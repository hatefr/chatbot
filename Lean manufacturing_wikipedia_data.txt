Lean manufacturing

Lean manufacturing  is a production method aimed primarily at reducing times within the production system as well as response times from suppliers and to customers. It is closely related to another concept called  just-in-time manufacturing (JIT manufacturing in short). Just-in-time manufacturing tries to match production to demand by only supplying goods which have been ordered and focuses on efficiency, productivity (with a commitment to continuous improvement) and reduction of "wastes" for the producer and supplier of goods. Lean manufacturing adopts the just-in-time approach and additionally focuses on reducing cycle, flow and throughput times by further eliminating activities which do not add any value for the customer. Lean manufacturing also involves people who work outside of the manufacturing process, such as in marketing and customer service. 
Lean manufacturing is particularly related to the operational model implemented in the post-war 1950s and 1960s by the Japanese automobile company Toyota called Toyota Production System (TPS), known in the USA as "The Toyota Way". Toyota's system was erected on the two pillars of just-in-time inventory management and automated quality control. The seven "wastes" (muda in Japanese), first formulated by Toyota engineer Shigeo Shingo, are the waste of superfluous inventory of raw material and finished goods, the waste of overproduction (producing more than what is needed now), the waste of over-processing (processing or making parts beyond the standard expected by customer), the waste of transportation (unnecessary movement of people and goods inside the system), the waste of excess motion (mechanizing or automating before improving the method), the waste of waiting (inactive working periods due to job queues), and the waste of making defective products (reworking to fix avoidable defects in products and processes).The term Lean was coined in 1988 by American businessman John Krafcik in his article "Triumph of the Lean Production System", and defined in 1996 by American researchers James Womack and Daniel Jones to consist of five key principles: "Precisely specify value by specific product, identify the value stream for each product, make value flow without interruptions, let customer pull value from the producer, and pursue perfection."Companies employ the strategy to increase efficiency. By receiving goods only as they need them for the production process, it reduces inventory costs and wastage, and increases productivity and profit. The downside is that it requires producers to forecast demand accurately as the benefits can be nullified by minor delays in the supply chain. It may also impact negatively on workers due to added stress and inflexible conditions. A successful operation depends on a company having regular outputs, high-quality processes, and reliable suppliers.

History
Fredrick Taylor and Henry Ford documented their observations relating to these topics, and Shigeo Shingo and Taiichi Ohno applied their enhanced thoughts on the subject at Toyota in the late 1940s after the World War II. The resulting methods were researched from the mid-20th century and dubbed Lean by John Krafcik in 1988, and then were defined in The Machine that Changed the World and further detailed by James Womack and Daniel Jones in Lean Thinking (1996).

Japan, the Origins of Lean
The exact reasons for adoption of just-in-time manufacturing in Japan are clear, some Americans think and suggest it started with a requirement to solve the lack of standardization, which wasn't true. Japanese companies needed an immediate solution for the extreme situation they were living after losing the World War II. American supply chain specialist Gergard Plenert has offered four quite vague reasons, paraphrased here. During Japan's post–World War II rebuilding (of economy, infrastructure, industry, political, and social-emotional stability): 

Japan's lack of cash made it difficult for industry to finance the big-batch, large inventory production methods common elsewhere.
Japan lacked space to build big factories loaded with inventory.
The Japanese islands lack natural resources with which to build products.
Japan had high unemployment, which meant that labor efficiency methods were not an obvious pathway to industrial success.Thus, the Japanese "leaned out" their processes. "They built smaller factories ... in which the only materials housed in the factory were those on which work was currently being done. In this way, inventory levels were kept low, investment in in-process inventories was at a minimum, and the investment in purchased natural resources was quickly turned around so that additional materials were purchased." Plenert goes on to explain Toyota's key role in developing this lean or just-in-time production methodology.American industrialists recognized the threat of cheap offshore labor to American workers during the 1910s, and explicitly stated the goal of what is now called lean manufacturing as a countermeasure. Henry Towne, past president of the American Society of Mechanical Engineers, wrote in the foreword to Frederick Winslow Taylor's Shop Management (1911), "We are justly proud of the high wage rates which prevail throughout our country, and jealous of any interference with them by the products of the cheaper labor of other countries. To maintain this condition, to strengthen our control of home markets, and, above all, to broaden our opportunities in foreign markets where we must compete with the products of other industrial nations, we should welcome and encourage every influence tending to increase the efficiency of our productive processes."Continuous production improvement and incentives for such were documented in Taylor's Principles of Scientific Management (1911):

"... whenever a workman proposes an improvement, it should be the policy of the management to make a careful analysis of the new method, and if necessary conduct a series of experiments to determine accurately the relative merit of the new suggestion and of the old standard. And whenever the new method is found to be markedly superior to the old, it should be adopted as the standard for the whole establishment."
"...after a workman has had the price per piece of the work he is doing lowered two or three times as a result of his having worked harder and increased his output, he is likely entirely to lose sight of his employer's side of the case and become imbued with a grim determination to have no more cuts if soldiering [marking time, just doing what he is told] can prevent it."Shigeo Shingo cites reading Principles of Scientific Management in 1931 and being "greatly impressed to make the study and practice of scientific management his life's work"., Shingo and Taiichi Ohno were key to the design of Toyota's manufacturing process. Previously a textile company, Toyota moved into building automobiles in 1934. Kiichiro Toyoda, founder of Toyota Motor Corporation, directed the engine casting work and discovered many problems in their manufacturing, with wasted resources on repair of poor-quality castings. Toyota engaged in intense study of each stage of the process. In 1936, when Toyota won its first truck contract with the Japanese government, the processes encountered new problems, to which Toyota responded by developing Kaizen improvement teams, into what has become the Toyota Production System (TPS), and subsequently The Toyota Way.
Levels of demand in the postwar economy of Japan were low; as a result, the focus of mass production on lowest cost per item via economies of scale had little application. Having visited and seen supermarkets in the United States, Ohno recognized that the scheduling of work should not be driven by sales or production targets but by actual sales. Given the financial situation during this period, over-production had to be avoided, and thus the notion of "pull" (or "build-to-order" rather than target-driven "push") came to underpin production scheduling.

Evolution in the rest of the world
Just-in-time manufacturing was introduced in Australia in the 1950s by the British Motor Corporation (Australia) at its Victoria Park plant in Sydney, from where the idea later migrated to Toyota. News about just-in-time/Toyota production system reached other western countries from Japan in 1977 in two English-language articles: one referred to the methodology as the "Ohno system", after Taiichi Ohno, who was instrumental in its development within Toyota. The other article, by Toyota authors in an international journal, provided additional details. Finally, those and other publicity were translated into implementations, beginning in 1980 and then quickly multiplying throughout industry in the United States and other developed countries. A seminal 1980 event was a conference in Detroit at Ford World Headquarters co-sponsored by the Repetitive Manufacturing Group (RMG), which had been founded 1979 within the American Production and Inventory Control Society (APICS) to seek advances in manufacturing. The principal speaker, Fujio Cho (later, president of Toyota Motor Corp.), in explaining the Toyota system, stirred up the audience, and led to the RMG's shifting gears from things like automation to just-in-time/Toyota production system.At least some of audience's stirring had to do with a perceived clash between the new just-in-time regime and manufacturing resource planning (MRP II), a computer software-based system of manufacturing planning and control which had become prominent in industry in the 1960s and 1970s. Debates in professional meetings on just-in-time vs. MRP II were followed by published articles, one of them titled, "The Rise and Fall of Just-in-Time". Less confrontational was Walt Goddard's, "Kanban Versus MRP II—Which Is Best for You?" in 1982. Four years later, Goddard had answered his own question with a book advocating just-in-time. Among the best known of MRP II's advocates was George Plossl, who authored two articles questioning just-in-time's kanban planning method and the "japanning of America". But, as with Goddard, Plossl later wrote that "JIT is a concept whose time has come".Just-in-time/TPS implementations may be found in many case-study articles from the 1980s and beyond. An article in a 1984 issue of Inc. magazine relates how Omark Industries (chain saws, ammunition, log loaders, etc.) emerged as an extensive just-in-time implementer under its US home-grown name ZIPS (zero inventory production system). At Omark's mother plant in Portland, Oregon, after the work force had received 40 hours of ZIPS training, they were "turned loose" and things began to happen. A first step was to "arbitrarily eliminate a week's lead time [after which] things ran smoother. 'People asked that we try taking another week's worth out.' After that, ZIPS spread throughout the plant's operations 'like an amoeba.'" The article also notes that Omark's 20 other plants were similarly engaged in ZIPS, beginning with pilot projects. For example, at one of Omark's smaller plants making drill bits in Mesabi, Minnesota, "large-size drill inventory was cut by 92%, productivity increased by 30%, scrap and rework ... dropped 20%, and lead time ... from order to finished product was slashed from three weeks to three days." The Inc. article states that companies using just-in-time the most extensively include "the Big Four, Hewlett-Packard, Motorola, Westinghouse Electric, General Electric, Deere & Company, and Black and Decker".By 1986, a case-study book on just-in-time in the U.S. was able to devote a full chapter to ZIPS at Omark, along with two chapters on just-in-time at several Hewlett-Packard plants, and single chapters for Harley-Davidson, John Deere, IBM-Raleigh, North Carolina, and California-based Apple Inc., a Toyota truck-bed plant, and New United Motor Manufacturing joint venture between Toyota and General Motors.Two similar, contemporaneous books from the U.K. are more international in scope. One of the books, with both conceptual articles and case studies, includes three sections on just-in-time practices: in Japan (e.g., at Toyota, Mazda, and Tokagawa Electric); in Europe (jmg Bostrom, Lucas Electric, Cummins Engine, IBM, 3M, Datasolve Ltd., Renault, Massey Ferguson); and in the US and Australia (Repco Manufacturing-Australia, Xerox Computer, and two on Hewlett-Packard). The second book, reporting on what was billed as the First International Conference on just-in-time manufacturing, includes case studies in three companies: Repco-Australia, IBM-UK, and 3M-UK. In addition, a day two keynote address discussed just-in-time as applied "across all disciplines, ... from accounting and systems to design and production".: J1–J9

Rebranding as "lean"
John Krafcik coined the term Lean in his 1988 article, "Triumph of the Lean Production System". The article states: (a) Lean manufacturing plants have higher levels of productivity/quality than non-Lean and (b) "The level of plant technology seems to have little effect on operating performance" (page 51). According to the article, risks with implementing Lean can be reduced by: "developing a well-trained, flexible workforce, product designs that are easy to build with high quality, and a supportive, high-performance supplier network" (page 51).

Middle era and to the present
Three more books which include just-in-time implementations were published in 1993, 1995, and 1996, which are start-up years of the lean manufacturing/lean management movement that was launched in 1990 with publication of the book, The Machine That Changed the World. That one, along with other books, articles, and case studies on lean, were supplanting just-in-time terminology in the 1990s and beyond. The same period, saw the rise of books and articles with similar concepts and methodologies but with alternative names, including cycle time management, time-based competition, quick-response manufacturing, flow, and pull-based production systems.There is more to just-in-time than its usual manufacturing-centered explication. Inasmuch as manufacturing ends with order-fulfillment to distributors, retailers, and end users, and also includes remanufacturing, repair, and warranty claims, just-in-time's concepts and methods have application downstream from manufacturing itself. A 1993 book on "world-class distribution logistics" discusses kanban links from factories onward. And a manufacturer-to-retailer model developed in the U.S. in the 1980s, referred to as quick response, has morphed over time to what is called fast fashion.

Methodology
The strategic elements of lean can be quite complex, and comprise multiple elements. Four different notions of lean have been identified:
Lean as a fixed state or goal (being lean)
Lean as a continuous change process (becoming lean)
Lean as a set of tools or methods (doing lean/toolbox lean)
Lean as a philosophy (lean thinking)The other way to avoid market risk and control the supply efficiently is to cut down in stock. P&G has completed their goal to co-operate with Walmart and other wholesales companies by building the response system of stocks directly to the suppliers companies.In 1999, Spear and Bowen identified four rules which characterize the "Toyota DNA":

All work shall be highly specified as to content, sequence, timing, and outcome.
Every customer-supplier connection must be direct, and there must be an unambiguous yes or no way to send requests and receive responses.
The pathway for every product and service must be simple and direct.
Any improvement must be made in accordance with the scientific method, under the guidance of a teacher, at the lowest possible level in the organization.This is a fundamentally different approach from most improvement methodologies, and requires more persistence than basic application of the tools, which may partially account for its lack of popularity. The implementation of "smooth flow" exposes quality problems that already existed, and waste reduction then happens as a natural consequence, a system-wide perspective rather focusing directly upon the wasteful practices themselves. 
Takt time is the rate at which products need to be produced to meet customer demand. The JIT system is designed to produce products at the rate of takt time, which ensures that products are produced just in time to meet customer demand.Sepheri provides a list of methodologies of just-in-time manufacturing that "are important but not exhaustive":
Housekeeping: physical organization and discipline.
Make it right the first time: elimination of defects.
Setup reduction: flexible changeover approaches.
Lot sizes of one: the ultimate lot size and flexibility.
Uniform plant load: leveling as a control mechanism.
Balanced flow: organizing flow scheduling throughput.
Skill diversification: multi-functional workers.
Control by visibility: communication media for activity.
Preventive maintenance: flawless running, no defects.
Fitness for use: producibility, design for process.
Compact plant layout: product-oriented design.
Streamlining movements: smoothing materials handling.
Supplier networks: extensions of the factory.
Worker involvement: small group improvement activities.
Cellular manufacturing: production methods for flow.
Pull system: signal [kanban] replenishment/resupply systems.

Key principles and waste
Womack and Jones define Lean as "...a way to do more and more with less and less—less human effort, less equipment, less time, and less space—while coming closer and closer to providing customers exactly what they want" and then translate this into five key principles:
Value: Specify the value desired by the customer. "Form a team for each product to stick with that product during its entire production cycle", "Enter into a dialogue with the customer" (e.g. Voice of the customer)
The Value Stream: Identify the value stream for each product providing that value and challenge all of the wasted steps (generally nine out of ten) currently necessary to provide it
Flow: Make the product flow continuously through the remaining value-added steps
Pull: Introduce pull between all steps where continuous flow is possible
Perfection: Manage toward perfection so that the number of steps and the amount of time and information needed to serve the customer continually fallsLean is founded on the concept of continuous and incremental improvements on product and process while eliminating redundant activities. "The value of adding activities are simply only those things the customer is willing to pay for, everything else is waste, and should be eliminated, simplified, reduced, or integrated".On principle 2, waste, see seven basic waste types under The Toyota Way. Additional waste types are:

Faulty goods (manufacturing of goods or services that do not meet customer demand or specifications, Womack et al., 2003. See Lean services)
Waste of skills (Six Sigma)
Under-utilizing capabilities (Six Sigma)
Delegating tasks with inadequate training (Six Sigma)
Metrics (working to the wrong metrics or no metrics) (Mika Geoffrey, 1999)
Participation (not utilizing workers by not allowing them to contribute ideas and suggestions and be part of Participative Management) (Mika Geoffrey, 1999)
Computers (improper use of computers: not having the proper software, training on use and time spent surfing, playing games or just wasting time) (Mika Geoffrey, 1999)

Implementation
One paper suggests that an organization implementing Lean needs its own Lean plan as developed by the "Lean Leadership". This should enable Lean teams to provide suggestions for their managers who then makes the actual decisions about what to implement. Coaching is recommended when an organization starts off with Lean to impart knowledge and skills to shop-floor staff. Improvement metrics are required for informed decision-making.Lean philosophy and culture is as important as tools and methodologies. Management should not decide on solutions without understanding the true problem by consulting shop floor personnel.The solution to a specific problem for a specific company may not have generalized application. The solution must fit the problem.Value-stream mapping (VSM) and 5S are the most common approaches companies take on their first steps to Lean. Lean can be focused on specific processes, or cover the entire supply chain. Front-line workers should be involved in VSM activities. Implementing a series of small improvements incrementally along the supply chain can bring forth enhanced productivity.

Naming
Alternative terms for JIT manufacturing have been used. Motorola's choice was short-cycle manufacturing (SCM). IBM's was continuous-flow manufacturing (CFM), and demand-flow manufacturing (DFM), a term handed down from consultant John Constanza at his Institute of Technology in Colorado. Still another alternative was mentioned by Goddard, who said that "Toyota Production System is often mistakenly referred to as the 'Kanban System'", and pointed out that kanban is but one element of TPS, as well as JIT production.: 11 The wide use of the term JIT manufacturing throughout the 1980s faded fast in the 1990s, as the new term lean manufacturing became established,  as "a more recent name for JIT". As just one testament to the commonality of the two terms, Toyota production system (TPS) has been and is widely used as a synonym for both JIT and lean manufacturing.,

Objectives and benefits
Objectives and benefits of JIT manufacturing may be stated in two primary ways: first, in specific and quantitative terms, via published case studies; second, general listings and discussion.
A case-study summary from Daman Products in 1999 lists the following benefits: reduced cycle times 97%, setup times 50%, lead times from 4 to 8 weeks to 5 to 10 days, flow distance 90%. This was achieved via four focused (cellular) factories, pull scheduling, kanban, visual management, and employee empowerment.Another study from NCR (Dundee, Scotland) in 1998, a producer of make-to-order automated teller machines, includes some of the same benefits while also focusing on JIT purchasing: In switching to JIT over a weekend in 1998, eliminated buffer inventories, reducing inventory from 47 days to 5 days, flow time from 15 days to 2 days, with 60% of purchased parts arriving JIT and 77% going dock to line, and suppliers reduced from 480 to 165.Hewlett-Packard, one of western industry's earliest JIT implementers, provides a set of four case studies from four H-P divisions during the mid-1980s. The four divisions, Greeley, Fort Collins, Computer Systems, and Vancouver, employed some but not all of the same measures. At the time about half of H-P's 52 divisions had adopted JIT.

Use in other sectors
Lean principles have been successfully applied to various sectors and services, such as call centers and healthcare. In the former, lean's waste reduction practices have been used to reduce handle time, within and between agent variation, accent barriers, as well as attain near perfect process adherence. In the latter, several hospitals have adopted the idea of lean hospital, a concept that prioritizes the patient, thus increasing the employee commitment and motivation, as well as boosting medical quality and cost effectiveness.Lean principles also have applications to software development and maintenance as well as other sectors of information technology (IT). More generally, the use of lean in information technology has become known as Lean IT. Lean methods are also applicable to the public sector, but most results have been achieved using a much more restricted range of techniques than lean provides.The challenge in moving lean to services is the lack of widely available reference implementations to allow people to see how directly applying lean manufacturing tools and practices can work and the impact it does have. This makes it more difficult to build the level of belief seen as necessary for strong implementation. However, some research does relate widely recognized examples of success in retail and even airlines to the underlying principles of lean. Despite this, it remains the case that the direct manufacturing examples of 'techniques' or 'tools' need to be better 'translated' into a service context to support the more prominent approaches of implementation, which has not yet received the level of work or publicity that would give starting points for implementors. The upshot of this is that each implementation often 'feels its way' along as must the early industrial engineering practices of Toyota. This places huge importance upon sponsorship to encourage and protect these experimental developments.Lean management is nowadays implemented also in non-manufacturing processes and administrative processes. In non-manufacturing processes is still huge potential for optimization and efficiency increase. Some people have advocated using STEM resources to teach children Lean thinking instead of computer science.

Criticism
According to Williams, it becomes necessary to find suppliers that are close by or can supply materials quickly with limited advance notice. When ordering small quantities of materials, suppliers' minimum order policies may pose a problem, though.Employees are at risk of precarious work when employed by factories that utilize just-in-time and flexible production techniques. A longitudinal study of US workers since 1970 indicates employers seeking to easily adjust their workforce in response to supply and demand conditions respond by creating more nonstandard work arrangements, such as contracting and temporary work.Natural and human-made disasters will disrupt the flow of energy, goods and services. The down-stream customers of those goods and services will, in turn, not be able to produce their product or render their service because they were counting on incoming deliveries "just in time" and so have little or no inventory to work with. The disruption to the economic system will cascade to some degree depending on the nature and severity of the original disaster and may create shortages. The larger the disaster the worse the effect on just-in-time failures. Electrical power is the ultimate example of just-in-time delivery. A severe geomagnetic storm could disrupt electrical power delivery for hours to years, locally or even globally. Lack of supplies on hand to repair the electrical system would have catastrophic effects.The COVID-19 pandemic has caused disruption in JIT practices, with various quarantine restrictions on international trade and commercial activity in general interrupting supply while lacking stockpiles to handle the disruption; along with increased demand for medical supplies like personal protective equipment (PPE) and ventilators, and even panic buying, including of various domestically manufactured (and so less vulnerable) products like panic buying of toilet paper, disturbing regular demand. This has led to suggestions that stockpiles and diversification of suppliers should be more heavily focused.Critics of Lean argue that this management method has significant drawbacks, especially for the employees of companies operating under Lean. Common criticism of Lean is that it fails to take into consideration the employee's safety and well-being. Lean manufacturing is associated with an increased level of stress among employees, who have a small margin of error in their work environment which require perfection. Lean also over-focuses on cutting waste, which may lead management to cut sectors of the company that are not essential to the company's short-term productivity but are nevertheless important to the company's legacy. Lean also over-focuses on the present, which hinders a company's plans for the future.Critics also make negative comparison of Lean and 19th century scientific management, which had been fought by the labor movement and was considered obsolete by the 1930s. Finally, lean is criticized for lacking a standard methodology: "Lean is more a culture than a method, and there is no standard lean production model."After years of success of Toyota's Lean Production, the consolidation of supply chain networks has brought Toyota to the position of being the world's biggest carmaker in the rapid expansion. In 2010, the crisis of safety-related problems in Toyota made other carmakers that duplicated Toyota's supply chain system wary that the same recall issue might happen to them.
James Womack had warned Toyota that cooperating with single outsourced suppliers might bring unexpected problems.Lean manufacturing is different from lean enterprise. Recent research reports the existence of several lean manufacturing processes but of few lean enterprises. One distinguishing feature opposes lean accounting and standard cost accounting. For standard cost accounting, SKUs are difficult to grasp. SKUs include too much hypothesis and variance, i.e., SKUs hold too much indeterminacy. Manufacturing may want to consider moving away from traditional accounting and adopting lean accounting. In using lean accounting, one expected gain is activity-based cost visibility, i.e., measuring the direct and indirect costs at each step of an activity rather than traditional cost accounting that limits itself to labor and supplies.

See also
Notes
References
Anderson, Barry (ed.) 2012. Building Cars in Australia: Morris, Austin, BMC and Leyland 1950-1976. Sydney: Halstead Press.
Billesbach, Thomas J. 1987. Applicability of Just-in-Time Techniques in the Administrative Area. Doctoral dissertation, University of Nebraska. Ann Arbor, Mich., University Microfilms International.
Goddard, W. E. 2001. JIT/TQC—identifying and solving problems. Proceedings of the 20th Electrical Electronics Insulation Conference, Boston, October 7–10, 88–91.
Goldratt, Eliyahu M. and Fox, Robert E. (1986), The Race, North River Press, ISBN 0-88427-062-9
Hall, Robert W. 1983. Zero Inventories. Homewood, Ill.: Dow Jones-Irwin.
Hall, Robert W. 1987. Attaining Manufacturing Excellence: Just-in-Time, Total Quality, Total People Involvement. Homewood, Ill.: Dow Jones-Irwin.
Hay, Edward J. 1988. The Just-in-Time Breakthrough: Implementing the New Manufacturing Basics. New York: Wiley.
Hohner, Gregory (1988). "JIT/TQC: integrating product design with shop floor effectiveness". Industrial Engineering. 20 (9): 42–48.
Hum, Sin-Hoon (1991). "Industrial progress and the strategic significance of JIT and TQC for developing countries". International Journal of Operations & Production Management. 110 (5): 39–46. doi:10.1108/01443579110145320.
Hyer, Nancy; Wemmerlov, Urban (2001). Reorganizing the Factory: Competing Through Cellular Manufacturing. CRC Press. ISBN 9781563272288.
Jackson, Paul (1991). "White collar JIT at Security Pacific". Target. 7 (1): 32–37.
Ker, J. I., Wang, Y., Hajli, M. N., Song, J., Ker, C. W. (2014). Deploying Lean in Healthcare: Evaluating Information Technology Effectiveness in US Hospital Pharmacies
Lubben, R. T. 1988. Just-in-Time Manufacturing: An Aggressive Manufacturing Strategy. New York: McGraw-Hill.
MacInnes, Richard L. (2002) The Lean Enterprise Memory Jogger.
Mika, Geoffrey L. (1999) Kaizen Event Implementation Manual
Monden, Yasuhiro. 1982. Toyota Production System. Norcross, Ga: Institute of Industrial Engineers.
Ohno, Taiichi (1988), Toyota Production System: Beyond Large-Scale Production, Productivity Press, ISBN 0-915299-14-3
Ohno, Taiichi (1988), Just-In-Time for Today and Tomorrow, Productivity Press, ISBN 0-915299-20-8.
Page, Julian (2003) Implementing Lean Manufacturing Techniques.
Schonberger, Richard J. 1982. Japanese Manufacturing Techniques: Nine Hidden Lessons in Simplicity. New York: Free Press.
Shingo, Shingeo; Dillon, Andrew P. (1989). A Study of the Toyota Production System: From an Industrial Engineering Viewpoint. CRC Press. ISBN 9780915299171.
Suri, R. 1986. Getting from 'just in case' to 'just in time': insights from a simple model. 6 (3) 295–304.
Suzaki, Kyoshi. 1993. The New Shop Floor Management: Empowering People for Continuous Improvement. New York: Free Press.
Voss, Chris, and David Clutterbuck. 1989. Just-in-Time: A Global Status Report. UK: IFS Publications.
Wadell, William, and Bodek, Norman (2005), The Rebirth of American Industry, PCS Press, ISBN 0-9712436-3-8
Womack, James P.; Jones, Daniel T. (2003). Lean Thinking: Banish Waste and Create Wealth in Your Corporation. Simon and Schuster. ISBN 9781471111006. Archived from the original on October 22, 2021. Retrieved October 2, 2020.
Womack, James P.; Jones, Daniel T.; Roos, Daniel (1990). The Machine that Changed the World. New York: Rawson Associates. ISBN 9780892563500. Archived from the original on February 19, 2022. Retrieved October 2, 2020.

External links

Lean Enterprise Institute

5S (methodology)

5S is a workplace organization method that uses a list of five Japanese words: seiri (整理), seiton (整頓), seisō (清掃), seiketsu (清潔), and shitsuke (躾). These have been translated as 'sort', 'set in order', 'shine', 'standardize', and 'sustain'. The list describes how to organize a work space for efficiency and effectiveness by identifying and storing the items used, maintaining the area and items, and sustaining the new organizational system. The decision-making process usually comes from a dialogue about standardization, which builds understanding among employees of how they should do the work.
In some quarters, 5S has become 6S, the sixth element being safety (safe).Other than a specific stand-alone methodology, 5S is frequently viewed as an element of a broader construct known as visual control, visual workplace, or visual factory. Under those (and similar) terminologies, Western companies were applying underlying concepts of 5S before publication, in English, of the formal 5S methodology. For example, a workplace-organization photo from Tennant Company (a Minneapolis-based manufacturer) quite similar to the one accompanying this article appeared in a manufacturing-management book in 1986.

Origins
5S was developed in Japan and was identified as one of the techniques that enabled just-in-time manufacturing.Two major frameworks for understanding and applying 5S to business environments have arisen, one proposed by Takahashi and Osada, the other by Hiroyuki Hirano.
Hirano provided a structure to improve programs with a series of identifiable steps, each building on its predecessor.
Before this Japanese management framework, a similar "scientific management" was proposed by Alexey Gastev and the USSR Central Institute of Labour (CIT) in Moscow.

Each S
There are five 5S phases. They can be translated to English as 'sort', 'set in order', 'shine', 'standardize', and 'sustain'. Other translations are possible.

Sort (seiri 整理)
Seiri is sorting through all items in a location and removing all unnecessary items from the location.
Goals:

Reduce time loss looking for an item by reducing the number of unnecessary items.
Reduce the chance of distraction by unnecessary items.
Simplify inspection.
Increase the amount of available, useful space.
Increase safety by eliminating obstacles.Implementation:

Check all items in a location and evaluate whether or not their presence at the location is useful or necessary.
Remove unnecessary items as soon as possible. Place those that cannot be removed immediately in a 'red tag area' so that they are easy to remove later on.
Keep the working floor clear of materials except for those that are in use for production.

Set in order (seiton 整頓)
(Sometimes shown as Straighten)
Seiton is putting all necessary items in the optimal place for fulfilling their function in the workplace.
Goal:

Make the workflow smooth and easy.Implementation:

Arrange work stations in such a way that all tooling/equipment is in close proximity, in an easy to reach spot and in a logical order adapted to the work performed. Place components according to their uses, with the frequently used components being nearest to the workplace.
Arrange all necessary items so that they can be easily selected for use. Make it easy to find and pick up necessary items.
Assign fixed locations for items. Use clear labels, marks or hints so that items are easy to return to the correct location and so that it is easy to spot missing items.

Shine (seiso 清掃)
Seiso is sweeping or cleaning and inspecting the workplace, tools and machinery on a regular basis.
Goals:

Improves the production process efficiency and safety, reduces waste, prevents errors and defects.
Keep the workplace safe and easy to work in.
Keep the workplace clean and pleasing to work in.
When in place, anyone not familiar to the environment must be able to detect any problems within 15 metres (50 ft) in 5 seconds.Implementation:

Clean the workplace and equipment on a daily basis, or at another appropriate (high frequency) cleaning interval.
Inspect the workplace and equipment while cleaning.

Standardize (seiketsu 清潔)
Seiketsu is to standardize the processes used to sort, order and clean the workplace.
Goal:

Establish procedures and schedules to ensure the repetition of the first three 'S' practices.Implementation:

Develop a work structure that will support the new practices and make it part of the daily routine.
Ensure everyone knows their responsibilities of performing the sorting, organizing and cleaning.
Use photos and visual controls to help keep everything as it should be.
Review the status of 5S implementation regularly using audit checklists.

Sustain/self-discipline (shitsuke しつけ)
Shitsuke or sustain is the developed processes by self-discipline of the workers. Also translates as "do without being told".
Goal:

Ensure that the 5S approach is followed.Implementation:

Organize training sessions.
Perform regular audits to ensure that all defined standards are being implemented and followed.
Implement improvements whenever possible. Worker inputs can be very valuable for identifying improvements.

Variety of applications
5S methodology has expanded from manufacturing and is now being applied to a wide variety of industries including health care, education, and government. Visual management and 5S can be particularly beneficial in health care because a frantic search for supplies to treat an in-trouble patient (a chronic problem in health care) can have dire consequences.
Although the origins of the 5S methodology are in manufacturing, it can also be applied to knowledge economy work, with information, software, or media in the place of physical product.

In lean product and process development
The output of engineering and design in a lean enterprise is information, the theory behind using 5S here is "Dirty, cluttered, or damaged surfaces attract the eye, which spends a fraction of a second trying to pull useful information from them every time we glance past. Old equipment hides the new equipment from the eye and forces people to ask which to use".

See also
Japanese aesthetics
Just-in-time manufacturing
Kaikaku
Kaizen
Kanban
Lean manufacturing
Muda
Gogyo (traditional Japanese philosophy)


== References ==

A3 problem solving

A3 problem solving is a structured problem-solving and continuous-improvement approach, first employed at Toyota and typically used by lean manufacturing practitioners. It provides a simple and strict procedure that guides problem solving by workers. The approach typically uses a single sheet of ISO A3-size paper, which is the source of its name.

See also
Notes
References
Shook, John (October 2008). Managing to learn: using the A3 management process to solve problems, gain agreement, mentor and lead. Cambridge, MA: Lean Enterprise Institute. ISBN 9781934109205. OCLC 276865965.
Sobek, Durward K.; Smalley, Art (2008). Understanding A3 thinking: a critical component of Toyota's PDCA management system. A Productivity Press book. Boca Raton, FL: CRC Press/Productivity Press. ISBN 9781563273605. OCLC 183609519.
Matthews, Daniel D. (2011). The A3 workbook: unlock your problem-solving mind. A Productivity Press book. New York: CRC Press/Productivity Press. ISBN 9781439834893. OCLC 462926456.

ANSI/ISA-95

ANSI/ISA-95, or ISA-95 as it is more commonly referred, is an international standard from the International Society of Automation for developing an automated interface between enterprise and control systems. This standard has been developed for global manufacturers. It was developed to be applied in all industries, and in all sorts of processes, like batch processes, continuous and repetitive processes.
The objectives of ISA-95 are to provide consistent terminology that is a foundation for supplier and manufacturer communications, provide consistent information models, and to provide consistent operations models which is a foundation for clarifying application functionality and how information is to be used.
There are 5 parts of the ISA-95 standard.
ANSI/ISA-95.00.01-2000, Enterprise-Control System Integration Part 1: Models and Terminology consists of standard terminology and  object models, which can be used to decide which information should be exchanged.
The models help define boundaries between the enterprise systems and the control systems. They help address questions like which tasks can be executed by which function and what information must be exchanged between applications. Here is a  Archived 2014-11-05 at the Wayback Machine.
ISA-95 Models

Context
Hierarchy Models
Scheduling and control (Purdue)
Equipment hierarchy
Functional Data Flow Model
Manufacturing Functions
Data Flows
Object Models
Objects
Object Relationships
Object Attributes
Operations Activity Models
Operations Elements: PO, MO, QO, IO
Operations Data Flow Model
Operations Functions
Operations FlowsANSI/ISA-95.00.02-2001, Enterprise-Control System Integration Part 2: Object Model Attributes consists of attributes for every object that is defined in part 1. The objects and attributes of Part 2 can be used for the exchange of information between different systems, but these objects and attributes can also be used as the basis for relational databases.
ANSI/ISA-95.00.03-2005, Enterprise-Control System Integration, Part 3: Models of Manufacturing Operations Management focuses on the functions and activities at level 3 (Production / MES layer). It provides guidelines for describing and comparing  the production levels of different sites in a standardized way.
ISA-95.00.04 Object Models & Attributes Part 4 of ISA-95: "Object models and attributes for Manufacturing Operations Management" 
The SP95 committee is yet developing part 4 of ISA-95, which is entitled "Object Models and Attributes of Manufacturing Operations Management". This technical specification defines object models that determine which information is exchanged between MES activities (which are defined in part 3 by ISA-95). The models and attributes from part 4 are the basis for the design and the implementation of interface standards and make sure of a flexible lapse of the cooperation and information-exchange between the different MES activities.
ISA-95.00.05 B2M Transactions Part 5 of ISA-95: "Business to manufacturing transactions" Also part 5 of ISA-95 is yet in development. This technical specification defines operation between office and production automations-systems, which can be used together with the object models out part 1 & 2. The operations connect and organise the production objects and activities that are defined through earlier parts of the standard. Such operations take place on all levels within a business, but the focus of this technical specification lies on the interface between enterprise- and control systems. On the basis of models, the operation will be described and becomes the operation processing logically explained.
Within production areas activities are executed and information is passed back and forth. The standard provides reference models for production activities, quality activities, maintenance activities and inventory activities.

See also
International Society of Automation
IEC 62264
Manufacturing execution system
Manufacturing operations management

External links
Enterprise-Control System Integration (ISA95)
ANSI/ISA-95.00.01-2010 (IEC 62264-1 Mod), Enterprise-Control System Integration—Part 1: Models and Terminology
ANSI/ISA-95.00.02-2018, Enterprise-Control System Integration—Part 2: Object Model Attributes
ANSI/ISA-95.00.03-2013 (IEC 62264-3 Modified), Enterprise-Control System Integration—Part 3: Activity Models of Manufacturing Operations Management
ANSI/ISA-95.00.04-2018, Enterprise-Control System Integration—Part 4: Objects and attributes for manufacturing operations management integration
ANSI/ISA-95.00.05-2018, Enterprise-Control System Integration—Part 5: Business-to-Manufacturing Transactions
ANSI/ISA-95.00.06-2014, Enterprise-Control System Integration—Part 6: Messaging Service Model
ANSI/ISA-95.00.07-2017, Enterprise-Control System Integration—Part 7: Alias Service Model
ANSI/ISA-95.00.08-2020, Enterprise-Control System Integration—Part 8: Information Exchange Profiles
ISA-TR95.01-2018, Enterprise-Control System Integration—TR01: Master Data Profile Template

Agile manufacturing

Agile manufacturing is a term applied to an organization that has created the processes, tools, and training to enable it to respond quickly to customer needs and market changes while still controlling costs and quality. It is mostly related to lean manufacturing.
An enabling factor in becoming an agile manufacturer has been the development of manufacturing support technology that allows the marketers, the designers and the production personnel to share a common database of parts and products, to share data on production capacities and problems—particularly where small initial problems may have larger downstream effects.  It is a general proposition of manufacturing that the cost of correcting quality issues increases as the problem moves downstream, so that it is cheaper to correct quality problems at the earliest possible point in the process.
Agile manufacturing is seen as the next step after lean manufacturing in the evolution of production methodology. The key difference between the two is like between a thin and an athletic person, agile being the latter. One can be neither, one or both. In manufacturing theory, being both is often referred to as leagile.
According to Martin Christopher, when companies have to decide what to be, they have to look at the customer order cycle (COC) (the time the customers are willing to wait) and the leadtime for getting supplies. If the supplier has a short lead time, lean production is possible. If the COC is short, agile production is beneficial.
Agile manufacturing is an approach to manufacturing which is focused on meeting the needs of customers while maintaining high standards of quality and controlling the overall costs involved in the production of a particular product. This approach is geared towards companies working in a highly competitive environment, where small variations in performance and product delivery can make a huge difference in the long term to a company's survival and reputation among consumers.
This concept is closely related to lean manufacturing, in which the goal is to reduce waste as much as possible. In lean manufacturing, the company aims to cut all costs which are not directly related to the production of a product for the consumer. Agile manufacturing can include this concept, but it also adds an additional dimension, the idea that customer demands need to be met rapidly and effectively. In situations where companies integrate both approaches, they are sometimes said to be using "agile and lean manufacturing".
Companies which utilize an agile manufacturing approach tend to have very strong networks with suppliers and related companies, along with numerous cooperative teams which work within the company to deliver products effectively. They can retool facilities quickly, negotiate new agreements with suppliers and other partners in response to changing market forces, and take other steps to meet customer demands. This means that the company can increase production on products with a high consumer demand, as well as redesign products to respond to issues which have emerged on the open market.
Markets can change very quickly, especially in the global economy. A company which cannot adapt quickly to change may find itself left behind, and once a company starts to lose market share, it can fall rapidly. The goal of agile manufacturing is to keep a company ahead of the competition so that consumers think of that company first, which allows it to continue innovating and introducing new products, because it is financially stable and it has a strong customer support base.
Companies that want to switch to the use of agile manufacturing can take advantage of consultants who specialize in helping companies convert and improve existing systems. Consultants can offer advice and assistance which is tailored to the industry a company is involved in, and they usually focus on making companies competitive as quickly as possible with proved agile techniques. There are also a number of textbooks and manuals available with additional information on agile manufacturing techniques and approaches.
Another approach was developed combining the attributes of agility together with leanness across one supply chain is the hybrid lean-agile strategy. This blended lean-agile strategy hybridizes attributes of leanness (cost minimization, waste reduction, continuous improvement), agility (speed, flexibility, responsiveness) and leagility (mass customization, postponement) in one supply network. The significance of the hybridized lean aspect is higher upstream the supply chain than the agility dimension in the same supplier node, compared to downstream the supply chain at the distributor node closer to the customers, which operates in a more agile manner.

See also
Lean manufacturing
Flexible manufacturing system
Industrial engineering

References
L. Goldman, R.L. Nagel and K Preiss, Agile Competitors and Virtual Organizations - Strategies for Enriching the Customer, Van Nostrand Reinhold, 1995.
Martin Christopher. "Logistics and Supply Chain Management"

External links
MESA - Manufacturing Enterprise Solutions Association
Agile Methodologies for Production

American Society of Mechanical Engineers

The American Society of Mechanical Engineers (ASME) is an American  professional association that, in its own words, "promotes the art, science, and practice of multidisciplinary engineering and allied sciences around the globe" via "continuing education, training and professional development, codes and standards, research, conferences and publications, government relations, and other forms of outreach." ASME is thus an engineering society, a standards organization, a research and development organization, an advocacy organization, a provider of training and education, and a nonprofit organization. Founded as an engineering society focused on mechanical engineering in North America, ASME is today multidisciplinary and global.
ASME has over 85,000 members in more than 135 countries worldwide.ASME was founded in 1880 by Alexander Lyman Holley, Henry Rossiter Worthington, John Edison Sweet and Matthias N. Forney in response to numerous steam boiler pressure vessel failures. Known for setting codes and standards for mechanical devices, ASME conducts one of the world's largest technical publishing operations. It holds numerous technical conferences and hundreds of professional development courses each year and sponsors numerous outreach and educational programs. Georgia Tech president and women engineer supporter Blake R Van Leer was an executive member. Kate Gleason and Lydia Weld were the first two women members.

ASME codes and standards
ASME is one of the oldest standards-developing organizations in America. It produces approximately 600 codes and standards covering many technical areas, such as fasteners, plumbing fixtures, elevators, pipelines, and power plant systems and components. ASME's standards are developed by committees of subject matter experts using an open, consensus-based process. Many ASME standards are cited by government agencies as tools to meet their regulatory objectives. ASME standards are therefore voluntary, unless the standards have been incorporated into a legally binding business contract or incorporated into regulations enforced by an authority having jurisdiction, such as a federal, state, or local government agency.  ASME's standards are used in more than 100 countries and have been translated into numerous languages.

ASME boiler and pressure vessel code
The largest ASME standard, both in size and in the number of volunteers involved in its preparation, is the ASME Boiler and Pressure Vessel Code (BPVC). The BPVC provides rules for the design, fabrication, installation, inspection, care, and use of boilers, pressure vessels, and nuclear components. The code also includes standards on materials, welding and brazing procedures and qualifications, nondestructive examination, and nuclear in-service inspection.

Other notable standardization areas
Other Notable Standardization Areas include but not limited to are; Elevators and Escalators (A17 Series), Overhead and Mobile Cranes and related lifting and rigging equipment (B30 Series), Piping and Pipelines (B31 Series), Bio-processing Equipment (BPE), Valves Flanges, Fittings and Gaskets (B16), Nuclear Components and Processes Performance Test Codes.

Journals
Some of the journals published by ASME are the following:Journal of Micro and Nano Manufacturing
ASME Journal of Engineering and Science in Medical Diagnostics and Therapy
Journal of Solar Energy Engineering
Journal of Energy Resource Technology
Journal of Manufacturing Science and Engineering
Journal of Mechanisms and Robotics

Society awards
ASME offers four categories of awards: achievement awards to recognize "eminently distinguished engineering achievement"; literature awards for original papers; service awards for voluntary service to ASME; and unit awards, jointly awarded by six societies in recognition of advancement in the field of transportation. 
ASME Medal
Worcester Reed Warner Medal
Charles T. Main Student Leadership Award
Holley Medal
Honorary Member
Kate Gleason Award
Henry Laurence Gantt Medal
Leonardo Da Vinci Award
Lewis F. Moody Award
Melville Medal
Nadia Medal
Old Guard Early Career Award
Sia Nemat-Nasser Early Career Award
R. Tom Sawyer Award
Ralph Coats Roe Medal
Soichiro Honda Medal

Nadia Medal recipients
Satya N. Atluri (2012)
Huseyin Sehitoglu (2007)
George Z. Voyiadjis (2022)

ASME Fellows
ASME Fellow is a Membership Grade of Distinction conferred by The ASME Committee of Past Presidents to an ASME member with significant publications or innovations and distinguished scientific and engineering background. Over 3,000 members have attained the grade of Fellow. The ASME Fellow membership grade is the highest elected grade in ASME.

ASME E-Fests
ASME runs several annual E-Fests, or Engineering Festivals, taking the place of the Student Professional Development Conference (SPDC) series. In addition to the Human Powered Vehicle Challenge (HPVC), the Innovative Additive Manufacturing 3D Challenge (IAM3D), the Student Design Competition, and the Old Guard Competition, there are also talks, interactive workshops, and entertainment. These events allows students to network with working engineers, host contests, and promote ASME's benefits to students as well as professionals. E-Fests are held in four regions in the United States and internationally—western U.S, eastern U.S., Asia Pacific, and South America—with the E-Fest location for each region changing every year.

Student competitions
ASME holds a variety of competitions every year for engineering students from around the world.
Human Powered Vehicle Challenge (HPVC)
Student Design Competition (SDC)
Innovative Design Simulation Challenge (IDSC)
Innovative Additive Manufacturing 3D Challenge (IAM3D)
Old Guard Competitions
Innovation Showcase (IShow)
Student Design Expositions

Organization
ASME has four key offices in the United States, including its headquarters operation in New York, N.Y., and three international offices in Beijing, China; Brussels, Belgium, and New Delhi, India. ASME has two institutes and 32 technical divisions within its organizational structure.  Volunteer activity is organized into four sectors: 

Technical Events and Content
Public Affairs and Outreach
Standards and Certification
Student and Early Career Development

Controversy
In 1982, ASME was found to be the first non-profit organization to in violation of the Sherman Antitrust Act. The United States Supreme Court found the organization liable for more than $6 million in American Society of Mechanical Engineers v. Hydrolevel Corp.

See also
ASME Y14.41-2003 Digital Product Definition Data Practices
List of American Society of Mechanical Engineers academic journals
List of Historic Mechanical Engineering Landmarks
ASME Medal
ASME Boiler and Pressure Vessel Code
Uniform Mechanical Code
American Welding Society

References
Further reading
Calvert, Monte A. The Mechanical Engineer in America, 1830–1910: Professional Cultures in Conflict. Baltimore: The Johns Hopkins University Press, 1967.
Hutton, Frederick Remson (1915) A History of the American Society of Mechanical Engineers. ASME.
Sinclair, Bruce. A Centennial History of the American Society of Mechanical Engineers, 1880–1980.  Toronto: Toronto University Press, 1980.
John H. White (1979). A History of the American Locomotive: Its Development, 1830–1880. Courier Dover Publications. ISBN 978-0-486-23818-0.

External links

Official website 
ASME Peerlink (archived)
Society Awards

Apple Inc.

Apple Inc. is an American multinational technology company headquartered in Cupertino, California. As of March 2023, Apple is the world's biggest company by market capitalization, and with US$394.3 billion the largest technology company by 2022 revenue. As of June 2022, Apple is the fourth-largest personal computer vendor by unit sales; the largest manufacturing company by revenue; and the second-largest mobile phone manufacturer in the world. It is considered one of the Big Five American information technology companies, alongside Alphabet (parent company of Google), Amazon, Meta, and Microsoft.
Apple was founded as Apple Computer Company on April 1, 1976, by Steve Wozniak, Steve Jobs and Ronald Wayne to develop and sell Wozniak's Apple I personal computer. It was incorporated by Jobs and Wozniak as Apple Computer, Inc. in 1977. The company's second computer, the Apple II, became a best seller and one of the first mass-produced microcomputers. Apple went public in 1980 to instant financial success. The company developed computers featuring innovative graphical user interfaces, including the 1984 original Macintosh, announced that year in a critically acclaimed advertisement called "1984". By 1985, the high cost of its products, and power struggles between executives, caused problems. Wozniak stepped back from Apple and pursued other ventures, while Jobs resigned and founded NeXT, taking some Apple employees with him.
As the market for personal computers expanded and evolved throughout the 1990s, Apple lost considerable market share to the lower-priced duopoly of the Microsoft Windows operating system on Intel-powered PC clones (also known as "Wintel"). In 1997, weeks away from bankruptcy, the company bought NeXT to resolve Apple's unsuccessful operating system strategy and entice Jobs back to the company. Over the next decade, Jobs guided Apple back to profitability through a number of tactics including introducing the iMac, iPod, iPhone and iPad to critical acclaim, launching the "Think different" campaign and other memorable advertising campaigns, opening the Apple Store retail chain, and acquiring numerous companies to broaden the company's product portfolio. When Jobs resigned in 2011 for health reasons, and died two months later, he was succeeded as CEO by Tim Cook.
Apple became the first publicly traded U.S. company to be valued at over $1 trillion in August 2018, then at $2 trillion in August 2020, and at $3 trillion in January 2022. In June 2023, it was valued at just over $3 trillion. The company receives criticism regarding the labor practices of its contractors, its environmental practices, and its business ethics, including anti-competitive practices and materials sourcing. Nevertheless, the company has a large following and enjoys a high level of brand loyalty. It has also been consistently ranked as one of the world's most valuable brands.

History
1976–1980: Founding and incorporation
Apple Computer Company was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne as a partnership. The company's first product was the Apple I, a computer designed and hand-built entirely by Wozniak. To finance its creation, Jobs sold his Volkswagen Bus, and Wozniak sold his HP-65 calculator.: 57  Neither received the full selling price but in total earned $1,300 (equivalent to $6,700 in 2022). Wozniak debuted the first prototype Apple I at the Homebrew Computer Club in July 1976. The Apple I was sold as a motherboard with CPU, RAM, and basic textual-video chips—a base kit concept which would not yet be marketed as a complete personal computer. It went on sale soon after debut for $666.66 (equivalent to $3,400 in 2022).: 180  Wozniak later said he was unaware of the coincidental mark of the beast in the number 666, and that he came up with the price because he liked "repeating digits".Apple Computer, Inc. was incorporated on January 3, 1977, without Wayne, who had left and sold his share of the company back to Jobs and Wozniak for $800 only twelve days after having co-founded it. Multimillionaire Mike Markkula provided essential business expertise and funding of $250,000 (equivalent to $1,207,000 in 2022) to Jobs and Wozniak during the incorporation of Apple. During the first five years of operations, revenues grew exponentially, doubling about every four months. Between September 1977 and September 1980, yearly sales grew from $775,000 to $118 million, an average annual growth rate of 533%.The Apple II, also invented by Wozniak, was introduced on April 16, 1977, at the first West Coast Computer Faire. It differed from its major rivals, the TRS-80 and Commodore PET, because of its character cell-based color graphics and open architecture. While the Apple I and early Apple II models used ordinary audio cassette tapes as storage devices, they were superseded by the introduction of a 5+1⁄4-inch floppy disk drive and interface called the Disk II in 1978.The Apple II was chosen to be the desktop platform for the first "killer application" of the business world: VisiCalc, a spreadsheet program released in 1979. VisiCalc created a business market for the Apple II and gave home users an additional reason to buy an Apple II: compatibility with the office, but Apple II market share remained behind home computers made by competitors such as Atari, Commodore, and Tandy.On December 12, 1980, Apple (ticker symbol "AAPL") went public selling 4.6 million shares at $22 per share ($.10 per share when adjusting for stock splits as of September 3, 2022), generating over $100 million, which was more capital than any IPO since Ford Motor Company in 1956. By the end of the day, 300 millionaires were created, from a stock price of $29 per share and a market cap of $1.778 billion.

1980–1990: Success with Macintosh
A critical moment in the company's history came in December 1979 when Jobs and several Apple employees, including human–computer interface expert Jef Raskin, visited Xerox PARC in to see a demonstration of the Xerox Alto, a computer using a graphical user interface. Xerox granted Apple engineers three days of access to the PARC facilities in return for the option to buy 100,000 shares (22.4 million split-adjusted shares as of September 3, 2022) of Apple at the pre-IPO price of $10 a share. After the demonstration, Jobs was immediately convinced that all future computers would use a graphical user interface, and development of a GUI began for the Apple Lisa, named after Jobs's daughter.The Lisa division was plagued by infighting, and in 1982, Jobs was pushed off the project. The Lisa launched in 1983 and became the first mass marketed personal computer with a GUI, but was a commercial failure due to its high price and limited software library.Jobs, angered by being pushed off the Lisa team, took over the company's Macintosh division. Wozniak and Raskin had envisioned the Macintosh as a low-cost computer with a text-based interface like the Apple II, but a plane crash in 1981 forced Wozniak to step back from the project. Jobs quickly redefined the Macintosh as a graphical system that would be cheaper than the Lisa, undercutting his former division. Jobs was also hostile to the Apple II division, which at the time, generated most of the company's revenue.In 1984, Apple launched the Macintosh, the first personal computer without a bundled programming language. Its debut was signified by "1984", a $1.5 million television advertisement directed by Ridley Scott that aired during the third quarter of Super Bowl XVIII on January 22, 1984. This was hailed as a watershed event for Apple's success and was called a "masterpiece" by CNN and one of the greatest TV advertisements of all time by TV Guide.The advertisement created great interest in Macintosh, and sales were initially good, but began to taper off dramatically after the first three months as reviews started to come in. Jobs had required 128 kilobytes of RAM, which limited its speed and software in favor of aspiring for a projected price point of $1,000 (equivalent to $2,800 in 2022). The Macintosh shipped for $2,495 (equivalent to $7,000 in 2022), a price panned by critics due to its slow performance.: 195  In early 1985, this sales slump triggered a power struggle between Steve Jobs and CEO John Sculley, who had been hired away from Pepsi two years earlier by Jobs saying, "Do you want to sell sugar water for the rest of your life or come with me and change the world?" Sculley removed Jobs as the head of the Macintosh division, with unanimous support from the Apple board of directors.The board of directors instructed Sculley to contain Jobs and his ability to launch expensive forays into untested products. Rather than submit to Sculley's direction, Jobs attempted to oust him from leadership. Jean-Louis Gassée informed Sculley that Jobs had been attempting to organize a boardroom coup and called an emergency meeting at which Apple's executive staff sided with Sculley and stripped Jobs of all operational duties. Jobs resigned from Apple in September 1985 and took several Apple employees with him to found NeXT. Wozniak had also quit his active employment at Apple earlier in 1985 to pursue other ventures, expressing his frustration with Apple's treatment of the Apple II division and stating that the company had "been going in the wrong direction for the last five years". Wozniak remained employed by Apple as a representative, receiving a stipend estimated to be $120,000 per year. Jobs and Wozniak remained Apple shareholders after their departures.After the departures of Jobs and Wozniak in 1985, Sculley launched the Macintosh 512K that year with quadruple the RAM, and introduced the LaserWriter, the first reasonably priced PostScript laser printer. PageMaker, an early desktop publishing application taking advantage of the PostScript language, was also released by Aldus Corporation in July 1985. It has been suggested that the combination of Macintosh, LaserWriter, and PageMaker was responsible for the creation of the desktop publishing market.This dominant position in the desktop publishing market allowed the company to focus on higher price points, the so-called "high-right policy" named for the position on a chart of price vs. profits. Newer models selling at higher price points offered higher profit margin, and appeared to have no effect on total sales as power users snapped up every increase in speed. Although some worried about pricing themselves out of the market, the high-right policy was in full force by the mid-1980s, due to Jean-Louis Gassée's slogan of "fifty-five or die", referring to the 55% profit margins of the Macintosh II.: 79–80 This policy began to backfire late in the decade as desktop publishing programs appeared on IBM PC compatibles that offered some of the same functionality of the Macintosh at far lower price points. The company lost its dominant position in the desktop publishing market and estranged many of its original consumer customer base who could no longer afford their high-priced products. The Christmas season of 1989 was the first in the company's history to have declining sales, which led to a 20% drop in Apple's stock price.: 117–129  During this period, the relationship between Sculley and Gassée deteriorated, leading Sculley to effectively demote Gassée in January 1990 by appointing Michael Spindler as the chief operating officer. Gassée left the company later that year.

1990–1997: Decline and restructuring
The company pivoted strategy and in October 1990 introduced three lower-cost models, the Macintosh Classic, the Macintosh LC, and the Macintosh IIsi, all of which saw significant sales due to pent-up demand. In 1991, Apple introduced the hugely successful PowerBook with a design that set the current shape for almost all modern laptops. The same year, Apple introduced System 7, a major upgrade to the Macintosh operating system, adding color to the interface and introducing new networking capabilities.

The success of the lower-cost Macs and PowerBook brought increasing revenue. For some time, Apple was doing incredibly well, introducing fresh new products and generating increasing profits in the process. The magazine MacAddict named the period between 1989 and 1991 as the "first golden age" of the Macintosh.The success of Apple's lower-cost consumer models, especially the LC, also led to the cannibalization of higher-priced machines. To address this, management introduced several new brands, selling largely identical machines at different price points, aimed at different markets: the high-end Quadra series, the mid-range Centris series, and the consumer-marketed Performa series. This led to significant market confusion, as customers did not understand the difference between so many models.In the early 1990s, the Apple II series was discontinued, which was expensive to produce, and the company decided was still taking sales away from lower-cost Macintosh models. After the launch of the LC, Apple began encouraging developers to create applications for Macintosh rather than Apple II, and authorized salespersons to direct consumers away from Apple II and toward Macintosh. The Apple IIe was discontinued in 1993.Apple also experimented with several other unsuccessful consumer targeted products during the 1990s, including digital cameras, portable CD audio players, speakers, video game consoles, the eWorld online service, and TV appliances. Enormous resources were invested in the problem-plagued Newton tablet division, based on John Sculley's unrealistic market forecasts.Throughout this period, Microsoft continued to gain market share with Windows by focusing on delivering software to inexpensive personal computers, while Apple was delivering a richly engineered but expensive experience. Apple relied on high profit margins and never developed a clear response; instead, they sued Microsoft for using a GUI similar to the Apple Lisa in Apple Computer, Inc. v. Microsoft Corp. The lawsuit dragged on for years before it was finally dismissed.
The major product flops and the rapid loss of market share to Windows sullied Apple's reputation, and in 1993 Sculley was replaced as CEO by Michael Spindler.With Spindler at the helm, Apple, IBM, and Motorola formed the AIM alliance in 1994 with the goal of creating a new computing platform (the PowerPC Reference Platform; PReP), which used IBM and Motorola hardware coupled with Apple software. The AIM alliance hoped that PReP's performance and Apple's software would leave the PC far behind and thus counter the dominance of Windows. That year, Apple introduced the Power Macintosh, the first of many computers with Motorola's PowerPC processor.In the wake of the alliance, Apple opened up to the idea of allowing Motorola and other companies to build Macintosh clones. Over the next two years, 75 distinct Macintosh clone models were introduced. However, by 1996, Apple executives were worried that the clones were cannibalizing sales of their own high-end computers, where profit margins were highest.In 1996, Spindler was replaced by Gil Amelio as CEO. Hired for his reputation as a corporate rehabilitator, Amelio made deep changes, including extensive layoffs and cost-cutting.This period was also marked by numerous failed attempts to modernize the Macintosh operating system (MacOS). The original Macintosh operating system (System 1) was not built for multitasking (running several applications at once). The company attempted to correct this with by introducing cooperative multitasking in System 5, but the company still felt it needed a more modern approach. This led to the Pink project in 1988, A/UX that same year, Copland in 1994, and the attempted purchase of BeOS in 1996. Talks with Be stalled when the CEO, former Apple executive Jean-Louis Gassée, demanded $300 million instead of the $125 million Apple wanted to pay.Only weeks away from bankruptcy, Apple's board decided NeXTSTEP was a better choice for its next operating system and purchased NeXT in late 1996 for $400 million, bringing back Apple co-founder Steve Jobs.

1997–2007: Return to profitability
The NeXT acquisition was finalized on February 9, 1997, and the board brought Jobs back to Apple as an advisor. On July 9, 1997, Jobs staged a boardroom coup that resulted in Amelio's resignation after overseeing a three-year record-low stock price and crippling financial losses.
The board named Jobs as interim CEO and he immediately began a review of the company's products. Jobs would order 70% of the company's products to be cancelled, resulting in the loss of 3,000 jobs, and taking Apple back to the core of its computer offerings. The next month, in August 1997, Steve Jobs convinced Microsoft to make a $150 million investment in Apple and a commitment to continue developing software for the Mac. The investment was seen as an "antitrust insurance policy" for Microsoft who had recently settled with the Department of Justice over anti-competitive practices. Jobs also ended the Mac clone deals and in September 1997, purchased the largest clone maker, Power Computing. On November 10, 1997, Apple introduced the Apple Store website, which was tied to a new build-to-order manufacturing that had been successfully used by PC manufacturer Dell.The moves paid off for Jobs; at the end of his first year as CEO, the company turned a $309 million profit.

On May 6, 1998, Apple introduced a new all-in-one computer reminiscent of the original Macintosh: the iMac. The iMac was a huge success for Apple selling 800,000 units in its first five months and ushered in major shifts in the industry by abandoning legacy technologies like the 3+1⁄2-inch diskette, being an early adopter of the USB connector, and coming pre-installed with internet connectivity (the "i" in iMac) via Ethernet and a dial-up modem. The device also had a striking teardrop shape and translucent materials, designed by Jonathan Ive, who although hired by Amelio, would go on to work collaboratively with Jobs for the next decade to chart a new course the design of Apple's products.A little more than a year later on July 21, 1999, Apple introduced the iBook, a laptop for consumers. It was the culmination of a strategy established by Jobs to produce only four products: refined versions of the Power Macintosh G3 desktop and PowerBook G3 laptop for professionals, along with the iMac desktop and iBook laptop for consumers. Jobs felt the small product line allowed for a greater focus on quality and innovation.At around the same time, Apple also completed numerous acquisitions to create a portfolio of digital media production software for both professionals and consumers. Apple acquired of Macromedia's Key Grip digital video editing software project which was renamed Final Cut Pro when it was launched on the retail market in April 1999. The development of Key Grip also led to Apple's release of the consumer video-editing product iMovie in October 1999. Next, Apple successfully acquired the German company Astarte in April 2000, which had developed the DVD authoring software DVDirector, which Apple would sell as the professional-oriented DVD Studio Pro software product, and used the same technology to create iDVD for the consumer market. In 2000, Apple purchased the SoundJam MP audio player software from Casady & Greene. Apple renamed the program iTunes, while simplifying the user interface and adding the ability to burn CDs.2001 would be a pivotal year for the Apple with the company making three announcements that would change the course of the company.
The first announcement came on March 24, 2001, that Apple was nearly ready to release a new modern operating system, Mac OS X. The announcement came after numerous failed attempts in the early 1990s, and several years of development. Mac OS X was based on NeXTSTEP, OPENSTEP, and BSD Unix, with Apple aiming to combine the stability, reliability, and security of Unix with the ease of use afforded by an overhauled user interface, heavily influenced by NeXTSTEP. To aid users in migrating from Mac OS 9, the new operating system allowed the use of OS 9 applications within Mac OS X via the Classic Environment.In May 2001, the company opened its first two Apple Store retail locations in Virginia and California, offering an improved presentation of the company's products. At the time, many speculated that the stores would fail, but they went on to become highly successful, and the first of more than 500 stores around the world.On October 23, 2001, Apple debuted the iPod portable digital audio player. The product, which was first sold on November 10, 2001, was phenomenally successful with over 100 million units sold within six years.In 2003, Apple's iTunes Store was introduced. The service offered music downloads for 99¢ a song and integration with the iPod. The iTunes Store quickly became the market leader in online music services, with over five billion downloads by June 19, 2008. Two years later, the iTunes Store was the world's largest music retailer.In 2002, Apple purchased Nothing Real for their advanced digital compositing application Shake, as well as Emagic for the music productivity application Logic. The purchase of Emagic made Apple the first computer manufacturer to own a music software company. The acquisition was followed by the development of Apple's consumer-level GarageBand application. The release of iPhoto that year completed the iLife suite.
At the Worldwide Developers Conference keynote address on June 6, 2005, Jobs announced that Apple would move away from PowerPC processors, and the Mac would transition to Intel processors in 2006. On January 10, 2006, the new MacBook Pro and iMac became the first Apple computers to use Intel's Core Duo CPU. By August 7, 2006, Apple made the transition to Intel chips for the entire Mac product line—over one year sooner than announced. The Power Mac, iBook, and PowerBook brands were retired during the transition; the Mac Pro, MacBook, and MacBook Pro became their respective successors. On April 29, 2009, The Wall Street Journal reported that Apple was building its own team of engineers to design microchips. Apple also introduced Boot Camp in 2006 to help users install Windows XP or Windows Vista on their Intel Macs alongside Mac OS X.Apple's success during this period was evident in its stock price. Between early 2003 and 2006, the price of Apple's stock increased more than tenfold, from around $6 per share (split-adjusted) to over $80. When Apple surpassed Dell's market cap in January 2006, Jobs sent an email to Apple employees saying Dell's CEO Michael Dell should eat his words. Nine years prior, Dell had said that if he ran Apple he would "shut it down and give the money back to the shareholders".

2007–2011: Success with mobile devices
During his keynote speech at the Macworld Expo on January 9, 2007, Jobs announced the renaming of Apple Computer, Inc. to Apple Inc., because the company had shifted its emphasis from computers to consumer electronics. This event also saw the announcement of the iPhone and the Apple TV. The company sold 270,000 iPhone units during the first 30 hours of sales, and the device was called "a game changer for the industry".In an article posted on Apple's website on February 6, 2007, Jobs wrote that Apple would be willing to sell music on the iTunes Store without digital rights management (DRM), thereby allowing tracks to be played on third-party players if record labels would agree to drop the technology. On April 2, 2007, Apple and EMI jointly announced the removal of DRM technology from EMI's catalog in the iTunes Store, effective in May 2007. Other record labels eventually followed suit and Apple published a press release in January 2009 to announce that all songs on the iTunes Store are available without their FairPlay DRM.In July 2008, Apple launched the App Store to sell third-party applications for the iPhone and iPod Touch. Within a month, the store sold 60 million applications and registered an average daily revenue of $1 million, with Jobs speculating in August 2008 that the App Store could become a billion-dollar business for Apple. By October 2008, Apple was the third-largest mobile handset supplier in the world due to the popularity of the iPhone.On January 14, 2009, Jobs announced in an internal memo that he would be taking a six-month medical leave of absence from Apple until the end of June 2009 and would spend the time focusing on his health. In the email, Jobs stated that "the curiosity over my personal health continues to be a distraction not only for me and my family, but everyone else at Apple as well", and explained that the break would allow the company "to focus on delivering extraordinary products". Though Jobs was absent, Apple recorded its best non-holiday quarter (Q1 FY 2009) during the recession with revenue of $8.16 billion and profit of $1.21 billion.After years of speculation and multiple rumored "leaks", Apple unveiled a large screen, tablet-like media device known as the iPad on January 27, 2010. The iPad ran the same touch-based operating system as the iPhone, and all iPhone apps were compatible with the iPad. This gave the iPad a large app catalog on launch, though having very little development time before the release. Later that year on April 3, 2010, the iPad was launched in the U.S. It sold more than 300,000 units on its first day, and 500,000 by the end of the first week. In May of the same year, Apple's market cap exceeded that of competitor Microsoft for the first time since 1989.In June 2010, Apple released the iPhone 4, which introduced video calling using FaceTime, multitasking, and a new uninsulated stainless steel design that acted as the phone's antenna. Later that year, Apple again refreshed its iPod line of MP3 players by introducing a multi-touch iPod Nano, an iPod Touch with FaceTime, and an iPod Shuffle that brought back the clickwheel buttons of earlier generations. It also introduced the smaller, cheaper second generation Apple TV which allowed renting of movies and shows.On January 17, 2011, Jobs announced in an internal Apple memo that he would take another medical leave of absence for an indefinite period to allow him to focus on his health. Chief operating officer Tim Cook assumed Jobs's day-to-day operations at Apple, although Jobs would still remain "involved in major strategic decisions". Apple became the most valuable consumer-facing brand in the world. In June 2011, Jobs surprisingly took the stage and unveiled iCloud, an online storage and syncing service for music, photos, files, and software which replaced MobileMe, Apple's previous attempt at content syncing. This would be the last product launch Jobs would attend before his death.
On August 24, 2011, Jobs resigned his position as CEO of Apple. He was replaced by Cook and Jobs became Apple's chairman. Apple did not have a chairman at the time and instead had two co-lead directors, Andrea Jung and Arthur D. Levinson, who continued with those titles until Levinson replaced Jobs as chairman of the board in November after Jobs' death.

2011–present: Post-Jobs era, Tim Cook
On October 5, 2011, Steve Jobs died, marking the end of an era for Apple. The next major product announcement by Apple was on January 19, 2012, when Apple's Phil Schiller introduced iBook's Textbooks for iOS and iBook Author for Mac OS X in New York City. Jobs stated in the biography "Steve Jobs" that he wanted to reinvent the textbook industry and education.From 2011 to 2012, Apple released the iPhone 4S and iPhone 5, which featured improved cameras, an intelligent software assistant named Siri, and cloud-synced data with iCloud; the third- and fourth-generation iPads, which featured Retina displays; and the iPad Mini, which featured a 7.9-inch screen in contrast to the iPad's 9.7-inch screen. These launches were successful, with the iPhone 5 (released September 21, 2012) becoming Apple's biggest iPhone launch with over two million pre-orders and sales of three million iPads in three days following the launch of the iPad Mini and fourth-generation iPad (released November 3, 2012). Apple also released a third-generation 13-inch MacBook Pro with a Retina display and new iMac and Mac Mini computers.On August 20, 2012, Apple's rising stock price increased the company's market capitalization to a then-record $624 billion. This beat the non-inflation-adjusted record for market capitalization previously set by Microsoft in 1999. On August 24, 2012, a US jury ruled that Samsung should pay Apple $1.05 billion (£665m) in damages in an intellectual property lawsuit. Samsung appealed the damages award, which was reduced by $450 million and further granted Samsung's request for a new trial. On November 10, 2012, Apple confirmed a global settlement that dismissed all existing lawsuits between Apple and HTC up to that date, in favor of a ten-year license agreement for current and future patents between the two companies. It is predicted that Apple will make $280 million per year from this deal with HTC.In May 2014, the company confirmed its intent to acquire Dr. Dre and Jimmy Iovine's audio company Beats Electronics—producer of the "Beats by Dr. Dre" line of headphones and speaker products, and operator of the music streaming service Beats Music—for $3 billion, and to sell their products through Apple's retail outlets and resellers. Iovine believed that Beats had always "belonged" with Apple, as the company modeled itself after Apple's "unmatched ability to marry culture and technology." The acquisition was the largest purchase in Apple's history.
During a press event on September 9, 2014, Apple introduced a smartwatch, the Apple Watch. Initially, Apple marketed the device as a fashion accessory and a complement to the iPhone, that would allow people to look at their smartphones less. Over time, the company has focused on developing health and fitness-oriented features on the watch, in an effort to compete with dedicated activity trackers.
In January 2016, it was announced that one billion Apple devices were in active use worldwide.On June 6, 2016, Fortune released Fortune 500, its list of companies ranked on revenue generation. In the trailing fiscal year of 2015, Apple was listed as the top tech company. It ranked third, overall, with $233 billion in revenue. This represents a movement upward of two spots from the previous year's list.In June 2017, Apple announced the HomePod, its smart speaker aimed to compete against Sonos, Google Home, and Amazon Echo. Towards the end of the year, TechCrunch reported that Apple was acquiring Shazam, a company that introduced its products at WWDC and specializing in music, TV, film and advertising recognition. The acquisition was confirmed a few days later, reportedly costing Apple $400 million, with media reports that the purchase looked like a move to acquire data and tools bolstering the Apple Music streaming service. The purchase was approved by the European Union in September 2018.Also in June 2017, Apple appointed Jamie Erlicht and Zack Van Amburg to head the newly formed worldwide video unit. In November 2017, Apple announced it was branching out into original scripted programming: a drama series starring Jennifer Aniston and Reese Witherspoon, and a reboot of the anthology series Amazing Stories with Steven Spielberg. In June 2018, Apple signed the Writers Guild of America's minimum basic agreement and Oprah Winfrey to a multi-year content partnership. Additional partnerships for original series include Sesame Workshop and DHX Media and its subsidiary Peanuts Worldwide, as well as a partnership with A24 to create original films.During the Apple Special Event in September 2017, the AirPower wireless charger was announced alongside the iPhone X, 8 and Watch Series 3. The AirPower was intended to wirelessly charge multiple devices, simultaneously. Though initially set to release in early 2018, the AirPower would be canceled in March 2019, marking the first cancellation of a device under Cook's leadership.On August 19, 2020, Apple's share price briefly topped $467.77, making it the first US company with a market capitalization of $2 trillion.
During its annual WWDC keynote speech on June 22, 2020, Apple announced it would move away from Intel processors, and the Mac would transition to processors developed in-house. The announcement was expected by industry analysts, and it has been noted that Macs featuring Apple's processors would allow for big increases in performance over current Intel-based models. On November 10, 2020, the MacBook Air, MacBook Pro, and the Mac Mini became the first Mac devices powered by an Apple-designed processor, the Apple M1.In April 2022, it was reported that Samsung Electro-Mechanics would be collaborating with Apple on its M2 chip instead of LG Innotek. Developer logs showed that at least nine Mac models with four different M2 chips were being tested.The Wall Street Journal reported that an effort to develop its own chips left Apple better prepared to deal with the semiconductor shortage that emerged during the pandemic era and led to increased profitability, with sales of Mac computers that included M1 chips rising sharply in 2020 and 2021. It also inspired other companies like Tesla, Amazon, and Meta Platforms to pursue a similar path.In April 2022, Apple opened an online store that allowed anyone in the US to view repair manuals and order replacement parts for specific recent iPhones, although the difference in cost between this method and official repair is anticipated to be minimal.In May 2022, a trademark was filed for RealityOS, an operating system reportedly intended for virtual and augmented reality headsets, first mentioned in 2017. According to Bloomberg, the headset may come out in 2023. Further insider reports state that the device uses iris scanning for payment confirmation and signing into accounts.On June 18, 2022, the Apple Store in Towson, Maryland became the first to unionize in the U.S., with the employees voting to join the International Association of Machinists and Aerospace Workers.On July 7, 2022, Apple added Lockdown Mode to macOS 13 and iOS 16, as a response to the earlier Pegasus revelations; the mode increases security protections for high-risk users against targeted zero-day malware.Apple launched a buy now, pay later service called 'Apple Pay Later' for its Apple Wallet users in March 2023. The program allows its users to apply for loans between $50 and $1,000 to make online or in-app purchases and then repaying them through four installments spread over six weeks without any interest or fees.

Products
Mac
The Mac is Apple's family of personal computers. Macs are known for their ease of use and distinctive aluminium, minimalist designs. Macs have been popular among students, creative professionals, and software engineers. The current lineup consists of the MacBook Air and MacBook Pro laptops, and the iMac, Mac mini, Mac Studio and Mac Pro desktop computers.
Often described as a walled garden, Macs use Apple silicon chips, run the macOS operating system, and include Apple software like the Safari web browser, iMovie for home movie editing, GarageBand for music creation, and the iWork productivity suite. Apple also sells pro apps: Final Cut Pro for video production, Logic Pro for musicians and producers, and Xcode for software developers.
Apple also sells a variety of accessories for Macs, including the Pro Display XDR, Apple Studio Display, Magic Mouse, Magic Trackpad, and Magic Keyboard.

iPhone
The iPhone is Apple's line of smartphones, which run the iOS operating system. The first iPhone was unveiled by Steve Jobs on January 9, 2007. Since then, new models have been released annually. When it was introduced, its multi-touch screen was described as "revolutionary" and a "game-changer" for the mobile phone industry. The device has been credited with creating the app economy.
As of 2022, the iPhone has 15% market share, yet represents 50% of global smartphone revenues, with Android phones accounting for the rest. The iPhone has generated large profits for the company, and is credited with helping to make Apple one of the world's most valuable publicly traded companies.

iPad
The iPad is Apple's line of tablets which run iPadOS. The first-generation iPad was announced on January 27, 2010. The iPad is mainly marketed for consuming multimedia, creating art, working on documents, videoconferencing, and playing games. The iPad lineup consists of several base iPad models, and the smaller iPad Mini, upgraded iPad Air, and high-end iPad Pro. Apple has consistently improved the iPad's performance, with the iPad Pro adopting the same M1 and M2 chips as the Mac; but the iPad still receives criticism for its limited OS.As of September 2020, Apple has sold more than 500 million iPads, though sales peaked in 2013. The iPad still remains the most popular tablet computer by sales as of the second quarter of 2020, and accounted for nine percent of the company's revenue as of the end of 2021.Apple sells several iPad accessories, including the Apple Pencil, Smart Keyboard, Smart Keyboard Folio, Magic Keyboard, and several adapters.

Other products
Apple makes several other products that it categorizes as "Wearables, Home and Accessories". These products include the AirPods line of wireless headphones, Apple TV digital media players, Apple Watch smartwatches, Beats headphones and HomePod Mini smart speakers.
As of the end of 2021, this broad line of products comprises about 11% of the company's revenues.At WWDC 2023, Apple introduced its new VR headset, Vision Pro, along with visionOS. Apple announced that it will be partnering with Unity to bring existing 3D apps to Vision Pro using Unity's PolySpatial technology.

Services
Apple offers a broad line of services, including advertising in the App Store and Apple News app, the AppleCare+ extended warranty plan, the iCloud+ cloud-based data storage service, payment services through the Apple Card credit card and the Apple Pay processing platform, digital content services including Apple Books, Apple Fitness+, Apple Music, Apple News+, Apple TV+, and the iTunes Store.
As of the end of 2021, services comprise about 19% of the company's revenue. Many of the services have been launched as of 2019 when Apple announced it would be making a concerted effort to expand its service revenues.

Marketing
Branding
According to Steve Jobs, the company's name was inspired by his visit to an apple farm while on a fruitarian diet. Jobs thought the name "Apple" was "fun, spirited and not intimidating". Steve Jobs and Steve Wozniak were fans of the Beatles, but Apple Inc. had name and logo trademark issues with Apple Corps Ltd., a multimedia company started by the Beatles in 1968. This resulted in a series of lawsuits and tension between the two companies. These issues ended with the settling of their lawsuit in 2007.Apple's first logo, designed by Ron Wayne, depicts Sir Isaac Newton sitting under an apple tree. It was almost immediately replaced by Rob Janoff's "rainbow Apple", the now-familiar rainbow-colored silhouette of an apple with a bite taken out of it. On August 27, 1999, Apple officially dropped the rainbow scheme and began to use monochromatic logos nearly identical in shape to the previous rainbow incarnation.Apple evangelists were actively engaged by the company at one time, but this was after the phenomenon had already been firmly established. Apple evangelist Guy Kawasaki has called the brand fanaticism "something that was stumbled upon," while Ive claimed in 2014 that "people have an incredibly personal relationship" with Apple's products.Fortune magazine named Apple the most admired company in the United States in 2008, and in the world from 2008 to 2012. On September 30, 2013, Apple surpassed Coca-Cola to become the world's most valuable brand in the Omnicom Group's "Best Global Brands" report. Boston Consulting Group has ranked Apple as the world's most innovative brand every year as of 2005.As of January 2021, 1.65 billion Apple products were in active use. In February 2023 that number exceeded 2 billion devices.

Advertising
Apple's first slogan, "Byte into an Apple", was coined in the late 1970s. From 1997 to 2002, the slogan "Think different" was used in advertising campaigns, and is still closely associated with Apple. Apple also has slogans for specific product lines—for example, "iThink, therefore iMac" was used in 1998 to promote the iMac, and "Say hello to iPhone" has been used in iPhone advertisements. "Hello" was also used to introduce the original Macintosh, Newton, iMac ("hello (again)"), and iPod.From the introduction of the Macintosh in 1984, with the 1984 Super Bowl advertisement to the more modern Get a Mac adverts, Apple has been recognized for its efforts towards effective advertising and marketing for its products. However, claims made by later campaigns were criticized, particularly the 2005 Power Mac ads. Apple's product advertisements gained significant attention as a result of their eye-popping graphics and catchy tunes. Musicians who benefited from an improved profile as a result of their songs being included on Apple advertisements include Canadian singer Feist with the song "1234" and Yael Naïm with the song "New Soul".

Stores
The first Apple Stores were originally opened as two locations in May 2001 by then-CEO Steve Jobs, after years of attempting but failing store-within-a-store concepts. Seeing a need for improved retail presentation of the company's products, he began an effort in 1997 to revamp the retail program to get an improved relationship to consumers, and hired Ron Johnson in 2000. Jobs relaunched Apple's online store in 1997, and opened the first two physical stores in 2001. The media initially speculated that Apple would fail, but its stores were highly successful, bypassing the sales numbers of competing nearby stores and within three years reached US$1 billion in annual sales, becoming the fastest retailer in history to do so.Over the years, Apple has expanded the number of retail locations and its geographical coverage, with 499 stores across 22 countries worldwide as of December 2017. Strong product sales have placed Apple among the top-tier retail stores, with sales over $16 billion globally in 2011. Apple Stores underwent a period of significant redesign, beginning in May 2016. This redesign included physical changes to the Apple Stores, such as open spaces and re-branded rooms, as well as changes in function to facilitate interaction between consumers and professionals.Many Apple Stores are located inside shopping malls, but Apple has built several stand-alone "flagship" stores in high-profile locations. It has been granted design patents and received architectural awards for its stores' designs and construction, specifically for its use of glass staircases and cubes. The success of Apple Stores have had significant influence over other consumer electronics retailers, who have lost traffic, control and profits due to a perceived higher quality of service and products at Apple Stores. Due to the popularity of the brand, Apple receives a large number of job applications, many of which come from young workers. Although Apple Store employees receive above-average pay, are offered money toward education and health care, and receive product discounts, there are limited or no paths of career advancement.

Market power
On March 16, 2020, France fined Apple €1.1 billion for colluding with two wholesalers to stifle competition and keep prices high by handicapping independent resellers. The arrangement created aligned prices for Apple products such as iPads and personal computers for about half the French retail market. According to the French regulators, the abuses occurred between 2005 and 2017 but were first discovered after a complaint by an independent reseller, eBizcuss, in 2012.On August 13, 2020, Epic Games, the maker of the popular game Fortnite, sued Apple and Google after its hugely popular video game was removed from Apple and Google's App Store. The suits came after both Apple and Google blocked the game after it introduced a direct payment system, effectively shutting out the tech titans from collecting fees. In September 2020, Epic Games founded the Coalition for App Fairness together with thirteen other companies, which aims for better conditions for the inclusion of apps in the app stores. Later, in December 2020, Facebook agreed to assist Epic in their legal game against Apple, planning to support the company by providing materials and documents to Epic. Facebook had, however, stated that the company would not participate directly with the lawsuit, although did commit to helping with the discovery of evidence relating to the trial of 2021. In the months prior to their agreement, Facebook had been dealing with feuds against Apple relating to the prices of paid apps as well as privacy rule changes. Head of ad products for Facebook Dan Levy commented, saying that "this is not really about privacy for them, this is about an attack on personalized ads and the consequences it's going to have on small-business owners," commenting on the full-page ads placed by Facebook in various newspapers in December 2020.

Customer privacy
Apple has a pro-privacy stance, actively making privacy-conscious features and settings part of its conferences, promotional campaigns, and public image. With its iOS 8 mobile operating system in 2014, the company started encrypting all contents of iOS devices through users' passcodes, making it impossible at the time for the company to provide customer data to law enforcement requests seeking such information. With the popularity rise of cloud storage solutions, Apple began a technique in 2016 to do deep learning scans for facial data in photos on the user's local device and encrypting the content before uploading it to Apple's iCloud storage system. It also introduced "differential privacy", a way to collect crowdsourced data from many users, while keeping individual users anonymous, in a system that Wired described as "trying to learn as much as possible about a group while learning as little as possible about any individual in it". Users are explicitly asked if they want to participate, and can actively opt-in or opt-out.With Apple's release of an update to iOS 14, Apple required all developers of iPhone, iPad, and iPod Touch applications to directly ask iPhone users permission to track them. The feature, titled "App Tracking Transparency", received heavy criticism from Facebook, whose primary business model revolves around the tracking of users' data and sharing such data with advertisers so users can see more relevant ads, a technique commonly known as targeted advertising. Despite Facebook's measures, including purchasing full-page newspaper advertisements protesting App Tracking Transparency, Apple released the update in mid-spring 2021. A study by Verizon subsidiary Flurry Analytics reported only 4% of iOS users in the United States and 12% worldwide have opted into tracking.However, Apple aids law enforcement in criminal investigations by providing iCloud backups of users' devices, and the company's commitment to privacy has been questioned by its efforts to promote biometric authentication technology in its newer iPhone models, which do not have the same level of constitutional privacy as a passcode in the United States.Prior to the release of iOS 15, Apple announced new efforts at combating child sexual abuse material on iOS and Mac platforms. Parents of minor iMessage users can now be alerted if their child sends or receives nude photographs. Additionally, on-device hashing would take place on media destined for upload to iCloud, and hashes would be compared to a list of known abusive images provided by law enforcement; if enough matches were found, Apple would be alerted and authorities informed. The new features received praise from law enforcement and victims rights advocates, however privacy advocates, including the Electronic Frontier Foundation, condemned the new features as invasive and highly prone to abuse by authoritarian governments.Ireland's Data Protection Commission launched a privacy investigation to examine whether Apple complied with the EU's GDPR law following an investigation into how the company processes personal data with targeted ads on its platform.In December 2019, a report found that the iPhone 11 Pro continues tracking location and collecting user data even after users have disabled location services. In response, an Apple engineer said the Location Services icon "appears for system services that do not have a switch in settings."According to published reports by Bloomberg News on March 30, 2022, Apple turned over data such as phone numbers, physical addresses, and IP addresses to hackers posing as law enforcement officials using forged documents. The law enforcement requests sometimes included forged signatures of real or fictional officials. When asked about the allegations, an Apple representative referred the reporter to a section of the company policy for law enforcement guidelines, which stated, "We review every data request for legal sufficiency and use advanced systems and processes to validate law enforcement requests and detect abuse."

Corporate affairs
Business trends
The key trends for Apple are (as of the financial year ending September 24):

Leadership
Senior management
As of March 16, 2021, the management of Apple Inc. includes:
Tim Cook (chief executive officer)
Jeff Williams (chief operating officer)
Luca Maestri (senior vice president and chief financial officer)
Katherine L. Adams (senior vice president and general counsel)
Eddy Cue (senior vice president – Internet Software and Services)
Craig Federighi (senior vice president – Software Engineering)
John Giannandrea (senior vice president – Machine Learning and AI Strategy)
Deirdre O'Brien (senior vice president – Retail + People)
John Ternus (senior vice president – Hardware Engineering)
Greg Josiwak (senior vice president – Worldwide Marketing)
Johny Srouji (senior vice president – Hardware Technologies)
Sabih Khan (senior vice president – Operations)

Board of directors
As of January 20, 2023, the board of directors of Apple Inc. includes:
Arthur D. Levinson (chairman)
Tim Cook (executive director and CEO)
James A. Bell
Al Gore
Alex Gorsky
Andrea Jung
Monica Lozano
Ronald Sugar
Susan Wagner

Previous CEOs
Michael Scott (1977–1981)
Mike Markkula (1981–1983)
John Sculley (1983–1993)
Michael Spindler (1993–1996)
Gil Amelio (1996–1997)
Steve Jobs (1997–2011)

Corporate culture
Apple is one of several highly successful companies founded in the 1970s that bucked the traditional notions of corporate culture. Jobs often walked around the office barefoot even after Apple became a Fortune 500 company. By the time of the "1984" television advertisement, Apple's informal culture had become a key trait that differentiated it from its competitors. According to a 2011 report in Fortune, this has resulted in a corporate culture more akin to a startup rather than a multinational corporation. In a 2017 interview, Wozniak credited watching Star Trek and attending Star Trek conventions in his youth as inspiration for co-founding Apple.As the company has grown and been led by a series of differently opinionated chief executives, it has arguably lost some of its original character. Nonetheless, it has maintained a reputation for fostering individuality and excellence that reliably attracts talented workers, particularly after Jobs returned. Numerous Apple employees have stated that projects without Jobs's involvement often took longer than others.The Apple Fellows program awards employees for extraordinary technical or leadership contributions to personal computing. Recipients include Bill Atkinson, Steve Capps, Rod Holt, Alan Kay, Guy Kawasaki, Al Alcorn, Don Norman, Rich Page, Steve Wozniak, and Phil Schiller.At Apple, employees are intended to be specialists who are not exposed to functions outside their area of expertise. Jobs saw this as a means of having "best-in-class" employees in every role. For instance, Ron Johnson—Senior Vice President of Retail Operations until November 1, 2011—was responsible for site selection, in-store service, and store layout, yet had no control of the inventory in his stores. This was done by Tim Cook, who had a background in supply-chain management. Apple is known for strictly enforcing accountability. Each project has a "directly responsible individual" or "DRI" in Apple jargon. As an example, when iOS senior vice president Scott Forstall refused to sign Apple's official apology for numerous errors in the redesigned Maps app, he was forced to resign. Unlike other major U.S. companies, Apple provides a relatively simple compensation policy for executives that does not include perks enjoyed by other CEOs like country club fees or private use of company aircraft. The company typically grants stock options to executives every other year.In 2015, Apple had 110,000 full-time employees. This increased to 116,000 full-time employees the next year, a notable hiring decrease, largely due to its first revenue decline. Apple does not specify how many of its employees work in retail, though its 2014 SEC filing put the number at approximately half of its employee base. In September 2017, Apple announced that it had over 123,000 full-time employees.Apple has a strong culture of corporate secrecy, and has an anti-leak Global Security team that recruits from the National Security Agency, the Federal Bureau of Investigation, and the United States Secret Service.In December 2017, Glassdoor said Apple was the 48th best place to work, having originally entered at rank 19 in 2009, peaking at rank 10 in 2012, and falling down the ranks in subsequent years.In 2023, Bloomberg's Mark Gurman revealed the existence of Apple's Exploratory Design Group (XDG), which was working to add glucose monitoring to the Apple Watch. Gurman compared XDG to Alphabet's X "moonshot factory".

Offices
Apple Inc.'s world corporate headquarters are located in Cupertino, in the middle of California's Silicon Valley, at Apple Park, a massive circular groundscraper building with a circumference of one mile (1.6 km). The building opened in April 2017 and houses more than 12,000 employees. Apple co-founder Steve Jobs wanted Apple Park to look less like a business park and more like a nature refuge, and personally appeared before the Cupertino City Council in June 2011 to make the proposal, in his final public appearance before his death.

Apple also operates from the Apple Campus (also known by its address, 1 Infinite Loop), a grouping of six buildings in Cupertino that total 850,000 square feet (79,000 m2) located about 1 mile (1.6 km) to the west of Apple Park. The Apple Campus was the company's headquarters from its opening in 1993, until the opening of Apple Park in 2017. The buildings, located at 1–6 Infinite Loop, are arranged in a circular pattern around a central green space, in a design that has been compared to that of a university.
In addition to Apple Park and the Apple Campus, Apple occupies an additional thirty office buildings scattered throughout the city of Cupertino, including three buildings as prior headquarters: Stephens Creek Three from 1977 to 1978, Bandley One from 1978 to 1982, and Mariani One from 1982 to 1993. In total, Apple occupies almost 40% of the available office space in the city.Apple's headquarters for Europe, the Middle East and Africa (EMEA) are located in Cork in the south of Ireland, called the Hollyhill campus. The facility, which opened in 1980, houses 5,500 people and was Apple's first location outside of the United States. Apple's international sales and distribution arms operate out of the campus in Cork.Apple has two campuses near Austin, Texas: a 216,000-square-foot (20,100 m2) campus opened in 2014 houses 500 engineers who work on Apple silicon and a 1.1-million-square-foot (100,000 m2) campus opened in 2021 where 6,000 people work in technical support, supply chain management, online store curation, and Apple Maps data management.
The company also has several other locations in Boulder, Colorado, Culver City, California, Herzliya (Israel), London, New York, Pittsburgh, San Diego, and Seattle that each employ hundreds of people.

Litigation
Apple has been a participant in various legal proceedings and claims since it began operation. In particular, Apple is known for and promotes itself as actively and aggressively enforcing its intellectual property interests. Some litigation examples include Apple v. Samsung, Apple v. Microsoft, Motorola Mobility v. Apple Inc., and Apple Corps v. Apple Computer. Apple has also had to defend itself against charges on numerous occasions of violating intellectual property rights. Most have been dismissed in the courts as shell companies known as patent trolls, with no evidence of actual use of patents in question. On December 21, 2016, Nokia announced that in the U.S. and Germany, it has filed a suit against Apple, claiming that the latter's products infringe on Nokia's patents. Most recently, in November 2017, the United States International Trade Commission announced an investigation into allegations of patent infringement in regards to Apple's remote desktop technology; Aqua Connect, a company that builds remote desktop software, has claimed that Apple infringed on two of its patents. In January 2022, Ericsson sued Apple over payment of royalty of 5G technology.

Finances
Apple is the world's largest technology company by revenue, the world's largest technology company by total assets, and the world's second-largest mobile phone manufacturer after Samsung.In its fiscal year ending in September 2011, Apple Inc. reported a total of $108 billion in annual revenues—a significant increase from its 2010 revenues of $65 billion—and nearly $82 billion in cash reserves. On March 19, 2012, Apple announced plans for a $2.65-per-share dividend beginning in fourth quarter of 2012, per approval by their board of directors.The company's worldwide annual revenue in 2013 totaled $170 billion. In May 2013, Apple entered the top ten of the Fortune 500 list of companies for the first time, rising 11 places above its 2012 ranking to take the sixth position. As of 2016, Apple has around US$234 billion of cash and marketable securities, of which 90% is located outside the United States for tax purposes.Apple amassed 65% of all profits made by the eight largest worldwide smartphone manufacturers in quarter one of 2014, according to a report by Canaccord Genuity. In the first quarter of 2015, the company garnered 92% of all earnings.On April 30, 2017, The Wall Street Journal reported that Apple had cash reserves of $250 billion, officially confirmed by Apple as specifically $256.8 billion a few days later.As of August 3, 2018, Apple was the largest publicly traded corporation in the world by market capitalization. On August 2, 2018, Apple became the first publicly traded U.S. company to reach a $1 trillion market value. Apple was ranked No. 4 on the 2018 Fortune 500 rankings of the largest United States corporations by total revenue.In July 2022, Apple reported an 11% decline in Q3 profits compared to 2021. Its revenue in the same period rose 2% year-on-year to $83 billion, though this figure was also lower than in 2021, where the increase was at 36%. The general downturn is reportedly caused by the slowing global economy and supply chain disruptions in China.In May 2023, Apple reported a decline in its sales for the first quarter of 2023. Compared to that of 2022, revenue for 2023 fell by 3%. This is Apple's second consecutive quarter of sales decline. This fall is attributed to the slowing economy and consumers putting off purchases of iPads and computers due to increased pricing. However, iPhone sales held up with a year-on-year increase of 1.5%. According to Apple, demands for such devices were strong, particularly in Latin America and South Asia.

Taxes
Apple has created subsidiaries in low-tax places such as Ireland, the Netherlands, Luxembourg, and the British Virgin Islands to cut the taxes it pays around the world. According to The New York Times, in the 1980s Apple was among the first tech companies to designate overseas salespeople in high-tax countries in a manner that allowed the company to sell on behalf of low-tax subsidiaries on other continents, sidestepping income taxes. In the late 1980s, Apple was a pioneer of an accounting technique known as the "Double Irish with a Dutch sandwich", which reduces taxes by routing profits through Irish subsidiaries and the Netherlands and then to the Caribbean.British Conservative Party Member of Parliament Charlie Elphicke published research on October 30, 2012, which showed that some multinational companies, including Apple Inc., were making billions of pounds of profit in the UK, but were paying an effective tax rate to the UK Treasury of only 3 percent, well below standard corporate tax rates. He followed this research by calling on the Chancellor of the Exchequer George Osborne to force these multinationals, which also included Google and The Coca-Cola Company, to state the effective rate of tax they pay on their UK revenues. Elphicke also said that government contracts should be withheld from multinationals who do not pay their fair share of UK tax.According to a US Senate report on the company's offshore tax structure concluded in May 2013, Apple has held billions of dollars in profits in Irish subsidiaries to pay little or no taxes to any government by using an unusual global tax structure. The main subsidiary, a holding company that includes Apple's retail stores throughout Europe, has not paid any corporate income tax in the last five years. "Apple has exploited a difference between Irish and U.S. tax residency rules", the report said.On May 21, 2013, Apple CEO Tim Cook defended his company's tax tactics at a Senate hearing.Apple says that it is the single largest taxpayer in the U.S., with an effective tax rate of approximately of 26% as of Q2 FY2016. In an interview with the German newspaper FAZ in October 2017, Tim Cook stated that Apple was the biggest taxpayer worldwide.In 2016, after a two-year investigation, the European Commission claimed that Apple's use of a hybrid Double Irish tax arrangement constituted "illegal state aid" from Ireland, and ordered Apple to pay 13 billion euros ($14.5 billion) in unpaid taxes, the largest corporate tax fine in history. This was later annulled, after the European General Court ruled that the Commission had provided insufficient evidence. In 2018, Apple repatriated $285 billion to America, resulting in a $38 billion tax payment spread over the following 8 years.

Charity
Apple is a partner of (PRODUCT)RED, a fundraising campaign for AIDS charity. In November 2014, Apple arranged for all App Store revenue in a two-week period to go to the fundraiser, generating more than US$20 million, and in March 2017, it released an iPhone 7 with a red color finish.Apple contributes financially to fundraisers in times of natural disasters. In November 2012, it donated $2.5 million to the American Red Cross to aid relief efforts after Hurricane Sandy, and in 2017 it donated $5 million to relief efforts for both Hurricane Irma and Hurricane Harvey, as well as for the 2017 Central Mexico earthquake. The company has also used its iTunes platform to encourage donations in the wake of environmental disasters and humanitarian crises, such as the 2010 Haiti earthquake, the 2011 Japan earthquake, Typhoon Haiyan in the Philippines in November 2013, and the 2015 European migrant crisis. Apple emphasizes that it does not incur any processing or other fees for iTunes donations, sending 100% of the payments directly to relief efforts, though it also acknowledges that the Red Cross does not receive any personal information on the users donating and that the payments may not be tax deductible.On April 14, 2016, Apple and the World Wide Fund for Nature (WWF) announced that they have engaged in a partnership to, "help protect life on our planet." Apple released a special page in the iTunes App Store, Apps for Earth. In the arrangement, Apple has committed that through April 24, WWF will receive 100% of the proceeds from the applications participating in the App Store via both the purchases of any paid apps and the In-App Purchases. Apple and WWF's Apps for Earth campaign raised more than $8 million in total proceeds to support WWF's conservation work. WWF announced the results at WWDC 2016 in San Francisco.During the COVID-19 pandemic, Apple's CEO Cook announced that the company will be donating "millions" of masks to health workers in the United States and Europe.On January 13, 2021, Apple announced a $100 million "Racial Equity and Justice Initiative" to help combat institutional racism worldwide.

Environment
Apple Energy
Apple Energy, LLC is a wholly-owned subsidiary of Apple Inc. that sells solar energy. As of June 6, 2016, Apple's solar farms in California and Nevada have been declared to provide 217.9 megawatts of solar generation capacity. In addition to the company's solar energy production, Apple has received regulatory approval to construct a landfill gas energy plant in North Carolina. Apple will use the methane emissions to generate electricity. Apple's North Carolina data center is already powered entirely with energy from renewable sources.

Energy and resources
In 2010, Climate Counts, a nonprofit organization dedicated to directing consumers toward the greenest companies, gave Apple a score of 52 points out of a possible 100, which puts Apple in their top category "Striding". This was an increase from May 2008, when Climate Counts only gave Apple 11 points out of 100, which placed the company last among electronics companies, at which time Climate Counts also labeled Apple with a "stuck icon", adding that Apple at the time was "a choice to avoid for the climate-conscious consumer".Following a Greenpeace protest, Apple released a statement on April 17, 2012, committing to ending its use of coal and shifting to 100% renewable clean energy. By 2013, Apple was using 100% renewable energy to power their data centers. Overall, 75% of the company's power came from clean renewable sources.In May 2015, Greenpeace evaluated the state of the Green Internet and commended Apple on their environmental practices saying, "Apple's commitment to renewable energy has helped set a new bar for the industry, illustrating in very concrete terms that a 100% renewable Internet is within its reach, and providing several models of intervention for other companies that want to build a sustainable Internet."As of 2016, Apple states that 100% of its U.S. operations run on renewable energy, 100% of Apple's data centers run on renewable energy and 93% of Apple's global operations run on renewable energy. However, the facilities are connected to the local grid which usually contains a mix of fossil and renewable sources, so Apple carbon offsets its electricity use. The Electronic Product Environmental Assessment Tool (EPEAT) allows consumers to see the effect a product has on the environment. Each product receives a Gold, Silver, or Bronze rank depending on its efficiency and sustainability. Every Apple tablet, notebook, desktop computer, and display that EPEAT ranks achieves a Gold rating, the highest possible. Although Apple's data centers recycle water 35 times, the increased activity in retail, corporate and data centers also increase the amount of water use to 573 million US gal (2.2 million m3) in 2015.During an event on March 21, 2016, Apple provided a status update on its environmental initiative to be 100% renewable in all of its worldwide operations. Lisa P. Jackson, Apple's vice president of Environment, Policy and Social Initiatives who reports directly to CEO, Tim Cook, announced that as of March 2016, 93% of Apple's worldwide operations are powered with renewable energy. Also featured was the company's efforts to use sustainable paper in their product packaging; 99% of all paper used by Apple in the product packaging comes from post-consumer recycled paper or sustainably managed forests, as the company continues its move to all paper packaging for all of its products. Apple working in partnership with Conservation Fund, have preserved 36,000 acres of working forests in Maine and North Carolina. Another partnership announced is with the World Wildlife Fund to preserve up to 1,000,000 acres (4,000 km2) of forests in China. Featured was the company's installation of a 40 MW solar power plant in the Sichuan province of China that was tailor-made to coexist with the indigenous yaks that eat hay produced on the land, by raising the panels to be several feet off of the ground so the yaks and their feed would be unharmed grazing beneath the array. This installation alone compensates for more than all of the energy used in Apple's Stores and Offices in the whole of China, negating the company's energy carbon footprint in the country. In Singapore, Apple has worked with the Singaporean government to cover the rooftops of 800 buildings in the city-state with solar panels allowing Apple's Singapore operations to be run on 100% renewable energy. Liam was introduced to the world, an advanced robotic disassembler and sorter designed by Apple Engineers in California specifically for recycling outdated or broken iPhones. Reuses and recycles parts from traded in products.Apple announced on August 16, 2016, that Lens Technology, one of its major suppliers in China, has committed to power all its glass production for Apple with 100 percent renewable energy by 2018. The commitment is a large step in Apple's efforts to help manufacturers lower their carbon footprint in China. Apple also announced that all 14 of its final assembly sites in China are now compliant with UL's Zero Waste to Landfill validation. The standard, which started in January 2015, certifies that all manufacturing waste is reused, recycled, composted, or converted into energy (when necessary). Since the program began, nearly 140,000 metric tons of waste have been diverted from landfills.On July 21, 2020, Apple announced its plan to become carbon neutral across its entire business, manufacturing supply chain, and product life cycle by 2030. In the next 10 years, Apple will try to lower emissions with a series of innovative actions, including: low carbon product design, expanding energy efficiency, renewable energy, process and material innovations, and carbon removal.In April 2021, Apple said that it had started a $200 million fund in order to combat climate change by removing 1 million metric tons of carbon dioxide from the atmosphere each year.In February 2022, the NewClimate Institute, a German environmental policy think tank, published a survey evaluating the transparency and progress of the climate strategies and carbon neutrality pledges announced by 25 major companies in the United States that found that Apple's carbon neutrality pledge and climate strategy was unsubstantiated and misleading.

Toxins
Following further campaigns by Greenpeace, in 2008, Apple became the first electronics manufacturer to eliminate all polyvinyl chloride (PVC) and brominated flame retardants (BFRs) in its complete product line. In June 2007, Apple began replacing the cold cathode fluorescent lamp (CCFL) backlit LCD displays in its computers with mercury-free LED-backlit LCD displays and arsenic-free glass, starting with the upgraded MacBook Pro. Apple offers comprehensive and transparent information about the CO2e, emissions, materials, and electrical usage concerning every product they currently produce or have sold in the past (and which they have enough data needed to produce the report), in their portfolio on their homepage. Allowing consumers to make informed purchasing decisions on the products they offer for sale. In June 2009, Apple's iPhone 3GS was free of PVC, arsenic, and BFRs. All Apple products now have mercury-free LED-backlit LCD displays, arsenic-free glass, and non-PVC cables. All Apple products have EPEAT Gold status and beat the latest Energy Star guidelines in each product's respective regulatory category.In November 2011, Apple was featured in Greenpeace's Guide to Greener Electronics, which ranks electronics manufacturers on sustainability, climate and energy policy, and how "green" their products are. The company ranked fourth of fifteen electronics companies (moving up five places from the previous year) with a score of 4.6/10. Greenpeace praised Apple's sustainability, noting that the company exceeded its 70% global recycling goal in 2010. Apple continues to score well on product ratings, with all of their products now being free of PVC plastic and BFRs. However, the guide criticized Apple on the Energy criteria for not seeking external verification of its greenhouse gas emissions data, and for not setting any targets to reduce emissions. In January 2012, Apple requested that its cable maker, Volex, begin producing halogen-free USB and power cables.

Green bonds
In February 2016, Apple issued a $1.5 billion green bond (climate bond), the first ever of its kind by a U.S. tech company. The green bond proceeds are dedicated to the financing of environmental projects.

Supply chain
Apple products were made in America in Apple-owned factories until the late 1990s; however, as a result of outsourcing initiatives in the 2000s, almost all of its manufacturing is now handled abroad. According to a report by The New York Times, Apple insiders "believe the vast scale of overseas factories, as well as the flexibility, diligence and industrial skills of foreign workers, have so outpaced their American counterparts that "Made in the USA" is no longer a viable option for most Apple products".The company's manufacturing, procurement, and logistics enable it to execute massive product launches without having to maintain large, profit-sapping inventories. In 2011, Apple's profit margins were 40 percent, compared with between 10 and 20 percent for most other hardware companies. Cook's catchphrase to describe his focus on the company's operational arm is: "Nobody wants to buy sour milk."In May 2017, the company announced a $1 billion funding project for "advanced manufacturing" in the United States, and subsequently invested $200 million in Corning Inc., a manufacturer of toughened Gorilla Glass technology used in its iPhone devices. The following December, Apple's chief operating officer, Jeff Williams, told CNBC that the "$1 billion" amount was "absolutely not" the final limit on its spending, elaborating that "We're not thinking in terms of a fund limit... We're thinking about, where are the opportunities across the U.S. to help nurture companies that are making the advanced technology— and the advanced manufacturing that goes with that— that quite frankly is essential to our innovation."As of 2021, Apple uses components from 43 countries. The majority of assembling is done by Taiwanese original design manufacturer firms Foxconn, Pegatron, Wistron and Compal Electronics with factories mostly located inside China, but also Brazil, and India.Taiwan Semiconductor Manufacturing Co., (TSMC) is a pure-play semiconductor manufacturing company. They make the majority of Apple's smartphone SoCs, with Samsung Semiconductor, playing a minority role. Apple, alone accounted for over 25% of TSMC's total income in 2021. Apple's Bionic lineup of smartphone SoCs, are currently made exclusively by TSMC from the A7 bionic onwards, previously manufacturing was shared with Samsung. The M series of Apple SoC for consumer computers and tablets is made by TSMC as well.During the Mac's early history Apple generally refused to adopt prevailing industry standards for hardware, instead creating their own. This trend was largely reversed in the late 1990s, beginning with Apple's adoption of the PCI bus in the 7500/8500/9500 Power Macs. Apple has since joined the industry standards groups to influence the future direction of technology standards such as USB, AGP, HyperTransport, Wi-Fi, NVMe, PCIe and others in its products. FireWire is an Apple-originated standard that was widely adopted across the industry after it was standardized as IEEE 1394 and is a legally mandated port in all Cable TV boxes in the United States.Apple has gradually expanded its efforts in getting its products into the Indian market. In July 2012, during a conference call with investors, CEO Tim Cook said that he "[loves] India", but that Apple saw larger opportunities outside the region. India's requirement that 30% of products sold be manufactured in the country was described as "really adds cost to getting product to market". In May 2016, Apple opened an iOS app development center in Bangalore and a maps development office for 4,000 staff in Hyderabad. In March, The Wall Street Journal reported that Apple would begin manufacturing iPhone models in India "over the next two months", and in May, the Journal wrote that an Apple manufacturer had begun production of iPhone SE in the country, while Apple told CNBC that the manufacturing was for a "small number" of units. In April 2019, Apple initiated manufacturing of iPhone 7 at its Bengaluru facility, keeping in mind demand from local customers even as they seek more incentives from the government of India. At the beginning of 2020, Tim Cook announced that Apple schedules the opening of its first physical outlet in India for 2021, while an online store is to be launched by the end of the year.During the 2022 COVID-19 protests in China, Chinese state-owned company Wingtech was reported by The Wall Street Journal to gain an additional foothold in Apple's supply chain following protests at a Foxconn factory in the Zhengzhou Airport Economy Zone.

Worker organizations
In 2006, one complex of factories in Shenzhen, China that assembled the iPod and other items had over 200,000 workers living and working within it. Employees regularly worked more than 60 hours per week and made around $100 per month. A little over half of the workers' earnings was required to pay for rent and food from the company.Apple immediately launched an investigation after the 2006 media report, and worked with their manufacturers to ensure acceptable working conditions. In 2007, Apple started yearly audits of all its suppliers regarding worker's rights, slowly raising standards and pruning suppliers that did not comply. Yearly progress reports have been published as of 2008. In 2011, Apple admitted that its suppliers' child labor practices in China had worsened.The Foxconn suicides occurred between January and November 2010, when 18 Foxconn (Chinese: 富士康) employees attempted suicide, resulting in 14 deaths—the company was the world's largest contract electronics manufacturer, for clients including Apple, at the time. The suicides drew media attention, and employment practices at Foxconn were investigated by Apple. Apple issued a public statement about the suicides, and company spokesperson Steven Dowling said:

[Apple is] saddened and upset by the recent suicides at Foxconn ... A team from Apple is independently evaluating the steps they are taking to address these tragic events and we will continue our ongoing inspections of the facilities where our products are made.
The statement was released after the results from the company's probe into its suppliers' labor practices were published in early 2010. Foxconn was not specifically named in the report, but Apple identified a series of serious labor violations of labor laws, including Apple's own rules, and some child labor existed in a number of factories. Apple committed to the implementation of changes following the suicides.Also in 2010, workers in China planned to sue iPhone contractors over poisoning by a cleaner used to clean LCD screens. One worker claimed that he and his coworkers had not been informed of possible occupational illnesses. After a high suicide rate in a Foxconn facility in China making iPads and iPhones, albeit a lower rate than that of China as a whole, workers were forced to sign a legally binding document guaranteeing that they would not kill themselves. Workers in factories producing Apple products have also been exposed to hexane, a neurotoxin that is a cheaper alternative than alcohol for cleaning the products.A 2014 BBC investigation found excessive hours and other problems persisted, despite Apple's promise to reform factory practice after the 2010 Foxconn suicides. The Pegatron factory was once again the subject of review, as reporters gained access to the working conditions inside through recruitment as employees. While the BBC maintained that the experiences of its reporters showed that labor violations were continuing as of 2010, Apple publicly disagreed with the BBC and stated: "We are aware of no other company doing as much as Apple to ensure fair and safe working conditions".In December 2014, the Institute for Global Labour and Human Rights published a report which documented inhumane conditions for the 15,000 workers at a Zhen Ding Technology factory in Shenzhen, China, which serves as a major supplier of circuit boards for Apple's iPhone and iPad. According to the report, workers are pressured into 65-hour work weeks which leaves them so exhausted that they often sleep during lunch breaks. They are also made to reside in "primitive, dark and filthy dorms" where they sleep "on plywood, with six to ten workers in each crowded room." Omnipresent security personnel also routinely harass and beat the workers.In 2019, there were reports stating that some of Foxconn's managers had used rejected parts to build iPhones and that Apple was investigating the issue.

See also
List of Apple Inc. media events
Pixar

References
Bibliography
Further reading
External links
Official website 
Business data for Apple Inc.: 
Apple Inc. companies grouped at OpenCorporates

Balance sheet

In financial accounting, a balance sheet (also known as statement of financial position or statement of financial condition) is a summary of the financial balances of an individual or organization, whether it be a sole proprietorship, a business partnership, a corporation, private limited company or other organization such as government or not-for-profit entity. Assets, liabilities and ownership equity are listed as of a specific date, such as the end of its financial year. A balance sheet is often described as a "snapshot of a company's financial condition".  It is the summary of each and every financial statement of an organization.
Of the four basic financial statements, the balance sheet is the only statement which applies to a single point in time of a business's calendar year.
A standard company balance sheet has two sides: assets on the left, and financing on the right–which itself has two parts; liabilities and ownership equity.  The main categories of assets are usually listed first, and typically in order of liquidity. Assets are followed by the liabilities. The difference between the assets and the liabilities is known as equity or the net assets or the net worth or capital of the company and according to the accounting equation, net worth must equal assets minus liabilities.Another way to look at the balance sheet equation is that total assets equals liabilities plus owner's equity. Looking at the equation in this way shows how assets were financed: either by borrowing money (liability) or by using the owner's money (owner's or shareholders' equity). Balance sheets are usually presented with assets in one section and liabilities and net worth in the other section with the two sections "balancing".
A business operating entirely in cash can measure its profits by withdrawing the entire bank balance at the end of the period, plus any cash in hand. However, many businesses are not paid immediately; they build up inventories of goods and acquire buildings and equipment. In other words: businesses have assets and so they cannot, even if they want to, immediately turn these into cash at the end of each period. Often, these businesses owe money to suppliers and to tax authorities, and the proprietors do not withdraw all their original capital and profits at the end of each period. In other words, businesses also have liabilities.

Types
A balance sheet summarizes an organization's or individual's assets, equity and liabilities at a specific point in time. Two forms of balance sheet exist. They are the report form and  account form. Individuals and small businesses tend to have simple balance sheets. Larger businesses tend to have more complex balance sheets, and these are presented in the organization's annual report. Large businesses also may prepare balance sheets for segments of their businesses. A balance sheet is often presented alongside one for a different point in time (typically the previous year) for comparison.

Personal
A personal balance sheet lists current assets such as cash in checking accounts and savings accounts, long-term assets such as common stock and real estate, current liabilities such as loan debt and mortgage debt due, or overdue, long-term liabilities such as mortgage and other loan debt. Securities and real estate values are listed at market value rather than at historical cost or cost basis. Personal net worth is the difference between an individual's total assets and total liabilities.

US small business
A small business balance sheet lists current assets such as cash, accounts receivable, and inventory, fixed assets such as land, buildings, and equipment, intangible assets such as patents, and liabilities such as accounts payable, accrued expenses, and long-term debt. Contingent liabilities such as warranties are noted in the footnotes to the balance sheet. The small business's equity is the difference between total assets and total liabilities.

Public business entities structure
Guidelines for balance sheets of public business entities are given by the International Accounting Standards Board and numerous country-specific organizations/companies. The standard used by companies in the USA adheres to U.S. Generally Accepted Accounting Principles (GAAP). The Federal Accounting Standards Advisory Board (FASAB) is a United States federal advisory committee whose mission is to develop generally accepted accounting principles (GAAP) for federal financial reporting entities.
Balance sheet account names and usage depend on the organization's country and the type of organization. Government organizations do not generally follow standards established for individuals or businesses.If applicable to the business, summary values for the following items should be included in the balance sheet:
Assets are all the things the business owns. This will include property, tools, vehicles, furniture, machinery, and so on.

Assets
Current assets

Accounts receivable
Cash and cash equivalents
Inventories
Cash at bank, Petty Cash, Cash On Hand
Prepaid expenses for future services that will be used within a year
Revenue Earned In Arrears (Accrued Revenue) for services done but not yet received for the year
Loan To (Less than one financial period)Non-current assets (Fixed assets)

Property, plant and equipment
Investment property, such as real estate held for investment purposes
Intangible assets, such as patents, copyrights and goodwill
Financial assets (excluding investments accounted for using the equity method, accounts receivables, and cash and cash equivalents), such as notes receivables
Investments accounted for using the equity method
Biological assets, which are living plants or animals. Bearer biological assets are plants or animals which bear agricultural produce for harvest, such as apple trees grown to produce apples and sheep raised to produce wool.
Loan To (More than one financial period)

Liabilities
Accounts payable
Provisions for warranties or court decisions (contingent liabilities that are both probable and measurable)
Financial liabilities (excluding provisions and accounts payables), such as promissory notes and corporate bonds
Liabilities and assets for current tax
Deferred tax liabilities and deferred tax assets
Unearned revenue for services paid for by customers but not yet provided
Interests on loan stock
Creditors equity

Equity / capital
The net assets shown by the balance sheet equals the third part of the balance sheet, which is known as the shareholders' equity. It comprises:

Issued capital and reserves attributable to equity holders of the parent company (controlling interest)
Non-controlling interest in equityFormally, shareholders' equity is part of the company's liabilities: they are funds "owing" to shareholders (after payment of all other liabilities); usually, however, "liabilities" are used in the more restrictive sense of liabilities excluding shareholders' equity. The balance of assets and liabilities (including shareholders' equity) is not a coincidence.  Records of the values of each account in the balance sheet are maintained using a system of accounting known as double-entry bookkeeping. In this sense, shareholders' equity by construction must equal assets minus liabilities, and thus the shareholders' equity is considered to be a residual.
Regarding the items in the equity section, the following disclosures are required:

Numbers of shares authorized, issued and fully-paid, and issued but not fully paid
Par value of shares
Reconciliation of shares outstanding at the beginning and the end of the period
Description of rights, preferences, and restrictions of shares
Treasury shares, including shares held by subsidiaries and associates
Shares reserved for issuance under options and contracts
A description of the nature and purpose of each reserve within owners' equity

Substantiation
Balance sheet substantiation is the accounting process conducted by businesses on a regular basis to confirm that the balances held in the primary accounting system of record (e.g. SAP, Oracle, other ERP system's General Ledger) are reconciled (in balance with) with the balance and transaction records held in the same or supporting sub-systems.
Balance sheet substantiation includes multiple processes including reconciliation (at a transactional or at a balance level) of the account, a process of review of the reconciliation and any pertinent supporting documentation and a formal certification (sign-off) of the account in a predetermined form driven by corporate policy.
Balance sheet substantiation is an important process that is typically carried out on a monthly, quarterly and year-end basis. The results help to drive the regulatory balance sheet reporting obligations of the organization.
Historically, balance sheet substantiation has been a wholly manual process, driven by spreadsheets, email and manual monitoring and reporting. In recent years software solutions have been developed to bring a level of process automation, standardization and enhanced control to the balance sheet substantiation or account certification process. These solutions are suitable for organizations with a high volume of accounts and/or personnel involved in the Balance Sheet Substantiation process and can be used to drive efficiencies, improve transparency and help to reduce risk.
Balance sheet substantiation is a key control process in the SOX 404 top-down risk assessment.

Sample
The following balance sheet is a very brief example prepared in accordance with IFRS. It does not show all possible kinds of assets, liabilities and equity, but it shows the most usual ones. Because it shows goodwill, it could be a consolidated balance sheet. Monetary values are not shown, summary (subtotal) rows are missing as well.
Under IFRS items are always shown based on liquidity from the least liquid assets at the top, usually land and buildings to the most liquid, i.e. cash. Then liabilities and equity continue from the most immediate liability to be paid (usual account payable) to the least i.e. long-term debt such as mortgages and owner's equity at the very bottom.
Consolidated Statement of Finance Position of XYZ, Ltd.
 As of 31 December 2025

ASSETS
 Non-Current Assets (Fixed Assets)
  Property, Plant and Equipment (PPE)
     Less : Accumulated Depreciation
  Goodwill
  Intangible Assets (Patent, Copyright, Trademark, etc.)
     Less : Accumulated Amortization
  Investments in Financial assets due after one year
  Investments in Associates and Joint Ventures
  Other Non-Current Assets, e.g. Deferred Tax Assets, Lease Receivable and Receivables due after one year

 Current Assets
  Inventories
  Prepaid Expenses
  Investments in Financial assets due within one year
  Non-Current and Current Assets Held for sale
  Accounts Receivable (Debtors) due within one year
     Less : Allowances for Doubtful debts
  Cash and Cash Equivalents

TOTAL ASSETS (this will match/balance the total for Liabilities and Equity below)

LIABILITIES and EQUITY
 Current Liabilities (Creditors: amounts falling due within one year)
  Accounts Payable
  Current Income Tax Payable
  Current portion of Loans Payable
  Short-term Provisions
  Other Current Liabilities, e.g. Deferred income, Security deposits

 Non-Current Liabilities (Creditors: amounts falling due after more than one year)
  Loans Payable
  Issued Debt Securities, e.g. Notes/Bonds Payable
  Deferred Tax Liabilities
  Provisions, e.g. Pension Obligations
  Other Non-Current Liabilities, e.g. Lease Obligations

 EQUITY
  Paid-in Capital
    Share Capital (Ordinary Shares, Preference Shares)
    Share Premium
      Less: Treasury Shares
  Retained Earnings
  Revaluation Reserve
  Other Accumulated Reserves
  Accumulated Other Comprehensive Income
 
  Non-Controlling Interest

TOTAL LIABILITIES and EQUITY (this will match/balance the total for Assets above)

See also


== References ==

Bank

A bank is a financial institution that accepts deposits from the public and creates a demand deposit while simultaneously making loans. Lending activities can be directly performed by the bank or indirectly through capital markets.Whereas banks play an important role in financial stability and the economy of a country, most jurisdictions exercise a high degree of regulation over banks. Most countries have institutionalized a system known as fractional-reserve banking, under which banks hold liquid assets equal to only a portion of their current liabilities. In addition to other regulations intended to ensure liquidity, banks are generally subject to minimum capital requirements based on an international set of capital standards, the Basel Accords.
Banking in its modern sense evolved in the fourteenth century in the prosperous cities of Renaissance Italy but in many ways functioned as a continuation of ideas and concepts of credit and lending that had their roots in the ancient world. In the history of banking, a number of banking dynasties –  notably, the Medicis, the Fuggers, the Welsers, the Berenbergs, and the Rothschilds –  have played a central role over many centuries. The oldest existing retail bank is Banca Monte dei Paschi di Siena (founded in 1472), while the oldest existing merchant bank is Berenberg Bank (founded in 1590).

History
Banking as an archaic activity (or quasi-banking) is thought to have begun as early as the end of the 4th millennium BCE, to the 3rd millennia BCE.

Medieval
The present era of banking can be traced to medieval and early Renaissance Italy, to the rich cities in the centre and north like Florence, Lucca, Siena, Venice and Genoa. The Bardi and Peruzzi families dominated banking in 14th-century Florence, establishing branches in many other parts of Europe. Giovanni di Bicci de' Medici set up one of the most famous Italian banks, the Medici Bank, in 1397. The Republic of Genoa founded the earliest-known state deposit bank, and Banco di San Giorgio (Bank of St. George), in 1407 at Genoa, Italy.

Early modern
Fractional reserve banking and the issue of banknotes emerged in the 17th and 18th centuries. Merchants started to store their gold with the goldsmiths of London, who possessed private vaults, and who charged a fee for that service. In exchange for each deposit of precious metal, the goldsmiths issued receipts certifying the quantity and purity of the metal they held as a bailee; these receipts could not be assigned, only the original depositor could collect the stored goods.
Gradually the goldsmiths began to lend money out on behalf of the depositor, and promissory notes (which evolved into banknotes) were issued for money deposited as a loan to the goldsmith. Thus by the 19th century, we find in ordinary cases of deposits of money with banking corporations, or bankers, the transaction amounts to a mere loan or mutuum, and the bank is to restore, not the same money, but an equivalent sum, whenever it is demanded
and money, when paid into a bank, ceases altogether to be the money of the principal (see Parker v. Marchant, 1 Phillips 360); it is then the money of the banker, who is bound to return an equivalent by paying a similar sum to that deposited with him when he is asked for it.

The goldsmith paid interest on deposits. Since the promissory notes were payable on demand, and the advances (loans) to the goldsmith's customers were repayable over a longer time-period, this was an early form of fractional reserve banking. The promissory notes developed into an assignable instrument which could circulate as a safe and convenient form of money
backed by the goldsmith's promise to pay,
allowing goldsmiths to advance loans with little risk of default. Thus the goldsmiths of London became the forerunners of banking by creating new money based on credit.

The Bank of England originated the permanent issue of banknotes in 1695. The Royal Bank of Scotland established the first overdraft facility in 1728. By the beginning of the 19th century Lubbock's Bank had established a bankers' clearing house in London to allow multiple banks to clear transactions. The Rothschilds pioneered international finance on a large scale, financing the purchase of shares in the Suez canal for the British government in 1875.

Etymology
The word bank was taken into Middle English from Middle French banque, from Old Italian banco, meaning "table", from Old High German banc, bank "bench, counter". Benches were used as makeshift desks or exchange counters during the Renaissance by Florentine bankers, who used to make their transactions atop desks covered by green tablecloths.

Definition
The definition of a bank varies from country to country. See the relevant country pages for more information.
Under English common law, a banker is defined as a person who carries on the business of banking by conducting current accounts for their customers, paying cheques drawn on them and also collecting cheques for their customers.
In most common law jurisdictions there is a Bills of Exchange Act that codifies the law in relation to negotiable instruments, including cheques, and this Act contains a statutory definition of the term banker: banker includes a body of persons, whether incorporated or not, who carry on the business of banking' (Section 2, Interpretation). Although this definition seems circular, it is actually functional, because it ensures that the legal basis for bank transactions such as cheques does not depend on how the bank is structured or regulated.
The business of banking is in many common law countries not defined by statute but by common law, the definition above. In other English common law jurisdictions there are statutory definitions of the business of banking or banking business. When looking at these definitions it is important to keep in mind that they are defining the business of banking for the purposes of the legislation, and not necessarily in general. In particular, most of the definitions are from legislation that has the purpose of regulating and supervising banks rather than regulating the actual business of banking. However, in many cases, the statutory definition closely mirrors the common law one. Examples of statutory definitions:

"banking business" means the business of receiving money on current or deposit account, paying and collecting cheques drawn by or paid in by customers, the making of advances to customers, and includes such other business as the Authority may prescribe for the purposes of this Act; (Banking Act (Singapore), Section 2, Interpretation).
"banking business" means the business of either or both of the following:receiving from the general public money on current, deposit, savings or other similar account repayable on demand or within less than [3 months] ... or with a period of call or notice of less than that period;
paying or collecting cheques drawn by or paid in by customers.Since the advent of EFTPOS (Electronic Funds Transfer at Point Of Sale), direct credit, direct debit and internet banking, the cheque has lost its primacy in most banking systems as a payment instrument. This has led legal theorists to suggest that the cheque based definition should be broadened to include financial institutions that conduct current accounts for customers and enable customers to pay and be paid by third parties, even if they do not pay and collect cheques .

Standard business
Banks act as payment agents by conducting checking or current accounts for customers, paying cheques drawn by customers in the bank, and collecting cheques deposited to customers' current accounts. Banks also enable customer payments via other payment methods such as Automated Clearing House (ACH), Wire transfers or telegraphic transfer, EFTPOS, and automated teller machines (ATMs).
Banks borrow money by accepting funds deposited on current accounts, by accepting term deposits, and by issuing debt securities such as banknotes and bonds. Banks lend money by making advances to customers on current accounts, by making installment loans, and by investing in marketable debt securities and other forms of money lending.
Banks provide different payment services, and a bank account is considered indispensable by most businesses and individuals. Non-banks that provide payment services such as remittance companies are normally not considered as an adequate substitute for a bank account.
Banks issue new money when they make loans. In contemporary banking systems, regulators set a minimum level of reserve funds that banks must hold against the deposit liabilities created by the funding of these loans, in order to ensure that the banks can meet demands for payment of such deposits. These reserves can be acquired through the acceptance of new deposits, sale of other assets, or borrowing from other banks including the central bank.

Range of activities
Activities undertaken by banks include personal banking, corporate banking, investment banking, private banking, transaction banking, insurance, consumer finance, trade finance and other related.

Channels
Banks offer many different channels to access their banking and other services:

Branch, in-person banking in a retail location
Automated teller machine banking adjacent to or remote from the bank
Bank by mail: Most banks accept cheque deposits via mail and use mail to communicate to their customers
Online banking over the Internet to perform multiple types of transactions
Mobile banking is using one's mobile phone to conduct banking transactions
Telephone banking allows customers to conduct transactions over the telephone with an automated attendant, or when requested, with a telephone operator
Video banking performs banking transactions or professional banking consultations via a remote video and audio connection. Video banking can be performed via purpose built banking transaction machines (similar to an Automated teller machine) or via a video conference enabled bank branch clarification
Relationship manager, mostly for private banking or business banking, who visits customers at their homes or businesses
Direct Selling Agent, who works for the bank based on a contract, whose main job is to increase the customer base for the bank

Business models
A bank can generate revenue in a variety of different ways including interest, transaction fees and financial advice. Traditionally, the most significant method is via charging interest on the capital it lends out to customers. The bank profits from the difference between the level of interest it pays for deposits and other sources of funds, and the level of interest it charges in its lending activities.
This difference is referred to as the spread between the cost of funds and the loan interest rate. Historically, profitability from lending activities has been cyclical and dependent on the needs and strengths of loan customers and the stage of the economic cycle. Fees and financial advice constitute a more stable revenue stream and banks have therefore placed more emphasis on these revenue lines to smooth their financial performance.
In the past 20 years, American banks have taken many measures to ensure that they remain profitable while responding to increasingly changing market conditions.

First, this includes the Gramm–Leach–Bliley Act, which allows banks again to merge with investment and insurance houses. Merging banking, investment, and insurance functions allows traditional banks to respond to increasing consumer demands for "one-stop shopping" by enabling cross-selling of products (which, the banks hope, will also increase profitability).
Second, they have expanded the use of risk-based pricing from business lending to consumer lending, which means charging higher interest rates to those customers that are considered to be a higher credit risk and thus increased chance of default on loans. This helps to offset the losses from bad loans, lowers the price of loans to those who have better credit histories, and offers credit products to high risk customers who would otherwise be denied credit.
Third, they have sought to increase the methods of payment processing available to the general public and business clients. These products include debit cards, prepaid cards, smart cards, and credit cards. They make it easier for consumers to conveniently make transactions and smooth their consumption over time (in some countries with underdeveloped financial systems, it is still common to deal strictly in cash, including carrying suitcases filled with cash to purchase a home).However, with the convenience of easy credit, there is also an increased risk that consumers will mismanage their financial resources and accumulate excessive debt. Banks make money from card products through interest charges and fees charged to cardholders, and transaction fees to retailers who accept the bank's credit cards and debit cards for payments.This helps in making a profit and facilitates economic development as a whole.Recently, as banks have been faced with pressure from fintechs, new and additional business models have been suggested such as freemium, monetisation of data, white-labeling of banking and payment applications, or the cross-selling of complementary products.

Products
Retail
ATM card
Credit card
Debit card
Savings account
Recurring deposit account
Fixed deposit account
Money market account
Certificate of deposit (CD)
Individual retirement account (IRA)
Mortgage
Mutual fund
Personal loan (Secured and Unsecured Personal loan)
Time deposits
Current accounts
Cheque books
Automated teller machine (ATM)
National Electronic Fund Transfer (NEFT)
Real-time gross settlement (RTGS)

Business (or commercial/investment) banking
Business loan
Capital raising (equity / debt / hybrids)
Revolving credit
Risk management (foreign exchange (FX), interest rates, commodities, derivatives)
Term loan
Cash management services (lock box, remote deposit capture, merchant processing)
Credit services
Securities Services

Capital and risk
Banks face a number of risks in order to conduct their business, and how well these risks are managed and understood is a key driver behind profitability, and how much capital a bank is required to hold. Bank capital consists principally of equity, retained earnings and subordinated debt.
Some of the main risks faced by banks include:

Credit risk: risk of loss arising from a borrower who does not make payments as promised.
Liquidity risk: risk that a given security or asset cannot be traded quickly enough in the market to prevent a loss (or make the required profit).
Market risk: risk that the value of a portfolio, either an investment portfolio or a trading portfolio, will decrease due to the change in value of the market risk factors.
Operational risk: risk arising from the execution of a company's business functions.
Reputational risk: a type of risk related to the trustworthiness of the business.
Macroeconomic risk: risks related to the aggregate economy the bank is operating in.The capital requirement is a bank regulation, which sets a framework within which a bank or depository institution must manage its balance sheet. The categorisation of assets and capital is highly standardised so that it can be risk weighted.
After the financial crisis of 2007–2008, regulators force banks to issue Contingent convertible bonds (CoCos). These are hybrid capital securities that absorb losses in accordance with their contractual terms when the capital of the issuing bank falls below a certain level. Then debt is reduced and bank capitalisation gets a boost. Owing to their capacity to absorb losses, CoCos have the potential to satisfy regulatory capital requirement.

Banks in the economy
Economic functions
The economic functions of banks include:

Issue of money, in the form of banknotes and current accounts subject to cheque or payment at the customer's order. These claims on banks can act as money because they are negotiable or repayable on demand, and hence valued at par. They are effectively transferable by mere delivery, in the case of banknotes, or by drawing a cheque that the payee may bank or cash.
Netting and settlement of payments – banks act as both collection and paying agents for customers, participating in interbank clearing and settlement systems to collect, present, be presented with, and pay payment instruments. This enables banks to economise on reserves held for settlement of payments since inward and outward payments offset each other. It also enables the offsetting of payment flows between geographical areas, reducing the cost of settlement between them.
Credit quality improvement – banks lend money to ordinary commercial and personal borrowers (ordinary credit quality), but are high quality borrowers. The improvement comes from diversification of the bank's assets and capital which provides a buffer to absorb losses without defaulting on its obligations. However, banknotes and deposits are generally unsecured; if the bank gets into difficulty and pledges assets as security, to raise the funding it needs to continue to operate, this puts the note holders and depositors in an economically subordinated position.
Asset liability mismatch/Maturity transformation – banks borrow more on demand debt and short term debt, but provide more long-term loans. In other words, they borrow short and lend long. With a stronger credit quality than most other borrowers, banks can do this by aggregating issues (e.g. accepting deposits and issuing banknotes) and redemptions (e.g. withdrawals and redemption of banknotes), maintaining reserves of cash, investing in marketable securities that can be readily converted to cash if needed, and raising replacement funding as needed from various sources (e.g. wholesale cash markets and securities markets).
Money creation/destruction – whenever a bank gives out a loan in a fractional-reserve banking system, a new sum of money is created and conversely, whenever the principal on that loan is repaid money is destroyed.

Bank crisis
Banks are susceptible to many forms of risk which have triggered occasional systemic crises. These include liquidity risk (where many depositors may request withdrawals in excess of available funds), credit risk (the chance that those who owe money to the bank will not repay it), and interest rate risk (the possibility that the bank will become unprofitable, if rising interest rates force it to pay relatively more on its deposits than it receives on its loans).
Banking crises have developed many times throughout history when one or more risks have emerged for the banking sector as a whole. Prominent examples include the bank run that occurred during the Great Depression, the U.S. Savings and Loan crisis in the 1980s and early 1990s, the Japanese banking crisis during the 1990s, and the sub-prime mortgage crisis in the 2000s.
The 2023 global banking crisis is the latest of these crises: In March 2023,  liquidity shortages and bank insolvencies led to three bank failures in the United States, and within two weeks, several of the world's largest banks failed or were shut down by regulators

Size of global banking industry
Assets of the largest 1,000 banks in the world grew by 6.8% in the 2008–2009 financial year to a record US$96.4 trillion while profits declined by 85% to US$115 billion. Growth in assets in adverse market conditions was largely a result of recapitalisation. EU banks held the largest share of the total, 56% in 2008–2009, down from 61% in the previous year. Asian banks' share increased from 12% to 14% during the year, while the share of US banks increased from 11% to 13%. Fee revenue generated by global investment in banking totalled US$66.3 billion in 2009, up 12% on the previous year.The United States has the most banks in the world in terms of institutions (5,330 as of 2015) and possibly branches (81,607 as of 2015). This is an indicator of the geography and regulatory structure of the US, resulting in a large number of small to medium-sized institutions in its banking system. As of November 2009, China's top four banks have in excess of 67,000 branches (ICBC:18000+, BOC:12000+, CCB:13000+, ABC:24000+) with an additional 140 smaller banks with an undetermined number of branches.
Japan had 129 banks and 12,000 branches. In 2004, Germany, France, and Italy each had more than 30,000 branches – more than double the 15,000 branches in the United Kingdom.

Mergers and acquisitions
Between 1985 and 2018 banks engaged in around 28,798 mergers or acquisitions, either as the acquirer or the target company. The overall known value of these deals cumulates to around 5,169 bil. USD. In terms of value, there have been two major waves (1999 and 2007) which both peaked at around 460 bil. USD followed by a steep decline (-82% from 2007 until 2018).
Here is a list of the largest deals in history in terms of value with participation from at least one bank:

Regulation
Currently, commercial banks are regulated in most jurisdictions by government entities and require a special bank license to operate.

Usually, the definition of the business of banking for the purposes of regulation is extended to include acceptance of deposits, even if they are not repayable to the customer's order – although money lending, by itself, is generally not included in the definition.
Unlike most other regulated industries, the regulator is typically also a participant in the market, being either publicly or privately governed central bank. Central banks also typically have a monopoly on the business of issuing banknotes. However, in some countries, this is not the case. In the UK, for example, the Financial Services Authority licenses banks, and some commercial banks (such as the Bank of Scotland) issue their own banknotes in addition to those issued by the Bank of England, the UK government's central bank.

Banking law is based on a contractual analysis of the relationship between the bank (defined above) and the customer – defined as any entity for which the bank agrees to conduct an account.
The law implies rights and obligations into this relationship as follows:

The bank account balance is the financial position between the bank and the customer: when the account is in credit, the bank owes the balance to the customer; when the account is overdrawn, the customer owes the balance to the bank.

The bank agrees to pay the customer's checks up to the amount standing to the credit of the customer's account, plus any agreed overdraft limit.

The bank may not pay from the customer's account without a mandate from the customer, e.g. a cheque drawn by the customer.

The bank agrees to promptly collect the cheques deposited to the customer's account as the customer's agent and to credit the proceeds to the customer's account.

And, the bank has a right to combine the customer's accounts since each account is just an aspect of the same credit relationship.

The bank has a lien on cheques deposited to the customer's account, to the extent that the customer is indebted to the bank.

The bank must not disclose details of transactions through the customer's account – unless the customer consents, there is a public duty to disclose, the bank's interests require it, or the law demands it.

The bank must not close a customer's account without reasonable notice, since cheques are outstanding in the ordinary course of business for several days.These implied contractual terms may be modified by express agreement between the customer and the bank. The statutes and regulations in force within a particular jurisdiction may also modify the above terms or create new rights, obligations, or limitations relevant to the bank-customer relationship.
Some types of financial institutions, such as building societies and credit unions, may be partly or wholly exempt from bank license requirements, and therefore regulated under separate rules.
The requirements for the issue of a bank license vary between jurisdictions but typically include:

Minimum capital
Minimum capital ratio
'Fit and Proper' requirements for the bank's controllers, owners, directors, or senior officers
Approval of the bank's business plan as being sufficiently prudent and plausible.

Different types of banking
Banks' activities can be divided into:

retail banking, dealing directly with individuals and small businesses;
business banking, providing services to mid-market business;
corporate banking, directed at large business entities;
private banking, providing wealth management services to high-net-worth individuals and families;
investment banking, relating to activities on the financial markets.Most banks are profit-making, private enterprises. However, some are owned by the government, or are non-profit organisations.

Types of banks
Commercial banks: the term used for a normal bank to distinguish it from an investment bank. After the Great Depression, the U.S. Congress required that banks only engage in banking activities, whereas investment banks were limited to capital market activities. Since the two no longer have to be under separate ownership, some use the term "commercial bank" to refer to a bank or a division of a bank that mostly deals with deposits and loans from corporations or large businesses.
Community banks: locally operated financial institutions that empower employees to make local decisions to serve their customers and partners.
Community development banks: regulated banks that provide financial services and credit to under-served markets or populations.
Land development banks: The special banks providing long-term loans are called land development banks (LDB). The history of LDB is quite old. The first LDB was started at Jhang in Punjab in 1920. The main objective of the LDBs is to promote the development of land, agriculture and increase the agricultural production. The LDBs provide long-term finance to members  directly through their branches.
Credit unions or co-operative banks: not-for-profit cooperatives owned by the depositors and often offering rates more favourable than for-profit banks. Typically, membership is restricted to employees of a particular company, residents of a defined area, members of a certain union or religious organisations, and their immediate families.
Postal savings banks: savings banks associated with national postal systems.
Private banks: banks that manage the assets of high-net-worth individuals. Historically a minimum of US$1 million was required to open an account, however, over the last years, many private banks have lowered their entry hurdles to US$350,000 for private investors.
Offshore banks: banks located in jurisdictions with low taxation and regulation. Many offshore banks are essentially private banks.
Savings banks: in Europe, savings banks took their roots in the 19th or sometimes even in the 18th century. Their original objective was to provide easily accessible savings products to all strata of the population. In some countries, savings banks were created on public initiative; in others, socially committed individuals created foundations to put in place the necessary infrastructure. Nowadays, European savings banks have kept their focus on retail banking: payments, savings products, credits, and insurances for individuals or small and medium-sized enterprises. Apart from this retail focus, they also differ from commercial banks by their broadly decentralised distribution network, providing local and regional outreach – and by their socially responsible approach to business and society.
Building societies and Landesbanks: institutions that conduct retail banking.
Ethical banks: banks that prioritize the transparency of all operations and make only what they consider to be socially responsible investments.
A direct or internet-only bank is a banking operation without any physical bank branches. Transactions are usually accomplished using ATMs and electronic transfers and direct deposits through an online interface.

Types of investment banks
Investment banks "underwrite" (guarantee the sale of) stock and bond issues, provide investment management, and advise corporations on capital market activities such as M&A, trade for their own accounts, make markets, provide securities services to institutional clients.
Merchant banks were traditionally banks which engaged in trade finance. The modern definition, however, refers to banks which provide capital to firms in the form of shares rather than loans. Unlike venture caps, they tend not to invest in new companies.

Combination banks
Universal banks, more commonly known as financial services companies, engage in several of these activities. These big banks are very diversified groups that, among other services, also distribute insurance –  hence the term bancassurance, a portmanteau word combining "banque or bank" and "assurance", signifying that both banking and insurance are provided by the same corporate entity.

Other types of banks
Central banks are normally government-owned and charged with quasi-regulatory responsibilities, such as supervising commercial banks, or controlling the cash interest rate. They generally provide liquidity to the banking system and act as the lender of last resort in event of a crisis.
Islamic banks adhere to the concepts of Islamic law. This form of banking revolves around several well-established principles based on Islamic laws. All banking activities must avoid interest, a concept that is forbidden in Islam. Instead, the bank earns profit (markup) and fees on the financing facilities that it extends to customers.

Challenges within the banking industry
United States
The United States banking industry is one of the most heavily regulated and guarded in the world, with multiple specialised and focused regulators. All banks with FDIC-insured deposits have the Federal Deposit Insurance Corporation (FDIC) as a regulator. However, for soundness examinations (i.e., whether a bank is operating in a sound manner), the Federal Reserve is the primary federal regulator for Fed-member state banks; the Office of the Comptroller of the Currency (OCC) is the primary federal regulator for national banks. State non-member banks are examined by the state agencies as well as the FDIC.: 236  National banks have one primary regulator – the OCC.
Each regulatory agency has its own set of rules and regulations to which banks and thrifts must adhere.
The Federal Financial Institutions Examination Council (FFIEC) was established in 1979 as a formal inter-agency body empowered to prescribe uniform principles, standards, and report forms for the federal examination of financial institutions. Although the FFIEC has resulted in a greater degree of regulatory consistency between the agencies, the rules and regulations are constantly changing.
In addition to changing regulations, changes in the industry have led to consolidations within the Federal Reserve, FDIC, OTS, and OCC. Offices have been closed, supervisory regions have been merged, staff levels have been reduced and budgets have been cut. The remaining regulators face an increased burden with an increased workload and more banks per regulator. While banks struggle to keep up with the changes in the regulatory environment, regulators struggle to manage their workload and effectively regulate their banks. The impact of these changes is that banks are receiving less hands-on assessment by the regulators, less time spent with each institution, and the potential for more problems slipping through the cracks, potentially resulting in an overall increase in bank failures across the United States.
The changing economic environment has a significant impact on banks and thrifts as they struggle to effectively manage their interest rate spread in the face of low rates on loans, rate competition for deposits and the general market changes, industry trends and economic fluctuations. It has been a challenge for banks to effectively set their growth strategies with the recent economic market. A rising interest rate environment may seem to help financial institutions, but the effect of the changes on consumers and businesses is not predictable and the challenge remains for banks to grow and effectively manage the spread to generate a return to their shareholders.
The management of the banks' asset portfolios also remains a challenge in today's economic environment. Loans are a bank's primary asset category and when loan quality becomes suspect, the foundation of a bank is shaken to the core. While always an issue for banks, declining asset quality has become a big problem for financial institutions.

There are several reasons for this, one of which is the lax attitude some banks have adopted because of the years of "good times." The potential for this is exacerbated by the reduction in the regulatory oversight of banks and in some cases depth of management. Problems are more likely to go undetected, resulting in a significant impact on the bank when they are discovered. In addition, banks, like any business, struggle to cut costs and have consequently eliminated certain expenses, such as adequate employee training programs.
Banks also face a host of other challenges such as ageing ownership groups. Across the country, many banks' management teams and boards of directors are ageing. Banks also face ongoing pressure from shareholders, both public and private, to achieve earnings and growth projections. Regulators place added pressure on banks to manage the various categories of risk. Banking is also an extremely competitive industry. Competing in the financial services industry has become tougher with the entrance of such players as insurance agencies, credit unions, cheque cashing services, credit card companies, etc.
As a reaction, banks have developed their activities in financial instruments, through financial market operations such as brokerage and have become big players in such activities.
Another major challenge is the ageing infrastructure, also called legacy IT. Backend systems were built decades ago and are incompatible with new applications. Fixing bugs and creating interfaces costs huge sums, as knowledgeable programmers become scarce.

Loan activities of banks
To be able to provide home buyers and builders with the funds needed, banks must compete for deposits. The phenomenon of disintermediation had to dollars moving from savings accounts and into direct market instruments such as U.S. Department of Treasury obligations, agency securities, and corporate debt. One of the greatest factors in recent years in the movement of deposits was the tremendous growth of money market funds whose higher interest rates attracted consumer deposits.To compete for deposits, US savings institutions offer many different types of plans:
Passbook or ordinary deposit accounts  –  permit any amount to be added to or withdrawn from the account at any time.
NOW and Super NOW accounts  –  function like checking accounts but earn interest. A minimum balance may be required on Super NOW accounts.
Money market accounts  –  carry a monthly limit of preauthorised transfers to other accounts or persons and may require a minimum or average balance.
Certificate accounts  –  subject to loss of some or all interest on withdrawals before maturity.
Notice accounts  –  the equivalent of certificate accounts with an indefinite term. Savers agree to notify the institution a specified time before withdrawal.
Individual retirement accounts (IRAs) and Keogh plans  –  a form of retirement savings in which the funds deposited and interest earned are exempt from income tax until after withdrawal.
Checking accounts  –  offered by some institutions under definite restrictions.
All withdrawals and deposits are completely the sole decision and responsibility of the account owner unless the parent or guardian is required to do otherwise for legal reasons.
Club accounts and other savings accounts  –  designed to help people save regularly to meet certain goals.

Types of accounts
Bank statements are accounting records produced by banks under the various accounting standards of the world. Under GAAP there are two kinds of accounts: debit and credit. Credit accounts are Revenue, Equity and Liabilities. Debit Accounts are Assets and Expenses. The bank credits a credit account to increase its balance, and debits a credit account to decrease its balance.The customer debits his or her savings/bank (asset) account in his ledger when making a deposit (and the account is normally in debit), while the customer credits a credit card (liability) account in his ledger every time he spends money (and the account is normally in credit). When the customer reads his bank statement, the statement will show a credit to the account for deposits, and debits for withdrawals of funds. The customer with a positive balance will see this balance reflected as a credit balance on the bank statement. If the customer is overdrawn, he will have a negative balance, reflected as a debit balance on the bank statement.

Brokered deposits
One source of deposits for banks is deposit brokers who deposit large sums of money on behalf of investors through trust corporations. This money will generally go to the banks which offer the most favourable terms, often better than those offered local depositors. It is possible for a bank to engage in business with no local deposits at all, all funds being brokered deposits. Accepting a significant quantity of such deposits, or "hot money" as it is sometimes called, puts a bank in a difficult and sometimes risky position, as the funds must be lent or invested in a way that yields a return sufficient to pay the high interest being paid on the brokered deposits. This may result in risky decisions and even in eventual failure of the bank. Banks which failed during 2008 and 2009 in the United States during the global financial crisis had, on average, four times more brokered deposits as a percent of their deposits than the average bank. Such deposits, combined with risky real estate investments, factored into the savings and loan crisis of the 1980s. Regulation of brokered deposits is opposed by banks on the grounds that the practice can be a source of external funding to growing communities with insufficient local deposits. There are different types of accounts: saving, recurring and current accounts.

Custodial accounts
Custodial accounts are accounts in which assets are held for a third party. For example, businesses that accept custody of funds for clients prior to their conversion, return, or transfer may have a custodial account at a bank for these purposes.

Globalisation in the banking industry
In modern times there have been huge reductions to the barriers of global competition in the banking industry. Increases in telecommunications and other financial technologies, such as Bloomberg, have allowed banks to extend their reach all over the world since they no longer have to be near customers to manage both their finances and their risk. The growth in cross-border activities has also increased the demand for banks that can provide various services across borders to different nationalities. Despite these reductions in barriers and growth in cross-border activities, the banking industry is nowhere near as globalised as some other industries. In the US, for instance, very few banks even worry about the Riegle–Neal Act, which promotes more efficient interstate banking. In the vast majority of nations around the globe, the market share for foreign owned banks is currently less than a tenth of all market shares for banks in a particular nation. One reason the banking industry has not been fully globalised is that it is more convenient to have local banks provide loans to small businesses and individuals. On the other hand, for large corporations, it is not as important in what nation the bank is in since the corporation's financial information is available around the globe.

See also
References
Further reading
Born, Karl Erich. International Banking in the 19th and 20th Centuries (St Martin's, 1983) online

External links

Guardian Datablog – World's Biggest Banks
Banking, Banks, and Credit Unions from UCB Libraries GovPubs (archived 11January 2012)
A Guide to the National Banking System (PDF). Office of the Comptroller of the Currency (OCC), Washington, D.C. Provides an overview of the national banking system of the US, its regulation, and the OCC.

Batch production

Batch production is a method of manufacturing where the products are made as specified groups or amounts, within a time frame. A batch can go through a series of steps in a large manufacturing process to make the final desired product. Batch production is used for many types of manufacturing that may need smaller amounts of production at a time to ensure specific quality standards or changes in the process. This is opposed to large mass production or continuous production methods where the product or process does not need to be checked or changed as frequently or periodically.

Characteristics
In the manufacturing batch production process, the machines are in chronological order directly related to the manufacturing process. The batch production method is also used so any temporary changes or modifications can be made to the product if necessary during the manufacturing process. For example, if a product needed a sudden change in material or details changed, it can be done in between batches. As opposed to assembly production or mass production where such changes cannot be easily made. The time between batches is called cycle time. Each batch may be assigned a lot number.

Advantages
Because batch production involves small batches, it is good for quality control. For example, if there is a mistake in the process, it can be fixed without as much loss compared to mass production. This can also save money by taking less risk for newer plans and products etc. As a result, this allows batch manufacturing to be changed or modified depending on company needs. In certain cases, batch production may require less expensive equipment, thus reducing the capital cost required to set up this type of system.

Disadvantages
There can be downtime between individual batches. Or if the product is constantly changing or being modified throughout the process, this also can cost downtime. Other disadvantages are that smaller batches need more planning, scheduling and control over the process and collecting data. Because of these factors, items made using batch production may have higher unit cost and take more time compared to continuous production.

See also
Lot number


== References ==

Black+Decker

Black+Decker is an American manufacturer of power tools, accessories, hardware, home improvement products, home appliances and fastening systems headquartered in Towson, Maryland, north of Baltimore, where the company was originally established in 1910. In March 2010, Black & Decker merged with Stanley Works to become Stanley Black & Decker. It remains a wholly owned subsidiary of that company.

History
1910–1974
1910 – "The Black & Decker Manufacturing Company" was founded by S. Duncan Black (1883–1951) and Alonzo G. Decker (1884–1956) as a small machine shop in Baltimore in September. Decker, who had only a seventh grade education, had met Black in 1906, when they were both 23-year-old workers at the Rowland Telegraph Company. With only $1,200 between them, one of their first jobs was designing machinery for making milk bottle caps and candy dipping.
1912 – The Black and Decker "Hexagon" logo symbol was introduced, symbolizing the head of a hexagonal bolt found in machine shops. It was used in one form or another from 1912 to 2014.
1917 – Black & Decker invented and patented the hand-held electric drill with a pistol grip and trigger switch.–– For many decades the director of design was Glenn Calvin Wilhide, a friend of Walter Gropius and other industrial designers of the day. Wilhide filed many US patents for Black & Decker.
1917 – The first factory was opened in Towson, Maryland.
1919 – Company reaches $1,000,000 in sales.
1928 – Acquired the Van Dorn Electric Tool Company of Cleveland, Ohio.
1936 – Common stock begins trading on the New York Stock Exchange.
1941 August – Wilhide's patent for a portable power driven tool unit granted.
1943 – Received the Army-Navy "E" Award for production, one of four World War II (1939/1941-1945) citations awarded to the company.
1949 – First Black & Decker U.S. trademark awarded.
1951 – Alonzo G. Decker, Sr. becomes president
1960 – Acquired DeWalt from American Machine and Foundry.

1975–2008
1975 – Francis P. Lucier succeeded son of one of the founders Alonzo G. Decker, Jr.(1908-2002), as chairman of the board, the first time a family member did not hold the post.
1984 – Acquired small-appliance business from General Electric Company.
1986 – Nolan D. Archibald is named chief executive officer.
1989 – Acquired the Emhart Corporation  which includes the brand names Kwikset, Price Pfister faucets, Molly wall anchors, POP rivets, True Temper (both hardware and sports equipment) and other consumer and commercial products. Inducted into the Space Foundation's Space Technology Hall of Fame for its cordless power tool achievements and contributions to NASA's Gemini and Apollo programs.
1990 – True Temper hardware is sold to Huffy, and then sold to US Industries, owner of Ames, which later became Ames True Temper, which is now owned by Griffon Corporation.
1996 – Sold small-appliance business to Windmere Durable Holdings. In May 2000, Windmere Durable Holdings changes its corporate name to Applica Inc.
2000 – Alonzo G. Decker, Jr. resigns from the board, at age 92, two years before his death.

2009–present
2010 – Black & Decker merges with Stanley Works to become Stanley Black & Decker.
2012 – Stanley Black & Decker sells its Hardware and Home Improvement group (HHI) to Spectrum Brands. Sale includes the lock business, as well as the related manufacturing subsidirary in Taiwan (Tong Lung). As of January 2023, Spectrum was in negotiations to sell its HHI assets to Swedish lock manufacturer Assa Abloy, but were making adjustments in response to the United States Department of Justice findings of antitrust issues.
2014 – Rebranded from Black & Decker to Black+Decker
2017 – Stanley Black & Decker purchases Craftsman from Sears (Sears, Roebuck & Company).
2017 – Stovekraft entered a licensing agreement with Black+Decker to sell the latter's products in the Indian market.

Brand portfolio
Recent
As of 2017, Stanley Black & Decker's brand portfolio included:
STANLEY (formerly known as The Stanley Works; started in 1843 as Stanley's Bolt Manufactory by Frederick Trent Stanley, and merged in 1920 with the Stanley Rule and Level Company founded by Henry Stanley in 1857)

Former
Baldwin (sold to Spectrum Brands)
Delta Machinery (sold to Chang Type Industrial)
DeVilbiss Air Power (sold to MAT Holdings)
Kwikset (sold to Spectrum Brands)
Price Pfister (sold to Spectrum Brands)
Weiser Lock (sold to Spectrum Brands)

References
Further reading
Black & Decker, 1989 Competition Commission report

External links

Official website

Business plan

A business plan is a formal written document containing the goals of a business, the methods for attaining those goals, and the time-frame for the achievement of the goals. It also describes the nature of the business, background information on the organization, the organization's financial projections, and the strategies it intends to implement to achieve the stated targets. In its entirety, this document serves as a road-map (a plan) that provides direction to the business.Written business plans are often required to obtain a bank loan or other kind of financing. Templates  and guides, such as the ones offered in the United States by the Small Business Administration can be used to facilitate producing a business plan.

Audience
Business plans may be internally or externally focused. Externally-focused plans draft goals that are important to outside stakeholders, particularly financial stakeholders. These plans typically have detailed information about the organization or the team making effort to reach its goals.  With for-profit entities, external stakeholders include investors and customers, for non-profits, external stakeholders refer to donors and clients, for government agencies, external stakeholders are the tax-payers, higher-level government agencies, and international lending bodies such as the International Monetary Fund, the World Bank, various economic agencies of the United Nations, and development banks.
Internally-focused business plans target intermediate goals required to reach the external goals. They may cover the development of a new product, a new service, a new IT system, a restructuring of finance, the refurbishing of a factory or the restructuring of an organization. An internally-focused business plan is often developed in conjunction with a balanced scorecard or OGSM or a list of critical success factors. This allows the success of the plan to be measured using non-financial measures.
Business plans that identify and target internal goals, but provide only general guidance on how they will be met are called strategic plans.
Operational plans describe the goals of an internal organization, working group or department. Project plans, sometimes known as project frameworks, describe the goals of a particular project.  They may also address the project's place within the organization's larger strategic goals.

Content
Business plans are decision-making tools.  The content and format of the business plan are determined by the goals and audience. For example, a business plan for a non-profit might discuss the fit between the business plan and the organization's mission.  Banks are quite concerned about defaults, so a business plan for a bank loan will build a convincing case for the organization's ability to repay the loan.  Venture capitalists are primarily concerned about initial investment, feasibility, and exit valuation.  A business plan for a project requiring equity financing will need to explain why current resources, upcoming growth opportunities, and sustainable competitive advantage will lead to a high exit valuation.Preparing a business plan draws on a wide range of knowledge from many different business disciplines: finance, human resource management, intellectual property management, supply chain management, operations management, and marketing, among others.   It can be helpful to view the business plan as a collection of sub-plans, one for each of the main business disciplines."... a good business plan can help to make a good business credible, understandable, and attractive to someone who is unfamiliar with the business. Writing a good business plan can't guarantee success, but it can go a long way toward reducing the odds of failure."

Presentation
The format of a business plan depends on its presentation context. It is common for businesses, especially start-ups, to have three or four formats for the same business plan.
An "elevator pitch" is a short summary of the plan's executive summary. This is often used as a teaser to awaken the interest of potential investors, customers, or strategic partners. It is called an elevator pitch as it is supposed to be content that can be explained to someone else quickly in an elevator. The elevator pitch should be between 30 and 60 seconds.A pitch deck is a slide show and oral presentation that is meant to trigger discussion and interest potential investors in reading the written presentation.  The content of the presentation is usually limited to the executive summary and a few key graphs showing financial trends and key decision-making benchmarks. If a new product is being proposed and time permits, a demonstration of the product may be included.A written presentation for external stakeholders is a detailed, well written, and pleasingly formatted plan targeted at external stakeholders.
An internal operational plan is a detailed plan describing planning details that are needed by management but may not be of interest to external stakeholders.  Such plans have a somewhat higher degree of candor and informality than the version targeted at external stakeholders and others.

Business plans for start-ups
Typical structure for a business plan for a start-up venture 
cover page and table of contents
executive summary
mission statement
business description
business environment analysis
SWOT analysis
industry background
competitor analysis
market analysis
marketing plan
operations plan
management summary
financial plan
achievements and milestonesTypical questions addressed by a business plan for a start-up venture 
What problem does the company's product or service solve? What niche will it fill?
What is the company's solution to the problem?
Who are the company's customers, and how will the company market and sell its products to them?
What is the size of the market for this solution?
What is the business model for the business (how will it make money)?
Who are the competitors and how will the company maintain a competitive advantage?
How does the company plan to manage its operations as it grows?
Who will run the company and what makes them qualified to do so?
What are the risks and threats confronting the business, and what can be done to mitigate them?
What are the company's capital and resource requirements?
What are the company's historical and projected financial statements?

Revising the business plan
Cost overruns and revenue shortfalls
Cost and revenue estimates are central to any business plan for deciding the viability of the planned venture. But costs are often underestimated and revenues overestimated resulting in later cost overruns, revenue shortfalls, and possibly non-viability. During the dot-com bubble 1997-2001 this was a problem for many technology start-ups. Reference class forecasting has been developed to reduce the risks of cost overruns and revenue shortfalls and thus generate more accurate business plans.

Legal and liability issues
Disclosure requirements
An externally targeted business plan should list all legal concerns and financial liabilities that might negatively affect investors.  Depending on the number of funds being raised and the audience to whom the plan is presented, failure to do this may have severe legal consequences.

Limitations on content and audience
Non-disclosure agreements (NDAs) with third parties, non-compete agreements, conflicts of interest, privacy concerns, and the protection of one's trade secrets may severely limit the audience to which one might show the business plan.  Alternatively, they may require each party to receive the business plan to sign a contract accepting special clauses and conditions.
This situation is complicated by the fact that many venture capitalists will refuse to sign an NDA before looking at a business plan, lest it put them in the untenable position of looking at two independently developed look-alike business plans, both claiming originality.  In such situations, one may need to develop two versions of the business plan: a stripped-down plan that can be used to develop a relationship and a detailed plan that is only shown when investors have sufficient interest and trust to sign a Non-disclosure agreement.

Open business plans
Traditionally business plans have been highly confidential and quite limited in the audience.
The business plan itself is generally regarded as a secret.
An open business plan is a business plan with an unlimited audience. The business plan is typically
web published and made available to all.
In the free software and open source business model, trade secrets, copyright and patents can no longer
be used as effective locking mechanisms to provide sustainable advantages to a particular business and therefore a secret business plan is less relevant in those models.

Uses
Education
Business plans are used in some primary and secondary programs to teach economic principles.Wikiversity has a Lunar Boom Town project where students of all ages can collaborate with designing and revising business models and practice evaluating them to learn practical business planning techniques and methodologyFundraisingFundraising is the primary purpose of many business plans since they are related to the inherent probable success/failure of the company risk.

Angel investors
Business loans
Grants
Startup company funding
Venture capitalInternal useManagement by objectives (MBO) is a process of agreeing upon objectives (as can be detailed within business plans) within an organization so that management and employees agree to the objectives and understand what they are in the organization.
Strategic planning is an organization's process of defining its strategy, or direction, and making decisions on allocating its resources to pursue this strategy, including its capital and people. Business plans can help decision-makers see how specific projects relate to the organization's strategic plan.
Total quality management (TQM) is a business management strategy aimed at embedding awareness of quality in all organizational processes. TQM has been widely used in manufacturing, education, call centers, government, and service industries, as well as NASA space and science programs.

Not for-profit businesses
The business goals may be defined both for non-profit  or for-profit organizations.  For-profit business plans typically focus on financial goals, such as profit or creation of wealth.  Non-profit, as well as government agency business plans tend to focus on the "organizational mission" which is the basis for their governmental status or their non-profit, tax-exempt status, respectively—although non-profits may also focus on optimizing revenue.
The primary difference between profit and non-profit organizations is that "for-profit" organizations look to maximize wealth versus non-profit organizations, which look to provide a greater good to society. In non-profit organizations, creative tensions may develop in the effort to balance mission with "margin" (or revenue).

Satires
The business plan is the subject of many satires.  Satires are used both to express cynicism about business plans and as an educational tool to improve the quality of business plans.  For example,

In his presentation, Five Criteria For a Successful Business Plan in Biotech, Dr. Roger Bernier, uses Dilbert comic strips to remind people what not to do when researching and writing a business plan for a biotech start-up.
The "Gnomes" episode satirizes the business plans of the Dot-com era.
Chapter 26 of Neal Stephenson's 1999 novel Cryptonomicon begins with the business plan of a fictional high tech company, satirizing both the writing style and the physical form of slickly produced business publications like business plans and annual reports.

See also


== References ==

CONWIP

CONWIP (CONstant work in process) are pull-oriented production control systems. Such systems can be classified as pull and push systems (Spearman et al. 1990). In a push system, the production order is scheduled, and the material is pushed into the production line. In a pull system, the start of each product assembly process is triggered by the completion of another at the end of production line. This pull-variant is known for its ease of implementation.
CONWIP is a kind of single-stage kanban system and is also a hybrid push-pull system. While kanban systems maintain tighter control of system WIP through the individual cards at each workstation, CONWIP systems are easier to implement and adjust, since only one set of system cards is used to manage system WIP. CONWIP uses cards to control the number of WIPs. For example, no part is allowed to enter the system without a card (authority). After a finished part is completed at the last workstation, a card is transferred to the first workstation and a new part is pushed into the sequential process route. In their paper, Spearman et al. (1990) used a simulation to make a comparison among the CONWIP, kanban and push systems, and found that CONWIP systems can achieve a lower WIP level than kanban systems.

Card control policy
In a CONWIP system, a card is shared by all kinds of products.  However, Duenyas (1994) proposed a dedicated card control policy in CONWIP and he stated that this policy could perform as a multiple chain closed queuing network.

See also
Factory Physics
Material requirements planning (MRP)
Just-in-time manufacturing (JIT)
Kanban

References
Further reading
Spearman, Mark L. and Michael A Zazanis, "Push and Pull Production Systems: Issues and Comparisons", Op. Res., Vol40(3), pp 521–532, May–June 1992.

COVID-19 pandemic

The COVID-19 pandemic, also known as the coronavirus pandemic, is a global pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The novel virus was first identified in an outbreak in the Chinese city of Wuhan in December 2019. Attempts to contain it there failed, allowing the virus to spread to other areas of Asia and then worldwide in early 2020. The World Health Organization (WHO) declared the outbreak a public health emergency of international concern (PHEIC) on 30 January 2020. The WHO ended its PHEIC declaration on 5 May 2023. As of 14 November 2023, the pandemic had caused 771,820,173 cases and 6,978,162 confirmed deaths, ranking it fifth in the deadliest epidemics and pandemics in history.
COVID-19 symptoms range from asymptomatic to deadly, but most commonly include fever, nocturnal cough, and fatigue. Transmission of the virus is often through airborne particles. Mutations have produced many strains (variants) with varying degrees of infectivity and virulence.COVID-19 vaccines were widely deployed in various countries beginning in December 2020. Treatments include novel antiviral drugs and symptom control. Common mitigation measures during the public health emergency included travel restrictions, lockdowns, business restrictions and closures, workplace hazard controls, mask mandates, quarantines, testing systems, and contact tracing of the infected.
The pandemic caused severe social and economic disruption around the world, including the largest global recession since the Great Depression. Widespread supply shortages, including food shortages, were caused by supply chain disruptions and panic buying. Reduced human activity led to an unprecedented decrease in pollution. Educational institutions and public areas were partially or fully closed in many jurisdictions, and events were cancelled or postponed during 2020 and 2021. Many white-collar workers began working from home. Misinformation circulated through social media and mass media, and political tensions intensified. The pandemic raised issues of racial and geographic discrimination, health equity, and the balance between public health imperatives and individual rights.

Terminology
In epidemiology, a pandemic is defined as "an epidemic occurring over a very wide area, crossing international boundaries, and usually affecting a large number of people". During the COVID-19 pandemic, as with other pandemics, the meaning of this term has been challenged.During the initial outbreak in Wuhan, the virus and disease were commonly referred to as "coronavirus", "Wuhan coronavirus", "the coronavirus outbreak" and the "Wuhan coronavirus outbreak", with the disease sometimes called "Wuhan pneumonia". In January 2020, the WHO recommended 2019-nCoV and 2019-nCoV acute respiratory disease as interim names for the virus and disease per 2015 international guidelines against using geographical locations (e.g. Wuhan, China), animal species, or groups of people in disease and virus names in part to prevent social stigma. WHO finalized the official names COVID-19 and SARS-CoV-2 on 11 February 2020. Tedros Adhanom Ghebreyesus explained: CO for corona, VI for virus, D for disease and 19 for when the outbreak was first identified (31 December 2019). WHO additionally uses "the COVID-19 virus" and "the virus responsible for COVID-19" in public communications.WHO named variants of concern and variants of interest using Greek letters. The initial practice of naming them according to where the variants were identified (e.g. Delta began as the "Indian variant") is no longer common. A more systematic naming scheme reflects the variant's PANGO lineage (e.g., Omicron's lineage is B.1.1.529) and is used for other variants.

Epidemiology
Background
SARS-CoV-2 is a virus closely related to bat coronaviruses, pangolin coronaviruses, and SARS-CoV. The first known outbreak (the 2019–2020 COVID-19 outbreak in mainland China) started in Wuhan, Hubei, China, in December 2019. Many early cases were linked to people who had visited the Huanan Seafood Wholesale Market there, but it is possible that human-to-human transmission began earlier.The scientific consensus is that the virus is most likely of a zoonotic origin, from bats or another closely related mammal. Controversies about the origins of the virus, including the lab leak theory, heightened geopolitical divisions, notably between the United States and China.The earliest known infected person fell ill on 1 December 2019. That individual did not have a connection with the later wet market cluster. However, an earlier case may have occurred on 17 November. Two-thirds of the initial case cluster were linked with the market. Molecular clock analysis suggests that the index case is likely to have been infected between mid-October and mid-November 2019.

Cases
Official "case" counts refer to the number of people who have been tested for COVID-19 and whose test has been confirmed positive according to official protocols whether or not they experienced symptomatic disease. Due to the effect of sampling bias, studies which obtain a more accurate number by extrapolating from a random sample have consistently found that total infections considerably exceed the reported case counts. Many countries, early on, had official policies to not test those with only mild symptoms. The strongest risk factors for severe illness are obesity, complications of diabetes, anxiety disorders, and the total number of conditions.During the start of the COVID-19 pandemic it was not clear whether young people were less likely to be infected, or less likely to develop symptoms and be tested. A retrospective cohort study in China found that children and adults were just as likely to be infected.Among more thorough studies, preliminary results from 9 April 2020, found that in Gangelt, the centre of a major infection cluster in Germany, 15 percent of a population sample tested positive for antibodies. Screening for COVID-19 in pregnant women in New York City, and blood donors in the Netherlands, found rates of positive antibody tests that indicated more infections than reported. Seroprevalence-based estimates are conservative as some studies show that persons with mild symptoms do not have detectable antibodies.Initial estimates of the basic reproduction number (R0) for COVID-19 in January 2020 were between 1.4 and 2.5, but a subsequent analysis claimed that it may be about 5.7 (with a 95 percent confidence interval of 3.8 to 8.9).In December 2021, the number of cases continued to climb due to several factors, including new COVID-19 variants. As of that 28 December, 282,790,822 individuals worldwide had been confirmed as infected. As of 14 April 2022, over 500 million cases were confirmed globally. Most cases are unconfirmed, with the Institute for Health Metrics and Evaluation estimating the true number of cases as of early 2022 to be in the billions.

Deaths
As of 10 March 2023, more than 6.88 million deaths had been attributed to COVID-19. The first confirmed death was in Wuhan on 9 January 2020. These numbers vary by region and over time, influenced by testing volume, healthcare system quality, treatment options, government response, time since the initial outbreak, and population characteristics, such as age, sex, and overall health.Multiple measures are used to quantify mortality. Official death counts typically include people who died after testing positive. Such counts exclude deaths without a test. Conversely, deaths of people who died from underlying conditions following a positive test may be included. Countries such as Belgium include deaths from suspected cases, including those without a test, thereby increasing counts.Official death counts have been claimed to underreport the actual death toll, because excess mortality (the number of deaths in a period compared to a long-term average) data show an increase in deaths that is not explained by COVID-19 deaths alone. Using such data, estimates of the true number of deaths from COVID-19 worldwide have included a range from 16.5 to 26.8 million (≈20.2 million) by 3 February 2023 by The Economist, as well as over 18.5 million by 1 April 2023 by the Institute for Health Metrics and Evaluation and ≈18.2 million (earlier) deaths between 1 January 2020, and 31 December 2021, by a comprehensive international study. Such deaths include deaths due to healthcare capacity constraints and priorities, as well as reluctance to seek care (to avoid possible infection). Further research may help distinguish the proportions directly caused by COVID-19 from those caused by indirect consequences of the pandemic.In May 2022, the WHO estimated the number of excess deaths by the end of 2021 to be 14.9 million compared to 5.4 million reported COVID-19 deaths, with the majority of the unreported 9.5 million deaths believed to be direct deaths due the virus, rather than indirect deaths. Some deaths were because people with other conditions could not access medical services.A December 2022 WHO study estimated excess deaths from the pandemic during 2020 and 2021, again concluding ≈14.8 million excess early deaths occurred, reaffirming and detailing their prior calculations from May as well as updating them, addressing criticisms. These numbers do not include measures like years of potential life lost and may make the pandemic 2021's leading cause of death.The time between symptom onset and death ranges from 6 to 41 days, typically about 14 days. Mortality rates increase as a function of age. People at the greatest mortality risk are the elderly and those with underlying conditions.

Infection fatality ratio (IFR)
The infection fatality ratio (IFR) is the cumulative number of deaths attributed to the disease divided by the cumulative number of infected individuals (including asymptomatic and undiagnosed infections and excluding vaccinated infected individuals). It is expressed in percentage points. Other studies refer to this metric as the infection fatality risk.In November 2020, a review article in Nature reported estimates of population-weighted IFRs for various countries, excluding deaths in elderly care facilities, and found a median range of 0.24% to 1.49%. IFRs rise as a function of age (from 0.002% at age 10 and 0.01% at age 25, to 0.4% at age 55, 1.4% at age 65, 4.6% at age 75, and 15% at age 85). These rates vary by a factor of ≈10,000 across the age groups. For comparison, the IFR for middle-aged adults is two orders of magnitude higher than the annualised risk of a fatal automobile accident and much higher than the risk of dying from seasonal influenza.In December 2020, a systematic review and meta-analysis estimated that population-weighted IFR was 0.5% to 1% in some countries (France, Netherlands, New Zealand, and Portugal), 1% to 2% in other countries (Australia, England, Lithuania, and Spain), and about 2.5% in Italy. This study reported that most of the differences reflected corresponding differences in the population's age structure and the age-specific pattern of infections. There have also been reviews that have compared the fatality rate of this pandemic with prior pandemics, such as MERS-CoV.For comparison the infection mortality rate of seasonal flu in the United States is 0.1%, which is 13 times lower than COVID-19.

Case fatality ratio (CFR)
Another metric in assessing death rate is the case fatality ratio (CFR), which is the ratio of deaths to diagnoses. This metric can be misleading because of the delay between symptom onset and death and because testing focuses on symptomatic individuals.Based on Johns Hopkins University statistics, the global CFR is 1.02 percent (6,881,955 deaths for 676,609,955 cases) as of 10 March 2023. The number varies by region and has generally declined over time.

Disease
Variants
Several variants have been named by WHO and labelled as a variant of concern (VoC) or a variant of interest (VoI). Many of these variants have shared the more infectious D614G. As of May 2023, the WHO had downgraded all variants of concern to previously circulating as these were no longer detected in new infections. Sub-lineages of the Omicron variant (BA.1 - BA.5) were considered separate VoCs by the WHO until they were downgraded in March 2023 as no longer widely circulating.

Signs and symptoms
Symptoms of COVID-19 are variable, ranging from mild symptoms to severe illness. Common symptoms include headache, loss of smell and taste, nasal congestion and runny nose, cough, muscle pain, sore throat, fever, diarrhoea, and breathing difficulties. People with the same infection may have different symptoms, and their symptoms may change over time. Three common clusters of symptoms have been identified: one respiratory symptom cluster with cough, sputum, shortness of breath, and fever; a musculoskeletal symptom cluster with muscle and joint pain, headache, and fatigue; a cluster of digestive symptoms with abdominal pain, vomiting, and diarrhoea. In people without prior ear, nose, and throat disorders, loss of taste combined with loss of smell is associated with COVID-19 and is reported in as many as 88% of cases.

Transmission
The disease is mainly transmitted via the respiratory route when people inhale droplets and small airborne particles (that form an aerosol) that infected people exhale as they breathe, talk, cough, sneeze, or sing. Infected people are more likely to transmit COVID-19 when they are physically close to other non-infected individuals. However, infection can occur over longer distances, particularly indoors.

Cause
SARS‑CoV‑2 belongs to the broad family of viruses known as coronaviruses. It is a positive-sense single-stranded RNA (+ssRNA) virus, with a single linear RNA segment. Coronaviruses infect humans, other mammals, including livestock and companion animals, and avian species.Human coronaviruses are capable of causing illnesses ranging from the common cold to more severe diseases such as Middle East respiratory syndrome (MERS, fatality rate ≈34%). SARS-CoV-2 is the seventh known coronavirus to infect people, after 229E, NL63, OC43, HKU1, MERS-CoV, and the original SARS-CoV.

Diagnosis
The standard method of testing for presence of SARS-CoV-2 is a nucleic acid test, which detects the presence of viral RNA fragments. As these tests detect RNA but not infectious virus, its "ability to determine duration of infectivity of patients is limited." The test is typically done on respiratory samples obtained by a nasopharyngeal swab; however, a nasal swab or sputum sample may also be used. The WHO has published several testing protocols for the disease.

Prevention
Preventive measures to reduce the chances of infection include getting vaccinated, staying at home or spending more time outdoors, avoiding crowded places, keeping distance from others, wearing a mask in public, ventilating indoor spaces, managing potential exposure durations, washing hands with soap and water often and for at least twenty seconds, practicing good respiratory hygiene, and avoiding touching the eyes, nose, or mouth with unwashed hands.Those diagnosed with COVID-19 or who believe they may be infected are advised by healthcare authorities to stay home except to get medical care, call ahead before visiting a healthcare provider, wear a face mask before entering the healthcare provider's office and when in any room or vehicle with another person, cover coughs and sneezes with a tissue, regularly wash hands with soap and water and avoid sharing personal household items.

Vaccines
A COVID-19 vaccine is intended to provide acquired immunity against severe acute respiratory syndrome coronavirus 2 (SARS‑CoV‑2), the virus that causes coronavirus disease 2019 (COVID-19). Prior to the COVID-19 pandemic, an established body of knowledge existed about the structure and function of coronaviruses causing diseases like severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS). This knowledge accelerated the development of various vaccine platforms during early 2020. The initial focus of SARS-CoV-2 vaccines was on preventing symptomatic and severe illness. The COVID-19 vaccines are widely credited for their role in reducing the severity and death caused by COVID-19.As of March 2023, more than 5.5 billion people had received one or more doses (11.8 billion in total) in over 197 countries. The Oxford-AstraZeneca vaccine was the most widely used. According to a June 2022 study, COVID-19 vaccines prevented an additional 14.4 million to 19.8 million deaths in 185 countries and territories from 8 December 2020 to 8 December 2021.On 8 November 2022, the first recombinant protein-based COVID-19 vaccine (Novavax's booster Nuvaxovid) was authorized for use in adults in the United Kingdom. It has subsequently received endorsement/authorization from the WHO, US, European Union, and Australia.On 12 November 2022, the WHO released its Global Vaccine Market Report. The report indicated that "inequitable distribution is not unique to COVID-19 vaccines"; countries that are not economically strong struggle to obtain vaccines.On 14 November 2022, the first inhalable vaccine was introduced, developed by Chinese biopharmaceutical company CanSino Biologics, in the city of Shanghai, China.

Treatment
For the first two years of the pandemic, no specific and effective treatment or cure was available. In 2021, the European Medicines Agency's (EMA) Committee for Medicinal Products for Human Use (CHMP) approved the oral antiviral protease inhibitor, Paxlovid (nirmatrelvir plus AIDS drug ritonavir), to treat adult patients. FDA later gave it an EUA.Most cases of COVID-19 are mild. In these, supportive care includes medication such as paracetamol or NSAIDs to relieve symptoms (fever, body aches, cough), adequate intake of oral fluids and rest. Good personal hygiene and a healthy diet are also recommended.Supportive care in severe cases includes treatment to relieve symptoms, fluid therapy, oxygen support and prone positioning, and medications or devices to support other affected vital organs. More severe cases may need treatment in hospital. In those with low oxygen levels, use of the glucocorticoid dexamethasone is recommended to reduce mortality. Noninvasive ventilation and, ultimately, admission to an intensive care unit for mechanical ventilation may be required to support breathing. Extracorporeal membrane oxygenation (ECMO) has been used to address the issue of respiratory failure.Existing drugs such as hydroxychloroquine, lopinavir/ritonavir, and ivermectin are not recommended by US or European health authorities, as there is no good evidence they have any useful effect. The antiviral remdesivir is available in the US, Canada, Australia, and several other countries, with varying restrictions; however, it is not recommended for use with mechanical ventilation, and is discouraged altogether by the World Health Organization (WHO), due to limited evidence of its efficacy.

Prognosis
The severity of COVID-19 varies. It may take a mild course with few or no symptoms, resembling other common upper respiratory diseases such as the common cold. In 3–4% of cases (7.4% for those over age 65) symptoms are severe enough to cause hospitalization. Mild cases typically recover within two weeks, while those with severe or critical diseases may take three to six weeks to recover. Among those who have died, the time from symptom onset to death has ranged from two to eight weeks. Prolonged prothrombin time and elevated C-reactive protein levels on admission to the hospital are associated with severe course of COVID-19 and with a transfer to intensive care units (ICU).Between 5% and 50% of COVID-19 patients experience long COVID, a condition characterized by long-term consequences persisting after the typical convalescence period of the disease. The most commonly reported clinical presentations are fatigue and memory problems, as well as malaise, headaches, shortness of breath, loss of smell, muscle weakness, low fever and cognitive dysfunction.

Strategies
Many countries attempted to slow or stop the spread of COVID-19 by recommending, mandating or prohibiting behaviour changes, while others relied primarily on providing information. Measures ranged from public advisories to stringent lockdowns. Outbreak control strategies are divided into elimination and mitigation. Experts differentiate between elimination strategies (known as "zero-COVID") that aim to completely stop the spread of the virus within the community, and mitigation strategies (commonly known as "flattening the curve") that attempt to lessen the effects of the virus on society, but which still tolerate some level of transmission within the community. These initial strategies can be pursued sequentially or simultaneously during the acquired immunity phase through natural and vaccine-induced immunity.Nature reported in 2021 that 90 percent of researchers who responded to a survey "think that the coronavirus will become endemic".

Containment
Containment is undertaken to stop an outbreak from spreading into the general population. Infected individuals are isolated while they are infectious. The people they have interacted with are contacted and isolated for long enough to ensure that they are either not infected or no longer contagious. Screening is the starting point for containment. Screening is done by checking for symptoms to identify infected individuals, who can then be isolated or offered treatment. The Zero-COVID strategy involves using public health measures such as contact tracing, mass testing, border quarantine, lockdowns and mitigation software to stop community transmission of COVID-19 as soon as it is detected, with the goal of getting the area back to zero detected infections and resuming normal economic and social activities. Successful containment or suppression reduces Rt to less than 1.

Mitigation
Should containment fail, efforts focus on mitigation: measures taken to slow the spread and limit its effects on the healthcare system and society.
Successful mitigation delays and decreases the epidemic peak, known as "flattening the epidemic curve". This decreases the risk of overwhelming health services and provides more time for developing vaccines and treatments. Individual behaviour changed in many jurisdictions. Many people worked from home instead of at their traditional workplaces.

Non-pharmaceutical interventions
Non-pharmaceutical interventions that may reduce spread include personal actions such as wearing face masks, self-quarantine, and hand hygiene; community measures aimed at reducing interpersonal contacts such as closing workplaces and schools and cancelling large gatherings; community engagement to encourage acceptance and participation in such interventions; as well as environmental measures such as surface cleaning.

Other measures
More drastic actions, such as quarantining entire populations and strict travel bans have been attempted in various jurisdictions. The Chinese and Australian government approaches have included many lockdowns and are widely considered the most strict. The New Zealand government response included the most severe travel restrictions. As part of its K-Quarantine program, South Korea introduced mass screening and localised quarantines, and issued alerts on the movements of infected individuals. The Singaporean government's response included so-called "circuit breaker lockdowns" and financial support for those affected while also imposing large fines for those who broke quarantine.

Contact tracing
Contact tracing attempts to identify recent contacts of newly infected individuals, and to screen them for infection; the traditional approach is to request a list of contacts from infectees, and then telephone or visit the contacts. Contact tracing was widely used during the Western African Ebola virus epidemic in 2014.Another approach is to collect location data from mobile devices to identify those who have come in significant contact with infectees, which prompted privacy concerns. On 10 April 2020, Google and Apple announced an initiative for privacy-preserving contact tracing. In Europe and in the US, Palantir Technologies initially provided COVID-19 tracking services.

Health care
WHO described increasing capacity and adapting healthcare as a fundamental mitigation. The ECDC and WHO's European regional office issued guidelines for hospitals and primary healthcare services for shifting resources at multiple levels, including focusing laboratory services towards testing, cancelling elective procedures, separating and isolating patients, and increasing intensive care capabilities by training personnel and increasing ventilators and beds. The pandemic drove widespread adoption of telehealth.

Improvised manufacturing
Due to supply chain capacity limitations, some manufacturers began 3D printing material such as nasal swabs and ventilator parts. In one example, an Italian startup received legal threats due to alleged patent infringement after reverse-engineering and printing one hundred requested ventilator valves overnight. Individuals and groups of makers created and shared open source designs, and manufacturing devices using locally sourced materials, sewing, and 3D printing. Millions of face shields, protective gowns, and masks were made. Other ad hoc medical supplies included shoe covers, surgical caps, powered air-purifying respirators, and hand sanitizer. Novel devices were created such as ear savers, non-invasive ventilation helmets, and ventilator splitters.

Herd immunity
In July 2021, several experts expressed concern that achieving herd immunity may not be possible because Delta can transmit among vaccinated individuals. CDC published data showing that vaccinated people could transmit Delta, something officials believed was less likely with other variants. Consequently, WHO and CDC encouraged vaccinated people to continue with non-pharmaceutical interventions such as masking, social distancing, and quarantining if exposed.

History
2019
The outbreak was discovered in Wuhan in November 2019. It is possible that human-to-human transmission was happening before the discovery. Based on a retrospective analysis starting from December 2019, the number of cases in Hubei gradually increased, reaching 60 by 20 December and at least 266 by 31 December.A pneumonia cluster was observed on 26 December and treated by Chinese pulmonologist Zhang Jixian. She informed the Wuhan Jianghan CDC on 27 December. After analyzing pneumonia patient samples, a genetic sequencing company named Vision Medicals reported the discovery of a novel coronavirus to the China CDC (CCDC) on 28 December.On 30 December, a test report from CapitalBio Medlab addressed to Wuhan Central Hospital reported an erroneous positive result for SARS, causing doctors there to alert authorities. Eight of those doctors, including Li Wenliang (who was also punished on 3 January), were later admonished by the police for spreading false rumours. Director of the Emergency Department at the Central Hospital of Wuhan, Ai Fen, was also reprimanded. That evening, Wuhan Municipal Health Commission (WMHC) issued a notice about "the treatment of pneumonia of unknown cause". The next day, WMHC made the announcement public, confirming 27 cases—enough to trigger an investigation.On 31 December, the WHO office in China was notified about the cluster of unknown pneumonia cases and immediately launched an investigation.Official Chinese sources claimed that the early cases were mostly linked to the Huanan Seafood Wholesale Market, which also sold live animals. In May 2020, CCDC director George Gao initially ruled out the market as a possible origin, as animal samples collected there had tested negative.

2020
On 11 January, WHO was notified by the Chinese National Health Commission that the outbreak was associated with exposures in the market, and that China had identified a new type of coronavirus, which it isolated on 7 January.Initially, the number of cases doubled approximately every seven and a half days. In early and mid-January, the virus spread to other Chinese provinces, helped by the Chinese New Year migration. Wuhan was a transport hub and major rail interchange. On 10 January, the virus' genome was shared publicly. A retrospective study published in March found that 6,174 people had reported symptoms by 20 January. A 24 January report indicated human transmission was likely occurring, and recommended personal protective equipment for health workers. It also advocated testing, given the outbreak's "pandemic potential". On 31 January, the first published modelling study warned of inevitable "independent self-sustaining outbreaks in major cities globally" and called for "large-scale public health interventions."On 30 January, 7,818 infections had been confirmed, leading WHO to declare the outbreak a Public Health Emergency of International Concern (PHEIC). On 11 March, WHO announced its assessment that the situation could be characterized as a pandemic.By 31 January, Italy indicated its first confirmed infections had occurred, in two tourists from China. On 19 March, Italy overtook China as the country with the most reported deaths. By 26 March, the United States had overtaken China and Italy as the country with the highest number of confirmed infections. Genomic analysis indicated that the majority of New York's confirmed infections came from Europe, rather than directly from Asia. Testing of prior samples revealed a person who was infected in France on 27 December 2019 and a person in the United States who died from the disease on 6 February.
In October, WHO reported that one in ten people around the world may have been infected, or 780 million people, while only 35 million infections had been confirmed.On 9 November, Pfizer released trial results for a candidate vaccine, showing a 90 percent effectiveness in preventing infection. That day, Novavax submitted an FDA Fast Track application for their vaccine.On 14 December, Public Health England reported that a variant had been discovered in the UK's southeast, predominantly in Kent. The variant, later named Alpha, showed changes to the spike protein that could make the virus more infectious. As of 13 December, 1,108 infections had been confirmed in the UK.On 4 February 2020, US Secretary of Health and Human Services Alex Azar waived liability for vaccine manufacturers in all cases except those involving "willful misconduct".

2021
On 2 January, the Alpha variant, first discovered in the UK, had been identified in 33 countries. On 6 January, the Gamma variant was first identified in Japanese travellers returning from Brazil. On 29 January, it was reported that the Novavax vaccine was 49 percent effective against the Beta variant in a clinical trial in South Africa. The CoronaVac vaccine was reported to be 50.4 percent effective in a Brazil clinical trial.
On 12 March, several countries stopped using the Oxford-AstraZeneca COVID-19 vaccine- due to blood clotting problems, specifically cerebral venous sinus thrombosis (CVST). On 20 March, the WHO and European Medicines Agency found no link to thrombosis, leading several countries to resume administering the vaccine. In March WHO reported that an animal host was the most likely origin, without ruling out other possibilities. The Delta variant was first identified in India. In mid-April, the variant was first detected in the UK and two months later it had become a full-fledged third wave in the country, forcing the government to delay reopening that was originally scheduled for June.
On 10 November, Germany advised against the Moderna vaccine for people under 30, due to a possible association with myocarditis. On 24 November, the Omicron variant was detected in South Africa; a few days later the World Health Organization declared it a VoC (variant of concern). The new variant is more infectious than the Delta variant.

2022
On 1 January, Europe passed 100 million cases amidst a surge in the Omicron variant. Later that month, the WHO recommended the rheumatoid arthritis drug Baricitinib for severe or critical patients. It also recommended the monoclonal antibody Sotrovimab in patients with non-severe disease, but only those who are at highest risk of hospitalization.On 24 January, the Institute for Health Metrics and Evaluation estimated that about 57% of the world's population had been infected by COVID-19. By 6 March, it was reported that the total worldwide death count had surpassed 6 million people. By 6 July, Omicron subvariants BA.4 and BA.5 had spread worldwide. WHO Director-General Tedros Ghebreyesus stated on 14 September 2022, that "[The world has] never been in a better position to end the pandemic", citing the lowest number of weekly reported deaths since March 2020. He continued, "We are not there yet. But the end is in sight—we can see the finish line".On 21 October, the United States surpassed 99 million cases of COVID-19, the most cases of any country. By 30 October, the worldwide daily death toll was 424, the lowest since 385 deaths were reported on 12 March 2020. 17 November marked the three-year anniversary since health officials in China first detected COVID-19.On 11 November, the WHO reported that deaths since the month of February had dropped 90 percent. Director-General Tedros said this was "cause for optimism". On 3 December, the WHO indicated that, "at least 90% of the world's population has some level of immunity to Sars-CoV-2". In early December, China began lifting some of its most stringent lockdown measures. Subsequent data from China's health authorities revealed that 248 million people, nearly 18 percent of its population, had been infected in the first 20 days of that month. On 29 December, the US joined Italy, Japan, Taiwan and India in requiring negative COVID-19 test results from all people traveling from China due to the new surge in cases. The EU refused similar measures, stating that the BF7 omicron variant had already spread throughout Europe without becoming dominant.

2023
On 4 January 2023, the World Health Organization said the information shared by China during the recent surge in infections lacked data, such as hospitalization rates. On 10 January, the WHO's Europe office said the recent viral surge in China posed "no immediate threat." On 16 January, the WHO recommended that China monitor excess mortality to provide "a more comprehensive understanding of the impact of COVID-19."On 30 January, the three-year anniversary of the original declaration, the World Health Organization determined that COVID-19 still met the criteria for a public health emergency of international concern (PHEIC).On 19 March, WHO Director-General Tedros indicated he was "confident" the COVID-19 pandemic would cease to be a public health emergency by the end of the year. On 5 May, the WHO downgraded COVID-19 from being a global health emergency, though it continued to refer to it as a pandemic. The WHO does not make official declarations of when pandemics end. The decision came after Tedros convened with the International Health Regulations Emergency Committee, wherein the Committee noted that due to the decrease in deaths and hospitalisations, and the prevalence of vaccinations and the level of general immunity, it was time to remove the emergency designation and "transition to long-term management". Tedros agreed, and the WHO reduced the classification to an "established and ongoing health issue". In a press conference, Tedros remarked that the diminishing threat from COVID-19 had "allowed most countries to return to life as we knew it before COVID-19".In September the WHO said it had observed "concerning" trends in COVID-19 case numbers and hospitalisations, although analysis was hampered because many countries were no longer recording COVID-19 case statistics.In the United Kingdom in October, following a steep rise in COVID-19 cases, healthcare facilities began reinstating face-masking policies for visitors and patients.

Responses
National reactions ranged from strict lockdowns to public education campaigns. WHO recommended that curfews and lockdowns should be short-term measures to reorganise, regroup, rebalance resources, and protect the health care system.
As of 26 March 2020, 1.7 billion people worldwide were under some form of lockdown. This increased to 3.9 billion people by the first week of April—more than half the world's population.In several countries, protests rose against restrictions such as lockdowns. A February 2021 study found that protests against restrictions were likely to directly increase the spread of the virus.

Asia
As of the end of 2021, Asia's peak had come at the same time and at the same level as the world as a whole, in May 2021. However, cumulatively they had experienced only half of the global average in cases.
China opted for containment, instituting strict lockdowns to eliminate viral spread.
The vaccines distributed in China included the BIBP, WIBP, and CoronaVac. It was reported on 11 December 2021, that China had vaccinated 1.162 billion of its citizens, or 82.5% of the total population of the country against COVID-19. China's large-scale adoption of zero-COVID had largely contained the first waves of infections of the disease. When the waves of infections due to the Omicron variant followed, China was almost alone in pursuing the strategy of zero-Covid to combat the spread of the virus in 2022. Lockdown continued to be employed in November to combat a new wave of cases; however, protests erupted in cities across China over the country's stringent measures, and in December that year, the country relaxed its zero-COVID policy. On 20 December 2022, the Chinese State Council narrowed its definition of what would be counted as a COVID-19 death to include solely respiratory failure, which led to skepticism by health experts of the government's total death count at a time when hospitals reported being overwhelmed with cases following the abrupt discontinuation of zero-COVID.The first case in India was reported on 30 January 2020. India ordered a nationwide lockdown starting 24 March 2020, with a phased unlock beginning 1 June 2020. Six cities accounted for around half of reported cases—Mumbai, Delhi, Ahmedabad, Chennai, Pune and Kolkata. Post-lockdown, the Government of India introduced a contact tracking app called Aarogya Setu to help authorities manage contact tracing and vaccine distribution. India's vaccination program was considered to be the world's largest and most successful with over 90% of citizens getting the first dose and another 65% getting the second dose. A second wave hit India in April 2021, straining healthcare services. On 21 October 2021, it was reported that the country had surpassed 1 billion vaccinations.
Iran reported its first confirmed cases on 19 February 2020, in Qom. Early measures included the cancellation/closure of concerts and other cultural events, Friday prayers, and school and university campuses. Iran became a centre of the pandemic in February 2020. More than ten countries had traced their outbreaks to Iran by 28 February, indicating a more severe outbreak than the 388 reported cases. The Iranian Parliament closed, after 23 of its 290 members tested positive on 3 March 2020. At least twelve sitting or former Iranian politicians and government officials had died by 17 March 2020. By August 2021, the pandemic's fifth wave peaked, with more than 400 deaths in 1 day.COVID-19 was confirmed in South Korea on 20 January 2020. Military bases were quarantined after tests showed three infected soldiers. South Korea introduced what was then considered the world's largest and best-organised screening programme, isolating infected people, and tracing and quarantining contacts. Screening methods included mandatory self-reporting by new international arrivals through mobile application, combined with drive-through testing, and increasing testing capability to 20,000 people/day. Despite some early criticisms, South Korea's programme was considered a success in controlling the outbreak without quarantining entire cities.

Europe
The COVID-19 pandemic arrived in Europe with its first confirmed case in Bordeaux, France, on 24 January 2020, and subsequently spread widely across the continent. By 17 March 2020, every country in Europe had confirmed a case, and all had reported at least one death, with the exception of Vatican City. Italy was the first European nation to experience a major outbreak in early 2020, becoming the first country worldwide to introduce a national lockdown. By 13 March 2020, the World Health Organization (WHO) declared Europe the epicentre of the pandemic and it remained so until the WHO announced it had been overtaken by South America on 22 May. By 18 March 2020, more than 250 million people were in lockdown in Europe. Despite deployment of COVID-19 vaccines, Europe became the pandemic's epicentre once again in late 2021.The Italian outbreak began on 31 January 2020, when two Chinese tourists tested positive for SARS-CoV-2 in Rome. Cases began to rise sharply, which prompted the government to suspend flights to and from China and declare a state of emergency. On 22 February 2020, the Council of Ministers announced a new decree-law to contain the outbreak, which quarantined more than 50,000 people in northern Italy. On 4 March, the Italian government ordered schools and universities closed as Italy reached a hundred deaths. Sport was suspended completely for at least one month. On 11 March, Italian Prime Minister Giuseppe Conte closed down nearly all commercial activity except supermarkets and pharmacies. On 19 April, the first wave ebbed, as 7-day deaths declined to 433. On 13 October, the Italian government again issued restrictive rules to contain the second wave. On 10 November, Italy surpassed 1 million confirmed infections. On 23 November, it was reported that the second wave of the virus had led some hospitals to stop accepting patients.
The virus was first confirmed to have spread to Spain on 31 January 2020, when a German tourist tested positive for SARS-CoV-2 on La Gomera in the Canary Islands. Post-hoc genetic analysis has shown that at least 15 strains of the virus had been imported, and community transmission began by mid-February. On 29 March, it was announced that, beginning the following day, all non-essential workers were ordered to remain at home for the next 14 days. The number of cases increased again in July in a number of cities including Barcelona, Zaragoza and Madrid, which led to reimposition of some restrictions but no national lockdown. By September 2021, Spain was one of the countries with the highest percentage of its population vaccinated (76% fully vaccinated and 79% with the first dose). Italy is ranked second at 75%.Sweden differed from most other European countries in that it mostly remained open. Per the Swedish constitution, the Public Health Agency of Sweden has autonomy that prevents political interference and the agency favoured remaining open. The Swedish strategy focused on longer-term measures, based on the assumption that after lockdown the virus would resume spreading, with the same result. By the end of June, Sweden no longer had excess mortality.Devolution in the United Kingdom meant that each of its four countries developed its own response. England's restrictions were shorter-lived than the others. The UK government started enforcing social distancing and quarantine measures on 18 March 2020. On 16 March, Prime Minister Boris Johnson advised against non-essential travel and social contact, praising work from home and avoiding venues such as pubs, restaurants, and theatres. On 20 March, the government ordered all leisure establishments to close, and promised to prevent unemployment. On 23 March, Johnson banned gatherings and restricted non-essential travel and outdoor activity. Unlike previous measures, these restrictions were enforceable by police through fines and dispersal of gatherings. Most non-essential businesses were ordered to close. On 24 April 2020, it was reported that a promising vaccine trial had begun in England; the government pledged more than £50 million towards research. On 16 April 2020, it was reported that the UK would have first access to the Oxford vaccine, due to a prior contract; should the trial be successful, some 30 million doses would be available. On 2 December 2020, the UK became the first developed country to approve the Pfizer vaccine; 800,000 doses were immediately available for use. In August 2022 it was reported that viral infection cases had declined in the UK.

North America
The virus arrived in the United States on 13 January 2020. Cases were reported in all North American countries after Saint Kitts and Nevis confirmed a case on 25 March, and in all North American territories after Bonaire confirmed a case on 16 April.
Per Our World in Data, 103,436,829 confirmed cases have been reported in the United States with 1,138,309 deaths, the most of any country, and the nineteenth-highest per capita worldwide. COVID-19 is the deadliest pandemic in US history; it was the third-leading cause of death in the US in 2020, behind heart disease and cancer. From 2019 to 2020, US life expectancy dropped by 3 years for Hispanic Americans, 2.9 years for African Americans, and 1.2 years for white Americans. These effects have persisted as US deaths due to COVID-19 in 2021 exceeded those in 2020. In the United States, COVID-19 vaccines became available under emergency use in December 2020, beginning the national vaccination program. The first COVID-19 vaccine was officially approved by the Food and Drug Administration on 23 August 2021. By 18 November 2022, while cases in the U.S. had declined, COVID variants BQ.1/BQ.1.1 had become dominant in the country.In March 2020, as cases of community transmission were confirmed across Canada, all of its provinces and territories declared states of emergency. Provinces and territories, to varying degrees, implemented school and daycare closures, prohibitions on gatherings, closures of non-essential businesses and restrictions on entry. Canada severely restricted its border access, barring travellers from all countries with some exceptions. Cases surged across Canada, notably in the provinces of British Columbia, Alberta, Quebec and Ontario, with the formation of the Atlantic Bubble, a travel-restricted area of the country (formed of the four Atlantic provinces). Vaccine passports were adopted in all provinces and two of the territories. Per a report on 11 November 2022, Canadian health authorities saw a surge in influenza, while COVID-19 was expected to rise during winter.

South America
The COVID-19 pandemic was confirmed to have reached South America on 26 February 2020, when Brazil confirmed a case in São Paulo. By 3 April, all countries and territories in South America had recorded at least one case. On 13 May 2020, it was reported that Latin America and the Caribbean had reported over 400,000 cases of COVID-19 infection with 23,091 deaths. On 22 May 2020, citing the rapid increase of infections in Brazil, the World Health Organization WHO declared South America the epicentre of the pandemic. As of 16 July 2021, South America had recorded 34,359,631 confirmed cases and 1,047,229 deaths from COVID-19. Due to a shortage of testing and medical facilities, it is believed that the outbreak is far larger than the official numbers show.The virus was confirmed to have spread to Brazil on 25 February 2020, when a man from São Paulo who had traveled to Italy tested positive for the virus. The disease had spread to every federative unit of Brazil by 21 March. On 19 June 2020, the country reported its one millionth case and nearly 49,000 reported deaths. One estimate of under-reporting was 22.62% of total reported COVID-19 mortality in 2020. As of 14 November 2023, Brazil, with 37,721,749 confirmed cases and 704,659 deaths, has the third-highest number of confirmed cases and second-highest death toll from COVID-19 in the world, behind only those of the United States and India.

Africa
The COVID-19 pandemic was confirmed to have spread to Africa on 14 February 2020, with the first confirmed case announced in Egypt. The first confirmed case in sub-Saharan Africa was announced in Nigeria at the end of February 2020. Within three months, the virus had spread throughout the continent; Lesotho, the last African sovereign state to have remained free of the virus, reported its first case on 13 May 2020. By 26 May, it appeared that most African countries were experiencing community transmission, although testing capacity was limited. Most of the identified imported cases arrived from Europe and the United States rather than from China where the virus originated. Many preventive measures were implemented by different countries in Africa including travel restrictions, flight cancellations, and event cancellations. Despite fears, Africa reported lower death rates than other, more economically developed regions.In early June 2021, Africa faced a third wave of COVID infections with cases rising in 14 countries. By 4 July the continent recorded more than 251,000 new COVID cases, a 20% increase from the prior week and a 12% increase from the January peak. More than sixteen African countries, including Malawi and Senegal, recorded an uptick in new cases. The World Health Organization labelled it Africa's 'Worst Pandemic Week Ever'. In October 2022, WHO reported that most countries on the African continent will miss the goal of 70 percent vaccination by the end of 2022.

Oceania
The COVID-19 pandemic was confirmed to have reached Oceania on 25 January 2020, with the first confirmed case reported in Melbourne, Australia. It has since spread elsewhere in the region. Australia and New Zealand were praised for their handling of the pandemic in comparison to other Western nations, with New Zealand and each state in Australia wiping out all community transmission of the virus several times even after re-introduction into the community.As a result of the high transmissibility of the Delta variant, however, by August 2021, the Australian states of New South Wales and Victoria had conceded defeat in their eradication efforts. In early October 2021, New Zealand also abandoned its elimination strategy. In November and December, following vaccination efforts, the remaining states of Australia, excluding Western Australia, voluntarily gave up COVID-zero to open up state and international borders. The open borders allowed the Omicron Variant of COVID-19 to enter quickly and cases subsequently exceeded 120,000 a day. By early March 2022, with cases exceeding 1,000 a day, Western Australia conceded defeat in its eradication strategy and opened its borders. Despite record cases, Australian jurisdictions slowly removed restrictions such as close contact isolation, mask wearing and density limits by April 2022.On 9 September 2022 restrictions were significantly relaxed. The aircraft mask mandate was scrapped nationwide and daily reporting transitioned to weekly reporting. On 14 September, COVID-19 disaster payment for isolating persons was extended for mandatory isolation. By 22 September, all states had ended mask mandates on public transport, including in Victoria, where the mandate had lasted for approximately 800 days. On 30 September 2022, all Australian leaders declared the emergency response finished and announced the end of isolation requirements. These changes were due in part to high levels of 'hybrid immunity' and low case numbers.

Antarctica
Due to its remoteness and sparse population, Antarctica was the last continent to have confirmed cases of COVID-19. The first cases were reported in December 2020, almost a year after the first cases of COVID-19 were detected in China. At least 36 people were infected in the first outbreak in 2020, with several other outbreaks taking place in 2021 and 2022.

United Nations
The United Nations Conference on Trade and Development (UNSC) was criticised for its slow response, especially regarding the UN's global ceasefire, which aimed to open up humanitarian access to conflict zones. The United Nations Security Council was criticized due to the inadequate manner in which it dealt with the COVID-19 pandemic, namely the poor ability to create international collaboration during this crisis.On 23 March 2020, United Nations Secretary-General António Manuel de Oliveira Guterres appealed for a global ceasefire; 172 UN member states and observers signed a non-binding supporting statement in June, and the UN Security Council passed a resolution supporting it in July.On 29 September 2020, Guterres urged the International Monetary Fund to help certain countries via debt relief and also call for countries to increase contributions to develop vaccines.

WHO
The WHO spearheaded initiatives such as the COVID-19 Solidarity Response Fund to raise money for the pandemic response, the UN COVID-19 Supply Chain Task Force, and the solidarity trial for investigating potential treatment options for the disease. The COVAX program, co-led by the WHO, GAVI, and the Coalition for Epidemic Preparedness Innovations (CEPI), aimed to accelerate the development, manufacture, and distribution of COVID-19 vaccines, and to guarantee fair and equitable access across the world.

Restrictions
The pandemic shook the world's economy, with especially severe economic damage in the United States, Europe and Latin America. A consensus report by American intelligence agencies in April 2021 concluded, "Efforts to contain and manage the virus have reinforced nationalist trends globally, as some states turned inward to protect their citizens and sometimes cast blame on marginalized groups." COVID-19 inflamed partisanship and polarisation around the world as bitter arguments exploded over how to respond. International trade was disrupted amid the formation of no-entry enclaves.

Travel restrictions
The pandemic led many countries and regions to impose quarantines, entry bans, or other restrictions, either for citizens, recent travellers to affected areas, or for all travellers. Travel collapsed worldwide, damaging the travel sector. The effectiveness of travel restrictions was questioned as the virus spread across the world. One study found that travel restrictions only modestly affected the initial spread, unless combined with other infection prevention and control measures. Researchers concluded that "travel restrictions are most useful in the early and late phase of an epidemic" and "restrictions of travel from Wuhan unfortunately came too late". The European Union rejected the idea of suspending the Schengen free travel zone.

Repatriation of foreign citizens
Several countries repatriated their citizens and diplomatic staff from Wuhan and surrounding areas, primarily through charter flights. Canada, the United States, Japan, India, Sri Lanka, Australia, France, Argentina, Germany and Thailand were among the first to do so. Brazil and New Zealand evacuated their own nationals and others. On 14 March, South Africa repatriated 112 South Africans who tested negative, while four who showed symptoms were left behind. Pakistan declined to evacuate its citizens.On 15 February, the US announced it would evacuate Americans aboard the Diamond Princess cruise ship, and on 21 February, Canada evacuated 129 Canadians from the ship. In early March, the Indian government began repatriating its citizens from Iran. On 20 March, the United States began to withdraw some troops from Iraq.

Impact
Economics
The pandemic and responses to it damaged the global economy. On 27 February 2020, worries about the outbreak crushed US stock indexes, which posted their sharpest falls since 2008.Tourism collapsed due to travel restrictions, closing of public places including travel attractions, and advice of governments against travel. Airlines cancelled flights, while British regional airline Flybe collapsed. The cruise line industry was hard hit, and train stations and ferry ports closed. International mail stopped or was delayed.The retail sector faced reductions in store hours or closures. Retailers in Europe and Latin America faced traffic declines of 40 percent. North America and Middle East retailers saw a 50–60 percent drop. Shopping centres faced a 33–43 percent drop in foot traffic in March compared to February. Mall operators around the world coped by increasing sanitation, installing thermal scanners to check the temperature of shoppers, and cancelling events.Hundreds of millions of jobs were lost, including more than 40 million jobs in the US. According to a report by Yelp, about 60% of US businesses that closed will stay shut permanently. The International Labour Organization (ILO) reported that the income generated in the first nine months of 2020 from work across the world dropped by 10.7 percent, or $3.5 trillion.

Supply shortages
Pandemic fears led to panic buying, emptying groceries of essentials such as food, toilet paper, and bottled water. Panic buying stemmed from perceived threat, perceived scarcity, fear of the unknown, coping behaviour and social psychological factors (e.g. social influence and trust).Supply shortages were due to disruption to factory and logistic operations; shortages were worsened by supply chain disruptions from factory and port shutdowns, and labour shortages.Shortages continued as managers underestimated the speed of economic recovery after the initial economic crash. The technology industry, in particular, warned of delays from underestimates of semiconductor demand for vehicles and other products.According to WHO Secretary-General Tedros Ghebreyesus, demand for personal protective equipment (PPE) rose one hundredfold, pushing prices up twentyfold. PPE stocks were exhausted everywhere.In September 2021, the World Bank reported that food prices remained generally stable and the supply outlook remained positive. However, the poorest countries witnessed a sharp increase in food prices, reaching the highest level since the pandemic began. The Agricultural Commodity Price Index stabilized in the third quarter but remained 17% higher than in January 2021.By contrast, petroleum products were in surplus at the beginning of the pandemic, as demand for gasoline and other products collapsed due to reduced commuting and other trips. The 2021 global energy crisis was driven by a global surge in demand as the world economy recovered. Energy demand was particularly strong in Asia.

Arts and cultural heritage
The performing arts and cultural heritage sectors were profoundly affected by the pandemic. Both organisations' and individuals' operations have been impacted globally. By March 2020, across the world and to varying degrees, museums, libraries, performance venues, and other cultural institutions had been indefinitely closed with their exhibitions, events and performances cancelled or postponed. A 2021 UNESCO report estimated ten million job losses worldwide in the culture and creative industries. Some services continued through digital platforms, such as live streaming concerts or web-based arts festivals.

Politics
The pandemic affected political systems, causing suspensions of legislative activities, isolations or deaths of politicians, and rescheduled elections. Although they developed broad support among epidemiologists, NPIs (non-pharmaceutical interventions) were controversial in many countries. Intellectual opposition came primarily from other fields, along with heterodox epidemiologists.

Brazil
The pandemic (and the response of Brazilian politicians to it) led to widespread panic, confusion, and pessimism in Brazil. When questioned regarding record deaths in the country in April 2020, Brazilian President Jair Bolsonaro said "So what? I'm sorry. What do you want me to do about it?" Bolsonaro disregarded WHO-recommended mitigation techniques and instead downplayed the risks of the virus, promoted increased economic activity, spread misinformation about the efficacy of masks, vaccines and public health measures, and distributed unproven treatments including hydroxychloroquine and ivermectin. A series of federal health ministers resigned or were dismissed after they refused to implement Bolsonaro's policies.Disagreements between federal and state governments led to a chaotic and delayed response to the rapid spread of the virus, exacerbated by preexisting social and economic disparities in the country. Employment, investment and valuation of the Brazilian real plummeted to record lows. Brazil was also heavily affected by the Delta and Omicron variants. At the height of the outbreak in the spring of 2021, 3,000+ Brazilians were dying per day. Bolsonaro's loss to Lula da Silva in the 2022 presidential election is widely credited to the former's mishandling of the pandemic.

China
Multiple provincial-level administrators of the Chinese Communist Party (CCP) were dismissed over their handling of quarantine measures. Some commentators claimed this move was intended to protect CCP General Secretary Xi Jinping. The US intelligence community claimed that China intentionally under-reported its COVID-19 caseload. The Chinese government maintained that it acted swiftly and transparently. Journalists and activists in China who reported on the pandemic were detained by authorities, including Zhang Zhan, who was arrested and tortured.

Italy
In early March 2020, the Italian government criticised the EU's lack of solidarity with Italy. On 22 March 2020, after a phone call with Italian Prime Minister Giuseppe Conte, Russian President Vladimir Putin ordered the Russian army to send military medics, disinfection vehicles, and other medical equipment to Italy. In early April, Norway and EU states like Romania and Austria started to offer help by sending medical personnel and disinfectant, and European Commission President Ursula von der Leyen offered an official apology to the country .

United States
Beginning in mid-April 2020, protestors objected to government-imposed business closures and restrictions on personal movement and assembly. Simultaneously, essential workers protested unsafe conditions and low wages by participating in a brief general strike. Some political analysts claimed that the pandemic contributed to President Donald Trump's 2020 defeat.The outbreak prompted calls for the United States to adopt social policies common in other wealthy countries, including universal health care, universal child care, paid sick leave, and higher levels of funding for public health. The Kaiser Family Foundation estimated that preventable hospitalizations of unvaccinated Americans in the second half of 2021 cost US$13.8 billion.There were also protest in regards to vaccine mandates in the United States. In January 2022, the US Supreme Court struck down an OSHA rule that mandated vaccination or a testing regimen for all companies with greater than 100 employees.

Other countries
The number of journalists imprisoned or detained increased worldwide; some detentions were related to the pandemic. The planned NATO "Defender 2020" military exercise in Germany, Poland and the Baltic states, the largest NATO war exercise since the end of the Cold War, was held on a reduced scale.The Iranian government was heavily affected by the virus, which infected some two dozen parliament members and political figures. Iran President Hassan Rouhani wrote a public letter to world leaders asking for help on 14 March 2020, due to a lack of access to international markets. Saudi Arabia, which had launched a military intervention in Yemen in March 2015, declared a ceasefire.Diplomatic relations between Japan and South Korea worsened. South Korea criticised Japan's "ambiguous and passive quarantine efforts" after Japan announced travellers from South Korea must quarantine for two weeks. South Korean society was initially polarised on President Moon Jae-in's response to the crisis; many Koreans signed petitions calling for Moon's impeachment or praising his response.Some countries passed emergency legislation. Some commentators expressed concern that it could allow governments to strengthen their grip on power. In Hungary, the parliament voted to allow Prime Minister Viktor Orbán to rule by decree indefinitely, suspend parliament and elections, and punish those deemed to have spread false information. In countries such as Egypt, Turkey, and Thailand, opposition activists and government critics were arrested for allegedly spreading fake news. In India, journalists criticising the government's response were arrested or issued warnings by police and authorities.

Food systems
The pandemic disrupted food systems worldwide, hitting at a time when hunger and undernourishment were rising- an estimated 690 million people lacked food security in 2019. Food access fell – driven by falling incomes, lost remittances, and disruptions to food production. In some cases, food prices rose. The pandemic and its accompanying lockdowns and travel restrictions slowed movement of food aid. According to the WHO, 811 million people were undernourished in 2020, "likely related to the fallout of COVID-19".

Education
The pandemic impacted educational systems in many countries. Many governments temporarily closed educational institutions, often replaced by online education. Other countries, such as Sweden, kept their schools open. As of September 2020, approximately 1.077 billion learners were affected due to school closures. School closures impacted students, teachers, and families with far-reaching economic and societal consequences. They shed light on social and economic issues, including student debt, digital learning, food insecurity, and homelessness, as well as access to childcare, health care, housing, internet, and disability services. The impact was more severe for disadvantaged children. The Higher Education Policy Institute reported that around 63% of students claimed worsened mental health as a result of the pandemic.

Health
The pandemic impacted global health for many conditions. Hospital visits fell. Visits for heart attack symptoms declined by 38% in the US and 40% in Spain. The head of cardiology at the University of Arizona said, "My worry is some of these people are dying at home because they're too scared to go to the hospital." People with strokes and appendicitis were less likely to seek treatment. Medical supply shortages impacted many people. The pandemic impacted mental health, increasing anxiety, depression, and post-traumatic stress disorder, affecting healthcare workers, patients and quarantined individuals.In late 2022, during the first northern hemisphere autumn and winter seasons following the widespread relaxation of global public health measures, North America and Europe experienced a surge in respiratory viruses and coinfections in both adults and children. This formed the beginnings of the 2022–2023 pediatric care crisis and what some experts have termed a "tripledemic" of seasonal influenza, Respiratory Syncytial Virus (RSV), and SARS-CoV-2 throughout North America. In the United Kingdom, pediatric infections also began to spike beyond pre-pandemic levels, albeit with different illnesses, such as Group A streptococcal infection and resultant scarlet fever. As of mid-December 2022, 19 children in the UK had died due to Strep A and the wave of infections had begun to spread into North America and Mainland Europe.The B/Yamagata lineage of influenza B might have become extinct in 2020/2021 due to COVID-19 pandemic measures. There have been no naturally occurring cases confirmed since March 2020. In 2023, the World Health Organization concluded that protection against the Yamagata lineage was no longer necessary in the seasonal flu vaccine, reducing the number of lineages targeted by the vaccine from four to three.

Environment
The pandemic and the reaction to it positively affected the environment and climate as a result of reduced human activity. During the "anthropause", fossil fuel use decreased, resource consumption declined, and waste disposal improved, generating less pollution. Planned air travel and vehicle transportation declined. In China, lockdowns and other measures resulted in a 26% decrease in coal consumption, and a 50% reduction in nitrogen oxides emissions.A wide variety of largely mammalian species, both captive and wild, have been shown to be susceptible to SARS-CoV-2, with some encountering particularly fatal outcomes. In particular, both farmed and wild mink have developed highly symptomatic and severe COVID-19 infections, with a mortality rate as high as 35–55% according to one study. White-tailed deer, on the other hand, have largely avoided severe outcomes but have effectively become natural reservoirs of the virus, with large numbers of free-ranging deer infected throughout the US and Canada, including approximately 80% of Iowa's wild deer herd. An August 2023 study appeared to confirm the status of white-tailed deer as a disease reservoir, noting that the viral evolution of SARS-CoV-2 in deer occurs at triple the rate of its evolution in humans and that infection rates remained high, even in areas rarely frequented by humans.

Discrimination and prejudice
Heightened prejudice, xenophobia, and racism toward people of Chinese and East Asian descent were documented around the world. Reports from February 2020, when most confirmed cases were confined to China, cited racist sentiments about Chinese people 'deserving' the virus. Individuals of Asian descent in Europe and North America reported increasing instances of racially-motivated abuse and assaults as a result of the pandemic. US President Donald Trump was criticised for referring to SARS-CoV-2 as the "Chinese Virus" and "Kung Flu", terms which were condemned as being racist and xenophobic.Age-based discrimination against older adults increased during the pandemic. This was attributed to their perceived vulnerability and subsequent physical and social isolation measures, which, coupled with their reduced social activity, increased dependency on others. Similarly, limited digital literacy left the elderly more vulnerable to isolation, depression, and loneliness.In a correspondence published in The Lancet in 2021, German epidemiologist Günter Kampf described the harmful effects of "inappropriate stigmatisation of unvaccinated people, who include our patients, colleagues, and other fellow citizens", noting the evidence that vaccinated individuals play a large role in transmission. American bioethicist Arthur Caplan responded to Kampf, writing "Criticising [the unvaccinated] who... wind up in hospitals and morgues in huge numbers, put stress on finite resources, and prolong the pandemic... is not stigmatising, it is deserved moral condemnation."In January 2022, Amnesty International urged Italy to change their anti-COVID-19 restrictions to avoid discrimination against unvaccinated people, saying that "the government must continue to ensure that the entire population can enjoy its fundamental rights." The restrictions included mandatory vaccination over the age of 50, and mandatory vaccination to use public transport.

Lifestyle changes
The pandemic triggered massive changes in behaviour, from increased Internet commerce to cultural changes in the workplace. Online retailers in the US posted $791.70 billion in sales in 2020, an increase of 32.4% from $598.02 billion the year before. Home delivery orders increased, while indoor restaurant dining shut down due to lockdown orders or low sales. Hackers, cybercriminals and scammers took advantage of the changes to launch new online attacks.Education in some countries temporarily shifted from physical attendance to video conferencing. Massive layoffs shrank the airline, travel, hospitality, and other industries. Despite most corporations implementing measures to address COVID-19 in the workplace, a poll from Catalyst found that as many as 68% of employees around the world felt that these policies were only performative and "not genuine".The pandemic led to a surge in remote work. According to a Gallup poll, only 4% of US employees were fully remote before the pandemic, compared to 43% in May 2020. Among white collar workers, that shift was more pronounced, with 6% increasing to 65% in the same period. That trend continued in later stages of the pandemic, with many workers choosing to remain remote even after workplaces reopened. Many Nordic, European, and Asian companies increased their recruitment of international remote workers even as the pandemic waned, partially to save on labor costs. This also led to a talent drain in the global south and in remote areas in the global north. High cost of living and dense urban areas also lost office real estate value due to remote worker exodus. By May 2023, due to increasing layoffs and concerns over productivity, some white collar workplaces in the US had resorted to performance review penalties and indirect incentives (e.g. donations to charity) to encourage workers to return to the office.

Historiography
A 2021 study noted that the COVID-19 pandemic had increased interest in epidemics and infectious diseases among both historians and the general public. Prior to the pandemic, these topics were usually overlooked by "general" history and only received attention in the history of medicine. Many comparisons were made between the COVID-19 and 1918 influenza pandemics, including the development of anti-mask movements, the widespread promotion of misinformation and the impact of socioeconomic disparities.

Religion
In some areas, religious groups exacerbated the spread of the virus, through large gatherings and the dissemination of misinformation. Some religious leaders decried what they saw as violations of religious freedom. In other cases, religious identity was a beneficial factor for health, increasing compliance with public health measures and protecting against the negative effects of isolation on mental wellbeing.

Information dissemination
Some news organizations removed their online paywalls for some or all of their pandemic-related articles and posts. Many scientific publishers provided pandemic-related journal articles to the public free of charge as part of the National Institutes of Health's COVID-19 Public Health Emergency Initiative. According to one estimate from researchers at the University of Rome, 89.5% of COVID-19-related papers were open access, compared to an average of 48.8% for the ten most deadly human diseases. The share of papers published on preprint servers prior to peer review increased dramatically.

Misinformation
Misinformation and conspiracy theories about the pandemic were widespread; they travelled through mass media, social media and text messaging. WHO declared an "infodemic" of incorrect information. Cognitive biases, such as confirmation bias, were linked to conspiracy beliefs, including COVID-19 vaccine hesitancy.

Culture and society
The COVID-19 pandemic had a major impact on popular culture. It was included in the narratives of ongoing pre-pandemic television series and become a central narrative in new ones, with mixed results. Writing for The New York Times about the then-upcoming BBC sitcom Pandemonium on 16 December 2020, David Segal asked, "Are we ready to laugh about Covid-19? Or rather, is there anything amusing, or recognizable in a humorous way, about life during a plague, with all of its indignities and setbacks, not to mention its rituals and rules."The pandemic had driven some people to seek peaceful escapism in media, while others were drawn towards fictional pandemics (e.g. zombie apocalypses) as an alternate form of escapism. Common themes have included contagion, isolation and loss of control. Many drew comparisons to the fictional film Contagion (2011), praising its accuracies while noting some differences, such as the lack of an orderly vaccine rollout.As people turned to music to relieve emotions evoked by the pandemic, Spotify listenership showed that classical, ambient and children's genres grew, while pop, country and dance remained relatively stable.

Transition to later phases
On 5 May 2023, the WHO declared that the pandemic was no longer a public health emergency of international concern. This led several media outlets to incorrectly report that this meant the pandemic was "over". The WHO commented to Full Fact that it was unlikely to declare the pandemic over "in the near future" and mentioned cholera, which it considers to have continued to be a pandemic since 1961. The WHO does not have an official category for pandemics or make declarations of when pandemics start or end.In June 2023, Hans Kluge, director of the WHO in Europe, commented that "While the international public health emergency may have ended, the pandemic certainly has not". The WHO in Europe launched a transition plan to manage the public health response to COVID-19 in the coming years and prepare for possible future emergencies.

Future endemic phase
In June 2022, an article in Human Genomics said that the pandemic was still "raging", but that "now is the time to explore the transition from the pandemic to the endemic phase."A March 2022 review declared the transition to endemic status to be "inevitable". A June 2022 review predicted that the virus that causes COVID-19 would become the fifth endemic seasonal coronavirus, alongside four other human coronaviruses. A February 2023 review of the four common cold coronaviruses concluded that the virus would become seasonal and, like the common cold, cause less severe disease for most people. As of 2023 the transition to endemic COVID-19 may take years or decades.

Long-term effects
Economic
Despite strong economic rebounds following the initial lockdowns in early 2020, towards the latter phases of the pandemic, many countries began to experience long-term economic effects. Several countries saw high inflation rates which had global impacts, particularly in developing countries. Some economic impacts such as supply chain and trade operations were seen as more permanent as the pandemic exposed major weaknesses in these systems.In Australia, the pandemic caused an increase in occupational burnout in 2022.During the pandemic, a large percentage of workers in Canada came to prefer working from home, which had an impact on the traditional work model. Some corporations made efforts to force workers to return to work on-site, while some embraced the idea.

Travel
There was a "travel boom" causing air travel to recover at rates faster than anticipated, and the aviation industry became profitable in 2023 for the first time since 2019, before the pandemic. However, economic issues meant some predicted that the boom would begin to slow down. Business travel on airlines was still below pre-pandemic levels and is predicted not to recover.

Health
An increase in excess deaths from underlying causes not related to COVID-19 has been largely blamed on systematic issues causing delays in health care and screening during the pandemic, which has resulted in an increase of non-COVID-19 related deaths.

Immunizations
During the pandemic, millions of children missed out on vaccinations as countries focused efforts on combating COVID-19. Efforts were made to increase vaccination rates among children in low-income countries. These efforts were successful in increasing vaccination rates for some diseases, though the UN noted that post-pandemic measles vaccinations were still falling behind.Some of the decrease in immunization was driven by an increase in mistrust of public health officials. This was seen in both low-income and high-income countries. Several African countries saw a decline in vaccinations due to misinformation around the pandemic flowing into other areas. Immunization rates have yet to recover in the United States and the United Kingdom.

See also
Coronavirus diseases
Emerging infectious disease
Globalization and disease
List of epidemics and pandemics

Notes
References
Further reading
External links
Health agencies
COVID-19 (Questions & Answers, instructional videos; Facts/MythBusters) by the World Health Organization (WHO)
COVID-19 by the Government of Canada
COVID-19 (Q&A) by the European Centre for Disease Prevention and Control
COVID-19 (Q&A) by the Ministry of Health, Singapore
COVID-19 (Q&A) by the US Centers for Disease Control and Prevention (CDC)
COVID-19 Information for the Workplace by the US National Institute for Occupational Safety and Health (NIOSH)

Data and graphs
Coronavirus disease (COVID-19) situation reports and map by the World Health Organization (WHO)
COVID-19 Resource Center, map, and historical data by Johns Hopkins University
COVID-19 data sets published by the European Centre for Disease Prevention and Control (ECDC)
COVID-19 Observer based on Johns Hopkins University data
COVID-19 Statistics and Research published by Our World in Data
COVID-19 Tracker from Stat News
COVID-19 Projections for many countries published by Institute for Health Metrics and Evaluation

Medical journals
Coronavirus (COVID-19) by The New England Journal of Medicine
Coronavirus (COVID-19) Hub by BMJ Publishing Group
Coronavirus Disease 2019 (COVID-19) by JAMA: The Journal of the American Medical Association
COVID-19: Novel Coronavirus Outbreak Archived 24 September 2020 at the Wayback Machine by Wiley Publishing
COVID-19 pandemic (2019–20) Collection by Public Library of Science (PLOS)
COVID-19 Portfolio, a curated collection of publications and preprints by National Institutes of Health (NIH)
COVID-19 Research Highlights by Springer Nature
COVID-19 Resource Centre by The Lancet
Novel Coronavirus Information Center by Elsevier

Cash conversion cycle

In management accounting, the Cash conversion cycle (CCC) measures how long a firm will be deprived of cash if it increases its investment in inventory in order to expand customer sales. It is thus a measure of the liquidity risk entailed by growth. However, shortening the CCC creates its own risks: while a firm could even achieve a negative CCC by collecting from customers before paying suppliers, a policy of strict collections and lax payments is not always sustainable.

Definition
CCC is days between disbursing cash and collecting cash in connection with undertaking a discrete unit of operations.

Derivation
Cashflows insufficient. The term "Cash Conversion Cycle" refers to the timespan between a firm's disbursing and collecting cash.  However, the CCC cannot be directly observed in cashflows, because these are also influenced by investment and financing activities; it must be derived from Statement of Financial Position data associated with the firm's operations.
Equation describes retailer. Although the term "cash conversion cycle" technically applies to a firm in any industry, the equation is generically formulated to apply specifically to a retailer. Since a retailer's operations consist of buying and selling inventory, the equation models the time between

(1) disbursing cash to satisfy the accounts payable created by purchase of inventory, and
(2) collecting cash to satisfy the accounts receivable generated by that sale.Equation describes a firm that buys and sells on account. Also, the equation is written to accommodate a firm that buys and sells on account.  For a cash-only firm, the equation would only need data from sales operations (e.g. changes in inventory), because disbursing cash would be directly measurable as purchase of inventory, and collecting cash would be directly measurable as sale of inventory.  However, no such 1:1 correspondence exists for a firm that buys and sells on account:  Increases and decreases in inventory do not occasion cashflows but accounting vehicles (payables and receivables, respectively); increases and decreases in cash will remove these accounting vehicles (receivables and payables, respectively) from the books.  Thus, the CCC must be calculated by tracing a change in cash through its effect upon receivables, inventory, payables, and finally back to cash—thus, the term cash conversion cycle, and the observation that these four accounts "articulate" with one another.

Taking these four transactions in pairs, analysts draw attention to five important intervals, referred to as conversion cycles (or conversion periods):

the Cash conversion cycle emerges as interval C→D (i.e. disbursing cash→collecting cash).
the Payables conversion period (or "Days payables outstanding") emerges as interval A→C (i.e. owing cash→disbursing cash)
the Operating cycle emerges as interval A→D (i.e. owing cash→collecting cash)
the Inventory conversion period or "Days inventory outstanding" emerges as interval A→B (i.e. owing cash→being owed cash)
the Receivables conversion period (or "Days sales outstanding") emerges as interval B→D (i.e.being owed cash→collecting cash)Knowledge of any three of these conversion cycles permits derivation of the fourth (leaving aside the operating cycle, which is just the sum of the inventory conversion period and the receivables conversion period.)
Hence,

In calculating each of these three constituent conversion cycles, the equation Time = Level/Rate is used (since each interval roughly equals the Time needed for its Level to be achieved at its corresponding Rate).

Its LEVEL "during the period in question" is estimated as the average of its levels in the two balance-sheets that surround the period: (Lt1+Lt2)/2.
To estimate its Rate, note that Accounts Receivable grows only when revenue is accrued; and Inventory shrinks and Accounts Payable grows by an amount equal to the COGS expense (in the long run, since COGS actually accrues sometime after the inventory delivery, when the customers acquire it).
Payables conversion period:  Rate = [inventory increase + COGS], since these are the items for the period that can increase "trade accounts payables," i.e. the ones that grew its inventory.
An exception is made when calculating this interval:  although a period average for the Level of inventory is used, any increase in inventory contributes to its Rate of change.  This is because the purpose of the CCC is to measure the effects of inventory growth on cash outlays.  If inventory grew during the period, this would be important to know.
Inventory conversion period: Rate = COGS, since this is the item that (eventually) shrinks inventory.
Receivables conversion period: Rate = revenue, since this is the item that can grow receivables (sales).

Aims
The aim of studying cash conversion cycle and its calculation is to change the policies relating to credit purchase and credit sales. The standard of payment of credit purchase or getting cash from debtors can be changed on the basis of reports of cash conversion cycle. If it tells good cash liquidity position, past credit policies can be maintained. Its aim is also to study cash flow of business. Cash flow statement and cash conversion cycle study will be helpful for cash flow analysis. The CCC readings can be compared among different companies in the same industry segment to evaluate the quality of cash management.

See also
Days in inventory
Days payable outstanding
Days sales outstanding
Working capital

References
External links
AFP Payments Guide to Unlocking the Cash Conversion Cycle
Measuring the Cash Conversion Cycle in an International Supply Chain
The Cash Conversion Cycle (Walmart, Target, Costco, and Amazon.com)
Net Trade Cycle

Cellular manufacturing

Cellular manufacturing is a process of manufacturing which is a subsection of just-in-time manufacturing and lean manufacturing encompassing group technology. The goal of cellular manufacturing is to move as quickly as possible, make a wide variety of similar products, while making as little waste as possible. Cellular manufacturing involves the use of multiple "cells" in an assembly line fashion. Each of these cells is composed of one or multiple different machines which accomplish a certain task. The product moves from one cell to the next, each station completing part of the manufacturing process.  Often the cells are arranged in a "U-shape" design because this allows for the overseer to move less and have the ability to more readily watch over the entire process. One of the biggest advantages of cellular manufacturing is the amount of flexibility that it has. Since most of the machines are automatic, simple changes can be made very rapidly. This allows for a variety of scaling for a product, minor changes to the overall design, and in extreme cases, entirely changing the overall design. These changes, although tedious, can be accomplished extremely quickly and precisely.A cell is created by consolidating the processes required to create a specific output, such as a part or a set of instructions. These cells allow for the reduction of extraneous steps in the process of creating the specific output, and facilitate quick identification of problems and encourage communication of employees within the cell in order to resolve issues that arise quickly. Once implemented, cellular manufacturing has been said to reliably create massive gains in productivity and quality while simultaneously reducing the amount of inventory, space and lead time required to create a product. It is for this reason that the one-piece-flow cell has been called "the ultimate in lean production."

History
Cellular manufacturing is derivative of principles of group technology, which were proposed by American industrialist Ralph Flanders in 1925 and adopted in Russia by the scientist Sergei Mitrofanov in 1933 (whose book on the subject was translated into English in 1959). Burbidge actively promoted group technology in the 1970s. "Apparently, Japanese firms began implementing cellular manufacturing sometime in the 1970s," and in the 1980s cells migrated to the United States as an element of just-in-time (JIT) production.One of the first English-language books to discuss cellular manufacturing, that of Hall in 1983, referred to a cell as a “U-line,” for the common, or ideal, U-shaped configuration of a cell—ideal because that shape puts all cell processes and operatives into a cluster, affording high visibility and contact. By 1990 cells had come to be treated as foundation practices in JIT manufacturing, so much so that Harmon and Peterson, in their book, Reinventing the Factory, included a section entitled, "Cell: Fundamental Factory of the Future". Cellular manufacturing was carried forward in the 1990s, when just-in-time was renamed lean manufacturing. Finally, when JIT/lean became widely attractive in the service sector, cellular concepts found their way into that realm; for example, Hyer and Wemmerlöv's final chapter is devoted to office cells.

Cell design
Cells are created in a workplace to facilitate flow. This is accomplished by bringing together operations or machines or people involved in a processing sequence of a products natural flow and grouping them close to one another, distinct from other groups. This grouping is called a cell. These cells are used to improve many factors in a manufacturing setting by allowing one-piece flow to occur. An example of one-piece flow would be in the production of a metallic case part that arrives at the factory from the vendor in separate pieces, requiring assembly. First, the pieces would be moved from storage to the cell, where they would be welded together, then polished, then coated, and finally packaged. All of these steps would be completed in a single cell, so as to minimize various factors (called non-value-added processes/steps) such as time required to transport materials between steps. Some common formats of single cells are: the U-shape (good for communication and quick movement of workers), the straight line, or the L-shape. The number of workers inside these formations depend on current demand and can be modulated to increase or decrease production. For example, if a cell is normally occupied by two workers and demand is doubled, four workers should be placed in the cell. Similarly, if demand halves, one worker will occupy the cell. Since cells have a variety of differing equipment, it is therefore a requirement that any employee is skilled at multiple processes.
While there exist many advantages to forming cells, there are some obvious benefits. It is quickly evident from observation of cells where inefficiencies lie, such as when an employee is too busy or relatively inactive. Resolving these inefficiencies can increase production and productivity by up to and above 100% in many cases. In addition to this, formation of cells consistently frees up floor space in the manufacturing/assembly environment (by having inventory only where it is absolutely required), improves safety in the work environment (due to smaller quantities of product/inventory being handled), improves morale (by imparting feelings of accomplishment and satisfaction in employees), reduces cost of inventory, and reducing inventory obsolescence.When formation of a cell would be too difficult, a simple principle is applied in order to improve efficiencies and flow, that is, to perform processes in a specific location and gather materials to that point at a rate dictated by an average of customer demand (this rate is called the takt time). This is referred to as the Pacemaker Process.Despite the advantages of designing for one-piece-flow, the formation of a cell must be carefully considered before implementation. Use of costly and complex equipment that tends to break down can cause massive delays in the production and will ruin output until they can be brought back online.
"A cell is a small organizational unit...designed to exploit similarities in how you process information, make products, and serve customers. Manufacturing cells [closely locate] people and equipment required for processing families of like products. [Prior to cellularization, parts] may have traveled miles to visit all the equipment and labor needed for their fabrication... After reorganization, families of similar parts are produced together within the physical confines of cells that house most or all of the required resources,...facilitating the rapid flow and efficient processing of material and information... Furthermore, cell operators can be cross-trained in several machines, engage in job rotation, and assume responsibilities for tasks [that] previously belonged to supervisors and support staff [including] activities such as planning and scheduling, quality control, trouble-shooting, parts ordering, interfacing with customers and suppliers, and record-keeping."
The short travel distances within cells serve to quicken the flows. Moreover, the compactness of a cell minimizes space that might allow build-ups of inventory between cell stations. To formalize that advantage, cells often have designed-in rules or physical devices that limit the amount of inventory between stations. Such a rule is known, in JIT/lean parlance, as kanban (from the Japanese), which establishes a maximum number of units allowable between a providing and a using work station. (Discussion and illustrations of cells in combinations with kanban are found in) The simplest form, kanban squares, are marked areas on floors or tables between work stations. The rule, applied to the producing station: "If all squares are full, stop. If not, fill them up."An office cell applies the same ideas: clusters of broadly trained cell-team members that, in concert, quickly handle all of the processing for a family of services or customers.A virtual cell is a variation in which all cell resources are not brought together in a physical space. In a virtual cell, as in the standard model, team members and their equipment are dedicated to a family of products or services. Although people and equipment are physically dispersed, as in a job shop, their narrow product focus aims for and achieves quick throughput, with all its advantages, just as if the equipment were moved into a cellular cluster. Lacking the visibility of physical cells, virtual cells may employ the discipline of kanban rules in order to tightly link the flows from process to process.
A simple but rather complete description of cell implementation comes from a 1985 booklet of 96 pages by Kone Corp. in Finland, producer of elevators, escalators, and the like. Excerpts follow: 

"The first step involved creating cells in the assembly, electrical and chemical testing departments. In April 1984 six cells, identified by different colors, were established... All devices manufactured in cells are identified by the cell's color, and all feed-back from quality control is directed straight to the workers of the cell concerned... The second step, in summer, 1984, was to "cellularize" manufacture of the analyzer subassemblies [that are] needed in the analyzer cells, and to test them if necessary. Production of the five sub-assembly cells consists exclusively of certain analyzer sub-units. The parts and materials are located in the cells... Material control between the cells is based on the pull system and actual demand. In the analyzer cells there is a buffer consisting of two pieces for each (roughly 25 different) sub-unit. When one piece is taken into assembly, a new one is ordered from the corresponding unit-cell. The order is made [using] a magnetic [kanban] button, which identifies the ordering cell (by color), unit (by code), and order date... When the manufacturing cell has completed the order, the unit is taken with the [kanban] button to its place on the ordering cell shelf. Orders from the unit cells to the sub-cells are based on the same principle. The only difference is that the buffer size is six sub-units. This [procedure] was implemented in August, 1984."

Implementation process
In order to implement cellular manufacturing, a number of steps must be performed. First, the parts to be made must be grouped by similarity (in design or manufacturing requirements) into families. Then a systematic analysis of each family must be performed; typically in the form of production flow analysis (PFA) for manufacturing families, or in the examination of design/product data for design families. This analysis can be time-consuming and costly, but is important because a cell needs to be created for each family of parts. Clustering of machines and parts is one of the most popular production flow analysis methods. The algorithms for machine part grouping include Rank Order Clustering, Modified Rank Order Clustering, and Similarity coefficients.
There are also a number of mathematical models and algorithms to aid in planning a cellular manufacturing center, which take into account a variety of important variables such as, "multiple plant locations, multi-market allocations with production planning and various part mix." Once these variables are determined with a given level of uncertainty, optimizations can be performed to minimize factors such as, "total cost of holding, inter-cell material handling, external transportation, fixed cost for producing each part in each plant, machine and labor salaries."

Difficulties in creating flow
The key to creating flow is continuous improvement to production processes. Upon implementation of cellular manufacturing, management commonly "encounters strong resistance from production workers". It will be beneficial to allow the change to cellular manufacturing to happen gradually. In this process.
It is also difficult to fight the desire to have some inventory on hand. It is tempting, since it would be easier to recover from an employee suddenly having to take sick leave. Unfortunately, in cellular manufacturing, it is important to remember the main tenets: "You sink or swim together as a unit" and that "Inventory hides problems and inefficiencies." If the problems are not identified and subsequently resolved, the process will not improve.
Another common set of problems stems from the need to transfer materials between operations. These problems include, "exceptional elements, number of voids, machine distances, bottleneck machines and parts, machine location and relocation, part routing, cell load variation, inter and intracellular material transferring, cell reconfiguring, dynamic part demands, and operation and completion times." These difficulties need to be considered and addressed to create efficient flow in cellular manufacturing.

Benefits and costs
Cellular manufacturing brings scattered processes together to form short, focused paths in concentrated physical space. So constructed, by logic a cell reduces flow time, flow distance, floor space, inventory, handling, scheduling transactions, and scrap and rework (the latter because of quick discovery of nonconformities). Moreover, cells lead to simplified, higher validity costing, since the costs of producing items are contained within the cell rather than scattered in distance and the passage of reporting time.Cellular manufacturing facilitates both production and quality control. Cells that are underperforming in either volume or quality can be easily isolated and targeted for improvement. The segmentation of the production process allows problems to be easily located and it is more clear which parts are affected by the problem.
There are also a number of benefits for employees working in cellular manufacturing. The small cell structure improves group cohesiveness and scales the manufacturing process down to a more manageable level for the workers. Workers can more easily see problems or possible improvements within their own cells and tend to be more self-motivated to propose changes. Additionally, these improvements that are instigated by the workers themselves cause less and less need for management, so over time overhead costs can be reduced. Furthermore, the workers often are able to rotate between tasks within their cell, which offers variety in their work. This can further increase efficiency because work monotony has been linked to absenteeism and reduced production quality.Case studies in just-in-time and lean manufacturing are replete with impressive quantitative measures along those lines. For example, BAE Systems, Platform Solutions (Fort Wayne, Ind.), producing aircraft engine monitors and controls, implemented cells for 80 percent of production, reducing customer lead time 90 percent, work-in-process inventory 70 percent, space for one product family from 6,000 square feet to 1,200 square feet, while increasing product reliability 300 percent, multi-skilling the union-shop work force, and being designated an Industry Week Best Plant for the year 2000. By five years later, rework and scrap had been cut 50 percent, new product introduction cycles 60 percent, and transactions 90 percent, while also increasing inventory turns three-fold and service turn times 30 percent, and being awarded a Shingo Prize for the year 2005.It appears to be difficult to isolate how much of those benefits accrue from cellular organization itself; among many case studies researched for this article few include attempts at isolating the benefits. One exception is the contention, at Steward, Inc. (Chattanooga, Tenn.), producing nickel zinc ferrite parts for electromagnetic interference suppression. According to case study authors, cells resulted in reductions of cycle time from 14 to 2 days, work-in-process inventories by 80 percent, finished inventories by 60 percent, lateness by 96 percent, and space by 56 percent.Another cellular case study includes quantitative estimates of the extent to which cells contributed to overall benefits. At Hughes Ground Systems Group (Fullerton, Calif.), producing circuit cards for defense equipment, the first cell, which began as a pilot project with 15 volunteers, was launched in 1987. One month later a second cell began, and by 1992 all production employees, numbering about 150, had been integrated into seven cells. Prior to cells, circuit card cycle time, from kit release to shipment to the customer, had been 38 weeks. After the cells had taken over the full production sequence (mechanical assembly, wave solder, thermal cycle, and conformal coat), cycle time had fallen to 30.5 weeks, of which production manager John Reiss attributed 20 weeks to use of a "WIP chart system" by the cell teams and the other 10.5 weeks to the cellular organization itself. Later, when it seemed that the cells were overly large and cumbersome, cell sizes were shrunk by two-thirds, resulting in “micro cells” that cut cycle time by another 1.5 weeks. Finally, by adopting certain other improvements, cycle times had decreased to four weeks. Other improvements included reducing work-in-process inventory from 6 or 7 days to one day and percent defective from 0.04 to 0.01 Switching from a functional (job-shop) layout to cells often costs has a minus net cost, inasmuch as the cell reduces costs of transport, work-in-process and finished inventory, transactions, and rework. When large, heavy, expensive pieces of equipment (sometimes called “monuments” in lean lingo) must be moved, however, the initial costs can be high to the point where cells are not feasible.There are a number of possible limitations to implementing cellular manufacturing. Some argue that cellular manufacturing can lead to a decrease in production flexibility. Cells are typically designed to maintain a specific flow volume of parts being produced. Should the demand or necessary quantity decrease, the cells may have to be realigned to match the new requirements, which is a costly operation, and one not typically required in other manufacturing setups.

See also
Cross-training (business)
World class manufacturing
Production flow analysis

References
Further reading
Anbumalar, V.; Raja Chandra Sekar, M (December 2015). "METHODS FOR SOLVING CELL FORMATION, STATIC LAYOUT AND DYNAMIC LAYOUT CELLULAR MANUFACTURING SYSTEM PROBLEMS: A REVIEW" (PDF). Asian Journal of Science and Technology.
Black, J. T. (1991). The Design of the Factory with a Future, New York, NY: McGraw-Hill, Inc., 1991.
Black, J. T. (2000). 'Lean Manufacturing Implementation', in Paul M. Swamidass (ed.), Innovations in competitive manufacturing, Boston, Mass.; London: Kluwer Academic, 177–86.
Burbidge, J.L. (1978), The Principles of Production Control, MacDonald and Evans, England, ISBN 0-7121-1676-1.
Brandon, John. (1996). Cellular Manufacturing: Integrating Technology and Management,  Somerset, England: Research Studies Press LTD.
Feld, William M., (2001). Lean Manufacturing: tools, techniques, and how to use them, Boca Raton, FL; Alexandria, VA: St. Lucie Press; Apics.
Hyer, N.; Brown, K.A. 2003. Work cells with staying power: lessons for process-complete operations. California Management Review 46/1 (Fall): 37–52.
Houshyar, A. Nouri; Leman, Z; Pakzad Moghadam, H; Sulaiman, R (August 2014). "Review on Cellular Manufacturing System and its Components". International Journal of Engineering and Advanced Technology (IJEAT).
İşlier, Attila (2015-01-01). "Cellular Manufacturing Systems: Organization, Trends And Innovative Methods". Alphanumeric Journal 3 (2). ISSN 2148-2225
Irani, Shahrukh. (1999). Handbook of Cellular Manufacturing Systems, New York, NY: John Wiley & Sons, Inc., 1999.
Kannan, V.R. 1996. A virtual cellular manufacturing approach to batch production. Decision Sciences. 27 (3), 519–539.
McLean, C.R., H.M. Bloom, and T.H. Hopp. 1982. The virtual manufacturing cell. Proceedings of the Fourth IFAC/IFIP Conference on Information Control Problems in Manufacturing Technology. Gaithersburg, Md. (October).
Singh, Nanua and Divakar Rajamani. (1996). Cellular Manufacturing Systems Design, Planning and Control, London, UK: Chapman & Hall.
Schonberger, R.J. 2004. Make work cells work for you. Quality Progress 3/74 (April 2004): 58–63.
Swamdimass, Paul M. and Darlow, Neil R. (2000). 'Manufacturing Strategy', in Paul M. Swamidass (ed.), Innovations in competitive manufacturing, Boston, Mass.; London: Kluwer Academic, 17–24.

CiteSeerX

CiteSeerX (formerly called CiteSeer) is a public search engine and digital library for scientific and academic papers, primarily in the fields of computer and information science.
CiteSeer's goal is to improve the dissemination and access of academic and scientific literature. As a non-profit service that can be freely used by anyone, it has been considered as part of the open access movement that is attempting to change academic and scientific publishing to allow greater access to scientific literature. CiteSeer freely provided Open Archives Initiative metadata of all indexed documents and links indexed documents when possible to other sources of metadata such as DBLP and the ACM Portal. To promote open data, CiteSeerX shares its data for non-commercial purposes under a Creative Commons license.CiteSeer is considered as a predecessor of academic search tools such as Google Scholar and Microsoft Academic Search. CiteSeer-like engines and archives usually only harvest documents from publicly available websites and do not crawl publisher websites. For this reason, authors whose documents are freely available are more likely to be represented in the index.
CiteSeer changed its name to ResearchIndex at one point and then changed it back.

History
CiteSeer and CiteSeer.IST
CiteSeer was created by researchers Lee Giles, Kurt Bollacker and Steve Lawrence in 1997 while they were at the NEC Research Institute (now NEC Labs), Princeton, New Jersey, US. CiteSeer's goal was to actively crawl and harvest academic and scientific documents on the web and use autonomous citation indexing to permit querying by citation or by document, ranking them by citation impact. At one point, it was called ResearchIndex.
CiteSeer became public in 1998 and had many new features unavailable in academic search engines at that time. These included:

Autonomous Citation Indexing automatically created a citation index that can be used for literature search and evaluation.
Citation statistics and related documents were computed for all articles cited in the database, not just the indexed articles.
Reference linking, allowing browsing of the database using citation links.
Citation context showed the context of citations to a given paper, allowing a researcher to quickly and easily see what other researchers have to say about an article of interest.
Related documents were shown using citation and word based measures, and an active and continuously updated bibliography is shown for each document.CiteSeer was granted a United States patent # 6289342, titled "Autonomous citation indexing and literature browsing using citation context", on September 11, 2001. The patent was filed on May 20, 1998, and has priority to January 5, 1998. A continuation patent (US Patent # 6738780) was filed on May 16, 2001, and granted on May 18, 2004.After NEC, in 2004 it was hosted as CiteSeer.IST on the World Wide Web at the College of Information Sciences and Technology, The Pennsylvania State University, and had over 700,000 documents. For enhanced access, performance and research, similar versions of CiteSeer were supported at universities such as the Massachusetts Institute of Technology, University of Zürich and the National University of Singapore. However, these versions of CiteSeer proved difficult to maintain and are no longer available. Because CiteSeer only indexes freely available papers on the web and does not have access to publisher metadata, it returns fewer citation counts than sites, such as Google Scholar, that have publisher metadata.
CiteSeer had not been comprehensively updated since 2005 due to limitations in its architecture design. It had a representative sampling of research documents in computer and information science but was limited in coverage because it was limited to papers that are publicly available, usually at an author's homepage, or those submitted by an author. To overcome some of these limitations, a modular and open source architecture for CiteSeer was designed – CiteSeerX.

CiteSeerX
CiteSeerX replaced CiteSeer and all queries to CiteSeer were redirected. CiteSeerX is a public search engine and digital library and repository for scientific and academic papers, primarily with a focus on computer and information science. However, recently CiteSeerX has been expanding into other scholarly domains such as economics, physics and others. Released in 2008, it was loosely based on the previous CiteSeer search engine and digital library and is built with a new open source infrastructure, SeerSuite, and new algorithms and their implementations. It was developed by researchers Isaac Councill and C. Lee Giles at the College of Information Sciences and Technology, Pennsylvania State University. It continues to support the goals outlined by CiteSeer to actively crawl and harvest academic and scientific documents on the public web and to use a citation inquiry by citations and ranking of documents by the impact of citations. Currently, Lee Giles, Prasenjit Mitra, Susan Gauch, Min-Yen Kan, Pradeep Teregowda, Juan Pablo Fernández Ramírez, Pucktada Treeratpituk, Jian Wu, Douglas Jordan, Steve Carman, Jack Carroll, Jim Jansen, and Shuyi Zheng are or have been actively involved in its development. Recently, a table search feature was introduced. It has been funded by the National Science Foundation, NASA, and Microsoft Research.
CiteSeerX continues to be rated as one of the world's top repositories, and was rated number 1 in July 2010. It currently has over 6 million documents with nearly 6 million unique authors and 120 million citations.CiteSeerX also shares its software, data, databases and metadata with other researchers, currently by Amazon S3 and by rsync. Its new modular open source architecture and software (available previously on SourceForge but now on GitHub) is built on Apache Solr and other Apache and open source tools, which allows it to be a testbed for new algorithms in document harvesting, ranking, indexing, and information extraction.
CiteSeerX caches some PDF files that it has scanned.  As such, each page includes a DMCA link which can be used to report copyright violations.

Current features
Automated information extraction
CiteSeerX uses automated information extraction tools, usually built on machine learning methods such ParsCit, to extract scholarly document metadata such as title, authors, abstract, citations, etc. As such, there are sometime errors in authors and titles. Other academic search engines have similar errors.

Focused crawling
CiteSeerX crawls publicly available scholarly documents primarily from author webpages and other open resources, and does not have access to publisher metadata. As such, citation counts in CiteSeerX are usually less than those in Google Scholar and Microsoft Academic Search who have access to publisher metadata.

Usage
CiteSeerX has nearly one million users worldwide based on unique IP addresses and has millions of hits daily. Annual downloads of document PDFs were nearly 200 million for 2015.

Data
CiteSeerX data is regularly shared under a Creative Commons BY-NC-SA license with researchers worldwide and has been and is used in many experiments and competitions.
Thanks to its OAI-PMH endpoint, CiteSeerX is an open archive and its content is indexed like an institutional repository in academic search engines, for instance BASE and Unpaywall consumers.

Other SeerSuite-based search engines
The CiteSeer model had been extended to cover academic documents in business with SmealSearch and in e-business with eBizSearch.  However, these were not maintained by their sponsors. An older version of both of these could be once found at BizSeer.IST but is no longer in service.
Other Seer-like search and repository systems have been built for chemistry, ChemXSeer and for archaeology, ArchSeer. Another had been built for robots.txt file search, BotSeer. All of these are built on the open source tool SeerSuite, which uses the open source indexer Lucene.

See also
References
Further reading
Giles, C. Lee; Bollacker, Kurt D.; Lawrence, Steve (1998). "CiteSeer: an automatic citation indexing system". Proceedings of the Third ACM Conference on Digital Libraries. pp. 89–98. CiteSeerX 10.1.1.30.6847. doi:10.1145/276675.276685. ISBN 978-0-89791-965-4. S2CID 514080.

External links

Official website of CiteSeerX

Clawback

The term clawback or claw back refers to any money or benefits that have been given out, but are required to be returned (clawed back) due to special circumstances or events, such as the monies having been received as the result of a financial crime, or where there is a clawback provision in the executive compensation contract.In law, clawback is most commonly known as restitution.

From government grantees
In the past, clawback phenomena have been used primarily in securing tax incentives, abatements, tax refunds, and grants. Clawbacks are distinguished from repayments or refunds as they involve a penalty, in addition to a repayment.
The use of tax incentives for attracting jobs and capital investment has grown over the past decades to include performance measures from which to gauge a company's growth. Typical measures are:

number of created jobs over 5
annual payroll;
amount of capital investment
amount of depreciated value .More unusual measures are retaining a headquarters at a specific site for a period of time, amount of production increase or production cost decrease per unit, or the requirement to bring a given technology to a commercial market. The recipient will be required to return the monetary value of the incentive plus a penalty and/or interest to the grantor of the incentive, usually a local or state taxing authority. As the use of incentives mature over time, it is sometimes alleged that the triggering of clawbacks for non-performance will likely become more ubiquitous.
Clawbacks can be understood to be the contractual elements that stand between the drive for economic development and community development and the slippery slope of corporate welfare. They are highly controversial and are utilized as community-based guarantees for some expectation of performance. The site location industry normally tries to eliminate or reduce any such promises as part of their negotiations.

From employees
Clawback provision
A clawback provision is a contractual clause typically included in employment contracts by financial firms, by which money already paid to an employee must be paid back to the employer under certain conditions.
The employees' bonuses are, in a clawback scheme, tied specifically to the performance (or lack thereof) of the financial product(s) the individual(s) may have created and/or sold as part of his or her job expecting a high profit. If the product does indeed do well over a long period of time, and permanently improves the nature of the firm, the bonuses paid to the individual are allowed to be retained by the individual. However, if the product fails, and damages the nature of the firm—even years down the line from the product's inception—then the firm has the right to revoke, reclaim, or otherwise repossess some or all of the bonus amount(s). However, research shows managers who are subject to clawback provisions that are newly in place in a company often try to offset their increased risk of bonus clawback by demanding an increase in base salary that is not subject to being clawed back.The prevalence of clawback provisions among Fortune 100 companies increased from lower than 3% prior to 2005, to 82% in 2010. The growing popularity of clawback provisions is likely, at least in part, due to the Sarbanes–Oxley Act of 2002, which requires the U.S. Securities and Exchange Commission (SEC) to pursue the repayment of incentive compensation from senior executives who are involved in a fraud. In practice, the Securities and Exchange Commission has enforced its clawback powers in only a small number of cases.The Dodd–Frank Act of 2010 mandates that the SEC require that U.S. public companies include a clawback provision in their executive compensation contracts that is triggered by any accounting restatement, regardless of fault (whereas the clawback provisions per the Sarbanes–Oxley Act only applied to intentional fraud). As of mid-2015, this portion of the Dodd–Frank Act had yet to be implemented.

Faithless servant clawback
Under the faithless servant doctrine, an employee who commits a crime in his work or fails to follow the company code of conduct or code of ethics is subject to having all of his compensation clawed back by the employer. In Morgan Stanley v. Skowron, 989 F. Supp. 2d 356 (S.D.N.Y. 2013), applying New York's faithless servant doctrine, the court held that a hedge fund's portfolio manager engaging in insider trading in violation of his company's code of conduct, which also required him to report his misconduct, must repay his employer the full $31 million his employer paid him as compensation during his period of faithlessness. The court called the insider trading the "ultimate abuse of a portfolio manager's position." The judge also wrote: "In addition to exposing Morgan Stanley to government investigations and direct financial losses, Skowron's behavior damaged the firm's reputation, a valuable corporate asset."

Implications
The usual objective of a clawback provision is to deter managers from publishing incorrect accounting information. Academic research finds that voluntarily adopted clawback provisions appear to be effective at reducing both intentional and unintentional accounting errors. The same study also finds that investors have greater confidence in a firm's financial statements after clawback adoption, and that boards of directors place greater weight on accounting numbers in executive bonuses after a clawback is in place (i.e., pay for performance sensitivity increases).
According to a December 2010 New Yorker magazine article, the clawback phenomenon pursued by banks and other financial groups directly and/or indirectly responsible for the financial crisis has been used by the chief administrators of those institutions in order to make the case that they are taking tangible self-corrective action to both prevent another crisis (by supposedly dis-incentivizing the sorts of shady investment-product behavior displayed by their people in the past) and to appropriately punish any potential future activity of a similar sort. However, some professional economists have argued that it is unlikely that either result will become the case, and that employee clawbacks are better seen as a public relations tactic until the impact of the financial crisis fades and similar abuses of the financial system can resume, with minimal or no detection by outside forces.

Notable cases
In the United States, clawbacks were rarely used until 2006. Major cases included a $600 million clawback affecting William W. McGuire of UnitedHealth Group, $500 million affecting Dennis Kozlowski of Tyco, and in 2019 clawbacks of compensation for the former CEO of Wells Fargo John Stumpf as well as a colleague.

From investors
Clawback lawsuits in US courts, especially from innocent individuals and entities who profited from financial crimes of others, have increased in the years since 2000.The yearslong clawback undertaken after the Madoff investment scandal, which attempted to transfer money back from the financial winners to the financial losers among those who had invested in Bernie Madoff's Ponzi scheme, is notable both for the size and success of the operation. A team of lawyers headed by Irving Picard were able to recover over $13 billion, or about 75%, of the estimated $19 billion collectively lost by investors, and transfer it back to those investors who had claimed losses. This was a far higher percentage than the usual recovery rate for investor clawbacks, which typically ranges from 5 to 30 percent. Of the recovered money, $7.2 billion came from the estate of just one investor, Jeffry Picower; it was the largest civil forfeiture payment in U.S. history.

Other clawback types
Clawback provisions are also used in bankruptcy matters where insiders may have raided assets prior to a filing,. The aim of the clause is to secure an option for an employer or trustee to limit bonuses, compensation, or other remuneration in case of catastrophic shifts in business, bankruptcy, and national crisis such as the financial crisis of 2007–2008.

In various countries
Italy and the Netherlands have several clawback regimes, and there are two clawback regimes in the United Kingdom.  The French clawback regime is limited. In Belgium, their enforceability is unclear.


== References ==

Computer-aided lean management

Computer-aided lean management, in business management, is a methodology of developing and using software-controlled, lean systems integration.  Its goal is to drive innovation towards cost and cycle-time savings.  It attempts to create an efficient use of capital and resources through the development and use of one integrated system model to run a business's planning, engineering, design, maintenance, and operations.

Overview
Computer-Aided Lean Management (CALM) is a management philosophy that uses computational software  to reduce risk and inefficiencies.  CALM acts on uncertainties and business inefficiencies to increase profitability through the use of computational decision-making tools that enable opportunities for additional value creation.  It is based on the application of software to enable continuous improvement through an Integrated System Model (ISM) of the business’s physical assets, business processes, and machine learning.   This unique integration of software applications using lean principles was developed in the aerospace industry and has migrated to the energy industry.
The creation of an integrated system model removes the barriers posed by the silos or stovepipes inherent in the departmentalization of most companies.  Integration enables lean uses of information for the creation of actionable knowledge.  CALM strives to create such a lean management approach to running the company through the rigors of software enforcement.  From this software enforcement comes clear policy and procedures that are adhered to, activity-based costing, measurement of effectiveness, and the capability of using advanced algorithms for dramatic improvements in optimization of resources.  CALM creates business capabilities through software to enable technology application, streamlining of processes, and a lean organizational structure.  The methodology is based on a commonsense approach for running a business, by measuring of actions taken and using those measurements to design improved processes in order to drive out inefficiencies.

History
CALM was inspired by lean processes and techniques that were already dominant management technologies with a wide diversity of applications and successes. Motorola and General Electric (GE) had been known for the concepts of Six Sigma; Boeing had been managing mass (using modular and flexible assembly options), and Toyota put it all together into a truly lean business through its Toyota Production System.  Boeing in turn took the Toyota model and added computer-aided enforcement of lean methodologies throughout the manufacturing process.
One of the major sources for CALM's outgrowth was integrated definition (IDEF) modeling in aerospace manufacturing that was pioneered by the U.S. Air Force in the 1970s. IDEF is a methodology designed to model the end-to-end decisions, actions, and activities of an organization or system so that costs, performance, and cycle times can be optimized. IDEF methods have been adapted for wider use in automotive, aerospace, pharmaceuticals, and even software development industries.  IDEF methods serve as a starting point to understand lean management through semantic data modeling. The IDEF process begins by mapping the as-is functions of an enterprise, creating a graphical model, or road map, that shows what controls each important function, who performs it, what resources are required for carrying it out, what it produces, how much it costs, and what relationships it has to other functions of the organization. IDEF simulations of the to-be enterprise have been found to be efficient at streamlining and modernizing both companies and governmental agencies.
Perhaps the best-developed evolution of the IDEF model beyond Toyota was at Boeing. Their project life-cycle process has grown into a rigorous software system that links people, tasks, tools, materials, and the environmental impact of any newly planned project, before any building is allowed to begin. Routinely, more than half of the time for any given project is spent building the precedence diagrams, or three-dimensional process maps, integrating with outside suppliers, and designing the implementation plan-all on the computer. Once real activity is initiated, an action tracker is used to monitor inputs and outputs versus the schedule and delivery metrics in real time throughout the organization. When the execution of a new airplane design begins, it is so well organized that it consistently cuts both costs and build time in half for each successive generation of airframe. And, of course, it is paperless. Boeing created a complex lean management process called 'define and control airplane configuration/manufacturing resource management' (DCAC/MRM). The process was built with the help of the operations research and computer sciences departments of the University of Pittsburgh. The manufacture of the Boeing 777 was ultimately a success, and it became the precursor to succeeding generations of CALM at Boeing.  Boeing is four generations beyond that airplane now, and they have succeeded in cutting the time and cost for each new generation of airplane. Boeing’s successes in conversion from inefficient silos of manufacturing to a lean and efficient operation have become legendary.
The methodology of CALM has recently been applied to field orientated infrastructure based businesses with highly interdependent systems, such as electric utilities where a smart grid concept is being researched and developed. The management of infrastructure-based industries like oil, gas, electricity, water, transportation, and renewables requires massive investments in interdependent, physical infrastructure, as well as simultaneous attention to disparate market forces.  In infrastructure businesses that manage field assets, uncertainty is the prime impediment to profitability, rather than the maintenance of efficient supply chains or the management of factory assembly lines.  These businesses are dominated by risk from uncertainties such as weather, market variations, transportation disruptions, government actions, logistic difficulties, geology, and asset reliability.  CALM has been applied to deal with these types of infrastructure based challenges.

References
Anderson, Boulanger, Johnson, Kressner (2008), Computer-Aided Lean Management for the Energy Industry, ISBN 978-1-59370-157-4
Anderson, R. Boulanger, A, Johnson, J., Kressner, A,. Getting lean and efficient.  Energy Biz Magazine – July/August 2006
Lean Energy Management – 12 Part Series – Oil & Gas Journal – Penn Well 2003–2007
Gross, P., R. Anderson, et al. 2007 Predicting electricity distribution feeder failures using machine learning susceptibility analysis – International Association of Artificial Intelligence.

External links
Lean Energy Initiative: "Lean Energy Initiative" – Columbia University – Lamont–Doherty Earth Observatory
LAI: MIT – The Lean Advancement Initiative – lots of articles, manuals and case studies
AndersonBoulanger.pdf: "Chapter 1 of Computer-Aided Lean Management for the Energy Industry"
Boeing Frontiers on DCAC/MRM
Predicting Electricity Distribution Feeder Failures
Real-time Ranking of Electric Feeders using Expert Advice

Continuous-flow manufacturing

Continuous-flow manufacturing, or repetitive-flow manufacturing, is an approach to discrete manufacturing that contrasts with batch production.  It is associated with a just-in-time and kanban production approach, and calls for an ongoing examination and improvement efforts which ultimately requires integration of all elements of the production system. The goal is an optimally balanced production line with little waste, the lowest possible cost, on-time and defect-free production.
This strategy is typically applied in discrete manufacturing as an attempt to handle production volumes comprising discrete units of product in a flow which is more naturally found in process manufacturing.  The basic fact is that in most cases, discrete units of a solid product cannot be handled in the same way as continuous quantities of liquid, gas or powder.
Discrete manufacturing is more likely to be performed in batches of product units that are routed from process to process in the factory.  Each process may add value to the batch during a run-time or work-time. There is usually some time spent waiting for the process during a queue-time or wait-time.  The larger the batch, the longer each unit has to wait for the rest of the batch to be completed, before it can go forward to the next process.  This queue-time is waste, Muda, and represents time lost that is not value-added in the eyes of the customer.  This waste is one of the most important elements targeted for reduction and elimination in lean manufacturing.
Reducing the batch size in discrete manufacturing is therefore a desirable goal: it improves the speed of response to the customer, whilst improving the ratio of value-added to non value-added work.  However, it should be balanced against the finite capacity of resources at the value-adding processes.  Capacity is consumed by changeover whenever a process is required to perform work on a different part or product model than the preceding one.  Time consumed in changeover is also considered waste, and it reduces the amount of resource capacity that is available to perform value-adding work. Reducing batch sizes can also increase handling time, risk and complexity in planning and controlling production.
The paradigm aim is to achieve single-piece flow where a single discrete unit of product flows from process to process.  In effect, the batch quantity is one.  If there is no change in part or product model, then this objective needs to be balanced against the additional handling time, and the work-centres that perform the process will typically have to be arranged in close proximity to one another in a flow-line.  This is often a characteristic of Repetitive-flow manufacturing and most manual assembly work is performed this way in the modern factory.
If there is a change in part or product model, then the process engineer should also consider to balance the changeover time with run-time.  If the changeover time is long, as it might be on a machine, batch size reduction is typically preceded with setup reduction techniques such as Single-Minute Exchange of Die.
One methodology for Repetitive-flow manufacturing is Demand Flow Technology which combines the principles of Repetitive-flow and demand-driven manufacturing.  The production planning and control is linked to a pull signal that is triggered from a customer order or consumption of finished goods stock.  A pull signal can also link a process to the down-stream, and synchronize the flow to the demand of the customer.

References
External links
WVU: Center for Entrepreneurial Studies and Development. Repetitive-Flow Manufacturing

Continual improvement process

A continual improvement process, also often called a continuous improvement process (abbreviated as CIP or CI), is an ongoing effort to improve products, services, or processes. These efforts can seek "incremental" improvement over time or "breakthrough" improvement all at once. Delivery (customer valued) processes are constantly evaluated and improved in the light of their efficiency, effectiveness and flexibility.
Some see continual improvement processes as a meta-process for most management systems (such as business process management, quality management, project management, and program management). W. Edwards Deming, a pioneer of the field, saw it as part of the 'system' whereby feedback from the process and customer were evaluated against organisational goals. The fact that it can be called a management process does not mean that it needs to be executed by 'management'; but rather merely that it makes decisions about the implementation of the delivery process and the design of the delivery process itself.
A broader definition is that of the Institute of Quality Assurance who defined "continuous improvement as a gradual never-ending change which is: '... focused on increasing the effectiveness and/or efficiency of an organisation to fulfil its policy and objectives. It is not limited to quality initiatives. Improvement in business strategy, business results, customer, employee and supplier relationships can be subject to continual improvement. Put simply, it means 'getting better all the time'.' ": 498 The key features of continual improvement processs in general are:

Feedback: The core principle of continual process improvement is the (self) reflection of processes
Efficiency: The purpose of continual improvement process is the identification, reduction, and elimination of suboptimal processes
Evolution: The emphasis of continual improvement process is on incremental, continual steps rather than giant leaps

Kaizen
Some successful implementations use the approach known as kaizen (the translation of kai ('change') zen ('good') is 'improvement'). This method became famous from Imai's 1986 book Kaizen: The Key to Japan's Competitive Success.Key features of kaizen include:

Improvements are based on many small changes rather than the radical changes that might arise from Research and Development
As the ideas come from the workers themselves, they are less likely to be radically different, and therefore easier to implement
Small improvements are less likely to require major capital investment than major process changes
The ideas come from the talents of the existing workforce, as opposed to using research, consultants or equipment – any of which could be very expensive
All employees should continually be seeking ways to improve their own performance
It helps encourage workers to take ownership for their work, and can help reinforce team working, thereby improving worker motivation.The elements above are the more tactical elements of continual improvement processes. The more strategic elements include deciding how to increase the value of the delivery process output to the customer (effectiveness) and how much flexibility is valuable in the process to meet changing needs.

PDCA
The PDCA (plan, do, check, act) or (plan, do, check, adjust) cycle supports continuous improvement and kaizen. It provides a process for improvement which can be used since the early design (planning) stage of any process, system, product or service.

PDSA
The PDSA (plan, do, study, act) cycle is often credited to W. Edwards Deming and often called the Deming cycle though W. Edwards Deming referred to it as the Shewhart cycle. Walter A. Shewhart back in the 1920s was working at Western Electric Company with W. Edwards Deming and Joseph M. Juran. Shewhart took the standard academic scientific method of inductive and deductive thinking, used in hypothesis testing, and converted it to a simple notion. When one does something, they plan it, do it, study it, and act on its results – the PDSA cycle. This was a far simpler notion to use and inform the shop floor of Western Electric while building telephones, where many workers would not and could not understand the scientific method. In fact, the PDSA notion could easily be applied to everyday life; driving a car to work. Thus the PDSA cycle was very easy to relate to by Western's workforce and gained the buy-in needed.

In environmental management
The continual improvement process concept is also used in environmental management systems (EMS), such as ISO 14000 and EMAS. The term "continual improvement", not "continuous improvement", is used in ISO 14000, and is understood to refer to an ongoing series of small or large-scale improvements which are each done discretely, i.e. in a step-wise fashion. Several differences exist between the CIP concept as it is applied in quality management and environmental management. Continual improvement in environmental management systems aims to improve the natural consequences of products and activities, not the products and activities as such. Secondly, there is no client-orientation in EMS-related continual improvement processes. Also, continual improvement processes in environmental management systems is not limited to small, incremental improvements as in kaizen, it also includes innovations of any scale.

ISO change from continuous to continual
In the late 1990s, the developers of the ISO 9001:2000 standard—which addressed quality management systems and principles—debated whether or not to update the use of the word continuous to continual. ISO Technical Committee 176 and regulatory representatives ultimately decided that "continuous was unenforceable because it meant an organization had to improve minute by minute, whereas, continual improvement meant step-wise improvement or improvement in segments". The committee reportedly did not base the change on dictionary definitions or the standard's vocabulary. This change ran contrary to the common usage of continuous in the standard and other prior business management documentation.The concept of continual improvement is the core of the British Standards Institute's 2019 publication: BS 8624 Guide to Continual improvement: Methods for quantification. BS 8624 describes requirements for continual improvement and provides methods and examples of recognized techniques.

See also
Benchmarking
ISO/IEC 15504 for software development process/management
Lean manufacturing
Minimum viable product
Perpetual beta
Training Within Industry (TWI), a service in USA from 1940 to 1945 within the War Manpower Commission which provided consulting services to war-related industries
Operational excellence


== References ==

Corporate action

A corporate action is an event initiated by a public company that brings or could bring an actual change to the securities—equity or debt—issued by the company. Corporate actions are typically agreed upon by a company's board of directors and authorized by the shareholders. For some events, shareholders or bondholders are permitted to vote on the event. Examples of corporate actions include stock splits, dividends, mergers and acquisitions, rights issues, and spin-offs.Some corporate actions such as a dividend (for equity securities) or coupon payment (for debt securities) may have a direct financial impact on the shareholders or bondholders; another example is a call (early redemption) of a debt security. Other corporate actions such as stock split may have an indirect financial impact, as the increased liquidity of shares may cause the price of the stock to decrease. Some corporate actions, such as name changes or ticker symbol changes to better reflect a company's business focus, have no direct financial impact on the shareholders; securities may be listed under a different security identifier (e.g. ISIN, CUSIP, Sedol) however. For example, "Apple Computers" changed its name to Apple Inc.

Overview
Types
There are three types of corporate actions: voluntary, mandatory, and mandatory with choice.
Mandatory corporate action: A mandatory corporate action is an event initiated by the board of directors of the corporation that affects all shareholders. Participation of shareholders are mandatory for these corporate actions. An example of a mandatory corporate action is cash dividend. A shareholder does not need to act to receive the dividend. Other examples of mandatory corporate actions include stock splits, mergers, pre-refunding, return of capital, bonus issue, asset ID change, and spin-offs. Strictly speaking, the word "mandatory" is not appropriate because the shareholder is not required to do anything; the shareholder is just a passive beneficiary in all the cases cited above. There is nothing the shareholder has to do or does in a Mandatory Corporate Action.
Voluntary corporate action: A voluntary corporate action is an action where the shareholders elect to participate in the action. A response is required for the corporation to process the action. An example of a voluntary corporate action is a tender offer. A corporation may request shareholders to tender their shares at a predetermined price. The shareholder may or may not participate in the tender offer. Shareholders send their responses to the corporation's agents, and the corporation will send the proceeds of the action to the shareholders who elect to participate.
Mandatory with choice corporate action: This corporate action is a mandatory corporate action where shareholders are given a chance to choose among several options. An example is cash or stock dividend option with one of the options as default. Shareholders may or may not submit their elections. In case a shareholder does not submit the election, the default option will be applied.Some market participants use a different method to distinguish the corporate action types. For example, "mandatory corporate action" and "mandatory with choice corporate action" may be used together. DTC uses the terms distributions, redemptions and reorganizations.

Purpose
The primary reasons companies use corporate actions are:

Return profits to shareholders: Cash dividends are a classic example where a public company declares a dividend to be paid on each outstanding share. Bonus is another case where the shareholder is rewarded. In a stricter sense, the bonus issue should not impact the share price but in reality, in rare cases, it does and results in an overall increase in value.
Influence the share price: If the price of a stock is too high or too low, the liquidity of the stock suffers. Stocks priced too high will not be affordable to all investors and stocks priced too low may be delisted. Corporate actions such as stock splits or reverse stock splits increase or decrease the number of outstanding shares to decrease or increase the stock price respectively. Buybacks are another example of influencing the stock price where a corporation buys back shares from the market in an attempt to reduce the number of outstanding shares thereby increasing the price.
Corporate restructuring: Corporations restructure in order to increase profitability. Examples include mergers (where two companies that are competitive or complementary join forces) and spin-offs (where a company breaks itself up in order to focus on its core competencies).

Impact
As an owner, the impact of a corporate action is usually measured in terms of changes to the securities and/or cash positions, so corporate actions can be divided into two categories:

Benefits: Actions that result in an increase to the position holder’s securities or cash position, without altering the underlying security. Examples include bonus issues, which is a Mandatory With Options Action/Event.
Reorganizations: Actions that reshape or restructure the beneficial owner's underlying securities position, which sometimes also results in a cash payout. Examples include equity restructures, conversions, and subscriptions.

Notification requirement
In order to keep investors and the market informed of corporate actions, they need to be announced. For public companies listed on exchanges, the exchanges themselves handle the announcement, notifying shareholders as well as making information about the corporate action available online. For companies that trade in the over-the-counter (OTC) marketplace, U.S. federal securities regulators task Financial Industry Regulatory Authority (FINRA), a self-regulatory organization, with processing the corporate action announcement.The event information flow for public companies where shareholders or bondholders can vote usually involves numerous parties. The information is first announced by the company to the exchange. Financial data companies which provide economic and financial data to customers collect such information and disseminate it via their own services to banks, institutional investors, managed service providers, and other market participants. In addition, the central securities depository (CSD) of the respective market collects the data and informs the CSD participants holding the respective share or bond in custody about the upcoming corporate action. The CSD sets a deadline for its participants by which the elections must be returned. The CSD participants then further disseminate the information to its clients (e.g. banks, institutional investors or private clients), which in turn must submit their election by the deadline set by the CSD participant.

References
External links
Securities Market Practice Group List of Corporate Action Events: Actions WG/A_Final Market Practices/4_SMPG_CA_EventTemplates_SR2014_V1_1.docx
Corporate Actions Glossary: [1]
List of voluntary corporate actions: [2]
ISO15022 MT564 message format for corporate actions data messages:[3]
SIX Financial Information Corporate Actions data offering: [4]
Assessing the Risk in the Corporate Actions Process: Industry Insight [5]

Corporate finance

Corporate finance is the area of finance that deals with the sources of funding, and the capital structure of corporations, the actions that managers take to increase the value of the firm to the shareholders, and the tools and analysis used to allocate financial resources. The primary goal of corporate finance is to maximize or increase shareholder value.Correspondingly, corporate finance comprises two main sub-disciplines. Capital budgeting is concerned with the setting of criteria about which value-adding projects should receive investment funding, and whether to finance that investment with equity or debt capital. Working capital management is the management of the company's monetary funds that deal with the short-term operating balance of current assets and current liabilities; the focus here is on managing cash, inventories, and short-term borrowing and lending (such as the terms on credit extended to customers).
The terms corporate finance and corporate financier are also associated with investment banking. The typical role of an investment bank is to evaluate the company's financial needs and raise the appropriate type of capital that best fits those needs. Thus, the terms "corporate finance" and "corporate financier" may be associated with transactions in which capital is raised in order to create, develop, grow or acquire businesses. 
Although it is in principle different from managerial finance which studies the financial management of all firms, rather than corporations alone, the main concepts in the study of corporate finance are applicable to the financial problems of all kinds of firms.
Financial management overlaps with the financial function of the accounting profession. However, financial accounting is the reporting of historical financial information, while financial management is concerned with the deployment of capital resources to increase a firm's value to the shareholders.

History
Corporate finance for the pre-industrial world began to emerge in the Italian city-states and the low countries of Europe from the 15th century.
The Dutch East India Company (also known by the abbreviation "VOC" in Dutch) was the first publicly listed company ever to pay regular dividends. 
The VOC was also the first recorded joint-stock company to get a fixed capital stock. Public markets for investment securities developed in the Dutch Republic during the 17th century.By the early 1800s, London acted as a center of corporate finance for companies around the world, which innovated new forms of lending and investment; see City of London § Economy. 
The twentieth century brought the rise of managerial capitalism and common stock finance, with share capital raised through listings, in preference to other sources of capital.
Modern corporate finance, alongside investment management, developed in the second half of the 20th century, particularly driven by innovations in theory and practice in the United States and Britain.
Here, see the later sections of History of banking in the United States and of History of private equity and venture capital.

Outline
The primary goal of financial management is to maximize or to continually increase shareholder value. Maximizing shareholder value requires managers to be able to balance capital funding between investments in "projects" that increase the firm's long term profitability and sustainability, along with paying excess cash in the form of dividends to shareholders. Managers of growth companies (i.e. firms that earn high rates of return on invested capital) will use most of the firm's capital resources and surplus cash on investments and projects so the company can continue to expand its business operations into the future. When companies reach maturity levels within their industry (i.e. companies that earn approximately average or lower returns on invested capital), managers of these companies will use surplus cash to payout dividends to shareholders. Managers must do an analysis to determine the appropriate allocation of the firm's capital resources and cash surplus between projects and payouts of dividends to shareholders, as well as paying back creditor related debt.Choosing between investment projects will thus be based upon several inter-related criteria. (1) Corporate management seeks to maximize the value of the firm by investing in projects which yield a positive net present value when valued using an appropriate discount rate in consideration of risk. (2) These projects must also be financed appropriately. (3) If no growth is possible by the company and excess cash surplus is not needed to the firm, then financial theory suggests that management should return some or all of the excess cash to shareholders (i.e., distribution via dividends).This "capital budgeting" is the planning of value-adding, long-term corporate financial projects relating to investments funded through and affecting the firm's capital structure.  Management must allocate the firm's limited resources between competing opportunities (projects).Capital budgeting is also concerned with the setting of criteria about which projects should receive investment funding to increase the value of the firm, and whether to finance that investment with equity or debt capital. Investments should be made on the basis of value-added to the future of the corporation. Projects that increase a firm's value may include a wide variety of different types of investments, including but not limited to, expansion policies, or mergers and acquisitions. When no growth or expansion is possible by a corporation and excess cash surplus exists and is not needed, then management is expected to pay out some or all of those surplus earnings in the form of cash dividends or to repurchase the company's stock through a share buyback program.A long-standing debate in corporate finance has focused on whether maximizing shareholder value or stakeholder value should be the primary focus of corporate managers, with stakeholders widely interpreted to refer to shareholders, employees, suppliers and the local community. In 2019, the Business Roundtable released a statement, signed by 181 prominent U.S. CEOs, which committed to lead their companies for "the benefit of all stakeholders". Despite intense debate and recent momentum for the stakeholder theory, shareholder theory still dominates corporate world strategy.

Capital structure
Achieving the goals of corporate finance requires that any corporate investment be financed appropriately. The sources of financing are, generically, capital self-generated by the firm and capital from external funders, obtained by issuing new debt and equity (and hybrid- or convertible securities). However, as above, since both hurdle rate and cash flows (and hence the riskiness of the firm) will be affected, the financing mix will impact the valuation of the firm, and a considered decision is required here. See Balance sheet, WACC.
Finally, there is much theoretical discussion as to other considerations that management might weigh here.

Sources of capital
Debt capital
Corporations may rely on borrowed funds (debt capital or credit) as sources of investment to sustain ongoing business operations or to fund future growth.  Debt comes in several forms, such as through bank loans, notes payable, or bonds issued to the public.  Bonds require the corporations to make regular interest payments (interest expenses) on the borrowed capital until the debt reaches its maturity date, therein the firm must pay back the obligation in full.  Debt payments can also be made in the form of sinking fund provisions, whereby the corporation pays annual installments of the borrowed debt above regular interest charges.  Corporations that issue callable bonds are entitled to pay back the obligation in full whenever the company feels it is in their best interest to pay off the debt payments.  If interest expenses cannot be made by the corporation through cash payments, the firm may also use collateral assets as a form of repaying their debt obligations (or through the process of liquidation).

Equity capital
Corporations can alternatively sell shares of the company to investors to raise capital.  Investors, or shareholders, expect that there will be an upward trend in value of the company (or appreciate in value) over time to make their investment a profitable purchase.  Shareholder value is increased when corporations invest equity capital and other funds into projects (or investments) that earn a positive rate of return for the owners.  Investors prefer to buy shares of stock in companies that will consistently earn a positive rate of return on capital in the future, thus increasing the market value of the stock of that corporation.  Shareholder value may also be increased when corporations payout excess cash surplus (funds from retained earnings that are not needed for business) in the form of dividends.

Preferred stock
Preferred stock is an equity security which may have any combination of features not possessed by common stock including properties of both an equity and a debt instrument, and is generally considered a hybrid instrument. Preferreds are senior (i.e. higher ranking) to common stock, but subordinate to bonds in terms of claim (or rights to their share of the assets of the company).Preferred stock usually carries no voting rights, but may carry a dividend and may have priority over common stock in the payment of dividends and upon liquidation. Terms of the preferred stock are stated in a "Certificate of Designation".
Similar to bonds, preferred stocks are rated by the major credit-rating companies. The rating for preferreds is generally lower, since preferred dividends do not carry the same guarantees as interest payments from bonds and they are junior to all creditors.Preferred stock is a special class of shares which may have any combination of features not possessed by common stock.
The following features are usually associated with preferred stock:
Preference in dividends
Preference in assets, in the event of liquidation
Convertibility to common stock.
Callability, at the option of the corporation
Nonvoting

Capitalization structure
As mentioned, the financing mix will impact the valuation of the firm: there are then two interrelated considerations here:

Management must identify the "optimal mix" of financing – the capital structure that results in maximum firm value, - but must also take other factors into account (see trade-off theory below). Financing a project through debt results in a liability or obligation that must be serviced, thus entailing cash flow implications independent of the project's degree of success. Equity financing is less risky with respect to cash flow commitments, but results in a dilution of share ownership, control and earnings. The cost of equity (see CAPM and APT) is also typically higher than the cost of debt - which is, additionally, a deductible expense – and so equity financing may result in an increased hurdle rate which may offset any reduction in cash flow risk.
Management must attempt to match the long-term financing mix to the assets being financed as closely as possible, in terms of both timing and cash flows. Managing any potential asset liability mismatch or duration gap entails matching the assets and liabilities respectively according to maturity pattern ("cashflow matching") or duration ("immunization"); managing this relationship in the short-term is a major function of working capital management, as discussed below. Other techniques, such as securitization, or hedging using interest rate- or credit derivatives, are also common. See: Asset liability management; Treasury management; Credit risk; Interest rate risk.

Related considerations
Much of the theory here, falls under the umbrella of the Trade-Off Theory in which firms are assumed to trade-off the tax benefits of debt with the bankruptcy costs of debt when choosing how to allocate the company's resources. However economists have developed a set of alternative theories about how managers allocate a corporation's finances.
One of the main alternative theories of how firms manage their capital funds is the Pecking Order Theory (Stewart Myers), which suggests that firms avoid external financing while they have internal financing available and avoid new equity financing while they can engage in new debt financing at reasonably low interest rates.
Also, the capital structure substitution theory hypothesizes that management manipulates the capital structure such that earnings per share (EPS) are maximized. An emerging area in finance theory is right-financing whereby investment banks and corporations can enhance investment return and company value over time by determining the right investment objectives, policy framework, institutional structure, source of financing (debt or equity) and expenditure framework within a given economy and under given market conditions.
One of the more recent innovations in this area from a theoretical point of view is the market timing hypothesis. This hypothesis, inspired by the behavioral finance literature, states that firms look for the cheaper type of financing regardless of their current levels of internal resources, debt and equity.

Investment and project valuation
In general, each "project's" value will be estimated using a discounted cash flow (DCF) valuation, and the opportunity with the highest value, as measured by the resultant net present value (NPV) will be selected (first applied in a corporate finance setting by Joel Dean in 1951). This requires estimating the size and timing of all of the incremental cash flows resulting from the project. Such future cash flows are then discounted to determine their present value (see Time value of money). These present values are then summed, and this sum net of the initial investment outlay is the NPV. See Financial modeling § Accounting for general discussion, and Valuation using discounted cash flows for the mechanics, with discussion re modifications for corporate finance.
The NPV is greatly affected by the discount rate. Thus, identifying the proper discount rate – often termed, the project "hurdle rate" – is critical to choosing appropriate projects and investments for the firm. The hurdle rate is the minimum acceptable return on an investment – i.e., the project appropriate discount rate. The hurdle rate should reflect the riskiness of the investment, typically measured by volatility of cash flows, and must take into account the project-relevant financing mix. Managers use models such as the CAPM or the APT to estimate a discount rate appropriate for a particular project, and use the weighted average cost of capital (WACC) to reflect the financing mix selected. (A common error in choosing a discount rate for a project is to apply a WACC that applies to the entire firm. Such an approach may not be appropriate where the risk of a particular project differs markedly from that of the firm's existing portfolio of assets.)
In conjunction with NPV, there are several other measures used as (secondary) selection criteria in corporate finance; see Capital budgeting § Ranked projects. These are visible from the DCF and include discounted payback period, IRR, Modified IRR, equivalent annuity, capital efficiency, and ROI. Alternatives (complements) to NPV, which more directly consider economic profit, include residual income valuation, MVA / EVA (Joel Stern, Stern Stewart & Co) and APV (Stewart Myers). With the cost of capital correctly and correspondingly adjusted, these valuations should yield the same result as the DCF. See also list of valuation topics.

Valuing flexibility
In many cases, for example R&D projects, a project may open (or close) various paths of action to the company, but this reality will not (typically) be captured in a strict NPV approach. Some analysts account for this uncertainty by adjusting the discount rate (e.g. by increasing the cost of capital) or the cash flows (using certainty equivalents, or applying (subjective) "haircuts" to the forecast numbers; see Penalized present value). Even when employed, however, these latter methods do not normally properly account for changes in risk over the project's lifecycle and hence fail to appropriately adapt the risk adjustment.  Management will therefore (sometimes) employ tools which place an explicit value on these options. So, whereas in a DCF valuation the most likely or average or scenario specific cash flows are discounted, here the "flexible and staged nature" of the investment is modelled, and hence "all" potential payoffs are considered. See further under Real options valuation. The difference between the two valuations is the "value of flexibility" inherent in the project.
The two most common tools are Decision Tree Analysis (DTA) and real options valuation (ROV); they may often be used interchangeably:

DTA values flexibility by incorporating possible events (or states) and consequent management decisions. (For example, a company would build a factory given that demand for its product exceeded a certain level during the pilot-phase, and outsource production otherwise. In turn, given further demand, it would similarly expand the factory, and maintain it otherwise. In a DCF model, by contrast, there is no "branching" – each scenario must be modelled separately.) In the decision tree, each management decision in response to an "event" generates a "branch" or "path" which the company could follow; the probabilities of each event are determined or specified by management. Once the tree is constructed: (1) "all" possible events and their resultant paths are visible to management; (2) given this "knowledge" of the events that could follow, and assuming rational decision making, management chooses the branches (i.e. actions) corresponding to the highest value path probability weighted; (3) this path is then taken as representative of project value. See Decision theory § Choice under uncertainty.
ROV is usually used when the value of a project is contingent on the value of some other asset or underlying variable. (For example, the viability of a mining project is contingent on the price of gold; if the price is too low, management will abandon the mining rights, if sufficiently high, management will develop the ore body. Again, a DCF valuation would capture only one of these outcomes.) Here: (1) using financial option theory as a framework, the decision to be taken is identified as corresponding to either a call option or a put option; (2) an appropriate valuation technique is then employed – usually a variant on the binomial options model or a bespoke simulation model, while Black–Scholes type formulae are used less often; see Contingent claim valuation. (3) The "true" value of the project is then the NPV of the "most likely" scenario plus the option value. (Real options in corporate finance were first discussed by Stewart Myers in 1977; viewing corporate strategy as a series of options was originally per Timothy Luehrman, in the late 1990s.)  See also § Option pricing approaches under Business valuation.

Quantifying uncertainty
Given the uncertainty inherent in project forecasting and valuation,

analysts will wish to assess the sensitivity of project NPV to the various inputs (i.e. assumptions) to the DCF model. In a typical sensitivity analysis the analyst will vary one key factor while holding all other inputs constant, ceteris paribus. The sensitivity of NPV to a change in that factor is then observed, and is calculated as a "slope": ΔNPV / Δfactor. For example, the analyst will determine NPV at various growth rates in annual revenue as specified (usually at set increments, e.g. -10%, -5%, 0%, 5%...), and then determine the sensitivity using this formula. Often, several variables may be of interest, and their various combinations produce a "value-surface" (or even a "value-space"), where NPV is then a function of several variables. See also Stress testing.
Using a related technique, analysts also run scenario based forecasts of NPV. Here, a scenario comprises a particular outcome for economy-wide, "global" factors (demand for the product, exchange rates, commodity prices, etc.) as well as for company-specific factors (unit costs, etc.). As an example, the analyst may specify various revenue growth scenarios (e.g. -5% for "Worst Case", +5% for "Likely Case" and +15% for "Best Case"), where all key inputs are adjusted so as to be consistent with the growth assumptions, and calculate the NPV for each. Note that for scenario based analysis, the various combinations of inputs must be internally consistent (see discussion at Financial modeling), whereas for the sensitivity approach these need not be so.  An application of this methodology is to determine an "unbiased" NPV, where management determines a (subjective) probability for each scenario – the NPV for the project is then the probability-weighted average of the various scenarios; see First Chicago Method. (See also rNPV, where cash flows, as opposed to scenarios, are probability-weighted.)
A further advancement which "overcomes the limitations of sensitivity and scenario analyses by examining the effects of all possible combinations of variables and their realizations" is to construct stochastic or probabilistic financial models – as opposed to the traditional static and deterministic models as above. For this purpose, the most common method is to use Monte Carlo simulation to analyze the project's NPV. This method was introduced to finance by David B. Hertz in 1964, although it has only recently become common: today analysts are even able to run simulations in spreadsheet based DCF models, typically using a risk-analysis add-in, such as @Risk or Crystal Ball. Here, the cash flow components that are (heavily) impacted by uncertainty are simulated, mathematically reflecting their "random characteristics". In contrast to the scenario approach above, the simulation produces several thousand random but possible outcomes, or trials, "covering all conceivable real world contingencies in proportion to their likelihood;" see Monte Carlo Simulation versus "What If" Scenarios. The output is then a histogram of project NPV, and the average NPV of the potential investment – as well as its volatility and other sensitivities – is then observed. This histogram provides information not visible from the static DCF: for example, it allows for an estimate of the probability that a project has a net present value greater than zero (or any other value).
Continuing the above example: instead of assigning three discrete values to revenue growth, and to the other relevant variables, the analyst would assign an appropriate probability distribution to each variable (commonly triangular or beta), and, where possible, specify the observed or supposed correlation between the variables. These distributions would then be "sampled" repeatedly – incorporating this correlation – so as to generate several thousand random but possible scenarios, with corresponding valuations, which are then used to generate the NPV histogram. The resultant statistics (average NPV and standard deviation of NPV) will be a more accurate mirror of the project's "randomness" than the variance observed under the scenario based approach. These are often used as estimates of the underlying "spot price" and volatility for the real option valuation as above; see Real options valuation § Valuation inputs. A more robust Monte Carlo model would include the possible occurrence of risk events (e.g., a credit crunch) that drive variations in one or more of the DCF model inputs.

Dividend policy
Dividend policy is concerned with financial policies regarding the payment of a cash dividend in the present or paying an increased dividend at a later stage.  Whether  to issue dividends, and what amount, is determined mainly on the basis of the company's unappropriated profit (excess cash) and influenced by the company's long-term earning power. When cash surplus exists and is not needed by the firm, then management is expected to pay out some or all of those surplus earnings in the form of cash dividends or to repurchase the company's stock through a share buyback program.
If there are no NPV positive opportunities, i.e. projects where returns exceed the hurdle rate, and excess cash surplus is not needed, then – finance theory suggests – management should return some or all of the excess cash to shareholders as dividends. This is the general case, however there are exceptions. For example, shareholders of a "growth stock", expect that the company will, almost by definition, retain most of the excess cash surplus so as to fund future projects internally to help increase the value of the firm.
Management must also choose the form of the dividend distribution, as stated, generally as cash dividends or via a share buyback. Various factors may be taken into consideration: where shareholders must pay tax on dividends, firms may elect to retain earnings or to perform a stock buyback, in both cases increasing the value of shares outstanding. Alternatively, some companies will pay "dividends" from stock rather than in cash; see Corporate action.  Financial theory suggests that the dividend policy should be set based upon the type of company and what management determines is the best use of those dividend resources for the firm to its shareholders.
As a general rule, then, shareholders of growth companies would prefer managers to retain earnings and pay no dividends (use excess cash to reinvest into the company's operations), whereas shareholders of value- or secondary stocks would prefer the management of these companies to payout surplus earnings in the form of cash dividends when a positive return cannot be earned through the reinvestment of undistributed earnings.  A share buyback program may be accepted when the value of the stock is greater than the returns to be realized from the reinvestment of undistributed profits.  In all instances, the appropriate dividend policy is usually directed by that which maximizes long-term shareholder value.

Working capital management
Managing the corporation's working capital position to sustain ongoing business operations is referred to as working capital management. These involve managing the relationship between a firm's short-term assets and its short-term liabilities.
In general this is as follows: As above, the goal of Corporate Finance is the maximization of firm value. In the context of long term, capital budgeting, firm value is enhanced through appropriately selecting and funding NPV positive investments. These investments, in turn, have implications in terms of cash flow and cost of capital.
The goal of Working Capital (i.e. short term) management is therefore to ensure that the firm is able to operate, and that it has sufficient cash flow to service long-term debt, and to satisfy both maturing short-term debt and upcoming operational expenses. In so doing, firm value is enhanced when, and if, the return on capital exceeds the cost of capital; See Economic value added (EVA).  Managing short term finance and long term finance is one task of a modern CFO.

Working capital
Working capital is the amount of funds that are necessary for an organization to continue its ongoing business operations, until the firm is reimbursed through payments for the goods or services it has delivered to its customers.  Working capital is measured through the difference between resources in cash or readily convertible into cash (Current Assets), and cash requirements (Current Liabilities). As a result, capital resource allocations relating to working capital are always current, i.e. short-term.
In addition to time horizon, working capital management differs from capital budgeting in terms of discounting and profitability considerations; decisions here are also "reversible" to a much larger extent. (Considerations as to risk appetite and return targets remain identical, although some constraints – such as those imposed by loan covenants – may be more relevant here).
The (short term) goals of working capital are therefore not approached on the same basis as (long term) profitability, and working capital management applies different criteria in allocating resources: the main considerations are (1) cash flow / liquidity and (2) profitability / return on capital (of which cash flow is probably the most important).

The most widely used measure of cash flow is the net operating cycle, or cash conversion cycle. This represents the time difference between cash payment for raw materials and cash collection for sales. The cash conversion cycle indicates the firm's ability to convert its resources into cash. Because this number effectively corresponds to the time that the firm's cash is tied up in operations and unavailable for other activities, management generally aims at a low net count. (Another measure is gross operating cycle which is the same as net operating cycle except that it does not take into account the creditors deferral period.)
In this context, the most useful measure of profitability is return on capital (ROC). The result is shown as a percentage, determined by dividing relevant income for the 12 months by capital employed; return on equity (ROE) shows this result for the firm's shareholders. As above, firm value is enhanced when, and if, the return on capital exceeds the cost of capital.

Management of working capital
Guided by the above criteria, management will use a combination of policies and techniques for the management of working capital. These policies aim at managing the current assets (generally cash and cash equivalents, inventories and debtors) and the short term financing, such that cash flows and returns are acceptable.
Cash management. Identify the cash balance which allows for the business to meet day to day expenses, but reduces cash holding costs.
Inventory management. Identify the level of inventory which allows for uninterrupted production but reduces the investment in raw materials – and minimizes reordering costs – and hence increases cash flow. See discussion under Inventory optimization and Supply chain management. Note that "inventory" is usually the realm of operations management: given the potential impact on cash flow, and on the balance sheet in general, finance typically "gets involved in an oversight or policing way".: 714 
Debtors management. There are two inter-related roles here:  (1) Identify the appropriate credit policy, i.e. credit terms which will attract customers, such that any impact on cash flows and the cash conversion cycle will be offset by increased revenue and hence Return on Capital (or vice versa); see Discounts and allowances.  (2) Implement appropriate credit scoring policies and techniques such that the risk of default on any new business is acceptable given these criteria.
Short term financing. Identify the appropriate source of financing, given the cash conversion cycle: the inventory is ideally financed by credit granted by the supplier; however, it may be necessary to utilize a bank loan (or overdraft), or to "convert debtors to cash" through "factoring"; see generally, trade finance.

Relationship with other areas in finance
Investment banking
Use of the term "corporate finance" varies considerably across the world. In the United States it is used, as above, to describe activities, analytical methods and techniques that deal with many aspects of a company's finances and capital. In the United Kingdom and Commonwealth countries, the terms "corporate finance" and "corporate financier" tend to be associated with investment banking – i.e. with transactions in which capital is raised for the corporation or shareholders; the services themselves are often referred to as advisory, financial advisory, deal advisory and transaction advisory services. See under Investment banking § Corporate finance for a listing of the various transaction-types here, and Financial analyst § Investment Banking for a description of the role.

Financial risk management
Financial risk management,  generically, is focused on measuring and managing  market risk, credit risk and operational risk.
Within corporates,  the scope is broadened to overlap enterprise risk management, 
and then addresses risks to the firm's overall strategic objectives,
focusing on the financial exposures and opportunities arising from business decisions, and their link to the firm’s appetite for risk, as well as their impact on share price.
The discipline is thus related to corporate finance, both re operations and funding, as below; and in large firms, the risk management function then overlaps "Corporate Finance", with the CRO consulted on capital-investment and other strategic decisions. 

Both areas share the goal of enhancing, and preserving, the firm's economic value.  Here, businesses actively manage any impact on profitability, cash flow, and hence firm value, due to credit and operational factors - this, overlapping "working capital management" to a large extent.  Firms then devote much time and effort to forecasting, analytics and performance monitoring.  (See also FP&A, "ALM" and treasury management.)
Firm exposure to market (and business) risk is a direct result of previous capital investments and funding decisions: where applicable here, typically in large corporates and under guidance from their investment bankers, firms actively manage and hedge  these exposures using traded financial instruments, usually  standard derivatives, creating interest rate-, commodity- and foreign exchange hedges. (See Hedge accounting, Cash flow hedge, Hedging irrelevance proposition.)

See also
Lists:

List of accounting topics
List of Corporate finance theorists
List of finance topics
List of corporate finance topics
List of valuation topics

References
Bibliography
Jonathan Berk; Peter DeMarzo (2013). Corporate Finance (3rd ed.). Pearson. ISBN 978-0132992473.
Peter Bossaerts; Bernt Arne Ødegaard (2006). Lectures on Corporate Finance (Second ed.). World Scientific. ISBN 978-981-256-899-1.
Richard Brealey; Stewart Myers; Franklin Allen (2013). Principles of Corporate Finance. Mcgraw-Hill. ISBN 978-0078034763.
Julie Dahlquist, Rainford Knight, Alan S. Adams (2022). Principles of Finance. ISBN 9781951693541.{{cite book}}:  CS1 maint: multiple names: authors list (link)
Aswath Damodaran (1996). Corporate Finance: Theory and Practice. Wiley. ISBN 978-0471076803.
João Amaro de Matos (2001). Theoretical Foundations of Corporate Finance. Princeton University Press. ISBN 9780691087948.
Joseph Ogden; Frank C. Jen; Philip F. O'Connor (2002). Advanced Corporate Finance. Prentice Hall. ISBN 978-0130915689.
C. Krishnamurti; S. R. Vishwanath (2010). Advanced Corporate Finance. MediaMatics. ISBN 978-8120336117.
Pascal Quiry; Yann Le Fur; Antonio Salvi; Maurizio Dallochio; Pierre Vernimmen (2011). Corporate Finance: Theory and Practice (3rd ed.). Wiley. ISBN 978-1119975588.
Stephen Ross, Randolph Westerfield, Jeffrey Jaffe (2012). Corporate Finance (10th ed.). Mcgraw-Hill. ISBN 978-0078034770.{{cite book}}:  CS1 maint: multiple names: authors list (link)
Joel M. Stern, ed. (2003). The Revolution in Corporate Finance (4th ed.). Wiley-Blackwell. ISBN 9781405107815.
Jean Tirole (2006). The Theory of Corporate Finance. Princeton University Press. ISBN 0691125562.
Ivo Welch (2017). Corporate Finance (4th ed.). ISBN 9780984004928.

Further reading
Jensen, Michael C.; Smith. Clifford W. (29 September 2000). The Theory of Corporate Finance: A Historical Overview. SSRN 244161. In The Modern Theory of Corporate Finance, edited by Michael C. Jensen and Clifford H. Smith Jr., pp. 2–20. McGraw-Hill, 1990. ISBN 0070591091
Graham, John R.; Harvey, Campbell R. (1999). "The Theory and Practice of Corporate Finance: Evidence from the Field". AFA 2001 New Orleans; Duke University Working Paper. SSRN 220251.

External links
Corporate Finance Overview - Corporate Finance Institute

Customer

In sales, commerce and economics, a customer (sometimes known as a client, buyer or purchaser) is the recipient of a good, service, product or an idea, obtained from a seller, vendor or supplier via a financial transaction or an exchange for money or some  other valuable consideration.

Etymology and terminology
Early societies relied on a gift economy based on favours. Later, as commerce developed, less permanent human relations were formed, depending more on transitory needs rather than enduring social desires. Customers are generally said to be the purchasers of goods and services, while clients are those who receive personalized advice and solutions. Although such distinctions have no contemporary semantic weight, agencies such as law firms, film studios, and health care providers tend to prefer client, while grocery stores, banks, and restaurants tend to prefer customer instead.

Client
The term client is derived from Latin clients or care meaning "to incline" or "to bend", and is related to the emotive idea of closure. It is widely believed that people only change their habits when motivated by greed and fear. Winning a client is, therefore, a singular event, which is why professional specialists who deal with particular problems tend to attract long-term clients rather than regular customers. Unlike regular customers, who buy merely on price and value, long-term clients buy on experience and trust.

Customer
Clients who habitually return to a seller develop customs that allow for regular, sustained commerce that allows the seller to develop statistical models to optimize production processes (which change the nature or form of goods or services) and supply chains (which change the location or formalize the changes of ownership or entitlement transactions). 
An "end customer" denotes the person at the end of a supply chain who ultimately purchases or utilised the goods or services.

Employer
A client paying for construction work is often referred to as an "employer".

Customer segmentation
In the 21st century customers are generally categorized into two types:

an entrepreneur or trader (sometimes a commercial Intermediary) - a dealer who purchases goods for re-sale.
an end user or ultimate customer who does not re-sell the things bought but is the actual consumer or an agent such as a Purchasing officer for the consumer.A customer may or may not also be a consumer, but the two notions are distinct.  A customer purchases goods; a consumer uses them.  An ultimate customer may be a consumer as well, but just as equally may have purchased items for someone else to consume.  An intermediate customer is not a consumer at all.  The situation is somewhat complicated in that ultimate customers of so-called industrial goods and services (who are entities such as government bodies, manufacturers, and educational and medical institutions) either themselves use up the goods and services that they buy, or incorporate them into other finished products, and so are technically consumers, too.  However, they are rarely called that, but are rather called industrial customers or business-to-business customers.  Similarly, customers who buy services rather than goods are rarely called consumers.Six Sigma doctrine places (active) customers in opposition to two other classes of people: not-customers and non-customers:

Customers of a given business have actively dealt with that business within a particular recent period that depends on the product sold.
Not-customers are either past customers who are no longer customers or potential customers who choose to interact with the competition.
Non-customers are people who are active in a different market segment entirely.Geoff Tennant, a Six Sigma consultant from the United Kingdom, uses the following analogy to explain the difference:  A supermarket's customer is the person buying milk at that supermarket; a not-customer buys milk from a competing supermarket, whereas a non-customer does not buy milk from supermarkets at all but rather "has milk delivered to the door in the traditional British way".Tennant also categorizes customers in another way that is employed outside the fields of marketing.  While marketers, market regulation, and economists use the intermediate/ultimate categorization, the field of customer service more often categorizes customers into two classes:

An external customer of an organization is a customer who is not directly connected to that organization.
An internal customer is a customer who is directly connected to an organization, and is usually (but not necessarily) internal to the organization.  Internal customers are usually stakeholders, employees, or shareholders, but the definition also encompasses creditors and external regulators.Before the introduction of the notion of an internal customer, external customers were, simply, customers. Quality-management writer Joseph M. Juran popularized the concept, introducing it in 1988 in the fourth edition of his Quality Control Handbook (Juran 1988).  The idea has since gained wide acceptance in the literature on total quality management and service marketing; and many organizations as of 2016 recognize the customer satisfaction of internal customers as a precursor to, and a prerequisite for, external customer satisfaction, with authors such as Tansuhaj, Randall & McCullough 1991 regarding service organizations which design products for internal customer satisfaction as better able to satisfy the needs of external customers. Research on the theory and practice of managing the internal customer continues as of 2016 in a variety of service-sector industries.

Arguments against use of the term "internal customers"
Leading authors in management and marketing, like Peter Drucker, Philip Kotler, W. Edwards Deming, etc., have not used the term "internal customer" in their works. They consider the "customer" as a very specific role in society which represents a crucial part in the relationship between the demand and the supply. Some of the most important characteristics of any customer are that: any customer is never in a subordination line with any supplier; any customer has equal positions with the supplier within negotiations, and any customer can accept or reject any offer for a service or a product. Peter Drucker wrote, "They are all people who can say no, people who have the choice to accept or reject what you offer."In opposition to the stated customer's characteristics, relationships between colleagues in a company are always based on subordination – direct or indirect. Company employees are obliged to follow the processes of their companies. Company employees do not have the authority to choose a unit/colleague to fulfill any task. Company employees are obliged to use an existing unit/colleague by using the company's structure and approved processes, therefore these internal relationships are not considered as an option.
Many authors in ITIL and Six Sigma methodologies define "internal customer" as an internal part of a company that uses the output of another part of a company as its input. But actually, this definition describes better a classical internal process rather than a relationship between a customer and a supplier. Peter Drucker considers that there are no customers inside organizations. He wrote "Inside an organization, there are only cost centers. The only profit center is a customer whose check has not bounced." In addition, William Deming advises managers, in his 9th point, to "Break down barriers between departments. They must work as a team", which means that there have to be teamwork in a company rather than a supplier/customer relationship. One more argument, even the ITIL methodology admits that "the term 'colleague' may be more accurate in describing how two internal groups are related to one another.".

See also
Client (business)
Customer centricity
Customer data integration
Customer delight
Customer relationship management
Early adopter
Guided selling
Procurement
Service level agreement
The customer is always right

Notes
References
Blythe, Jim (2008). Essentials of Marketing (4th ed.). Pearson Education. ISBN 978-0-273-71736-2.
Frain, John (1999). "Customers and customer buying behaviour". Introduction to marketing (4th ed.). Cengage Learning EMEA. ISBN 978-1-86152-147-7.
Kansal, B.B.; Rao, P.C.K. (2006). "Environmental Factors in Management". Preface to Management (Parragon Books). Ganga Dhar Chaudhary. ISBN 978-81-89091-00-2.
Kendall, Stephanie D. (2007). "Customer Service from the Customer's Perspective". In Fogli, Lawrence (ed.). Customer Service Delivery: Research and Best Practices. J-B SIOP Professional Practice Series. Vol. 20. John Wiley and Sons. ISBN 978-0-7879-8310-9.
Kelemen, Mihaela (2003). Managing quality: managerial and critical perspectives. SAGE. ISBN 978-0-7619-6904-4.
Papasolomou-Doukakis, Ioanna (2001). "Customer satisfaction". In Kitchen, Philip J.; Proctor, Tony (eds.). The informed student guide to marketing. ITBP Textbooks Series. Cengage Learning EMEA. ISBN 978-1-86152-546-8.
Reeves, Carol A.; Bednar, David A. (2005). "Defining Quality". In Wood, John Cunningham; Wood, Michael C. (eds.). Joseph M. Juran: critical evaluations in business and management. Routledge. ISBN 978-0-415-32571-4.
Reizenstein, Richard C. (2004). "Customer". In Stahl, Michael J. (ed.). Encyclopedia of health care management. Sage eReference. SAGE. ISBN 978-0-7619-2674-0.
Stracke, Christian (2006). "Process-oriented quality management". In Ehlers, Ulf-Daniel; Pawlowski, Jan Martin (eds.). Handbook on quality and standardisation in e-learning. Springer. ISBN 978-3-540-32787-5.
Tennant, Geoff (2001). Six Sigma: SPC and TQM in manufacturing and services. Gower Publishing. ISBN 978-0-566-08374-7.

Further reading
Juran, Joseph M. (1988). Quality Control Handbook (4th ed.). New York, NY: McGraw-Hill. ISBN 978-0-07-033176-1.
Tansuhaj, Patriya; Randall, Donna; McCullough, Jim (1991). "Applying the Internal Marketing Concept Within Large Organizations: As Applied to a Credit Union". Journal of Professional Services Marketing. Taylor & Francis. 6 (2): 193–202. doi:10.1300/J090v06n02_14.
Forget Demographics. Target Communities Instead (Marketing)

Cycle time variation

Cycle time variation is a metric and philosophy for continuous improvement with the aim of driving down the deviations in the time it takes to produce successive units on a production line.  It supports organizations' application of lean manufacturing or lean production by eliminating wasteful expenditure of resources. It is distinguished from some of the more common applications by its different focus of creating a structure for progressively reducing the sources of internal variation that leads to workarounds and disruption causing these wastes to accumulate in the first place.  Although it is often used as an indicator of lean progress, its use promotes a structured approach to reducing disruption that impacts efficiency, quality, and value.


== References ==

Daniel T. Jones (author)

Daniel T. Jones is an English author and researcher. He won the Shingo Prize for Operational Excellence in the Research and Professional Publication category multiple times for his books The Machine that Changed the World, Lean Thinking: Banish Waste and Create Wealth in Your Organization and Seeing the Whole: Mapping the Extended Value Stream.
He is also the founder of the Lean Enterprise Academy.

Education
He has a bachelor's degree in economics from the University of Sussex. In 2015 he received an honorary Doctorate of Science from the University of Buckingham in the United Kingdom.

Works
Daniel Jones along with James P. Womack researched the automotive industry. Their research work with Daniel Roos, a professor  at Massachusetts Institute of Technology on the automotive industry, found a three-to-one productivity difference between Japanese and American factories. Their research was published as a book, The Machine That Changed the World in 1991.

Bibliography
Books
A. Graves; Daniel T. Jones (1986). Comparison of international research and development in the automobile industry. International Motor Vehicle Program.
Roos, Daniel, Ph.D.; Womack, James P., Ph.D.; Jones, Daniel T.: The Machine That Changed the World : The Story of Lean Production, Harper Perennial (November 1991), ISBN 0060974176, ISBN 978-0060974176
James P. Womack; Daniel T. Jones (2002). Seeing the Whole: Mapping the Extended Value Stream, Volume 4. Taylor & Francis. ISBN 9780966784350.
James P. Womack; Daniel T. Jones (2010). Lean Thinking: Banish Waste and Create Wealth in Your Corporation. ISBN 9780743249270.
James P. Womack; Daniel T. Jones (2015). Lean Solutions: How Companies and Customers Can Create Value and Wealth Together. Simon and Schuster. ISBN 9780743277792.
Michael Balle; Daniel T. Jones (2016). Priolo, Roberto (ed.). Lead with Lean: On Lean Leadership and Practice. CreateSpace Independent Publishing Platform. ISBN 9781540480842.

See also
Lean manufacturing


== References ==

Discounts and allowances

Discounts and allowances are reductions to a basic price of goods or services.
They can occur anywhere in the distribution channel, modifying either the manufacturer's list price (determined by the manufacturer and often printed on the package), the retail price (set by the retailer and often attached to the product with a sticker), or the list price (which is quoted to a potential buyer, usually in written form).
There are many purposes for discounting, including to increase short-term sales, to move out-of-date stock, to reward valuable customers, to encourage distribution channel members to perform a function, or to otherwise reward behaviors that benefit the discount issuer. Some discounts and allowances are forms of sales promotion. Many are price discrimination methods that allow the seller to capture some of the consumer surplus.

Types
The most common types of discounts and allowances are listed below.

Dealing with payment
Prompt payment discount
Trade discounts are deductions in price given by the wholesaler or manufacturer to the retailer at the list price or catalogue price. Cash discounts are reductions in price given to the debtor to motivate the debtor to make payment within specified time. These discounts are intended to speed payment and thereby provide cash flow to the firm. They are sometimes used as a promotional device.

Examples
2/10 net 30 - this means the buyer must pay within 30 days of the invoice date, but will receive a 2% discount if they pay within 10 days of the invoice date.
3/7 EOM - this means the buyer will receive a cash discount of 3% if the bill is paid within 7 days after the end of the month indicated on the invoice date. If an invoice is received on or before the 25th day of the month, payment is due on the 7th day of the next calendar month. If a proper invoice is received after the 25th day of the month, payment is due on the 7th day of the second calendar month.
3/7 EOM net 30 - this means the buyer must pay within 30 days of the invoice date, but will receive a 3% discount if they pay within 7 days after the end of the month indicated on the invoice date. If an invoice is received on or before the 25th day of the month, payment is due on the 7th day of the next calendar month. If a proper invoice is received after the 25th day of the month, payment is due on the 7th day of the second calendar month.
2/15 net 40 ROG - this means the buyer must pay within 40 days of receipt of goods, but will receive a 2% discount if paid in 15 days of the invoice date. (ROG is short for "receipt of goods.")

Preferred payment method discount
Some retailers (particularly small retailers with low margins) offer discounts to customers paying with cash, to avoid paying fees on credit card transactions.

Partial payment discount
Similar to the trade discount, this is used when the seller wishes to improve cash flow or liquidity, but finds that the buyer typically is unable to meet the desired discount deadline. A partial discount for whatever payment the buyer makes helps the seller's cash flow partially.

Sliding scale
A discount offered based on one's ability to pay. More common with non-profit organizations than with for-profit retail.

Forward dating
This is where the purchaser doesn’t pay for the goods until well after they arrive. The date on the invoice is moved forward - example: purchase goods in November for sale during the December holiday season, but the payment date on the invoice is January 27.

Seasonal discount
These are price reductions given when an order is placed in a slack period (example: purchasing skis in April in the northern hemisphere, or in September in the southern hemisphere). On a shorter time scale, a happy hour may fall in this category. Retailers organize big discounts on almost every season in order to make space for new inventory for the upcoming season.  
Generally, this discount is referred to as "X-Dating" or "Ex-Dating". An example of X-Dating would be:

3/7 net 30 extra 10 - this means the buyer must pay within 30 days of the invoice date, but will receive a 3% discount if they pay within 7 days after the end of the month indicated on the invoice date plus an extra 10 days.

Dealing with trade
Bargaining
Bargaining is where the seller and the buyer negotiate a price below the original selling price.

Trade discount
Trade discounts, also called functional discounts, are payments to distribution channel members for performing some function. Examples of these functions are warehousing and shelf stocking. Trade discounts are often combined to include a series of functions, for example 20/12/5 could indicate a 20% discount for warehousing the product, an additional 12% discount for shipping the product, and an additional 5% discount for keeping the shelves stocked. Trade discounts are most frequent in industries where retailers hold the majority of the power in the distribution channel (referred to as channel captains).
Trade discounts are given to try to increase the volume of sales being made by the supplier.
The discount described as trade rate discount is sometimes called "trade discount".
Trade discount is the discount allowed on retail price of a product or something. 
for e.g. Retail price of a cream is 25 and trade discount is 2% on 25.

Trade rate discount
A trade rate discount, sometimes also called "trade discount", is offered by a seller to a buyer for purposes of trade or reselling, rather than to an end user. For example, a pharmacist might offer a discount for over-the-counter drugs to physicians who are purchasing them for dispensing to the physicians' own patients. A seller supplying both trade or resellers, and the general public will have a general list price for anybody, and will offer a trade discount to bona-fide trade customers.

Trade-in credit
Trade-in credit, also called trade-up credit, is a discount or credit granted for the return of something. The returned item may have little monetary value, as an old version of newer item being bought, or may be worth reselling as second-hand. The idea from a seller's viewpoint is to offer some discount but have the buyer showing some "counter action" to earn this special discount. Sellers like this as the discount granted is not just "given for free" and makes future price/value negotiations easier. Buyers have the advantage of getting some value for something no longer used. Examples can be found in many industries.

Dealing with quantity
These are price reductions given for bulk purchasing. The rationale behind them is to obtain economies of scale and pass some (or all) of these savings on to the customer. In some industries, buyer groups and co-ops have formed to take advantage of these discounts. Generally there are two types:

Cumulative quantity discount
Cumulative quantity discounts, also called accumulation discounts, are price reductions based on the quantity purchased over a set period of time. The expectation is that they will impose an implied switching cost and thereby bond the purchaser to the seller.

Non-cumulative quantity discount
These are price reductions based on the quantity of a single order. The expectation is that they will encourage larger orders, thus reducing billing, order filling, shipping, and sales personnel expenses.
If one has to buy more than one wants, we can distinguish between the surplus just not being used, or the surplus being a nuisance, e.g. because of having to carry a large container.

Dealing with customer characteristics
The following discounts have to do with specific characteristics of the customer.

Disability discount
A discount offered to customers with what is considered to be a disability.

Educational or student discount
These are price reductions given to members of educational institutions, usually students but possibly also to educators and to other institution staff. The provider's purpose is to build brand awareness early in a buyer's life, or build product familiarity so that after graduation the holder is likely to buy the same product, for own use or for an employer, at its normal price. Providers also offer student discounts as means of offering a product within the budget of a student, which would otherwise be too expensive, thus gaining extra sales. Students may be able to get discounts on products, services, entertainment, and more. Educational discounts may be given by merchants directly, or via a student discount program. Many brands like Apple, Dell, give exclusive discounts to students on their tech products, so that the students get to learn from the latest technology available making their work lesser. Additionally, travel websites also offer student discounts to help make travel more affordable for students. Some websites may also offer other perks for students, such as free cancellations or additional loyalty points. Students can get discounts not only from tech and travel but also from lifestyle brands.

Employee discount
A discount offered by a company to employees who buy its products.
In 2005, the American automakers ran an "employee discount" for all customers promotional campaign in order to entice buyers, with some success.

Military discount
A discount offered to customers who are or were members of a military service. Types of military discounts include discounts for active-duty military, veterans, retired military personnel, and military spouses or dependents. In the United States, military discounts frequently require proof of ID to show eligibility such as a DD Form 214, DD Form 215, or DD Form 217 from any branch of the Armed Forces, TRICARE Cards, Veterans Affairs Cards Uniformed Services Privilege and Identification Cards (USPIC) or other official documentation. Eligibility for military discounts can also be verified online or via mobile by verification companies. In Australia, DefCom Australia is a similar discount card.

Age-related discounts
Toddler discount, child discount, kid discount
A discount, or free service, offered to children younger than a certain age, commonly for admission to entertainments and attractions, restaurants, and hotels. There may be a requirement that the child be accompanied by an adult paying full price. Small children often travel free on public transport, and older ones may pay a substantially discounted price; proof of age may be required.

Young person's discount
Discounts are sometimes offered to young people below a certain age who are neither children nor in education.

Senior discount
A discount offered to customers who are above a certain relatively advanced age, typically a round number such as 50, 55, 60, 65, 70, and 75; the exact age varies in different cases. The rationale for a senior discount offered by companies is that the customer is assumed to be retired and living on a limited income, and unlikely to be willing to pay full price; sales at reduced price are better than no sales. Non-commercial organizations may offer concessionary prices as a matter of social policy. Free or reduced-rate travel is often available to older people (see, for example, Freedom Pass).
In the United States, most grocery stores offer senior discounts, starting for those age 50 or older, but most discounts are offered for those over 60.

First responder or healthcare worker discount
Discounts specially offered to firefighters, ambulance workers, police officers and other emergency services personnel are called first responder discounts. Hospital staff may sometimes receive discounts as well.

Special prices offered to friends of the seller
A discounted price offered to friends of the salesperson, an attitude which is parodied in the stereotype of a salesman saying "It costs [such-and such], but for you..." In Australia, New Zealand, and the UK, discounts to friends are known as "mates' rates." In French this discount is known as prix d'ami. In Spain this is known as "precio de amigo" in Spanish, or "preu d'amic" in Catalan. In German the term "Freundschaftspreis" is commonly used.

Special prices offered to local residents
Discounts are common in tourist destinations. In Hawaii, for example, many tourist attractions, hotels, and restaurants charge a deeply discounted price to someone who shows proof that they live in Hawaii; this is known as a "Kama'aina discount," meaning child of the land or a local resident.
It may be referred to in Hawaii or elsewhere as a resident discount.

Sometimes a document, typically a plastic card similar to a payment card, is issued as proof of eligibility for discounts. In other cases, existing documents proving status (as student, disabled, resident, etc.) are accepted. Documentation may not be required, for example, for people who are obviously young or old enough to qualify for age-related discounts. In some cases, the card may be issued to anyone who asks.

Coupons
A discount, either of a certain specified amount or a percentage to the holder of a voucher, usually with certain terms. Commonly, there are restrictions as for other discounts, such as being valid only if a certain quantity is bought or only if the customer is older than a specified age. Coupons are often printed in newspapers, brochures, and magazines, or can be downloaded from the Internet.

Rebates
A refund of part or sometimes the full price of the product following purchase, though some rebates are offered at the time of purchase. A particular case is the promise of a refund in full if applied for in a restricted date range some years in the future; the hope is that the promise will lure customers and increase sales, but that the majority will fail to meet the conditions for a valid claim.

Other
Promotional allowances (Trade-in allowances) - These are price reductions given to the buyer for performing some promotional activity. These include an allowance for creating and maintaining an in-store display or a co-op advertising allowance. Trade-in allowances are most common in the automobile industry, but they are also given for other durable goods.
Brokerage allowance - From the point of view of the manufacturer, any brokerage fee paid is similar to a promotional allowance. It is usually based on a percentage of the sales generated by the broker.

See also
Net 30
Ticket systems

References
Further reading
Shell, Ellen Ruppel, Cheap: The High Cost of Discount Culture, New York : Penguin Press, 2009. ISBN 978-1-59420-215-5

Distributed control system

A distributed control system (DCS) is a computerised control system for a process or plant usually with many control loops, in which autonomous controllers are distributed throughout the system, but there is no central operator supervisory control. This is in contrast to systems that use centralized controllers; either discrete controllers located at a central control room or within a central computer. The DCS concept increases reliability and reduces installation costs by localising control functions near the process plant, with remote monitoring and supervision.
Distributed control systems first emerged in large, high value, safety critical process industries, and were attractive because the DCS manufacturer would supply both the local control level and central supervisory equipment as an integrated package, thus reducing design integration risk. Today the functionality of Supervisory control and data acquisition (SCADA) and DCS systems are very similar, but DCS tends to be used on large continuous process plants where high reliability and security is important, and the control room is not geographically remote. Many machine control systems exhibit similar properties as plant and process control systems do.

Structure
The key attribute of a DCS is its reliability due to the distribution of the control processing around nodes in the system. This mitigates a single processor failure. If a processor fails, it will only affect one section of the plant process, as opposed to a failure of a central computer which would affect the whole process. This distribution of computing power local to the field Input/Output (I/O) connection racks also ensures fast controller processing times by removing possible network and central processing delays.
The accompanying diagram is a general model which shows functional manufacturing levels using computerised control.
Referring to the diagram;

Level 0 contains the field devices such as flow and temperature sensors, and final control elements, such as control valves
Level 1 contains the industrialised Input/Output (I/O) modules, and their associated distributed electronic processors.
Level 2 contains the supervisory computers, which  collect information from processor nodes on the system, and provide the operator control screens.
Level 3 is the production control level, which does not directly control the process, but is concerned with monitoring production and monitoring targets
Level 4 is the production scheduling level.Levels 1 and 2 are the functional levels of a traditional DCS, in which all equipment are part of an integrated system from a single manufacturer.
Levels 3 and 4 are not strictly process control in the traditional sense, but where production control and scheduling takes place.

Technical points
The processor nodes and operator graphical displays are connected over proprietary or industry standard networks, and network reliability is increased by dual redundancy cabling over diverse routes. This distributed topology also reduces the amount of field cabling by siting the I/O modules and their associated processors close to the process plant.
The processors receive information from input modules, process the information and decide control actions to be signalled by the output modules. The field inputs and outputs can be analog signals e.g. 4–20 mA DC current loop or two-state signals that switch either "on" or "off", such as relay contacts or a semiconductor switch.
DCSs are connected to sensors and actuators and use setpoint control to control the flow of material through the plant. A typical application is a PID controller fed by a flow meter and using a control valve as the final control element. The DCS sends the setpoint required by the process to the controller which instructs a valve to operate so that the process reaches and stays at the  desired setpoint. (see 4–20 mA schematic for example).
Large oil refineries and chemical plants have several thousand I/O points and employ very large DCS. Processes are not limited to fluidic flow through pipes, however, and can also include things like paper machines and their associated quality controls, variable speed drives and motor control centers, cement kilns, mining operations, ore processing facilities, and many others.
DCSs in very high reliability applications can have dual redundant processors with "hot" switch over on fault, to enhance the reliability of the control system.
Although 4–20 mA  has been the main field signalling standard, modern DCS systems can also support fieldbus digital protocols, such as Foundation Fieldbus, profibus, HART, modbus, PC Link, etc.
Modern DCSs also support neural networks and fuzzy logic applications. Recent research focuses on the synthesis of optimal distributed controllers, which optimizes a certain H-infinity or the H 2 control criterion.

Typical applications
Distributed control systems (DCS) are dedicated systems used in manufacturing processes that are continuous or batch-oriented.
Processes where a DCS might be used include:

Chemical plants
Petrochemical (oil) and refineries
Pulp and paper mills (see also: quality control system QCS)
Boiler controls and power plant systems
Nuclear power plants
Environmental control systems
Water management systems
Water treatment plants
Sewage treatment plants
Food and food processing
Agrochemical and fertilizer
Metal and mines
Automobile manufacturing
Metallurgical process plants
Pharmaceutical manufacturing
Sugar refining plants
Agriculture applications

History
Evolution of process control operations
Process control of large industrial plants has evolved through many stages. Initially, control would be from panels local to the process plant. However this required a large amount of human oversight to attend to these dispersed panels, and there was no overall view of the process. The next logical development was the transmission of all plant measurements to a permanently-staffed central control room. Effectively this was the centralisation of all the localised panels, with the advantages of lower manning levels and easier overview of the process. Often the controllers were behind the control room panels, and all automatic and manual control outputs were transmitted back to plant. However, whilst providing a central control focus, this arrangement was inflexible as each control loop had its own controller hardware, and continual operator movement within the control room was required to view different parts of the process.
With the coming of electronic processors and graphic displays it became possible to replace these discrete controllers with computer-based algorithms, hosted on a network of input/output racks with their own control processors. These could be distributed around plant, and communicate with the graphic display in the control room or rooms. The distributed control system was born.
The introduction of DCSs allowed easy interconnection and re-configuration of plant controls such as cascaded loops and interlocks, and easy interfacing with other production computer systems. It enabled sophisticated alarm handling, introduced automatic event logging, removed the need for physical records such as chart recorders, allowed the control racks to be networked and thereby located locally to plant to reduce cabling runs, and provided high level overviews of plant status and production levels.

Origins
Early minicomputers were used in the control of industrial processes since the beginning of the 1960s.  The IBM 1800, for example, was an early computer that had input/output hardware to gather process signals in a plant for conversion from field contact levels (for digital points) and analog signals to the digital domain.
The first industrial control computer system was built 1959 at the Texaco Port Arthur, Texas, refinery with an RW-300 of the Ramo-Wooldridge Company.In 1975, both Yamatake-Honeywell and Japanese electrical engineering firm Yokogawa introduced their own independently produced DCS's - TDC 2000 and CENTUM systems, respectively. US-based Bristol also introduced their UCS 3000 universal controller in 1975. In 1978 Valmet introduced their own DCS system called Damatic (latest generation named Valmet DNA). In 1980, Bailey (now part of ABB) introduced the NETWORK 90 system,  Fisher Controls (now part of Emerson Electric) introduced the PROVoX system, Fischer & Porter Company (now also part of ABB) introduced DCI-4000 (DCI stands for Distributed Control Instrumentation).
The DCS largely came about due to the increased availability of microcomputers and the proliferation of microprocessors in the world of process control.  Computers had already been applied to process automation for some time in the form of both direct digital control (DDC) and setpoint control. In the early 1970s Taylor Instrument Company, (now part of ABB) developed the 1010 system, Foxboro the FOX1 system, Fisher Controls the DC2 system and Bailey Controls the 1055 systems.  All of these were DDC applications implemented within minicomputers (DEC PDP-11, Varian Data Machines, MODCOMP etc.) and connected to proprietary Input/Output hardware. Sophisticated (for the time) continuous as well as batch control was implemented in this way. A more conservative approach was setpoint control, where process computers supervised clusters of analog process controllers. A workstation provided visibility into the process using text and crude character graphics. Availability of a fully functional graphical user interface was a way away.

Development
Central to the DCS model was the inclusion of control function blocks. Function blocks evolved from early, more primitive DDC concepts of "Table Driven" software. One of the first embodiments of object-oriented software, function blocks were self-contained "blocks" of code that emulated analog hardware control components and performed tasks that were essential to process control, such as execution of PID algorithms.  Function blocks continue to endure as the predominant method of control for DCS suppliers, and are supported by key technologies such as Foundation Fieldbus today.
Midac Systems, of Sydney, Australia, developed an objected-oriented distributed direct digital control system in 1982. The central system ran 11 microprocessors sharing tasks and common memory and connected to a serial communication network of distributed controllers each running two Z80s. The system was installed at the University of Melbourne.Digital communication between distributed controllers, workstations and other computing elements (peer to peer access) was one of the primary advantages of the DCS.  Attention was duly focused on the networks, which provided the all-important lines of communication that, for process applications, had to incorporate specific functions such as determinism and redundancy. As a result, many suppliers embraced the IEEE 802.4 networking standard. This decision set the stage for the wave of migrations necessary when information technology moved into process automation and IEEE 802.3 rather than IEEE 802.4 prevailed as the control LAN.

The network-centric era of the 1980s
In the 1980s, users began to look at DCSs as more than just basic process control. A very early example of a Direct Digital Control DCS was completed by the Australian business Midac in 1981–82 using R-Tec Australian designed hardware. The system installed at the University of Melbourne used a serial communications network, connecting campus buildings back to a control room "front end". Each remote unit ran two Z80 microprocessors, while the front end ran eleven Z80s in a parallel processing configuration with paged common memory to share tasks and that could run up to 20,000 concurrent control objects.
It was believed that if openness could be achieved and greater amounts of data could be shared throughout the enterprise that even greater things could be achieved. The first attempts to increase the openness of DCSs resulted in the adoption of the predominant operating system of the day: UNIX.  UNIX and its companion networking technology TCP-IP were developed by the US Department of Defense for openness, which was precisely the issue the process industries were looking to resolve.
As a result, suppliers also began to adopt Ethernet-based networks with their own proprietary protocol layers.  The full TCP/IP standard was not implemented, but the use of Ethernet made it possible to implement the first instances of object management and global data access technology.  The 1980s also witnessed the first PLCs integrated into the DCS infrastructure.  Plant-wide historians also emerged to capitalize on the extended reach of automation systems.  The first DCS supplier to adopt UNIX and Ethernet networking technologies was Foxboro, who introduced the I/A Series system in 1987.

The application-centric era of the 1990s
The drive toward openness in the 1980s gained momentum through the 1990s with the increased adoption of commercial off-the-shelf (COTS) components and IT standards.  Probably the biggest transition undertaken during this time was the move from the UNIX operating system to the Windows environment.  While the realm of the real time operating system (RTOS) for control applications remains dominated by real time commercial variants of UNIX or proprietary operating systems, everything above real-time control has made the transition to Windows.
The introduction of Microsoft at the desktop and server layers resulted in the development of technologies such as OLE for process control (OPC), which is now a de facto industry connectivity standard.  Internet technology also began to make its mark in automation and the  world, with most DCS HMI supporting Internet connectivity.  The 1990s were also known for the "Fieldbus Wars", where rival organizations competed to define what would become the IEC fieldbus standard for digital communication with field instrumentation instead of 4–20 milliamp analog communications.  The first fieldbus installations occurred in the 1990s.  Towards the end of the decade, the technology began to develop significant momentum, with the market consolidated around Ethernet I/P, Foundation Fieldbus and Profibus PA for process automation applications.  Some suppliers built new systems from the ground up to maximize functionality with fieldbus, such as Rockwell PlantPAx System, Honeywell with Experion & Plantscape SCADA systems, ABB with System 800xA, Emerson Process Management with the Emerson Process Management DeltaV control system, Siemens with the SPPA-T3000 or Simatic PCS 7, Forbes Marshall with the Microcon+ control system and Azbil Corporation with the Harmonas-DEO system. Fieldbus technics have been used to integrate machine, drives, quality and condition monitoring applications to one DCS with Valmet DNA system.The impact of COTS, however, was most pronounced at the hardware layer.  For years, the primary business of DCS suppliers had been the supply of large amounts of hardware, particularly I/O and controllers.  The initial proliferation of DCSs required the installation of prodigious amounts of this hardware, most of it manufactured from the bottom up by DCS suppliers.  Standard computer components from manufacturers such as Intel and Motorola, however, made it cost prohibitive for DCS suppliers to continue making their own components, workstations, and networking hardware.
As the suppliers made the transition to COTS components, they also discovered that the hardware market was shrinking fast.  COTS not only resulted in lower manufacturing costs for the supplier, but also steadily decreasing prices for the end users, who were also becoming increasingly vocal over what they perceived to be unduly high hardware costs. Some suppliers that were previously stronger in the PLC business, such as Rockwell Automation and Siemens, were able to leverage their expertise in manufacturing control hardware to enter the DCS marketplace with cost effective offerings, while the stability/scalability/reliability and functionality of these emerging systems are still improving. The traditional DCS suppliers introduced new generation DCS System based on the latest Communication and IEC Standards, which resulting in a trend of combining the traditional concepts/functionalities for PLC and DCS into a one for all solution—named "Process Automation System" (PAS). The gaps among the various systems remain at the areas such as: the database integrity, pre-engineering functionality, system maturity, communication transparency and reliability. While it is expected the cost ratio is relatively the same (the more powerful the systems are, the more expensive they will be), the reality of the automation business is often operating strategically case by case. The current next evolution step is called Collaborative Process Automation Systems.
To compound the issue, suppliers were also realizing that the hardware market was becoming saturated.  The life cycle of hardware components such as I/O and wiring is also typically in the range of 15 to over 20 years, making for a challenging replacement market.  Many of the older systems that were installed in the 1970s and 1980s are still in use today, and there is a considerable installed base of systems in the market that are approaching the end of their useful life.  Developed industrial economies in North America, Europe, and Japan already had many thousands of DCSs installed, and with few if any new plants being built, the market for new hardware was shifting rapidly to smaller, albeit faster growing regions such as China, Latin America, and Eastern Europe.
Because of the shrinking hardware business, suppliers began to make the challenging transition from a hardware-based business model to one based on software and value-added services.  It is a transition that is still being made today.  The applications portfolio offered by suppliers expanded considerably in the '90s to include areas such as production management, model-based control, real-time optimization, plant asset management (PAM), Real-time performance management (RPM) tools, alarm management, and many others.  To obtain the true value from these applications, however, often requires a considerable service content, which the suppliers also provide.

Modern systems (2010 onwards)
The latest developments in DCS include the following new technologies:

Wireless systems and protocols 
Remote transmission, logging and data historian
Mobile interfaces and controls
Embedded web-serversIncreasingly, and ironically, DCS are becoming centralised at plant level, with the ability to log into the remote equipment. This enables operator to control both at enterprise level ( macro ) and at the equipment level (micro), both within and outside the plant, because the importance of the physical location drops due to interconnectivity primarily thanks to wireless and remote access.
The more wireless protocols are developed and refined, the more they are included in DCS. DCS controllers are now often equipped with embedded servers and provide on-the-go web access. Whether DCS will lead Industrial Internet of Things (IIOT) or borrow key elements from remains to be seen.
Many vendors provide the option of a mobile HMI, ready for both Android and iOS. With these interfaces, the threat of security breaches and possible damage to plant and process are now very real.

See also
Annunciator panel
Building automation
EPICS
Industrial control system
Industrial safety system
Safety instrumented system (SIS)
TANGO


== References ==

Digital object identifier

A digital object identifier (DOI) is a persistent identifier or handle used to uniquely identify various objects, standardized by the International Organization for Standardization (ISO). DOIs are an implementation of the Handle System; they also fit within the URI system (Uniform Resource Identifier). They are widely used to identify academic, professional, and government information, such as journal articles, research reports, data sets, and official publications. DOIs have also been used to identify other types of information resources, such as commercial videos.A DOI aims to resolve to its target, the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL where the object is located. Thus, by being actionable and interoperable, a DOI differs from ISBNs or ISRCs which are identifiers only. The DOI system uses the indecs Content Model for representing metadata.
The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI should provide a more stable link than directly using its URL. But if its URL changes, the publisher must update the metadata for the DOI to maintain the link to the URL. It is the publisher's responsibility to update the DOI database. If they fail to do so, the DOI resolves to a dead link, leaving the DOI useless.The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000. Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs. The DOI system is implemented through a federation of registration agencies coordinated by the IDF. By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations, and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.

Nomenclature and syntax
A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.

prefix/suffixThe prefix identifies the registrant of the identifier and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a number greater than or equal to 1000, whose limit depends only on the total number of registrants. The prefix may be further subdivided with periods, like 10.NNNN.N.For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10" part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace, and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).
DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works such as licenses, parties to a transaction, etc.
The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.

Display
The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182) This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL – providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyperlinked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.Since DOI is a namespace within the Handle System, it is semantically correct to represent it as the URI info:doi/10.1000/182.

Content
Major content of the DOI system currently includes:

Scholarly materials (journal articles, books, ebooks, etc.) through Crossref, a consortium of around 3,000 publishers; Airiti, a leading provider of Chinese and Taiwanese electronic academic journals; and the Japan Link Center (JaLC)  an organization providing link management and DOI assignment for electronic academic journals in Japanese.
Research datasets through Datacite, a consortium of leading research libraries, technical information providers, and scientific data centers;
European Union official publications through the EU publications office;
The Chinese National Knowledge Infrastructure project at Tsinghua University and the Institute of Scientific and Technical Information of China (ISTIC), two initiatives sponsored by the Chinese government.
Permanent global identifiers for both commercial and non-commercial audio/visual content titles, edits, and manifestations through the Entertainment ID Registry, commonly known as EIDR.In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.Other registries include Crossref and the multilingual European DOI Registration Agency (mEDRA). Since 2015, RFCs can be referenced as doi:10.17487/rfc....

Features and benefits
The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated (although when the publisher of a journal changes, sometimes all the DOIs will be changed, with the old DOIs no longer working). It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.
The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system. DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request. However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents, that would have been available for no additional fee from alternative locations.The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.
The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.

Comparison with other identifier schemes
A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.The DOI system offers persistent, semantically interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" does not mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).
A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.

Resolution
DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.
To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.
Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as https://doi.org/ (preferred) or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.
Other DOI resolvers and HTTP Proxies include the Handle System and PANGAEA. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled (often author archived) version of a title and redirects the user to that instead of the publisher's version. Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016 (later Unpaywall). While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs, which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.

IDF organizational structure
The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system. It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.
The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.
Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.

Standardization
The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9. The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot, which was approved by 100% of those voting in a ballot closing on 15 November 2010. The final standard was published on 23 April 2012.DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:

URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.

See also
Notes
References
External links

Official website
Short DOI – DOI Foundation service for converting long DOIs to shorter equivalents
Factsheet: DOI System and Internet Identifier Specifications
CrossRef DOI lookup

Economic order quantity

Economic Order Quantity (EOQ), also known as Financial Purchase Quantity or Economic Buying Quantity (EPQ), is the order quantity that minimizes the total holding costs and ordering costs in inventory management. It is one of the oldest classical production scheduling models. The model was developed by Ford W. Harris in 1913, but R. H. Wilson, a consultant who applied it extensively, and K. Andler are given credit for their in-depth analysis.

Overview
EOQ applies only when demand for a product is constant over the year and each new order is delivered in full when inventory reaches zero. There is a fixed cost for each order placed, regardless of the number of units ordered; an order is assumed to contain only 1 unit. There is also a cost for each unit held in storage, commonly known as holding cost, sometimes expressed as a percentage of the purchase cost of the item. Although the EOQ formulation is straightforward,  factors such as transportation rates and quantity discounts factor into any real-world application.
The EOQ indicates the optimal number of units to order to minimize the total cost associated with the purchase, delivery, and storage of the product.
The required parameters to the solution are the total demand for the year, the purchase cost for each item, the fixed cost to place the order for a single item and the storage cost for each item per year. Note that the number of times an order is placed will also affect the total cost, though this number can be determined from the other parameters.

Variables
T
      
    
    {\displaystyle T}
   = total annual inventory cost

  
    
      
        P
      
    
    {\displaystyle P}
   = purchase unit price, unit production cost

  
    
      
        Q
      
    
    {\displaystyle Q}
   = order quantity

  
    
      
        
          Q
          
            ∗
          
        
      
    
    {\displaystyle Q^{*}}
   = optimal order quantity

  
    
      
        D
      
    
    {\displaystyle D}
   = annual demand quantity

  
    
      
        K
      
    
    {\displaystyle K}
   = fixed cost per order, setup cost (not per unit, typically cost of ordering and shipping and handling. This is not the cost of goods)

  
    
      
        h
      
    
    {\displaystyle h}
   = annual holding cost per unit, also known as carrying cost or storage cost (capital cost, warehouse space, refrigeration, insurance, opportunity cost (price x interest),  etc. usually not related to the unit production cost)

The total cost function and derivation of EOQ formula
The single-item EOQ formula finds the minimum point of the following cost function:
Total Cost = purchase cost or production cost + ordering cost + holding cost
Where:

Purchase cost: This is the variable cost of goods: purchase unit price × annual demand quantity. This is P × D
Ordering cost: This is the cost of placing orders: each order has a fixed cost K, and we need to order D/Q times per year. This is K × D/Q
Holding cost: the average quantity in stock (between fully replenished and empty) is Q/2, so this cost is h × Q/2
  
    
      
        T
        =
        P
        D
        +
        K
        
          
            D
            Q
          
        
        +
        h
        
          
            Q
            2
          
        
      
    
    {\displaystyle T=PD+K{\frac {D}{Q}}+h{\frac {Q}{2}}}
  .To determine the minimum point of the total cost curve, calculate the derivative of the total cost with respect to Q (assume all other variables are constant) and set it equal to 0:

  
    
      
        
          0
        
        =
        −
        
          
            
              D
              K
            
            
              Q
              
                2
              
            
          
        
        +
        
          
            h
            2
          
        
      
    
    {\displaystyle {0}=-{\frac {DK}{Q^{2}}}+{\frac {h}{2}}}
  Solving for Q gives Q* (the optimal order quantity):

  
    
      
        
          Q
          
            ∗
            2
          
        
        =
        
          
            
              2
              D
              K
            
            h
          
        
      
    
    {\displaystyle Q^{*2}={\frac {2DK}{h}}}
  Therefore:

Q* is independent of P; it is a function of only K, D, h.
The optimal value Q* may also be found by recognizing that

  
    
      
        T
        =
        
          
            
              D
              K
            
            Q
          
        
        +
        
          
            
              h
              Q
            
            2
          
        
        +
        P
        D
        =
        
          
            h
            
              2
              Q
            
          
        
        (
        Q
        −
        
          
            2
            D
            K
            
              /
            
            h
          
        
        
          )
          
            2
          
        
        +
        
          
            2
            h
            D
            K
          
        
        +
        P
        D
        ,
      
    
    {\displaystyle T={\frac {DK}{Q}}+{\frac {hQ}{2}}+PD={\frac {h}{2Q}}(Q-{\sqrt {2DK/h}})^{2}+{\sqrt {2hDK}}+PD,}
  where the non-negative quadratic term disappears for 
  
    
      
        Q
        =
        
          
            2
            D
            K
            
              /
            
            h
          
        
        ,
      
    
    {\textstyle Q={\sqrt {2DK/h}},}
   which provides the cost minimum 
  
    
      
        
          T
          
            m
            i
            n
          
        
        =
        
          
            2
            h
            D
            K
          
        
        +
        P
        D
        .
      
    
    {\displaystyle T_{min}={\sqrt {2hDK}}+PD.}

Example
annual requirement quantity (D) = 10000 units
Cost per order (K) = 40
Cost per unit (P)= 50
Yearly carrying cost per unit = 4
Market interest = 2%Economic order quantity = 
  
    
      
        
          
            
              
                2
                D
                ⋅
                K
              
              h
            
          
        
      
    
    {\displaystyle {\sqrt {\frac {2D\cdot K}{h}}}}
   
  
    
      
        =
        
          
            
              
                2
                ⋅
                10000
                ⋅
                40
              
              
                4
                +
                50
                ⋅
                2
                %
              
            
          
        
        =
        
          
            
              
                2
                ⋅
                10000
                ⋅
                40
              
              5
            
          
        
      
    
    {\displaystyle ={\sqrt {\frac {2\cdot 10000\cdot 40}{4+50\cdot 2\%}}}={\sqrt {\frac {2\cdot 10000\cdot 40}{5}}}}
   = 400 units
Number of orders per year (based on EOQ) 
  
    
      
        =
        
          
            10000
            400
          
        
        =
        25
      
    
    {\displaystyle ={\frac {10000}{400}}=25}
  
Total cost 
  
    
      
        =
        P
        ⋅
        D
        +
        K
        (
        D
        
          /
        
        E
        O
        Q
        )
        +
        h
        (
        E
        O
        Q
        
          /
        
        2
        )
      
    
    {\displaystyle =P\cdot D+K(D/EOQ)+h(EOQ/2)}
  
Total cost 
  
    
      
        =
        50
        ⋅
        10000
        +
        40
        ⋅
        (
        10000
        
          /
        
        400
        )
        +
        5
        ⋅
        (
        400
        
          /
        
        2
        )
        =
        502000
      
    
    {\displaystyle =50\cdot 10000+40\cdot (10000/400)+5\cdot (400/2)=502000}
  
If we check the total cost for any order quantity other than 400(=EOQ), we will see that the cost is higher.  For instance, supposing 500 units per order, then
Total cost 
  
    
      
        =
        50
        ⋅
        10000
        +
        40
        ⋅
        (
        10000
        
          /
        
        500
        )
        +
        5
        ⋅
        (
        500
        
          /
        
        2
        )
        =
        502050
      
    
    {\displaystyle =50\cdot 10000+40\cdot (10000/500)+5\cdot (500/2)=502050}
  
Similarly, if we choose 300 for the order quantity, then
Total cost 
  
    
      
        =
        50
        ⋅
        10000
        +
        40
        ⋅
        (
        10000
        
          /
        
        300
        )
        +
        5
        ⋅
        (
        300
        
          /
        
        2
        )
        =
        502083.33
      
    
    {\displaystyle =50\cdot 10000+40\cdot (10000/300)+5\cdot (300/2)=502083.33}
  
This illustrates that the economic order quantity is always in the best interests of the firm.

Extensions of the EOQ model
Quantity discounts
An important extension to the EOQ model is to accommodate quantity discounts. There are two main types of quantity discounts: (1) all-units and (2) incremental. Here is a numerical example:

Incremental unit discount: Units 1–100 cost $30 each; Units 101–199 cost $28 each; Units 200 and up cost $26 each. So when 150 units are ordered, the total cost is $30*100 + $28*50.
All units discount: an order of 1–1000 units costs $50 each; an order of 1001–5000 units costs $45 each; an order of more than 5000 units costs $40 each. So when 1500 units are ordered, the total cost is $45*1500.In order to find the optimal order quantity under different quantity discount schemes, one should use algorithms; these algorithms are developed under the assumption that the EOQ policy is still optimal with quantity discounts. Perera et al. (2017) establish this optimality and fully characterize the (s,S) optimality within the EOQ setting under general cost structures.

Design of optimal quantity discount schedules
In presence of a strategic customer, who responds optimally to discount schedules,  the design of an optimal quantity discount scheme by the supplier is complex and has to be done carefully.  This is particularly so when the demand at the customer is itself uncertain. An interesting effect called the "reverse bullwhip" takes place where an increase in consumer demand uncertainty actually reduces order quantity uncertainty at the supplier.

Backordering costs and multiple items
Several extensions can be made to the EOQ model, including backordering costs and multiple items. In the case backorders are permitted, the inventory carrying costs per cycle are:

  
    
      
        I
        C
        
          ∫
          
            0
          
          
            
              T
              
                1
              
            
          
        
        (
        Q
        −
        s
        −
        λ
        t
        )
        
        d
        t
        =
        
          
            
              I
              C
            
            
              2
              λ
            
          
        
        (
        Q
        −
        s
        
          )
          
            2
          
        
        ,
      
    
    {\displaystyle IC\int \limits _{0}^{T_{1}}(Q-s-\lambda t)\,dt={\frac {IC}{2\lambda }}(Q-s)^{2},}
  where s is the number of backorders when order quantity Q is delivered and 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   is the rate of demand. The backorder cost per cycle is:

  
    
      
        π
        s
        +
        
          
            
              π
              ^
            
          
        
        
          ∫
          
            0
          
          
            
              T
              
                2
              
            
          
        
        λ
        t
        d
        t
        =
        π
        s
        +
        
          
            1
            2
          
        
        
          
            
              π
              ^
            
          
        
        λ
        
          T
          
            2
          
          
            2
          
        
        =
        π
        s
        +
        
          
            
              
                
                  
                    π
                    ^
                  
                
              
              
                s
                
                  2
                
              
            
            
              2
              λ
            
          
        
        ,
      
    
    {\displaystyle \pi s+{\hat {\pi }}\int \limits _{0}^{T_{2}}\lambda tdt=\pi s+{\frac {1}{2}}{\hat {\pi }}\lambda T_{2}^{2}=\pi s+{\frac {{\hat {\pi }}s^{2}}{2\lambda }},}
  where 
  
    
      
        π
      
    
    {\displaystyle \pi }
   and 
  
    
      
        
          
            
              π
              ^
            
          
        
      
    
    {\displaystyle {\hat {\pi }}}
   are backorder costs, 
  
    
      
        
          T
          
            2
          
        
        =
        T
        −
        
          T
          
            1
          
        
      
    
    {\displaystyle T_{2}=T-T_{1}}
  , T being the cycle length and 
  
    
      
        
          T
          
            1
          
        
        =
        (
        Q
        −
        s
        )
        
          /
        
        λ
      
    
    {\displaystyle T_{1}=(Q-s)/\lambda }
  . The average annual variable cost is the sum of order costs, holding inventory costs and backorder costs:

  
    
      
        
          
            K
          
        
        =
        
          
            λ
            Q
          
        
        A
        +
        
          
            1
            
              2
              Q
            
          
        
        I
        C
        (
        Q
        −
        s
        
          )
          
            2
          
        
        +
        
          
            1
            Q
          
        
        [
        π
        λ
        s
        +
        
          
            1
            2
          
        
        
          
            
              π
              ^
            
          
        
        
          s
          
            2
          
        
        ]
      
    
    {\displaystyle {\mathcal {K}}={\frac {\lambda }{Q}}A+{\frac {1}{2Q}}IC(Q-s)^{2}+{\frac {1}{Q}}[\pi \lambda s+{\frac {1}{2}}{\hat {\pi }}s^{2}]}
  To minimize 
  
    
      
        
          
            K
          
        
      
    
    {\displaystyle {\mathcal {K}}}
   impose the partial derivatives equal to zero:

  
    
      
        
          
            
              ∂
              
                
                  K
                
              
            
            
              ∂
              Q
            
          
        
        =
        −
        
          
            1
            
              Q
              
                2
              
            
          
        
        
          [
          
            
              λ
            
            A
            +
            
              
                1
                2
              
            
            I
            C
            (
            Q
            −
            s
            
              )
              
                2
              
            
            +
            π
            λ
            s
            +
            
              
                1
                2
              
            
            
              
                
                  π
                  ^
                
              
            
            
              s
              
                2
              
            
          
          ]
        
        +
        
          
            
              I
              C
            
            Q
          
        
        (
        Q
        −
        s
        )
        =
        0
      
    
    {\displaystyle {\frac {\partial {\mathcal {K}}}{\partial Q}}=-{\frac {1}{Q^{2}}}\left[{\lambda }A+{\frac {1}{2}}IC(Q-s)^{2}+\pi \lambda s+{\frac {1}{2}}{\hat {\pi }}s^{2}\right]+{\frac {IC}{Q}}(Q-s)=0}
  

  
    
      
        
          
            
              ∂
              
                
                  K
                
              
            
            
              ∂
              s
            
          
        
        =
        −
        
          
            
              I
              C
            
            Q
          
        
        (
        Q
        −
        s
        )
        +
        
          
            1
            Q
          
        
        π
        λ
        +
        
          
            1
            Q
          
        
        
          
            
              π
              ^
            
          
        
        s
        =
        0
      
    
    {\displaystyle {\frac {\partial {\mathcal {K}}}{\partial s}}=-{\frac {IC}{Q}}(Q-s)+{\frac {1}{Q}}\pi \lambda +{\frac {1}{Q}}{\hat {\pi }}s=0}
  Substituting the second equation into the first gives the following quadratic equation:

  
    
      
        [
        
          
            
              
                π
                ^
              
            
          
          
            2
          
        
        +
        
          
            
              π
              ^
            
          
        
        I
        C
        ]
        
          s
          
            2
          
        
        +
        2
        π
        
          
            
              π
              ^
            
          
        
        λ
        s
        +
        (
        π
        λ
        
          )
          
            2
          
        
        −
        2
        λ
        A
        I
        C
        =
        0
      
    
    {\displaystyle [{\hat {\pi }}^{2}+{\hat {\pi }}IC]s^{2}+2\pi {\hat {\pi }}\lambda s+(\pi \lambda )^{2}-2\lambda AIC=0}
  If 
  
    
      
        
          
            
              π
              ^
            
          
        
        =
        0
      
    
    {\displaystyle {\hat {\pi }}=0}
   either s=0 or 
  
    
      
        s
        =
        ∞
      
    
    {\displaystyle s=\infty }
   is optimal. In the first case the optimal lot is given by the classic EOQ formula, in the second case an order is never placed and minimum yearly cost is given by 
  
    
      
        π
        λ
      
    
    {\displaystyle \pi \lambda }
  . If 
  
    
      
        π
        >
        
          
            
              
                2
                A
                I
                C
              
              λ
            
          
        
        =
        δ
      
    
    {\displaystyle \pi >{\sqrt {\frac {2AIC}{\lambda }}}=\delta }
   or 
  
    
      
        π
        λ
        >
        
          K
          
            w
          
        
      
    
    {\displaystyle \pi \lambda >K_{w}}
   
  
    
      
        
          s
          
            ∗
          
        
        =
        0
      
    
    {\displaystyle s^{*}=0}
   is optimal, if 
  
    
      
        π
        <
        δ
      
    
    {\displaystyle \pi <\delta }
   then there shouldn't be any inventory system. If 
  
    
      
        
          
            
              π
              ^
            
          
        
        ≠
        0
      
    
    {\displaystyle {\hat {\pi }}\neq 0}
   solving the preceding quadratic equation yields:

  
    
      
        
          s
          
            ∗
          
        
        =
        [
        
          
            
              π
              ^
            
          
        
        +
        I
        C
        
          ]
          
            −
            1
          
        
        
          (
          
            −
            π
            λ
            +
            
              
                [
                
                  (
                  2
                  λ
                  A
                  I
                  C
                  )
                  
                    (
                    
                      1
                      +
                      
                        
                          
                            I
                            C
                          
                          
                            
                              π
                              ^
                            
                          
                        
                      
                    
                    )
                  
                  −
                  
                    
                      
                        I
                        C
                      
                      
                        
                          π
                          ^
                        
                      
                    
                  
                  (
                  π
                  λ
                  
                    )
                    
                      2
                    
                  
                
                ]
              
              
                1
                
                  /
                
                2
              
            
          
          )
        
      
    
    {\displaystyle s^{*}=[{\hat {\pi }}+IC]^{-1}\left(-\pi \lambda +\left[(2\lambda AIC)\left(1+{\frac {IC}{\hat {\pi }}}\right)-{\frac {IC}{\hat {\pi }}}(\pi \lambda )^{2}\right]^{1/2}\right)}
  

  
    
      
        
          Q
          
            ∗
          
        
        =
        
          
            [
            
              
                
                  
                    
                      
                        π
                        ^
                      
                    
                  
                  +
                  I
                  C
                
                
                  
                    π
                    ^
                  
                
              
            
            ]
          
          
            1
            
              /
            
            2
          
        
        
          
            [
            
              
                
                  
                    2
                    λ
                    A
                  
                  
                    I
                    C
                  
                
              
              −
              
                
                  
                    (
                    π
                    λ
                    
                      )
                      
                        2
                      
                    
                  
                  
                    I
                    C
                    (
                    
                      
                        
                          π
                          ^
                        
                      
                    
                    +
                    I
                    C
                    )
                  
                
              
            
            ]
          
          
            1
            
              /
            
            2
          
        
      
    
    {\displaystyle Q^{*}=\left[{\frac {{\hat {\pi }}+IC}{\hat {\pi }}}\right]^{1/2}\left[{\frac {2\lambda A}{IC}}-{\frac {(\pi \lambda )^{2}}{IC({\hat {\pi }}+IC)}}\right]^{1/2}}
  If there are backorders the reorder point is: 
  
    
      
        
          r
          
            h
          
          
            ∗
          
        
        =
        μ
        −
        m
        
          Q
          
            ∗
          
        
        −
        
          s
          
            ∗
          
        
      
    
    {\displaystyle r_{h}^{*}=\mu -mQ^{*}-s^{*}}
  ; with m being the largest integer 
  
    
      
        m
        ≤
        
          
            τ
            T
          
        
      
    
    {\displaystyle m\leq {\frac {\tau }{T}}}
   and μ the lead time demand.
Additionally, the economic order interval can be determined from the EOQ and the economic production quantity model (which determines the optimal production quantity) can be determined in a similar fashion.
A version of the model, the Baumol-Tobin model, has also been used to determine the money demand function, where a person's holdings of money balances can be seen in a way parallel to a firm's holdings of inventory.Malakooti (2013) has introduced the multi-criteria EOQ models where the criteria could be minimizing the total cost, Order quantity (inventory), and Shortages.
A version taking the time-value of money into account was developed by Trippi and Lewin.

Imperfect quality
Another important extension of the EOQ model is to consider items with imperfect quality. Salameh and Jaber (2000) are the first to study the imperfect items in an EOQ model very thoroughly. They consider an inventory problem in which the demand is deterministic and there is a fraction of imperfect items in the lot and are screened by the buyer and sold by them at the end of the circle at discount price.

See also
Reorder point
Safety stock
Constant fill rate for the part being produced: Economic production quantity
Orders placed at regular intervals: Fixed time period model
Demand is random: classical Newsvendor model
Demand varies over time: Dynamic lot size model
Several products produced on the same machine: Economic lot scheduling problem
Renewal Demand and (s, S) Optimality by Perera, Janakiraman, and Niu [1]

References
Further reading
Harris, Ford W. Operations Cost (Factory Management Series), Chicago:  Shaw (1915)
Harris, Ford W. (1913). "How many parts to make at once". Factory, the Magazine of Management. 10: 135–136, 152.
Camp, W. E. "Determining the production order quantity", Management Engineering, 1922
Wilson, R. H. (1934). "A Scientific Routine for Stock Control". Harvard Business Review. 13: 116–28.
Plossel, George. Orlicky's Material Requirement's Planning. Second Edition. McGraw Hill. 1984. (first edition 1975)
Erlenkotter, Donald (2014). "Ford Whitman Harris's economical lot size model". International Journal of Production Economics. 155: 12–15. doi:10.1016/j.ijpe.2013.12.008. S2CID 153794306.
Perera, Sandun; Janakiraman, Ganesh; Niu, Shun-Chen (2017). "Optimality of (s,S) policies in EOQ models with general cost structures". International Journal of Production Economics. 187: 216–228. doi:10.1016/j.ijpe.2016.09.017.
Perera, Sandun; Janakiraman, Ganesh; Niu, Shun-Chen (2018). "Optimality of (s, S) Inventory Policies under Renewal Demand and General Cost Structures". Production and Operations Management. 27 (2): 368–383. doi:10.1111/poms.12795. hdl:2027.42/142450.
Tsan-Ming Choi (Ed.)  Handbook of EOQ Inventory Problems: Stochastic and Deterministic Models and Applications, Springer's International Series in Operations Research and Management Science, 2014. doi:10.1007/978-1-4614-7639-9.
Ventura, Robert; Samuel, Stephen (2016). "Optimization of fuel injection in GDI engine using economic order quantity and Lambert W function". Applied Thermal Engineering. 101: 112–20. doi:10.1016/j.applthermaleng.2016.02.024.

External links
The EOQ Model
http://www.inventoryops.com/economic_order_quantity.htm
http://www.scmfocus.com/supplyplanning/2014/04/10/economic-order-quantity-calculator/

Economic value added

In accounting, as part of financial statements analysis, economic value added is an estimate of a firm's economic profit, or the value created in excess of the required return of the company's shareholders. EVA is the net profit less the capital charge ($) for raising the firm's capital. The idea is that value is created when the return on the firm's economic capital employed exceeds the cost of that capital. This amount can be determined by making adjustments to GAAP accounting. There are potentially over 160 adjustments but in practice, only several key ones are made, depending on the company and its industry.

Calculation
EVA is net operating profit after taxes (or NOPAT) less a capital charge, the latter being the product of the cost of capital and the economic capital. The basic formula is:

  
    
      
        
          
            
              
                
                  EVA
                
              
              
                
                =
                (
                
                  ROIC
                
                −
                
                  WACC
                
                )
                ⋅
                (
                
                  total assets
                
                −
                
                  current liability
                
                )
              
            
            
              
              
                
                =
                
                  NOPAT
                
                −
                
                  WACC
                
                ⋅
                (
                
                  total assets
                
                −
                
                  current liability
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{EVA}}&=({\text{ROIC}}-{\text{WACC}})\cdot ({\text{total assets}}-{\text{current liability}})\\[8pt]&={\text{NOPAT}}-{\text{WACC}}\cdot ({\text{total assets}}-{\text{current liability}})\end{aligned}}}
  where:

  
    
      
        
          ROIC
        
        =
        
          
            NOPAT
            
              
                total assets
              
              −
              
                current liability
              
            
          
        
      
    
    {\displaystyle {\text{ROIC}}={\frac {\text{NOPAT}}{{\text{total assets}}-{\text{current liability}}}}}
   is the return on invested capital;

  
    
      
        (
        
          WACC
        
        )
        
      
    
    {\displaystyle ({\text{WACC}})\,}
   is the weighted average cost of capital (WACC);

  
    
      
        (
        
          total assets
        
        −
        
          current liability
        
        )
        
      
    
    {\displaystyle ({\text{total assets}}-{\text{current liability}})\,}
  is the economic capital employed (total assets − current liability);
NOPAT is the net operating profit after tax, with adjustments and translations, generally for the amortization of goodwill, the capitalization of brand advertising and other non-cash items.EVA calculation:
EVA = net operating profit after taxes – a capital charge [the residual income method]
therefore EVA = NOPAT – (c × capital), or alternatively

EVA = (r × capital) – (c × capital) so thatEVA = (r − c) × capital [the spread method, or excess return method]where

r = rate of return, and
c = cost of capital, or the weighted average cost of capital (WACC).NOPAT is profits derived from a company's operations after cash taxes but before financing costs and non-cash bookkeeping entries. It is the total pool of profits available to provide a cash return to those who provide capital to the firm.
Capital is the amount of cash invested in the business, net of depreciation. It can be calculated as the sum of interest-bearing debt and equity or as the sum of net assets less non-interest-bearing current liabilities (NIBCLs).
The capital charge is the cash flow required to compensate investors for the riskiness of the business given the amount of economic capital invested.
The cost of capital is the minimum rate of return on capital required to compensate investors (debt and equity) for bearing risk, their opportunity cost.
Another perspective on EVA can be gained by looking at a firm's return on net assets (RONA). RONA is a ratio that is calculated by dividing a firm's NOPAT by the amount of capital it employs (RONA = NOPAT/Capital) after making the necessary adjustments to the data reported by a conventional financial accounting system.

EVA = (RONA – required minimum return) × net investmentsIf RONA is above the threshold rate, EVA is positive.

Comparison with other approaches
Other approaches along similar lines include residual income valuation (RI) and residual cash flow. Although EVA is similar to residual income, under some definitions there may be minor technical differences between EVA and RI (for example, adjustments that might be made to NOPAT before it is suitable for the formula below). Residual cash flow is another, much older term for economic profit. In all three cases, money cost of capital refers to the amount of money rather than the proportional cost (% cost of capital); at the same time, the adjustments to NOPAT are unique to EVA.
Although in concept, these approaches are in a sense nothing more than the traditional, commonsense idea of "profit", the utility of having a more precise term such as EVA is that it makes a clear separation from dubious accounting adjustments that have enabled businesses such as Enron to report profits while actually approaching insolvency.
Other measures of shareholder value include:

Added value
Market value added
Total shareholder return

Market value added
The firm's market value added, is the added value an investment creates for its shareholders over the total capital invested by them. MVA is the discounted sum (present value) of all future expected economic value added:

  
    
      
        
          MVA
        
        =
        V
        −
        
          K
          
            0
          
        
        =
        
          ∑
          
            t
            =
            1
          
          
            ∞
          
        
        
          
            
              
                EVA
              
              
                t
              
            
            
              (
              1
              +
              c
              
                )
                
                  t
                
              
            
          
        
      
    
    {\displaystyle {\text{MVA}}=V-K_{0}=\sum _{t=1}^{\infty }{{\text{EVA}}_{t} \over (1+c)^{t}}}
  Note that MVA = PV of EVA.
More enlightening is that since MVA = NPV of free cash flow (FCF) it follows therefore that the

NPV of FCF = PV of EVA;since after all, EVA is simply the re-arrangement of the FCF formula.

Process-based costing
In 2012, Mocciaro Li Destri, Picone and Minà proposed a performance and cost measurement system that integrates the EVA criteria with process-based costing (PBC). The EVA-PBC methodology allows us to implement the EVA management logic not only at the firm level but also at lower levels of the organization. EVA-PBC methodology plays an interesting role in bringing strategy back into financial performance measures.

See also
Business valuation
Enterprise value
Opportunity cost
Value added

References
Further reading
G. Bennett Stewart III (2013). Best-Practice EVA. John Wiley & Sons.
G. Bennett Stewart III (1991). The Quest for Value. HarperCollins.
Erik Stern. The Value Mindset. Wiley.
Joel Stern and John Shiely (2001). The EVA Challenge. Wiley. ISBN 9780471405559.
Al Ehrbar (1988). EVA, the Real Key to Creating Wealth. Wiley.
E. Knappek. An hitchhiker guide to having fun times with Eva. Limited Edition.

External links
Stern Value Management Proprietary Tools
A Reading List on EVA/Value Based Management from Robert Korajczyk
Economic Value Added (EVA), Prof. Aswath Damodaran
EVA-WACC Tree Model Infographic Visual.ly
Economic Value Added: A simulation analysis of the trendy, owner-oriented management tool, Timo Salmi and Ilkka Virtanen, 2001
The Origins of EVA Chicago-Booth magazine
New Media video, by Florencia Roca: Interview to Joel Stern What is EVA?
EVA Calculator

Efficiency movement

The efficiency movement was a major movement in the United States, Britain and other industrial nations in the early 20th century that sought to identify and eliminate waste in all areas of the economy and society, and to develop and implement best practices. The concept covered mechanical, economic, social, and personal improvement. The quest for efficiency promised effective, dynamic management rewarded by growth.As a result of the influence of an early proponent, it is more often known as Taylorism.

United States
The efficiency movement played a central role in the Progressive Era in the United States, where it flourished 1890–1932. Adherents argued that all aspects of the economy, society and government were riddled with waste and inefficiency. Everything would be better if experts identified the problems and fixed them. The result was strong support for building research universities and schools of business and engineering, municipal research agencies, as well as reform of hospitals and medical schools, and the practice of farming. Perhaps the best known leaders were engineers Frederick Winslow Taylor (1856–1915), who used a stopwatch to identify the smallest inefficiencies, and Frank Bunker Gilbreth Sr. (1868–1924) who proclaimed there was always "one best way" to fix a problem.
Leaders including Herbert Croly, Charles R. van Hise, and Richard Ely sought to improve governmental performance by training experts in public service comparable to those in Germany, notably at the Universities of Wisconsin and Pennsylvania. Schools of business administration set up management programs oriented toward efficiency.

Municipal and state efficiency
Many cities set up "efficiency bureaus" to identify waste and apply the best practices.  For example, Chicago created an Efficiency Division (1910–16) within the city government's Civil Service Commission, and private citizens organized the Chicago Bureau of Public Efficiency (1910–32).  The former pioneered the study of "personal efficiency," measuring employees' performance through new scientific merit systems and efficiency movement State governments were active as well. For example, Massachusetts set up its "Commission on Economy and Efficiency" in 1912.  It made hundreds of recommendations.

Philanthropy
Leading philanthropists such as Andrew Carnegie and John D. Rockefeller actively promoted the efficiency movement.  In his many philanthropic pursuits, Rockefeller believed in supporting efficiency.  He said,

To help an inefficient, ill-located, unnecessary school is a waste ...it is highly probable that enough money has been squandered on unwise educational projects to have built up a national system of higher education adequate to our needs, if the money had been properly directed to that end.

Conservation
The conservation movement regarding national resources came to prominence during the Progressive Era.  According to historian Samuel P. Hays, the conservation movement was based on the "gospel of the efficiency".The Massachusetts Commission on Economy and Efficiency reflected the new concern with conservation. It said in 1912:

 The only proper basis for the protection of game birds, wild fowl and, indeed, all animals is an economic one, and must be based upon carefully constructed and properly enforced laws for the conservation of all species for the benefit of future generations of our citizens, rather than based on local opinion. …This expenditure for the protection of fish and game is clearly a wise economy, tending to prevent the annihilation of birds and other animals valuable to mankind which might otherwise become extinct.  It may be said that Massachusetts and her sister States have suffered irreparable loss by carelessly allowing, for generations past, indiscriminate waste of animal life.
President Roosevelt was the nation's foremost conservationist, putting the issue high on the national agenda by emphasizing the need to eliminate wasteful uses of limited natural resources. He worked with all the major figures of the movement, especially his chief advisor on the matter,  Gifford Pinchot.  Roosevelt was deeply committed to conserving natural resources, and is considered to be the nation's first conservation President.
In 1908, Roosevelt sponsored the Conference of Governors held in the White House, with a focus on natural resources and their most efficient use. Roosevelt delivered the opening address: "Conservation as a National Duty".
In contrast, environmentalist John Muir promulgated a very different view of conservation, rejecting the efficiency motivation.  Muir instead preached that nature was sacred and humans are intruders who should look but not develop. Working through the Sierra Club he founded, Muir tried to minimize commercial use of water resources and forests.  While Muir wanted nature preserved for the sake of pure beauty, Roosevelt subscribed to Pinchot's formulation, "to make the forest produce the largest amount of whatever crop or service will be most useful, and keep on producing it for generation after generation of men and trees."

National politics
In U.S. national politics, the most prominent figure was Herbert Hoover, a trained engineer who played down politics and believed dispassionate, nonpolitical experts could solve the nation's great problems, such as ending poverty.After 1929, Democrats blamed the Great Depression on Hoover and helped to somewhat discredit the movement.

Antitrust
Boston lawyer Louis Brandeis (1856–1941) argued bigness conflicted with efficiency and added a new political dimension to the Efficiency Movement. For instance, while fighting against legalized price fixing, Brandeis launched an effort to influence congressional policymaking with the help of his friend Norman Hapgood, who was then the editor of Harper's Weekly. He coordinated the publication of a series of articles (Competition Kills, Efficiency and the One-Price Article, and How Europe deals with the one-price goods), which were also distributed by the lobbying group American Fair Trade League to legislators, Supreme Court justices, governors, and twenty national magazines. For his works, he was asked to speak before a congressional committee considering the price-fixing bill he drafted. Here, he stated that "big business is not more efficient than little business" and that "it is a mistake to suppose that the department stores can do business cheaper than the little dealer." Brandeis ideas on which business is most efficient conflicted with Croly's positions, which favored efficiency driven by a kind of consolidation gained through large-scale economic operations.As early as 1895 Brandeis had warned of the harm that giant corporations could do to competitors, customers, and their own workers. The growth of industrialization was creating mammoth companies which he felt threatened the well-being of millions of Americans.  In The Curse of Bigness he argued, "Efficiency means greater production with less effort and at less cost, through the elimination of unnecessary waste, human and material. How else can we hope to attain our social ideals?" He also argued against an appeal to Congress by the state-regulated railroad industry in 1910 seeking an increase in rates. Brandeis explained that instead of passing along increased costs to the consumer, the railroads should pursue efficiency by reducing their overhead and streamlining their operations, initiatives that were unprecedented during the time.

Bedaux system
The Bedaux system, developed by Franco-American management consultant Charles Bedaux (1886–1944) built on the work of F. W. Taylor and Charles E. Knoeppel.Its distinctive advancement beyond these earlier thinkers was the Bedaux Unit or B, a universal measure for all manual work.The Bedaux System was influential in the United States in the 1920s and Europe in the 1930s and 1940s, especially in Britain.From the 1920s to the 1950s there were about one thousand companies in 21 countries worldwide that were run on the Bedaux System, including giants such as Swift's, Eastman Kodak, B.F. Goodrich, DuPont, Fiat, ICI and General Electric.

Relation to other movements
Later movements had echoes of the Efficiency Movement and were more directly inspired by Taylor and Taylorism.  Technocracy, for instance, and others flourished in the 1930s and 1940s.
Postmodern opponents of nuclear energy in the 1970s broadened their attack to try to discredit movements that saw salvation for human society in technical expertise alone, or which held that scientists or engineers had any special expertise to offer in the political realm.
Coming into usage in 1990, the Western term lean manufacturing (lean enterprise, lean production, or simply "lean") refers to a business idea that considered the expenditure of resources for anything other than the creation of value for the end customer to be wasteful, and thus a target for elimination. Today the Lean concept is broadening to include a greater range of strategic goals, not just cost-cutting and efficiency.

Britain
In engineering, the concept of efficiency was developed in Britain in the mid-18th century by John Smeaton (1724–1792). Called the "father of civil engineering", he studied water wheels and steam engines.  In the late 19th century there was much talk about improving the efficiency of the administration and economic performance of the British Empire.National Efficiency was an attempt to discredit the old-fashioned habits, customs and institutions that put the British at a handicap in competition with the world, especially with Germany, which was seen as the epitome of efficiency. In the early 20th century, "National Efficiency" became a powerful demand — a  movement supported by prominent figures across the political spectrum who disparaged sentimental humanitarianism and identified waste as a mistake that could no longer be tolerated.  The movement took place in two waves; the first wave from 1899 to 1905 was made urgent by the inefficiencies and failures in the Second Boer War (1899–1902).  Spectator magazine reported in 1902 there was "a universal outcry for efficiency in all departments of society, in all aspects of life".  The two most important themes were technocratic efficiency and managerial efficiency.  As White (1899) argued vigorously, the empire needed to be put on a business footing and administered to get better results.  The looming threat of Germany, which was widely seen as a much more efficient nation, added urgency after 1902.  Politically National Efficiency brought together modernizing Conservatives and Unionists, Liberals who wanted to modernize their party, and Fabians such as George Bernard Shaw and H. G. Wells, along with Beatrice and Sidney Webb, who had outgrown socialism and saw the utopia of a scientifically up-to-date society supervised by experts such as themselves.  Churchill in 1908 formed an alliance with the Webbs, announcing the goal of a "National Minimum", covering hours, working conditions, and wages – it was a safety net below which the individual would not be allowed to fall.Representative legislation included the Education Act 1902, which emphasized the role of experts in the schools system.  Higher education was an important initiative, typified by the growth of the London School of Economics, and the foundation of Imperial College.There was a pause in the movement between 1904 and 1909, when interest resumed.  The most prominent new leaders included Liberals Winston Churchill and David Lloyd George, whose influence brought a bundle of reform legislation that introduced the welfare state to Britain.
Much of the popular and elite support for National Efficiency grew out of concern for Britain's military position, especially with respect to Germany.  The Royal Navy underwent a dramatic modernization, most famously in the introduction of the Dreadnought, which in 1906 revolutionized naval warfare overnight.

Germany
In Germany the efficiency movement was called "rationalization" and it was a powerful social and economic force before 1933. In part it looked explicitly at American models, especially Fordism. The Bedaux system was widely adopted in the rubber and tire industry, despite strong resistance in the socialist labor movement to the Bedaux system. Continental AG, the leading rubber company in Germany, adopted the system and profited heavily from it, thus surviving the Great Depression relatively undamaged and improving its competitive capabilities. However most German businessmen preferred the home-grown REFA system which focused on the standardization of working conditions, tools, and machinery."Rationalization" meant higher productivity and greater efficiency, promising science would bring prosperity. More generally it promised a new level of modernity and was applied to economic production and consumption as well as public administration. Various versions of rationalization were promoted by industrialists and Social Democrats, by engineers and architects, by educators and academics, by middle class feminists and social workers, by government officials and politicians of many parties. It was ridiculed by the extremists in the Communist movement. As ideology and practice, rationalization challenged and transformed not only machines, factories, and vast business enterprises but also the lives of middle-class and working-class Germans.

Soviet Union
Ideas of Science Management was very popular in the Soviet Union. One of the leading theorists and practitioners of the Scientific Management in Soviet Russia was Alexei Gastev. The Central Institute of Labour (Tsentralnyi Institut Truda, or TsIT), founded by Gastev in 1921 with Vladimir Lenin's support, was a veritable citadel of socialist Taylorism.
Fascinated by Taylorism and Fordism, Gastev has led a popular movement for the “scientific organization of labor” (Nauchnaya Organizatsiya Truda, or NOT).
Because of its emphasis on the cognitive components of labor, some scholars consider Gastev's NOT to represent a Marxian variant of cybernetics. As with the concept of 'Organoprojection' (1919) by Pavel Florensky, underlying Nikolai Bernstein and Gastev's approach, lay a powerful man-machine metaphor.

Japan
W. Edwards Deming (1900–1993) brought the efficiency movement to Japan after World War II, teaching top management how to improve design (and thus service), product quality, testing and sales (the last through global markets), especially using statistical methods.  Deming then brought his methods back to the U.S. in the form of quality control called continuous improvement process.

Notes
Bibliography
Alexander, Jennifer K. The Mantra of Efficiency: From Waterwheel to Social Control, (2008), international perspective excerpt and text search
Bruce, Kyle, and Chris Nyland. "Scientific Management, Institutionalism, and Business Stabilization: 1903–1923," Journal of Economic Issues  Vol. 35, No. 4 (Dec., 2001), pp. 955–978 in JSTOR
Chandler, Alfred D. Jr. The Visible Hand: The Managerial Revolution in American Business (1977)
Fry, Brian R. Mastering Public Administration: From Max Weber to Dwight Waldo (1989) online edition
Hays, Samuel P. Conservation and the Gospel of Efficiency: The Progressive Conservation Movement 1890–1920 (1959).
Haber, Samuel. Efficiency and Uplift: Scientific Management in the Progressive Era, 1890–1920 (1964)
Hawley, Ellis W. "Herbert Hoover, the Commerce Secretariat, and the vision of the 'Associative State'." Journal of American History, (1974) 61: 116–140. in JSTOR
Jensen, Richard. "Democracy, Republicanism and Efficiency:  The Values of American Politics, 1885–1930," in Byron Shafer and Anthony Badger, eds, Contesting Democracy: Substance and Structure in American Political History, 1775–2000 (U of Kansas Press, 2001) pp 149–180; online version
Jordan, John M. Machine-Age Ideology: Social Engineering and American Liberalism, 1911–1939 (1994).
Kanigel, Robert. The One Best Way: Frederick Winslow Taylor and the Enigma of Efficiency. (Penguin, 1997).
Knoedler; Janet T. "Veblen and Technical Efficiency," Journal of Economic Issues, Vol. 31, 1997
Knoll, Michael: From Kidd to Dewey: The Origin and Meaning of Social Efficiency. Journal of Curriculum Studies  41 (June 2009), No. 3, pp. 361–391.
Lamoreaux, Naomi and Daniel M. G. Raft eds. Coordination and Information: Historical Perspectives on the Organization of Enterprise University of Chicago Press, 1995
Lee, Mordecai. Bureaus of Efficiency: Reforming Local Government in the Progressive Era (Marquette University Press, 2008) ISBN 978-0-87462-081-8
Merkle, Judith A. Management and Ideology: The Legacy of the International Scientific Management Movement (1980)
Nelson, Daniel. Frederick W. Taylor and the Rise of Scientific Management (1980).
Nelson, Daniel. Managers and Workers: Origins of the Twentieth-Century Factory System in the United States, 1880–1920 2d ed. (1995).
Noble, David F. America by Design (1979).
Nolan, Mary.  Visions of Modernity: American Business and the Modernization of Germany (1995)
Nolan, Mary. "Housework Made Easy: the Taylorized Housewife in Weimar Germany's Rationalized Economy," Feminist Studies. (1975) Volume: 16. Issue: 3. pp 549+
Searle, G. R.  The quest for national efficiency: a study in British politics and political thought, 1899–1914 (1971)
Stillman II, Richard J. Creating the American State: The Moral Reformers and the Modern Administrative World They Made (1998) online edition

Primary sources
Dewey, Melville. "Efficiency Society" Encyclopedia Americana (1918)online vol 9 p 720
Emerson, Harrington, "Efficiency Engineering" Encyclopedia Americana (1918) online vol 9 pp 714–20
Taylor, Frederick Winslow Principles of Scientific Management (1913)  online edition
Taylor, Frederick Winslow. Scientific Management: Early Sociology of Management and Organizations (2003), reprints Shop Management (1903), The Principles of Scientific Management (1911) and Testimony Before the Special House Committee (1912).
White, Arnold. Efficiency and empire (1901) online edition, influential study regarding the British Empire

Enterprise resource planning

Enterprise resource planning (ERP) is the integrated management of main business processes, often in real-time and mediated by software and technology. ERP is usually referred to as a category of business management software—typically a suite of integrated applications—that an organization can use to collect, store, manage and interpret data from many business activities. ERP systems can be local-based or cloud-based. Cloud-based applications have grown in recent years due to the increased efficiencies arising from information being readily available from any location with Internet access.
ERP provides an integrated and continuously updated view of core business processes using common databases maintained by a database management system. ERP systems track business resources—cash, raw materials, production capacity—and the status of business commitments: orders, purchase orders, and payroll. The applications that make up the system share data across various departments (manufacturing, purchasing, sales, accounting, etc.) that provide the data. ERP facilitates information flow between all business functions and manages connections to outside stakeholders.According to Gartner, the global ERP market size is estimated at $35 billion in 2021. Though early ERP systems focused on large enterprises, smaller enterprises increasingly use ERP systems.The ERP system integrates varied organizational systems and facilitates error-free transactions and production, thereby enhancing the organization's efficiency. However, developing an ERP system differs from traditional system development.
ERP systems run on a variety of computer hardware and network configurations, typically using a database as an information repository.

Origin
The Gartner Group first used the acronym ERP in the 1990s to include the capabilities of material requirements planning (MRP), and the later manufacturing resource planning (MRP II), as well as computer-integrated manufacturing. Without replacing these terms, ERP came to represent a larger whole that reflected the evolution of application integration beyond manufacturing.Not all ERP packages are developed from a manufacturing core; ERP vendors variously began assembling their packages with finance-and-accounting, maintenance, and human-resource components. By the mid-1990s ERP systems addressed all core enterprise functions. Governments and non–profit organizations also began to use ERP systems. An "ERP system selection methodology" is a formal process for selecting an enterprise resource planning (ERP) system. Existing methodologies include: Kuiper's funnel method, Dobrin's three-dimensional (3D) web-based decision support tool, and the Clarkston Potomac methodology.

Expansion
ERP systems experienced rapid growth in the 1990s. Because of the year 2000 problem many companies took the opportunity to replace their old systems with ERP.ERP systems initially focused on automating back office functions that did not directly affect customers and the public. Front office functions, such as customer relationship management (CRM), dealt directly with customers, or e-business systems such as e-commerce and e-government—or supplier relationship management (SRM) became integrated later, when the internet simplified communicating with external parties."ERP II" was coined in 2000 in an article by Gartner Publications entitled ERP Is Dead—Long Live ERP II. It describes web–based software that provides real–time access to ERP systems to employees and partners (such as suppliers and customers). The ERP II role expands traditional ERP resource optimization and transaction processing. Rather than just manage buying, selling, etc.—ERP II leverages information in the resources under its management to help the enterprise collaborate with other enterprises.
ERP II is more flexible than the first generation ERP. Rather than confine ERP system capabilities within the organization, it goes beyond the corporate walls to interact with other systems. Enterprise application suite is an alternate name for such systems. ERP II systems are typically used to enable collaborative initiatives such as supply chain management (SCM), customer relationship management (CRM) and business intelligence (BI) among business partner organizations through the use of various electronic business technologies. The large proportion of companies are pursuing a strong managerial targets in ERP system instead of acquire a ERP company.Developers now make more effort to integrate mobile devices with the ERP system. ERP vendors are extending ERP to these devices, along with other business applications, so that businesses don't have to rely on third-party applications. As an example, the e-commerce platform Shopify was able to make ERP tools from Microsoft and Oracle available on its app in October 2021.Technical stakes of modern ERP concern integration—hardware, applications, networking, supply chains. ERP now covers more functions and roles—including decision making, stakeholders' relationships, standardization, transparency, globalization, etc.

Characteristics
ERP systems typically include the following characteristics:

An integrated system
Operates in (or near) real time
A common database that supports all the applications
A consistent look and feel across modules
Installation of the system with elaborate application/data integration by the Information Technology (IT) department, provided the implementation is not done in small steps
Deployment options include: on-premises, cloud hosted, or SaaS

Functional areas
An ERP system covers the following common functional areas. In many ERP systems, these are called and grouped together as ERP modules:

Financial accounting: general ledger, fixed assets, payables including vouchering, matching and payment, receivables and collections, cash management, financial consolidation
Management accounting: budgeting, costing, cost management, activity based costing
Human resources: recruiting, training, rostering, payroll, benefits, retirement and pension plans, diversity management, retirement, separation
Manufacturing: engineering, bill of materials, work orders, scheduling, capacity, workflow management, quality control, manufacturing process, manufacturing projects, manufacturing flow, product life cycle management
Order processing: order to cash, order entry, credit checking, pricing, available to promise, inventory, shipping, sales analysis and reporting, sales commissioning
Supply chain management: supply chain planning, supplier scheduling, product configurator, order to cash, purchasing, inventory, claim processing, warehousing (receiving, putaway, picking and packing)
Project management: project planning, resource planning, project costing, work breakdown structure, billing, time and expense, performance units, activity management
Customer relationship management (CRM): sales and marketing, commissions, service, customer contact, call center support – CRM systems are not always considered part of ERP systems but rather business support systems (BSS)
Supplier relationship management (SRM): suppliers, orders, payments.
Data services: various "self–service" interfaces for customers, suppliers and/or employees
Management of school and educational institutes.

GRP - ERP use in government
Government resource planning (GRP) is the equivalent of an ERP for the public sector and an integrated office automation system for government bodies. The software structure, modularization, core algorithms and main interfaces do not differ from other ERPs, and ERP software suppliers manage to adapt their systems to government agencies.Both system implementations, in private and public organizations, are adopted to improve productivity and overall business performance in organizations, but comparisons (private vs. public) of implementations shows that the main factors influencing ERP implementation success in the public sector are cultural.

Best practices
Most ERP systems incorporate best practices. This means the software reflects the vendor's interpretation of the most effective way to perform each business process. Systems vary in how conveniently the customer can modify these practices.Use of best practices eases compliance with requirements such as IFRS, Sarbanes-Oxley, or Basel II. They can also help comply with de facto industry standards, such as electronic funds transfer. This is because the procedure can be readily codified within the ERP software and replicated with confidence across multiple businesses that share that business requirement.

Connectivity to plant floor information
ERP systems connect to real–time data and transaction data in a variety of ways. These systems are typically configured by systems integrators, who bring unique knowledge on process, equipment, and vendor solutions.
Direct integration—ERP systems have connectivity (communications to plant floor equipment) as part of their product offering. This requires that the vendors offer specific support for the plant floor equipment their customers operate.
Database integration—ERP systems connect to plant floor data sources through staging tables in a database. Plant floor systems deposit the necessary information into the database. The ERP system reads the information in the table. The benefit of staging is that ERP vendors do not need to master the complexities of equipment integration. Connectivity becomes the responsibility of the systems integrator.
Enterprise appliance transaction modules (EATM)—These devices communicate directly with plant floor equipment and with the ERP system via methods supported by the ERP system. EATM can employ a staging table, web services, or system–specific program interfaces (APIs). An EATM offers the benefit of being an off–the–shelf solution.
Custom–integration solutions—Many system integrators offer custom solutions. These systems tend to have the highest level of initial integration cost, and can have a higher long term maintenance and reliability costs. Long term costs can be minimized through careful system testing and thorough documentation. Custom–integrated solutions typically run on workstation or server-class computers.

Implementation
ERP's scope usually implies significant changes to staff work processes and practices. Generally, three types of services are available to help implement such changes: consulting, customization, and support. Implementation time depends on business size, number of modules, customization, the scope of process changes, and the readiness of the customer to take ownership for the project. Modular ERP systems can be implemented in stages. The typical project for a large enterprise takes about 14 months and requires around 150 consultants. Small projects can require months; multinational and other large implementations can take years. Customization can substantially increase implementation times.Besides that, information processing influences various business functions e.g. some large corporations like Walmart use a just in time inventory system. This reduces inventory storage and increases delivery efficiency, and requires up-to-date data. Before 2014, Walmart used a system called Inforem developed by IBM to manage replenishment.

Process preparation
Implementing ERP typically requires changes in existing business processes. Poor understanding of needed process changes prior to starting implementation is a main reason for project failure. The difficulties could be related to the system, business process, infrastructure, training, or lack of motivation.
It is therefore crucial that organizations thoroughly analyze processes before they deploy an ERP software. Analysis can identify opportunities for process modernization. It also enables an assessment of the alignment of current processes with those provided by the ERP system. Research indicates that risk of business process mismatch is decreased by:

Linking current processes to the organization's strategy
Analyzing the effectiveness of each process
Understanding existing automated solutionsERP implementation is considerably more difficult (and politically charged) in decentralized organizations, because they often have different processes, business rules, data semantics, authorization hierarchies, and decision centers. This may require migrating some business units before others, delaying implementation to work through the necessary changes for each unit, possibly reducing integration (e.g., linking via master data management) or customizing the system to meet specific needs.A potential disadvantage is that adopting "standard" processes can lead to a loss of competitive advantage. While this has happened, losses in one area are often offset by gains in other areas, increasing overall competitive advantage.

Configuration
Configuring an ERP system is largely a matter of balancing the way the organization wants the system to work, and the way the system is designed to work out of the box. ERP systems typically include many configurable settings that in effect modify system operations. For example, in the ServiceNow platform, business rules can be written requiring the signature of a business owner within 2 weeks of a newly completed risk assessment. The tool can be configured to automatically email notifications to the business owner, and transition the risk assessment to various stages in the process depending on the owner's responses or lack thereof.

Two-tier enterprise resource planning
Two-tier ERP software and hardware lets companies run the equivalent of two ERP systems at once: one at the corporate level and one at the division or subsidiary level. For example, a manufacturing company could use an ERP system to manage across the organization using independent global or regional distribution, production or sales centers, and service providers to support the main company's customers. Each independent center (or) subsidiary may have its own business operations cycles, workflows, and business processes.
Given the realities of globalization, enterprises continuously evaluate how to optimize their regional, divisional, and product or manufacturing strategies to support strategic goals and reduce time-to-market while increasing profitability and delivering value. With two-tier ERP, the regional distribution, production, or sales centers and service providers continue operating under their own business model—separate from the main company, using their own ERP systems. Since these smaller companies' processes and workflows are not tied to main company's processes and workflows, they can respond to local business requirements in multiple locations.Factors that affect enterprises' adoption of two-tier ERP systems include:

Manufacturing globalization, the economics of sourcing in emerging economies
Potential for quicker, less costly ERP implementations at subsidiaries, based on selecting software more suited to smaller companies
Extra effort, (often involving the use of enterprise application integration) is required where data must pass between two ERP systems Two-tier ERP strategies give enterprises agility in responding to market demands and in aligning IT systems at a corporate level while inevitably resulting in more systems as compared to one ERP system used throughout the organization.

Customization
ERP systems are theoretically based on industry best practices, and their makers intend that organizations deploy them "as is". ERP vendors do offer customers configuration options that let organizations incorporate their own business rules, but gaps in features often remain even after configuration is complete.
ERP customers have several options to reconcile feature gaps, each with their own pros/cons. Technical solutions include rewriting part of the delivered software, writing a homegrown module to work within the ERP system, or interfacing to an external system. These three options constitute varying degrees of system customization—with the first being the most invasive and costly to maintain. Alternatively, there are non-technical options such as changing business practices or organizational policies to better match the delivered ERP feature set. Key differences between customization and configuration include:

Customization is always optional, whereas the software must always be configured before use (e.g., setting up cost/profit center structures, organizational trees, purchase approval rules, etc.).
The software is designed to handle various configurations and behaves predictably in any allowed configuration.
The effect of configuration changes on system behavior and performance is predictable and is the responsibility of the ERP vendor. The effect of customization is less predictable. It is the customer's responsibility, and increases testing requirements.
Configuration changes survive upgrades to new software versions. Some customizations (e.g., code that uses pre–defined "hooks" that are called before/after displaying data screens) survive upgrades, though they require retesting. Other customizations (e.g., those involving changes to fundamental data structures) are overwritten during upgrades and must be re-implemented.Advantages of customization include:

Improving user acceptance
Potential to obtain competitive advantage vis-à-vis companies using only standard features.Customization's disadvantages include that it may:

Increase time and resources required to implement and maintain
Hinder seamless interfacing/integration between suppliers and customers due to the differences between systems
Limit the company's ability to upgrade the ERP software in the future
Create overreliance on customization, undermining the principles of ERP as a standardizing software platform

Extensions
ERP systems can be extended with third–party software, often via vendor-supplied interfaces. Extensions offer features such as:
product data management
product life cycle management
customer relations management
data mining
e-procurement

Data migration
Data migration is the process of moving, copying, and restructuring data from an existing system to the ERP system. Migration is critical to implementation success and requires significant planning. Unfortunately, since migration is one of the final activities before the production phase, it often receives insufficient attention. The following steps can structure migration planning:
Identify the data to be migrated.
Determine the migration timing.
Generate data migration templates for key data components
Freeze the toolset.
Decide on the migration-related setup of key business accounts.
Define data archiving policies and procedures.Often, data migration is incomplete because some of the data in the existing system is either incompatible or not needed in the new system. As such, the existing system may need to be kept as an archived database to refer back to once the new ERP system is in place.

Advantages
The most fundamental advantage of ERP is that the integration of a myriad of business processes saves time and expense. Management can make decisions faster and with fewer errors. Data becomes visible across the organization. Tasks that benefit from this integration include:
Sales forecasting, which allows inventory optimization.
Chronological history of every transaction through relevant data compilation in every area of operation.
Order tracking, from acceptance through fulfillment
Revenue tracking, from invoice through cash receipt
Matching purchase orders (what was ordered), inventory receipts (what arrived), and costing (what the vendor invoiced)ERP systems centralize business data, which:

Eliminates the need to synchronize changes between multiple systems—consolidation of finance, marketing, sales, human resource, and manufacturing applications
Brings legitimacy and transparency to each bit of statistical data
Facilitates standard product naming/coding
Provides a comprehensive enterprise view (no "islands of information"), making real–time information available to management anywhere, anytime to make proper decisions
Protects sensitive data by consolidating multiple security systems into a single structure

Benefits
ERP creates a more agile company that adapts better to change. It also makes a company more flexible and less rigidly structured so organization components operate more cohesively, enhancing the business—internally and externally.
ERP can improve data security in a closed environment. A common control system, such as the kind offered by ERP systems, allows organizations the ability to more easily ensure key company data is not compromised. This changes, however, with a more open environment, requiring further scrutiny of ERP security features and internal company policies regarding security.
ERP provides increased opportunities for collaboration. Data takes many forms in the modern enterprise, including documents, files, forms, audio and video, and emails. Often, each data medium has its own mechanism for allowing collaboration. ERP provides a collaborative platform that lets employees spend more time collaborating on content rather than mastering the learning curve of communicating in various formats across distributed systems.
ERP offers many benefits such as standardization of common processes, one integrated system, standardized reporting, improved key performance indicators (KPI), and access to common data. One of the key benefits of ERP; the concept of integrated system, is often misinterpreted by the business. ERP is a centralized system that provides tight integration with all major enterprise functions be it HR, planning, procurement, sales, customer relations, finance or analytics, as well to other connected application functions. In that sense ERP could be described as a centralized integrated enterprise system (CIES)

Disadvantages
Customization can be problematic. Compared to the best-of-breed approach, ERP can be seen as meeting an organization's lowest common denominator needs, forcing the organization to find workarounds to meet unique demands.
Re-engineering business processes to fit the ERP system may damage competitiveness or divert focus from other critical activities.
ERP can cost more than less integrated or less comprehensive solutions.
High ERP switching costs can increase the ERP vendor's negotiating power, which can increase support, maintenance, and upgrade expenses.
Overcoming resistance to sharing sensitive information between departments can divert management attention.
Integration of truly independent businesses can create unnecessary dependencies.
Extensive training requirements take resources from daily operations.
Harmonization of ERP systems can be a mammoth task (especially for big companies) and requires a lot of time, planning, and money.
Critical challenges include disbanding the project team very quickly after implementation, interface issues, lack of proper testing, time zone limitations, stress, offshoring, people's resistance to change, a short hyper-care period, and data cleansing.

Critical success factors
Critical success factors are limited number of areas in which results, if satisfactory, will ensure the organization's successful competitive performance. The CSF method has helped organizations specify their own critical information needs. Achieving satisfactory results in the key areas of critical success factors can ensure competitive advantage leading to improved organizational performance and overcome the challenges faced by organizations. Critical success factors theoretical foundation was improved upon, verified, and validated by several researchers, which underscored the importance of CSFs and its application to ERP project implementations.The application of critical success factors can prevent organizations from making costly mistakes, and the effective usage of CSFs can ensure project success and reduce failures during project implementations. Some of the important critical success factors related to ERP projects are: Know your data, longer and more integrated testing, utilization of the right people, longer stabilization period (hyper-care), clear communication, early buy-in from business, have a Lean Agile program, less customization, ERP projects must be business-driven and not IT-driven.

Adoption rates
Research published in 2011 based on a survey of 225 manufacturers, retailers and distributors found "high" rates of interest and adoption of ERP systems and that very few businesses were "completely untouched" by the concept of an ERP system. 27% of the companies survey had a fully operational system, 12% were at that time rolling out a system and 26% had an existing ERP system which they were extending or upgrading.

Postmodern ERP
The term "postmodern ERP" was coined by Gartner in 2013, when it first appeared in the paper series "Predicts 2014". According to Gartner's definition of the postmodern ERP strategy, legacy, monolithic and highly customized ERP suites, in which all parts are heavily reliant on each other, should sooner or later be replaced by a mixture of both cloud-based and on-premises applications, which are more loosely coupled and can be easily exchanged if needed.The basic idea is that there should still be a core ERP solution that would cover most important business functions, while other functions will be covered by specialist software solutions that merely extend the core ERP. This concept is similar to the so-called best-of-breed approach to software execution, but it shouldn't be confused with it. While in both cases, applications that make up the whole are relatively loosely connected and quite easily interchangeable, in the case of the latter there is no ERP solution whatsoever. Instead, every business function is covered by a separate software solution.There is, however, no golden rule as to what business functions should be part of the core ERP, and what should be covered by supplementary solutions. According to Gartner, every company must define their own postmodern ERP strategy, based on company's internal and external needs, operations and processes. For example, a company may define that the core ERP solution should cover those business processes that must stay behind the firewall, and therefore, choose to leave their core ERP on-premises. At the same time, another company may decide to host the core ERP solution in the cloud and move only a few ERP modules as supplementary solutions to on-premises.The main benefits that companies will gain from implementing postmodern ERP strategy are speed and flexibility when reacting to unexpected changes in business processes or on the organizational level. With the majority of applications having a relatively loose connection, it is fairly easy to replace or upgrade them whenever necessary. In addition to that, following the examples above, companies can select and combine cloud-based and on-premises solutions that are most suited for their ERP needs. The downside of postmodern ERP is that it will most likely lead to an increased number of software vendors that companies will have to manage, as well as pose additional integration challenges for the central IT.

See also
List of ERP software packages
Business process management
Comparison of project management software

References
Bibliography
External links
 Media related to Enterprise resource planning at Wikimedia Commons

Factoring (finance)

Factoring is a financial transaction and a type of debtor finance in which a business sells its accounts receivable (i.e., invoices) to a third party (called a factor) at a discount. A business will sometimes factor its receivable assets to meet its present and immediate cash needs. Forfaiting is a factoring arrangement used in international trade finance by exporters who wish to sell their receivables to a forfaiter. Factoring is commonly referred to as accounts receivable factoring, invoice factoring, and sometimes accounts receivable financing. Accounts receivable financing is a term more accurately used to describe a form of asset based lending against accounts receivable. The Commercial Finance Association is the leading trade association of the asset-based lending and factoring industries.In the United States, Factoring is not the same as invoice discounting (which is called an assignment of accounts receivable in American accounting – as propagated by FASB within GAAP). Factoring is the sale of receivables, whereas invoice discounting ("assignment of accounts receivable" in American accounting) is a borrowing that involves the use of the accounts receivable assets as collateral for the loan. However, in some other markets, such as the UK, invoice discounting is considered to be a form of factoring, involving the "assignment of receivables", that is included in official factoring statistics. It is therefore also not considered to be borrowing in the UK. In the UK the arrangement is usually confidential in that the debtor is not notified of the assignment of the receivable and the seller of the receivable collects the debt on behalf of the factor. In the UK, the main difference between factoring and invoice discounting is confidentiality. Scottish law differs from that of the rest of the UK, in that notification to the account debtor is required for the assignment to take place. The Scottish Law Commission reviewed this position and made proposals to the Scottish Ministers in 2018.

Overview
There are three parties directly involved: the factor who purchases the receivable, the one who sells the receivable, and the debtor who has a financial liability that requires him or her to make a payment to the owner of the invoice. The receivable, usually associated with an invoice for work performed or goods sold, is essentially a financial asset that gives the owner of the receivable the legal right to collect money from the debtor whose financial liability directly corresponds to the receivable asset.  The seller sells the receivables at a discount to the third party, the specialized financial organization (aka the factor) to obtain cash.  This process is sometimes used in manufacturing industries when the immediate need for raw material outstrips their available cash and ability to purchase "on account". Both invoice discounting and factoring are used by B2B companies to ensure they have the immediate cash flow necessary to meet their current and immediate obligations. Invoice factoring is not a relevant financing option for retail or B2C companies because they generally do not have business or commercial clients, a necessary condition for factoring.
The sale of the receivable transfers ownership of the receivable to the factor, indicating the factor obtains all of the rights associated with the receivables. Accordingly, the receivable becomes the factor's asset, and the factor obtains the right to receive the payments made by the debtor for the invoice amount, and is free to pledge or exchange the receivable asset without unreasonable constraints or restrictions. Usually, the account debtor is notified of the sale of the receivable, and the factor bills the debtor and makes all collections; however, non-notification factoring, where the client (seller) collects the accounts sold to the factor, as agent of the factor, also occurs. The arrangement is usually confidential in that the debtor is not notified of the assignment of the receivable and the seller of the receivable collects the debt on behalf of the factor.  If the factoring transfers the receivable "without recourse", the factor (purchaser of the receivable) must bear the loss if the account debtor does not pay the invoice amount. If the factoring transfers the receivable "with recourse", the factor has the right to collect the unpaid invoice amount from the transferor (seller). However, any merchandise returns that may diminish the invoice amount that is collectible from the accounts receivable are typically the responsibility of the seller, and the factor will typically hold back paying the seller for a portion of the receivable being sold (the "factor's holdback receivable") in order to cover the merchandise returns associated with the factored receivables until the privilege to return the merchandise expires.
There are four principal parts to the factoring transaction, all of which are recorded separately by an accountant who is responsible for recording the factoring transaction:

the "fee" paid to the factor,
the Interest Expense paid to the factor for the advance of money,
the "bad debt expense" associated with portion of the receivables that the seller expects will remain unpaid and uncollectable,
the "factor's holdback receivable" amount to cover merchandise returns, and (e) any additional "loss" or "gain" the seller must attribute to the sale of the receivables.  Sometimes the factor's charges paid by the seller (the factor's "client") covers a discount fee, additional credit risk the factor must assume, and other services provided. The factor's overall profit is the difference between the price it paid for the invoice and the money received from the debtor, less the amount lost due to non-payment.

Rationale
Factoring is a method used by some firms to obtain cash. Certain companies factor accounts when the available cash balance held by the firm is insufficient to meet current obligations and accommodate its other cash needs, such as new orders or contracts; in other industries, however, such as textiles or apparel, for example, financially sound companies factor their accounts simply because this is the historic method of financing. The use of factoring to obtain the cash needed to accommodate a firm's immediate cash needs will allow the firm to maintain a smaller ongoing cash balance. By reducing the size of its cash balances, more money is made available for investment in the firm's growth.
Debt factoring is also used as a financial instrument to provide better cash flow control especially if a company currently has a lot of accounts receivables with different credit terms to manage. A company sells its invoices at a discount to their face value when it calculates that it will be better off using the proceeds to bolster its own growth than it would be by effectively functioning as its "customer's bank." Accordingly, factoring occurs when the rate of return on the proceeds invested in production exceed the costs associated with factoring the receivables. Therefore, the trade-off between the return the firm earns on investment in production and the cost of utilizing a factor is crucial in determining both the extent factoring is used and the quantity of cash the firm holds on hand.
Many businesses have cash flow that varies. It might be relatively large in one period, and relatively small in another period. Because of this, businesses find it necessary to both maintain a cash balance on hand, and to use such methods as factoring, in order to enable them to cover their short term cash needs in those periods in which these needs exceed the cash flow. Each business must then decide how much it wants to depend on factoring to cover short falls in cash, and how large a cash balance it wants to maintain in order to ensure it has enough cash on hand during periods of low cash flow.
Generally, the variability in the cash flow will determine the size of the cash balance a business will tend to hold as well as the extent it may have to depend on such financial mechanisms as factoring. Cash flow variability is directly related to two factors:

The extent cash flow can change, and
The length of time cash flow can remain at a below average level.If cash flow can decrease drastically, the business will find it needs large amounts of cash from either existing cash balances or from a factor to cover its obligations during this period of time. Likewise, the longer a relatively low cash flow can last, the more cash is needed from another source (cash balances or a factor) to cover its obligations during this time. As indicated, the business must balance the opportunity cost of losing a return on the cash that it could otherwise invest, against the costs associated with the use of factoring.
The cash balance a business holds is essentially a demand for transactions money. As stated, the size of the cash balance the firm decides to hold is directly related to its unwillingness to pay the costs necessary to use a factor to finance its short term cash needs. The problem faced by the business in deciding the size of the cash balance it wants to maintain on hand is similar to the decision it faces when it decides how much physical inventory it should maintain. In this situation, the business must balance the cost of obtaining cash proceeds from a factor against the opportunity cost of the losing the Rate of Return it earns on investment within its business. The solution to the problem is:

  
    
      
        C
        B
        =
        
          
            
              
                i
                ∗
                n
                C
                F
              
              
                (
                2
                ∗
                r
                )
              
            
          
        
      
    
    {\displaystyle CB={\sqrt {\frac {i*nCF}{(2*r)}}}}
  where

  
    
      
        C
        B
      
    
    {\displaystyle CB}
   is the cash balance

  
    
      
        n
        C
        F
      
    
    {\displaystyle nCF}
   is the average negative cash flow in a given period

  
    
      
        i
      
    
    {\displaystyle i}
   is the [discount rate] that cover the factoring costs

  
    
      
        r
      
    
    {\displaystyle r}
   is the rate of return on the firm's assets.Today factoring's rationale still includes the financial task of advancing funds to smaller rapidly growing firms who sell to larger more credit-worthy organizations. While almost never taking possession of the goods sold, factors offer various combinations of money and supportive services when advancing funds.
Factors often provide their clients four key services: information on the creditworthiness of their prospective customers domestic and international, and, in nonrecourse factoring, acceptance of the credit risk for "approved" accounts; maintain the history of payments by customers (i.e., accounts receivable ledger); daily management reports on collections; and, make the actual collection calls. The outsourced credit function both extends the small firm's effective addressable marketplace and insulates it from the survival-threatening destructive impact of a bankruptcy or financial difficulty of a major customer. A second key service is the operation of the accounts receivable function. The services eliminate the need and cost for permanent skilled staff found within large firms. Although today even they are outsourcing such back-office functions. More importantly, these services insure entrepreneurs and owners against a major source of a liquidity crises and their equity.

Process
The factoring process can be broken up into two parts: the initial account setup and ongoing funding. Setting up a factoring account typically takes one to two weeks and involves submitting an application, a list of clients, an accounts receivable aging report and a sample invoice. The approval process involves detailed underwriting, during which time the factoring company can ask for additional documents, such as documents of incorporation, financials, and banks statements. If approved, the business will be set up with a maximum credit line from which they can draw. In the case of notification factoring, the arrangement is not confidential and approval is contingent upon successful notification; a process by which factoring companies send the business's client or account debtor a Notice of Assignment. The Notice of Assignment serves to

inform debtors that a factoring company is managing all of the business's receivables,
stake a claim on the financial rights for the receivables factored, and
update the payment address – usually a bank lock box.Once the account is set up, the business is ready to start funding invoices. Invoices are still approved on an individual basis, but most invoices can be funded in a business day or two, as long as they meet the factor's criteria. Receivables are funded in two parts. The first part is the "advance" and covers 80% to 85% of the invoice value. This is deposited directly to the business's bank account. The remaining 15% to 20% is rebated, less the factoring fees, as soon as the invoice is paid in full to the factoring company.

Accounts receivable discounting
Non-recourse factoring should not be confused with making a loan.  When a lender decides to extend credit to a company based on assets, cash flows, and credit history, the borrower must recognize a liability to the lender, and the lender recognizes the borrower's promise to repay the loan as an asset.  Factoring without recourse is a sale of a financial asset (the receivable), in which the factor assumes ownership of the asset and all of the risks associated with it, and the seller relinquishes any title to the asset sold. An example of factoring is the credit card. Factoring is like a credit card where the bank (factor) is buying the debt of the customer without recourse to the seller; if the buyer doesn't pay the amount to the seller the bank cannot claim the money from the seller or the merchant, just as the bank in this case can only claim the money from the debt issuer. Factoring is different from invoice discounting, which usually doesn't imply informing the debt issuer about the assignment of debt, whereas in the case of factoring the debt issuer is usually notified in what is known as notification factoring. One more difference between the factoring and invoice discounting is that in case of factoring the seller assigns all receivables of a certain buyer(s) to the factor whereas in invoice discounting the borrower (the seller) assigns a receivable balance, not specific invoices. A factor is therefore more concerned with the credit-worthiness of the company's customers. The factoring transaction is often structured as a purchase of a financial asset, namely the accounts receivable. A non-recourse factor assumes the "credit risk" that an account will not collect due solely to the financial inability of account debtor to pay. In the United States, if the factor does not assume the credit risk on the purchased accounts, in most cases a court will recharacterize the transaction as a secured loan.
When a company decides to factors account receivables invoices to a principles factors or broker, it needs to understands the risks and rewards involved with factoring. Amount of funding can vary depending on the specific accounts receivables, debtor and industry that factoring occurs in. Factors can limit and restrict funding in such occasions where the debtor is found not credit worthy, or the invoice amount represents too big of a portion of the business's annual income. Another area of concern is when the cost of invoice factoring is calculated. It's a compound of an administration charge and interest earned overtime as the debtor takes time to repay the original invoice. Not all factoring companies charge interest over the time it takes to collect from a debtor, in this case only the administration charge needs to be taken into account although this type of facility is comparatively rare. There are major industries which stand out in the factoring industry which are:
1. Distribution
2. Retail
3. Manufacturing
4. Transportation
5. Services
6. Construction
However, most businesses can apply invoice factoring successfully to their funding model.

Common factoring terms
Discount rate or factoring fee
The discount rate is the fee a factoring company charges to provide the factoring service. Since a formal factoring transaction involves the outright purchase of the invoice, the discount rate is typically stated as a percentage of the face value of the invoices. For instance, a factoring company may charge 5% for an invoice due in 45 days. In contrast, companies that do accounts receivable financing may charge per week or per month. Thus, an invoice financing company that charges 1% per week would result in a discount rate of 6–7% for the same invoice.

Advance rate
The advance rate is the percentage of an invoice that is paid out by the factoring company upfront. The difference between the face value of the invoice and the advance rates serves to protect factors against any losses and to ensure coverage for their fees. Once the invoice is paid, the factor gives the difference between the face value, advance amount and fees back to the business in the form of a factoring rebate.

Reserve account
Whereas the difference between the invoice face value and the advance serves as a reserve for a specific invoice, many factors also hold an ongoing reserve account which serves to further reduce the risk for the factoring company. This reserve account is typically 10–15% of the seller's credit line, but not all factoring companies hold reserve accounts.

Long-term contracts and minimums
While factoring fees and terms range widely, many factoring companies will have monthly minimums and require a long-term contract as a measure to guarantee a profitable relationship. Although shorter contract periods are now becoming more common, contracts and monthly minimums are typical with "whole ledger" factoring, which entails factoring all of a company's invoices or all of the company's invoices from a particular debtor.

Spot factoring
Spot factoring, or single invoice discounting, is an alternative to "whole ledger" and allows a company to factor a single invoice. The added flexibility for the business, and lack of predictable volume and monthly minimums for factoring providers means that spot factoring transactions usually carry a cost premium.

Treatment under GAAP
In the United States, under the Generally Accepted Accounting Principles (GAAP), receivables are considered "sold", under FASB ASC 860-10 (or under Statement of Financial Accounting Standards No. 140, paragraph 112), when the buyer has "no recourse". Moreover, to treat the transaction as a sale under GAAP, the seller's monetary liability under any "recourse" provision must be readily estimated at the time of the sale. Otherwise, the financial transaction is treated as a secured loan, with the receivables used as collateral.
When a nonrecourse transaction takes place, the accounts receivable balance is removed from the statement of financial position. The corresponding debits include the expense recorded on the income statement and the proceeds received from the factor.

History
Factoring as a fact of business life was underway in England prior to 1400, and it came to America with the Pilgrims, around 1620. It appears to be closely related to early merchant banking activities. The latter however evolved by extension to non-trade related financing such as sovereign debt. Like all financial instruments, factoring evolved over centuries. This was driven by changes in the organization of companies; technology, particularly air travel and non-face-to-face communications technologies starting with the telegraph, followed by the telephone and then computers. These also drove and were driven by modifications of the common law framework in England and the United States.Governments were latecomers to the facilitation of trade financed by factors. English common law originally held that unless the debtor was notified, the assignment between the seller of invoices and the factor was not valid. The Canadian Federal Government legislation governing the assignment of moneys owed by it still reflects this stance as does provincial government legislation modelled after it. As late as the current century, the courts have heard arguments that without notification of the debtor the assignment was not valid. In the United States, by 1949 the majority of state governments had adopted a rule that the debtor did not have to be notified, thus opening up the possibility of non-notification factoring arrangements.Originally the industry took physical possession of the goods, provided cash advances to the producer, financed the credit extended to the buyer and insured the credit strength of the buyer. In England the control over the trade thus obtained resulted in an Act of Parliament in 1696 to mitigate the monopoly power of the factors. With the development of larger firms who built their own sales forces, distribution channels, and knowledge of the financial strength of their customers, the needs for factoring services were reshaped and the industry became more specialized.
By the twentieth century in the United States factoring was still the predominant form of financing working capital for the then-high-growth-rate textile industry. In part this occurred because of the structure of the US banking system with its myriad of small banks and consequent limitations on the amount that could be advanced prudently by any one of them to a firm. In Canada, with its national banks the limitations were far less restrictive and thus factoring did not develop as widely as in the US. Even then, factoring also became the dominant form of financing in the Canadian textile industry.
By the first decade of the 21st century, a basic public policy rationale for factoring remains that the product is well-suited to the demands of innovative, rapidly growing firms critical to economic growth. A second public policy rationale is allowing fundamentally good business to be spared the costly, time-consuming trials and tribulations of bankruptcy protection for suppliers, employees and customers or to provide a source of funds during the process of restructuring the firm so that it can survive and grow.

Modern forms
In the latter half of the twentieth century the introduction of computers eased the accounting burdens of factors and then small firms. The same occurred for their ability to obtain information about debtor's creditworthiness. Introduction of the Internet and the web has accelerated the process while reducing costs. Today credit information and insurance coverage are instantly available online. The web has also made it possible for factors and their clients to collaborate in real time on collections. Acceptance of signed documents provided by facsimile as being legally binding has eliminated the need for physical delivery of "originals", thereby reducing time delays for entrepreneurs.
Traditionally, factoring has been a relationship driven business and factoring transactions have been largely manual and frequently involving a face to-face component as part of the relationship building process or due-diligence phase. This is especially true for small business factoring, in which the factoring companies tend to be locally or regionally focused. The geographic focus helps them better mitigate risks that because of their smaller scale, they otherwise couldn't afford to take.To make the arrangement economically profitable, most factoring companies have revenue minimums (e.g. at least $500,000 in annual revenue) and require annual contracts and monthly minimums. More recently, several online factoring companies have emerged, leveraging aggregation, analytics, automation to deliver the benefits of factoring with the convenience and ease afforded by the internet. Some companies use technology to automate some of the risk and back-office aspects of factoring and provide the service via a modern web interface for additional convenience. This enables them to serve a broader range of small businesses with significantly lower revenue requirements without the need for monthly minimums and long-term contracts. Many of these companies have direct software integrations with software programs such as Quickbooks, allowing businesses to immediately receive funding without an application.
The emergence of these modern forms has not been without controversy. Critics accurately point out that none of these new players have experienced a complete credit cycle and thus, their underwriting models have not been market tested by an economic contraction. What's more, some of these new models rely on a market place lending format. It's unclear if this source of capital will be stable over time, as other companies, most notably, Lending Club, had a difficult time attracting investors in early 2016, even though net returns seem higher on invoice finance platforms such as MarketInvoice and FundThrough than on business loan platforms such as Funding Circle.

Specialized factoring
With advances in technology, some invoice factoring providers have adapted to specific industries. This often affects additional services offered by the factor in order to best adapt the factoring service to the needs of the business. An example of this includes a recruitment specialist factor offering payroll and back office support with the factoring facility; a wholesale or /distribution factor may not offer this additional service. These differences can affect the cost of the facility, the approach the factor takes when collecting credit, the administration services included in the facility and the maximum size of invoices which can be factored.

Real estate
Since the 2007 United States recession one of the fastest-growing sectors in the factoring industry is real estate commission advances. Commission advances work the same way as factoring but are done with licensed real estate agents on their pending and future real estate commissions. Commission advances were first introduced in Canada but quickly spread to the United States. Typically, the process consists of an online application from a real estate agent, who signs a contract selling future commissions at a discount; the factoring company then wires the funds to the agent's bank account.

Medical factoring
The healthcare industry makes for a special case in which factoring is much needed because of long payment cycles from government, private insurance companies and other third party payers, but difficult because of  HIPAA requirements. For this reasons medical receivables factoring companies have developed to specifically target this niche.

Construction
Factoring is commonplace in the construction industry because of the long payment cycles that can stretch to 120 days and beyond. However, the construction industry has features that are risky for factoring companies. Because of the risks and exposure from mechanics' liens, danger of "paid-when-paid" terms, existence of progress billing, use of withholding, and exposure to economic cycles  most "generalist" factoring companies avoid construction receivables entirely. That has created another niche of factoring companies that specialize in construction receivables.

Haulage
Factoring is often used by haulage companies to cover upfront expenses, such as fuel.  Factoring companies that cater to this niche offer services to help accommodate drivers on the road, including the ability to verify invoices and fund on copies sent via scan, fax or email, and the option to place the funds directly onto a fuel card, which works like a debit card. Haulage factors also offer fuel advance programs that provide a cash advance to carriers upon confirmed pickup of the load.

Recruitment
In the recruitment sector factoring is an effective solution, often used by temporary recruitment agencies who must ensure that their business has the available funds each week to make payment to the workers they have placed. Specialist funding marketplaces like Raise enable Recruitment businesses to access the best funding options that they need to grow their agency.

Invoice payers (debtors)
Large firms and organizations such as governments usually have specialized processes to deal with one aspect of factoring, redirection of payment to the factor following receipt of notification from the third party (i.e., the factor) to whom they will make the payment. Many but not all in such organizations are knowledgeable about the use of factoring by small firms and clearly distinguish between its use by small rapidly growing firms and turnarounds.
Distinguishing between assignment of the responsibility to perform the work and the assignment of funds to the factor is central to the customer or debtor's processes. Firms have purchased from a supplier for a reason and thus insist on that firm fulfilling the work commitment. Once the work has been performed, however, it is a matter of indifference who is paid. For example, General Electric has clear processes to be followed which distinguish between their work and payment sensitivities. Contracts direct with the US government require an assignment of claims, which is an amendment to the contract allowing for payments to third parties (factors).

Risks
Risks to a factor include:
Counter-party credit risk related to clients and risk-covered debtors. Risk-covered debtors can be reinsured, which limit the risks of a factor. Trade receivables are a fairly low-risk asset due to their short duration.
External fraud by clients: fake invoicing, misdirected payments, pre-invoicing, not assigned credit notes, etc. A fraud insurance policy and subjecting the client to audit could limit the risks.
Legal, compliance and tax risks: large number of applicable laws and regulations in different countries
Operational risks, such as contractual disputes
Uniform Commercial Code (UCC-1) securing rights to assets.
IRS liens associated with payroll taxes, etc.
ICT risks: complicated, integrated factoring system, extensive data exchange with client

Reverse factoring
In reverse factoring or supply-chain finance, the buyer sells its debt to the factor. That way, the buyer secures the financing of the invoice, and the supplier gets a better interest rate.

See also
Capital formation
Invoice discounting


== References ==

Fast fashion

Fast fashion is the business model of replicating recent catwalk trends and high-fashion designs, mass-producing them at a low cost, and bringing them to retail quickly while demand is at its highest. The term fast fashion is also used generically to describe the products of this business model. Retailers who employ the fast fashion strategy include Primark, H&M, Shein, and Zara, all of which have become large multinationals by driving high turnover of inexpensive seasonal and trendy clothing that appeals to fashion-conscious consumers.
Fast fashion grew during the late 20th century as manufacturing of clothing became less expensive—the result of more efficient supply chains and new quick response manufacturing methods and greater reliance on low-cost labor from the apparel manufacturing industries of South, Southeast, and East Asia, where women make up 85-90% of the garment workforce. Labor practices in fast fashion are often exploitative, and due to the gender concentration of the garment industry, women are more vulnerable.Fast fashion's environmental impact has also been the subject of controversy. The global fashion industry is responsible for ~8–10% of global carbon emissions per year, to which fast fashion is a large contributor. The low cost of production favoring synthetic materials, chemicals, and minimal pollution abatement measures have led to excess waste.

Origins
Before the 1800s, fashion was a laborious, time-consuming process that required sourcing materials like wool, cotton, or leather, treating and preparing the materials by hand, then weaving or fashioning them into functional garments, also by hand. However, the Industrial Revolution changed the world of fashion by introducing new technology like the sewing machine and textile machines, which led to ready-made clothes and mass production factories. 
As a result, clothes became cheaper to make and buy, and easier and quicker to make. Meanwhile, localized dressmaking businesses emerged, catering to members of the middle class, and employing workroom employees along with garment workers, who worked from home for meager wages. These dress shops were early prototypes of the so-called ‘sweatshops’ that would become the foundation for twenty-first century clothing production. During World War II, the trend of more functional styles and fabric restrictions led to the standardized production of clothes. Once middle-class consumers grew accustomed to it, they became increasingly receptive to the idea of mass-produced clothing.
The fashion industry produced and ran clothes for four seasons a year until the mid-twentieth century, with designers working many months in advance to predict what customers would want. In the 1960s and 1970s, this method changed drastically as the younger generations started to create new trends. There was still a clear distinction between high-end and high street fashion. In the late 1990s and early 2000s, fast fashion became a booming industry in the United States with people enthusiastically partaking in consumerism. Fast fashion retailers such as Zara, H&M, Topshop, and, later, Primark took over high street fashion. Initially starting as small stores located in Europe, they were able to infiltrate and gain prominence in the U.S. market by examining and replicating the looks and design elements from runway shows and top fashion houses and quickly reproducing them, but at a fraction of the cost.The origins of the "fast fashion" phenomenon are not attributed to a single brand or company but rather involve several key players. One notable figure in this movement was Amancio Ortega, the founder of Zara. Established in 1963 in Galicia, Spain, Zara gained prominence by offering affordable imitations of high-end fashion trends alongside its own designs. In 1975, Ortega opened the first European retail outlet for his collections, pioneering a short-term production and distribution model. By the early 1990s, he had expanded to New York, and the New York Times coined the term "fast fashion" to describe Zara's approach, highlighting its ability to bring a designer's idea to store shelves in a mere 15 days.In the 2008 article "Fast Fashion Lessons," Donald Sull and Stefano Turconi studied how Zara pioneered an approach to navigate the volatile world of the fast fashion industry. According to Sull and Turconi, one of the reasons for the company's success was that it built a supply chain and production network where they maintained complicated and capital-intensive operations (like computer-guided fabric cutting) in-house, while it outsourced labour-intensive operations (like garment sewing) to a network of local subcontractors and seamstress operatives based in Galicia.Thus, with shorter lead times, the company responded very quickly when the sale of their products exceeded their expectations and cut off production for items that did not have high demand. They create a sense of urgency for consumers to purchase clothing because they constantly change their layout and stock, so it may not be in the store the next time they visit. The clothing is then only worn a few times before it is no longer in style, creating the need to constantly buy new items for cheap.Unlike many fashion companies, Zara hardly invests in television or press promotional campaigns and instead relies on store windows to convey their brand image, word of mouth, and locating their shops strategically in areas with high consumer traffic.The origin story of H&M shares common threads with Zara; technically, it is the world's longest-running retailer. In 1946, Erling Persson, a Swedish entrepreneur, traveled to New York City, where he was greatly intrigued and impressed by the high-volume fashion production he witnessed. The following year, Persson established a womenswear store called Hennes & Mauritz (or H&M) in Västerås, Sweden. Between 1960 and 1979, the company rapidly expanded, with 42 stores across Europe, and began producing clothing for women, men, and children.The foundation for expansion into the global market was laid in the 1980s when H&M acquired Rowells, a Swedish mail order company, and used its networks to sell fast fashion by catalogue and mail order. In the 1990s, H&M invested in large city billboard advertising, featuring celebrities and supermodels. H&M opened its flagship USA store on Fifth Avenue in New York City in 2000, marking the commencement of its expansion outside of Europe.Zaw Thiha Tun, a Vancouver-based investment advisor, examined the secret of H&M's success as a company and noted that its business model is unlike other fast fashion companies, such as Zara, as it does not manufacture any products in-house and rather, outsources production to more than 900 independent suppliers mainly located in Europe and Asia, which are in turn managed by 30 strategically-located oversight offices. They also depend on state-of-the-art IT infrastructure and networks to connect the central national and production offices. This method has been crucial to H&M's success: They do not have the overhead from owning factories or need to secure fabrics in advance, and they have been able to reduce their lead times through continuous developments in the buying process.

Concept
Fast fashion brands produce pieces to get the newest style on the market as soon as possible. They emphasize optimizing certain aspects of the supply chain for the trends to be designed and manufactured quickly and inexpensively and allow the mainstream consumer to buy current clothing styles at a lower price. This philosophy of quick manufacturing at an affordable price is used in large retailers such as SHEIN, H&M, Zara, C&A, Peacocks, Primark, ASOS, Forever 21, and Uniqlo.These retailers produce and sell products in small batches, keep surplus manufacturing capacity on hand, and frequently induce items to be out of stock, a practice designed to give retailers the ability to make substantial and immediate adjustments to manufacturing. For example, up to 85% of Zara's merchandise can be changed in the middle of the season: A fast fashion system like Zara's can quickly update designs, resulting in short product cycles where a garment does not sit on the stores' shelf for long periods, giving the store a sense of exclusivity and raising the attractiveness of an item.Fast fashion particularly came to the fore during the vogue for "boho chic" in the mid-2000s. According to the UK Environmental Audit Committee's report "Fixing Fashion", the practice "involves increased numbers of new fashion collections every year, quick turnarounds and often lower prices. Reacting rapidly to offer new products to meet consumer demand is crucial to this business model."Fast fashion has developed from a product-driven concept based on a manufacturing model referred to as "quick response" developed in the U.S. in the 1980s and moved to a market-based model of "fast fashion" in the late 1990s and the first part of the 21st century. The Zara brand name has become almost synonymous with the term, but other retailers worked with the concept before the label was applied, such as Benetton. Fast fashion has also become associated with disposable fashion because it has delivered designer product to a mass market at relatively low prices.The advancement of technology has allowed fast fashion to gain popularity over the last decade. Technology has allowed designers to create specifically what their consumers want according to what is "in" at the given moment. Every month, new things are trending and are displayed in stores to market towards the youth. Technology has the power to change all the issues within the fast fashion industry. Brands such as Zara have been listening to its consumers and thinking green to improve their environmental impact. As Nina Davis stated in 2020, "[Companies] are also adopting advanced technologies to improve supply chain efficiency and reduce their carbon footprint."

Slow fashion counter
The slow fashion or conscious fashion movement has risen in opposition to fast fashion, naming responsibility for pollution (both in the production of clothes and in the decay of synthetic fabrics), poor workmanship, and emphasizing very brief trends over classic style. Elizabeth L. Cline's 2012 book Overdressed: The Shockingly High Cost of Cheap Fashion was one of the first investigations into the human and environmental toll of fast fashion. The practice has also come under criticism for contributing to poor working conditions in developing countries. The 2013 Dhaka garment factory collapse in Bangladesh, the deadliest garment-related accident in world history, brought more attention to the safety impact of the fast fashion industry.In the rise of slow fashion, emphasis has been given to quality clothing that is better considered. In Spring/Summer 2020 season of fashion, high end designers led the movement of slow fashion by creating pieces that developed from environmental-friendly practices in the industry. Stella McCartney is one luxury designer who focuses on sustainable and ethical practices, and has done so since the nineties. British Vogue explained that the process of designing and creating clothing in slow fashion involves consciousness of materials, consumer demand, and the climate impact.In her 2016 article titled "Doing Good and Looking Good: Women in 'Fast Fashion' Activism", Rimi Khan criticized the slow fashion movement, particularly the work of high-profile designers and slow fashion advocates McCartney and Vivienne Westwood, as well as other well known industry professionals such as Livia Firth, for creating fashion products which cater to a mostly western, wealthy, and female demographic. Khan also pointed out that because most slow fashion products are significantly more expensive than fast fashion items, consumers are required to have a certain amount of disposable income in order to participate in the movement. Khan argues that by proposing a solution to fast-fashion that is largely inaccessible to many consumers, they are positioning wealthier women as "agents of change" in the movement against fast fashion, whereas the shopping habits of lower income women are often considered "problematic".Andrea Chang provided a similar critique of the slow fashion movement in her article "The Impact of Fast Fashion on Women". She wrote that the slow and ethical fashion movements place too much responsibility on the consumers of fast fashion clothing, most of whom are women, to influence the industry through their consumption. Chang suggests that because most consumers are limited in their ability to choose where and how they purchase clothing, largely due to financial factors, anti-fast fashion activists should target lawmakers, manufacturers, and investors with a stake in the fast fashion industry rather than create an alternative industry that is only accessible to some.

Economics
Fast fashion proves successful economically for the retail industry worldwide. The fast-fashion market in 2020 globally produced $25.1 billion. It was expected to increase at an annual compound growth rate (CAGR) of 21.9%, resulting in the global market increase to $31 billion in 2021. By 2030, it is estimated that the fast fashion industry will bring a revenue of $192 billion to the world's global economy.This economic growth from fast fashion is demonstrated through how companies like H&M or Shein strategize in manufacturing. Most fast fashion clothes exporters are from developing countries across Asia, such as India, Bangladesh, Vietnam, China, Indonesia, and Cambodia. Developing countries' economies rely on fast fashion consumption as most export earnings profit from ready-made clothes. China, for example, has gained a yearly profit of $158.4 billion from exporting such clothes. Additionally, the hazardous working circumstances in these employees endure have an adverse effect on their health, increasing the risk of illness and accidents among their coworkers and having a negative effect on the labor force around the world.

Manufacturing
The fast fashion industry is able to thrive economically through the low production costs of their manufacturers in Asia. One low production cost is the investment cost of materials to make a garment. Fast fashion invests in polyester and cotton fabric because they are inexpensive and durable. In 2020, polyester's global price per metric ton was $725 (or 32.9 cents per pound), and the global price for cotton in 2021 was 126 cents per pound.According to these statistics, polyester fabric is more affordable than cotton, but both are relativity cheaper than higher quality fabric such as silk or wool. One basic T-shirt would require .5 lbs of cotton material, resulting in less than $1 of cotton fabric used. Inexpensive materials allow the fast fashion industry to produce a high volume of affordable clothes at low production costs. Therefore, retailers profit from selling high volumes of affordable designer clothes by its increased markup price from the material cost.

Wage criticisms
The fast fashion industry faces criticism for hiring garments workers from developing countries for their low wages. There are more than 60 million workers that produce garments for the fast fashion retail, and 80% of those workers are women. MVO Netherlands researched in 2019 that workers' monthly wages in Ethiopia that manufacture for H&M, Gap, and JCPenney begins at $32, while an experienced worker is $122 a month. The lowest hourly wage for workers in developing countries is less than .50 cents. In developed countries like the United States, the average garment worker in Los Angeles, reported by the Garment Worker Center (GWC), is about $5.15 per hour despite the federal minimum wage being $7.25 per hour in 2016.Hence, workers' monthly income would be about $858 (if they worked 40 hours a week), which is a much higher salary than in developing countries but still lower than the U.S. standard of living in income conditions. To reach the target goals of consumer demands from the U.S. and Europe, garment laborers in developing countries, on average, are expected to work 11 hours a day. Fast fashion retailers face economic criticism for paying garment laborers from developing and developed countries unlivable wages while imposing long work hours.

Strategy
Management
Fashion is updated frequently to meet peoples demand for the availability of the newest and latest clothing styles. The efficiency is achieved through the retailers' understanding of the target market's wants, which is a high fashion-looking garment at a price at the lower end of the clothing sector. One of the largest causes of the high demand is the short trend cycles: The more an audience is exposed to new trends, the higher the demand grows. Primarily, the concept of category management has been used to align the retail buyer and the manufacturer in a more collaborative relationship.

Quick response method
Quick Response (QR)  was developed to improve manufacturing processes in the textile industry to remove time from the production system. The U.S. Apparel Manufacturing Association initiated the project in the early 1980s to address a competitive threat to its textile manufactures from imported textiles in countries with low labor costs. During the project, lead times in the manufacturing process were halved; the U.S. industry became more competitive for a time, and imports were lowered as a result.
The QR initiative was viewed by many as a protection mechanism for the American textile industry with the aim of improving manufacturing efficiencies.Quick response is now used to support fast fashion, creating new products while drawing consumers back to the retail experience for consecutive visits. Quick response also makes it possible for new technologies to increase production and efficiency, typified by the introduction of the complementary concept of Fast Fit. The Spanish mega chain Zara, owned by Inditex, has become the global model for how to decrease the time between design and production. This production shortcut enables the company to manufacture over 30,000 units of product every year to nearly 1,600 stores in 58 countries.New items are delivered twice a week to the stores, reducing the time between initial sale and replenishment. As a result, the shortened time period improves consumer's garment choices and product availability while significantly increasing the number of per customer visits per annum. In the case of Renner, a Brazilian chain, a new mini-collection is released every two months.

Delivery and waste
Fast fashion typically offers buyers quick shipping, meaning delivery can be same day or only take a few days. Due to constantly evolving trends, buyers need to have their item before it is no longer in style. Oftentimes, fast fashion brands will offer the buyer deals, where they can spend a certain amount of money to get free shipping. This creates a lot of impulse buying, resulting in the items being returned. However, fast fashion returns do not always get sold again. The company will likely throw the item out because it is no longer in style.

Marketing
Marketing is a key driver of fast fashion, creating the desire for consumption of new designs as close as possible to the point of creation. Marketing closes the gap between creation and consumption by promoting something fast, low-priced, and disposable. The continuous release of new products essentially makes the garments a highly cost-effective marketing tool that drives consumer visits, increases brand awareness, and results in higher rates of consumer purchases. Fast fashion companies have higher profit margins due to their lower % markdown percentage of 15% compared to competitors' 30% plus. The fast fashion business model reduces time cycles from production to consumption, stimulating sales through trends that change throughout the seasons. For example, the traditional fashion seasons followed the annual cycle of summer, autumn, winter and spring, but in fast fashion cycles have compressed into shorter periods of 4–6 weeks and in some cases less. Marketers have thus created more buying seasons in the same time-space.Companies use two marketing strategies, since the main difference is the amount of advertisement spending. While some companies invest in advertising, others like Primark operate with no advertising, investing in store layout and visual merchandising to create the instant hook. Research shows that 75 percent of consumers' decisions are made in front of a fixture within three seconds.

Social media marketing
In recent years, fast fashion retailers have taken a new approach to reaching consumers. Initially, social media's sole purpose was to act as a platform allowing people to connect with other users worldwide. However, social media has become a way for retailers to promote their products and impact consumer behavior. Now, consumers are able to look at products and businesses on social media before heading to a store or going online to make a purchase. Additionally, consumers can read real customer reviews on different social media accounts to get a better idea of the quality of the products as well as the customer service. Fast fashion retailers were quick to jump on the trend. Fast fashion retailers like Boohoo.com realized that social media advertisements could be a great way to reach their target audience, young girls. Such users were swarmed with fast fashion advertisements each time they opened Instagram. Companies like Boohoo hoped that the constant exposure to their products would influence users to not only visit their website, but also to buy clothing from them.Instead of posting pre-made ads on their accounts, fast fashion retailers realized that an effective way to advertise could be to use social media influencers. Social media influencers can be defined as "regular" individuals who have accrued a large number of followers across multiple social media platforms as a result of the content they post. For the most part, influencers focus their content on one subject area, like food or fashion and have become their own kind of "internet celebrities" whom followers value and whose opinions they trust. As a result, when social media influencers post content wearing an outfit from Shein, their followers may feel compelled to purchase clothing from that retailer too. Studies have shown that there is a correlation between following social media influencers and shopping more frequently. Even though some fast fashion retailers still have "celebrity ambassadors", many retailers have turned to social media influencers to promote their clothing.The world saw a surge in these social media marketing practices during the coronavirus pandemic. Shein quickly took center stage across numerous social media platforms. Social media users, specifically young women, could not go online without seeing something from this fast fashion website, and "Shein hauls" became one of the most popular trends on TikTok, with 4.7 billion #sheinhaul views as of March 2022. Haul videos consist of individuals recording themselves showing items they purchased (typically a large quantity) and posting the video on platforms like YouTube or TikTok. Amid a global pandemic, these billions of views allowed Shein to bring in about $10 billion in revenue that year.

Production
"Supermarket" market
The consumer in the fast fashion market thrives on constant change and the frequent availability of new products. Fast fashion is considered to be a "supermarket" segment within the larger sense of the fashion market. This term refers to fast fashion's nature to "race to make apparel an even smarter and quicker cash generator". Three crucial differentiating model factors exist within fast fashion consumption: market timing, cost, and the buying cycle. Timing's objective is to create the shortest production time possible. The quick turnover has increased the demand for the number of seasons presented in the stores. This demand also increases shipping and restocking time periods. Cost is still the consumer's primary buying decision. Costs are largely reduced by taking advantage of lower prices in markets in developing countries. In 2004, developing countries accounted for nearly 75 percent of all clothing exports and the removal of several import quotas has allowed companies to take advantage of the even lower cost of resources. The buying cycle is the final factor that affects the consumer. Traditionally, fashion buying cycles are based around long-term forecasts that occur one year to six months before the season.

Supply chain, vendor relationships and internal relationships
Supply chain
Supply chains are central to the creation of fast fashion, and supply chain systems are designed to add value and reduce cost in the process of moving goods from design concept to retail stores and through to consumption. The selection of a merchandising vendor is a key part in the process. Inefficiency primarily occurs when suppliers cannot respond quickly enough, and clothing ends up bottlenecked and in back stock. Two kinds of supply chains exist, agile and lean. In an agile supply chain, the principal characteristics include sharing information and technology. The collaboration results in the reduction in the amount of stock in megastores. A lean supply chain is characterized as the correct appropriation of the commodity for the product.

Vendor relationships
The companies in the fast fashion market also utilize a range of relationships with the suppliers. The product is first classified as "core" or "fashion".

Internal relationships
Productive internal relationships within the fast fashion companies are as important as the company's relationships with external suppliers, especially regarding the company's buyers. Traditionally with a "supermarket" market the buying is divided into multi-functional departments. The buying team uses the bottom-up approach when trend information is involved, meaning the information is only shared with the company's fifteen top suppliers. On the other hand, information about future aims, and strategies of production are shared downward within the buyer hierarchy so the team can consider lower cost production options.

Environmental impact
According to the United Nations Economic Commission for Europe, the fast fashion system provides opportunities for economic growth but the entire industry hinders sustainability efforts by contributing to 20% of wastewater. In addition, fast fashion is responsible for nearly 10 percent of global gas emissions. Providing insight, the Ellen MacArthur Foundation released study results on fashion and suggests a new circular system. A singular t-shirt requires over 2,000 liters of water to make. Clothing is not utilized to its full potential, the Ellen MacArthur Foundation explains that linear systems are contributing to unsustainable behavior and the future of fashion may need to transition towards a circular system of production and consumer behavior.Journalist Elizabeth L. Cline, author of Overdressed: The Shockingly High Cost of Cheap Fashion and one of the earliest critics of fast fashion, notes in her Atlantic article Where Does Discarded Clothing Go? that Americans are purchasing five times the amount of clothing than they did in 1980. Due to this rise in consumption, developed countries are producing more and more garments each season with the U.S. importing more than 1 billion garments annually from China alone. United Kingdom textile consumption surged by 37% from 2001 to 2005.
The Global Fashion Business Journal reported that in 2018, the global fiber production has reached the highest all-time, 107 million metric tons.The average American household produces 70 pounds (32 kg) of textile waste every year. The residents of New York City discard around 193,000 tons of clothing and textiles, which equates to 6% of all the city's garbage. In comparison, the European Union generates a total of 5.8 million tons of textiles each year. As a whole, the textile industry occupies roughly 5% of all landfill space. This means that the clothing industry produces about 92 million tons of textile waste annually, much of which is burned or goes into a landfill and less than 1% of used clothing is recycled into new garments. The clothing that is discarded into landfills is often made from non-biodegradable synthetic materials.Greenhouse gases and various pesticides and dyes are released into the environment by fashion-related operations. The United Nations estimated that the business of what we wear, including its long supply chains, is responsible for 10 percent of the greenhouse gas emissions heating our planet. The growing demand for quick fashion continuously adds effluent release from the textile factories, containing both dyes and caustic solutions. In comparison, greenhouse gas emissions from textile production companies is more than international flights and maritime shipping combined annually. The materials used not only affect the environment in textile products, but also the workers and the people who wear the clothes. The hazardous substances affect all aspects of life and release into the environments around them. Optoro estimates that 5 billion pounds of waste is generated through returns each year, contributing 15 million metric tons of carbon dioxide to the atmosphere. Fast fashion production has doubled since 2000, with brands such as Zara producing 24 collections a year and H&M producing about 12 to 16 collections a year.

Sustainability
Recycling
The speed of clothing consumption has increased substantially since the late 1990s across the world. All aspects of fast fashion have elements that are not environmentally friendly, the amounts of waste from disposal of textiles into the garbage system is increasing beyond the industries capabilities. The fast fashion industry currently has little to do with the end of life cycle of clothing, however, with recent social pressures some fast fashion companies collect and export their disposed textiles to developing countries for charity. As the production increases and charities are beginning to turn away fast fashion for being cheaply made, organizations are struggling to come up with sustainable solutions to continue against the social and soon governmental pressure. There are many organizations that provide educational tools on how to reuse and recycle textiles to interested individuals, such as "Human Bridge (charitable organization)". Additionally, the retail and textile chains that encourage recycling or reuse often provide incentives, such as Lindex, which offered a rebate to customers who turned in their clothes.There are the organizations that work to recycle the material into new usable materials for a wide variety of industry needs. Working with the Swedish Red Cross, the Swedish Prison and Probation Service is able to provide textile packing material to the shipping industry, additionally, more and more recycling programs like StenaRecycling are beginning to find new ways to use textiles to reach a large audience, being able to create construction materials, stuffing, and new and improved textiles.Polyester and cotton dominate the textile industry with the synthetic fiber polyester exceeding production of cotton since 2002. Fast fashion has caused a spike in textile waste, with no stop in production, waste management is needed. After clothing is reused until it's beyond usable for its given function, recycling it through a mechanical or chemical process is the next step. One concern with recycling textiles is the loss of "virgin material", however, chemical recycling can extract the "virgin materials" like protein-based and cellulosic fibers to produce new products. The deterioration of material to provide new products is the process of mechanical recycling.There are categories or types of recycling that can be done: upcycling, downcycling, closed-loop, and open-loop recycling. Upcycling is the process of using a textile to create something higher quality than the original. Downcycling is using a textile in a way that is less than the original value. Closed-loop recycling is the reuse of one textile over and over again to create the same piece. Open-loop recycling is the process of creating something new with the textile piece. The EU is currently taking initiative to enforce circularity, closed-loop recycling, in the clothing cycle encouraging a less wasteful lifestyle by supporting second-hand and organic clothing pieces, organic in this case being cotton, silk, etc. Even the US in New York City has begun working with natural fibers like bamboo and hemp to make not just clothing but bags as well.There are many technologies that assist in the recycling of textile products:

Anaerobic Digestion of Textile Waste - decomposition of organic cotton textile to collect methane and other biogas
Fermentation of Textile Waste for Ethanol Production - cotton fabric provides enhancement of bioethanol production
Composting of Textile Waste - cotton waste provides an excellent source of nutrients in compost
Fiber Regeneration from Textile Waste - recovery of glucose and polyester is possible and allows for reuse of material
Building/Construction Material from Textile Waste - use of textiles in building materials and construction
Thermal Recovery - incineration of remaining textiles to collect usable energy

Design strategies & techniques
According to FutureLearn, the following design strategies and techniques can be applied to make fast fashion more sustainable:

Zero Waste Pattern Cutting: This technique eliminates potential textile waste right at the design stage, where the pattern pieces are strategically laid like a jigsaw puzzle onto a precisely measured piece of fabric.
Minimal Seam Construction: This technique allows faster manufacturing time by lessening the number of seams that are necessary to stitch a garment.
Design for Disassembly (DfD): The main intention of this strategy involves designing a product in such a way that it can be easily taken apart at the end of its lifespan and this allows the use of fewer materials.
Craft preservation: This technique combines and incorporates ancestral craft techniques into modern designs and in a way it ensures preservation of traditional craftsmanship through innovation.
Pull Factor Framework: Brands such as L.L Bean and Harvey Nichols implemented a "Pull Factor Framework" which is a new methodology that strives to make sustainable innovation more enticing for consumers and producers alike.

Technology
Fast fashion brands like ASOS.com, Levi's, Macy's, North Face have turned to sizing technology that use algorithms to solve sizing issues, and give accurate size recommendations on their website to reduce environmental impact on returns. H&M's design team is implementing 3D design, 3D sampling and 3D prototyping to help cut waste, while artificial intelligence can be used to produce small garment runs for specific stores.Companies are helping support the circular system in fashion production and consumer behavior by renting out clothes to customers with recycled or reuse items. New York & Company Closet and American Eagle Style Drop are examples of rental services that can be offered to customers when subscribed to the program. Tulerie, a smartphone application offers borrowing, renting, or sharing of clothes in local communities across the globe; users have the opportunity to profit by renting clothes as well.

Overconsumption
In contrast to modern overconsumption, fast fashion traces its roots to World War II austerity, where high design was merged with utilitarian materials. The business model of fast fashion is based on consumers’ desire for new clothing to wear. In order to fulfill consumers' demand, fast fashion brands provide affordable prices and a wide range of clothing that reflects the latest trends. This ends up persuading consumers to buy more items which leads to the issue of overconsumption. Dana Thomas, author of Fashionopolis, stated that Americans spent 340 billion dollars on clothing in 2012, the same year of the Rana Plaza collapse.Planned obsolescence plays a key role in overconsumption. Based on the study of planned obsolescence in The Economist, fashion is deeply committed to planned obsolescence. Last year's skirts; for example, are designed to be replaced by this year's new models. In this case, fashion goods are purchased even when the old ones are still wearable. The quick response model and new supply chain practices of fast fashion even accelerate the speed of it. In recent years, the fashion cycle has steadily decreased as fast fashion retailers sell clothing that is expected to be disposed of after being worn only a few times.A 2014 article about fast fashion in Huffington Post pointed out that in order to make the fast moving trend affordable, fast-fashion merchandise is typically priced much lower than the competition, operating on a business model of low quality and high volume. Low quality goods make overconsumption more severe since those products have a shorter life span and would need to be replaced much more often. Furthermore, as both industry and consumers continue to embrace fast fashion, the volume of goods to be disposed of or recycled has increased substantially. However, most fast-fashion goods do not have the inherent quality to be considered as collectables for vintage or historic collections.

Labour concerns
Sweatshops
The fashion industry is known as the most labor dependent industry, as one in every six people works in acquiring raw materials and manufacturing clothing. There is an increasing concern for sweatshops as more fast fashion stores are lowering their prices and trends are fluctuating more frequently. Brands and store companies that use sweatshops are GAP, H&M, Zara, Abercrombie and Fitch and plenty of others.In particular H&M faced controversial issues and backlash regarding their sweatshops in Asian countries. H&M is the largest producer of clothing in under-developed South Asian and Southeast Asian countries such as India, Bangladesh and Cambodia. 500 employees in Indonesia left their work and protested for higher pay that was below the minimum wage for their country. Once a strike evolved, the factory removed their access to the building and paid men to harass the workers.Nike has received backlash over its use of sweatshops. Bangladesh – a country known for its cheap labor, is home to four million garment production workers in over 5000 factories, out of which 85% are women. Many of these factories do not have proper working conditions for essential workers. In 2013 a group of garment workers protested in Bangladesh for the poor quality of the building. A horrific tragedy took place in Rana Plaza factory, the building collapsed and killed over 1,000 workers. Not only did these workers have a badly manufactured building, they were overworked, Bangladesh is considered to have the lowest minimum wage from all the countries that export apparel.

Women and export processing zones
The International Labour Organization defines export processing zones as "industrial zones with special incentives set up to attract foreign investors, in which imported materials undergo some degree of processing before being re-exported". These zones have been used by developing countries to bolster foreign investment, and produce consumer goods that are labour-intensive, like clothing. Many export processing zones have been criticized for their substandard working conditions, low wages, and suspension of international and domestic labour laws. Women account for 70-90% of the working population in some export processing zones, such as in Sri Lanka, Bangladesh, and the Philippines. Despite their overrepresentation in export processing zone informal sector (informal economy) employment, women are still likely to earn less than men. Mainly, this discrepancy is due to employer's preferring to hire men in technical and managerial positions and women in lower-skilled production work. Moreover, employers tend to prefer hiring women for production jobs because they are seen as more compliant and less likely to join labour unions. In addition, a report that interviewed Sri Lankan women working in export processing zones found that gender-based violence "emerged as a dominant theme in their narratives". For example, 38% of women reported seeing or experiencing sexual harassment within their workplace. However, proponents of textile and garment production as a means for economic upgrading in developing countries (global value chain) have pointed out that clothing production work tends to have higher wages than other available jobs, such as agriculture or domestic service work, and therefore provides women with a larger degree of financial autonomy.

Film and media
The True Cost is a 2015 documentary film focusing on fast fashion that is directed by Andrew Morgan.
'How fast fashion adds to the world's clothing waste problem' is a short 2018 documentary created by Marketplace that is a part of the CBC News network.

Design legislation and lawsuits
United States
H.R. 5055
H.R. 5055, or Design Piracy Prohibition Act, was a bill proposed to protect the copyright of fashion designers in the United States. The bill was introduced into the United States House of Representatives on March 30, 2006. Under the bill designers would submit fashion sketches and/or photos to the U.S. Copyright Office within three months of the products' "publication". This publication includes everything from magazine advertisements to the garment's first public runway appearances. The bill would protect the designs for three years after the initial publication. If infringement of copyright occurred the infringer would be fined $250,000, or $5 per copy, whichever is a larger lump sum.

H.R. 2033
The Design Piracy Prohibition Act was reintroduced as H.R. 2033 during the first session of the 110th Congress on April 25, 2007. It had goals similar to H.R. 5055, as the bill proposed to protect certain types of apparel design through copyright protection of fashion design. The bill would grant fashion designs a three-year term of protection, based on registration with the U.S. Copyright Office. The fines of copyright infringement would continue to be $250,000 total or $5 per copied merchandise.

Lawsuits
As of 2007, Forever 21, one of the larger fast fashion retailers, was involved in several lawsuits over alleged violations of intellectual property rights. The lawsuits contended that certain pieces of merchandise at the retailer can effectively be considered infringements of designs from Diane von Furstenberg, Anna Sui and Gwen Stefani's Harajuku Lovers line as well as many other well-known designers. Forever 21 has not commented on the state of the litigation but initially said it was "taking steps to organize itself to prevent intellectual property violations".

See also
Impact of fast fashion in China
Cost per wear
Slow fashion
Digital fashion

References
Further reading
MacKinnon, J.B. (28 May 2021). "What would happen if the world stopped shopping?". Fast Company. Retrieved 4 July 2021.

Financial accounting

Financial accounting is a branch of accounting concerned with the summary, analysis and reporting of financial transactions related to a business. This involves the preparation of financial statements available for public use. Stockholders, suppliers, banks, employees, government agencies, business owners, and other stakeholders are examples of people interested in receiving such information for decision making purposes.
Financial accountancy is governed by both local and international accounting standards. Generally Accepted Accounting Principles (GAAP) is the standard framework of guidelines for financial accounting used in any given jurisdiction. It includes the standards, conventions and rules that accountants follow in recording and summarizing and in the preparation of financial statements.
On the other hand, International Financial Reporting Standards (IFRS) is a set of  accounting standards stating how particular types of transactions and other events should be reported in financial statements. IFRS are issued by the International Accounting Standards Board (IASB). With IFRS becoming more widespread on the international scene, consistency in financial reporting has become more prevalent between global organizations.
While financial accounting is used to prepare accounting information for people outside the organization or not involved in the day-to-day running of the company, managerial accounting provides accounting information to help managers make decisions to manage the business.

Objectives
Financial accounting and financial reporting are often used as synonyms.
1. According to International Financial Reporting Standards: the objective of financial reporting is:

To provide financial information that is useful to existing and potential investors, lenders and other creditors in making decisions about providing resources to the reporting entity.
2. According to the European Accounting Association:

Capital maintenance is a competing objective of financial reporting.
Financial accounting is the preparation of financial statements that can be consumed by the public and the relevant stakeholders. Financial information would be useful to users if such qualitative characteristics are present. When producing financial statements, the following must comply: 
Fundamental Qualitative Characteristics:

Relevance: Relevance is the capacity of the financial information to influence the decision of its users. The ingredients of relevance are the predictive value and confirmatory value. Materiality is a sub-quality of relevance. Information is considered material if its omission or misstatement could influence the economic decisions of users taken on the basis of the financial statements.
Faithful Representation: Faithful representation means that the actual effects of the transactions shall be properly accounted for and reported in the financial statements. The words and numbers must match what really happened in the transaction. The ingredients of faithful representation are completeness, neutrality and free from error. It signifies that the accountants have acted in good faith during the process of representation.Enhancing Qualitative Characteristics:

Verifiability: Verifiability implies consensus between the different knowledgeable and independent users of financial information. Such information must be supported by sufficient evidence to follow the principle of objectivity.
Comparability: Comparability is the uniform application of accounting methods across entities in the same industry. The principle of consistency is under comparability. Consistency is the uniform application of accounting across points in time within an entity.
Understandability: Understandability means that accounting reports should be expressed as clearly as possible and should be understood by those to whom the information is relevant.
Timeliness: Timeliness implies that financial information must be presented to the users before a decision is to be made.

Three components of financial statements
Statement of cash flows (cash flow statement)
The statement of cash flows considers the inputs and outputs in concrete cash within a stated period. The general template of a cash flow statement is as follows:
Cash Inflow - Cash Outflow + Opening Balance = Closing Balance
Example 1: in the beginning of September, Ellen started out with $5 in her bank account. During that same month, Ellen borrowed $20 from Tom. At the end of the month, Ellen bought a pair of shoes for $7. Ellen's cash flow statement for the month of September looks like this:

Cash inflow: $20
Cash outflow:$7
Opening balance: $5
Closing balance: $20 – $7 + $5 = $18Example 2: in the beginning of June, WikiTables, a company that buys and resells tables, sold 2 tables. They'd originally bought the tables for $25 each, and sold them at a price of $50 per table. The first table was paid out in cash however the second one was bought in credit terms. WikiTables' cash flow statement for the month of June looks like this:

Cash inflow: $50 - How much WikiTables received in cash for the first table. They didn't receive cash for the second table (sold in credit terms).
Cash outflow: $50 - How much they'd originally bought the 2 tables for.
Opening balance: $0
Closing balance: $50 – 2*$25 + $0 = $50–50=$0 - Indeed, the cash flow for the month of June for WikiTables amounts to $0 and not $50.Important: the cash flow statement only considers the exchange of actual cash, and ignores what the person in question owes or is owed.

Statement of financial performance (income statement, profit & loss (p&l) statement, or statement of operations)
The statement of profit or income statement represents the changes in value of a company's accounts over a set period (most commonly one fiscal year), and may compare the changes to changes in the same accounts over the previous period. All changes are summarized on the "bottom line" as net income, often reported as "net loss" when income is less than zero.
The net profit or loss is determined by:
Sales (revenue)
– cost of goods sold
– selling, general, administrative expenses (SGA)
– depreciation/ amortization
= earnings before interest and taxes (EBIT)
– interest and tax expenses
= profit/loss

Statement of financial position (balance sheet)
The balance sheet is the financial statement showing a firm's assets, liabilities and equity (capital) at a set point in time, usually the end of the fiscal year reported on the accompanying income statement. The total assets always equal the total combined liabilities and equity. This statement best demonstrates the basic accounting equation:
Assets = Liabilities + Equity

 The statement can be used to help show the financial position of a company because liability accounts are external claims on the firm's assets while equity accounts are internal claims on the firm's assets. 
Accounting standards often set out a general format that companies are expected to follow when presenting their balance sheets. International Financial Reporting Standards (IFRS) normally require that companies report current assets and liabilities separately from non-current amounts. A GAAP-compliant balance sheet must list assets and liabilities based on decreasing liquidity, from most liquid to least liquid. As a result, current assets/liabilities are listed first followed by non-current assets/liabilities. However, an IFRS-compliant balance sheet must list assets/liabilities based on increasing liquidity, from least liquid to most liquid. As a result, non-current assets/liabilities are listed first followed by current assets/liabilities.Current assets are the most liquid assets of a firm, which are expected to be realized within a 12-month period. Current assets include:

cash - physical money
accounts receivable - revenues earned but not yet collected
Merchandise inventory - consists of goods and services a firm currently owns until it ends up getting sold
Investee companies - expected to be held less than one financial period
prepaid expenses - expenses paid for in advance for use during that yearNon-current assets include fixed or long-term assets and intangible assets: 

fixed (long term) assets
property
building
equipment (such as factory machinery)
intangible assets
copyrights
trademarks
patents
goodwillLiabilities include:

current liabilities
trade accounts payable
dividends payable
employee salaries payable
interest (e.g. on debt) payable
long term liabilities
mortgage notes payable
bonds payableOwner's equity, sometimes referred to as net assets, is represented differently depending on the type of business ownership. Business ownership can be in the form of a sole proprietorship, partnership, or a corporation. For a corporation, the owner's equity portion usually shows common stock, and retained earnings (earnings kept in the company). Retained earnings come from the retained earnings statement, prepared prior to the balance sheet.

Statement of retained earnings (statement of changes in equity)
This statement is additional to the three main statements described above. It shows how the distribution of income and transfer of dividends affects the wealth of shareholders in the company. The concept of retained earnings means profits of previous years that are accumulated till current period. Basic proforma for this statement is as follows:
Retained earnings at the beginning of period
+ Net Income for the period
- Dividends
= Retained earnings at the end of period.

Basic concepts
The stable measuring assumption
One of the basic principles in accounting is "The Measuring Unit principle": The unit of measure in accounting shall be the base money unit of the most relevant currency. This principle also assumes the unit of measure is stable; that is, changes in its general purchasing power are not considered sufficiently important to require adjustments to the basic financial statements."
Historical Cost Accounting, i.e., financial capital maintenance in nominal monetary units, is based on the stable measuring unit assumption under which accountants simply assume that money, the monetary unit of measure, is perfectly stable in real value for the purpose of measuring (1) monetary items not inflation-indexed daily in terms of the Daily CPI and (2) constant real value non-monetary items not updated daily in terms of the Daily CPI during low and high inflation and deflation.

Units of constant purchasing power
The stable monetary unit assumption is not applied during hyperinflation. IFRS requires entities to implement capital maintenance in units of constant purchasing power in terms of IAS 29 Financial Reporting in Hyperinflationary Economies.
Financial accountants produce financial statements based on the accounting standards in a given jurisdiction. These standards may be the Generally Accepted Accounting Principles of a respective country, which are typically issued by a national standard setter, or International Financial Reporting Standards (IFRS), which are issued by the International Accounting Standards Board (IASB).
Financial accounting serves the following purposes:

producing general purpose financial statements
producing information used by the management of a business entity for decision making, planning and performance evaluation
producing financial statements for meeting regulatory requirements.

Objectives of financial accounting
Systematic recording of transactions: basic objective of accounting is to systematically record the financial aspects of business transactions (i.e. book-keeping).  These recorded transactions are later on classified and summarized logically for the preparation of financial statements and for their analysis and interpretation.
Ascertainment of result of above recorded transactions: accountant prepares profit and loss account to know the result of business operations for a particular period of time. If expenses exceed revenue then it is said that the business is running under loss. The profit and loss account helps the management and different stakeholders in taking rational decisions. For example, if business is not proved to be remunerative or profitable, the cause of such a state of affairs can be investigated by the management for taking remedial steps.
Ascertainment of the financial position of business: businessman is not only interested in knowing the result of the business in terms of profits or loss for a particular period but is also anxious to know that what he owes (liability) to the outsiders and what he owns (assets) on a certain date. To know this, accountant prepares a financial position statement of assets and liabilities of the business at a particular  point of time and helps in ascertaining the financial health of the business.
Providing information to the users for rational decision-making: accounting as a 'language of business' communicates the financial result of an enterprise to various stakeholders by means of financial statements. Accounting aims to meet the financial information needs of the decision-makers and helps them in rational decision-making.
To know the solvency position: by preparing the balance sheet, management not only reveals what is owned and owed by the enterprise, but also it gives the information regarding concern's ability to meet its liabilities in the short run (liquidity position) and also in the long-run (solvency position) as and when they fall due.

Graphic definition
The accounting equation (Assets = Liabilities + Owners' Equity) and financial statements are the main topics of financial accounting.
The trial balance, which is usually prepared using the double-entry accounting system, forms the basis for preparing the financial statements. All the figures in the trial balance are rearranged to prepare a profit & loss statement and balance sheet. Accounting standards determine the format for these accounts (SSAP, FRS, IFRS). Financial statements display the income and expenditure for the company and a summary of the assets, liabilities, and shareholders' or owners' equity of the company on the date to which the accounts were prepared.
Asset, expense, and dividend accounts have normal debit balances (i.e., debiting these types of accounts increases them).
Liability, revenue, and equity accounts have normal credit balances (i.e., crediting these types of accounts increases them).

0 = Dr Assets                            Cr Owners' Equity                Cr Liabilities  
          .       _____________________________/\____________________________       .
          .      /    Cr Retained Earnings (profit)         Cr Common Stock  \      .
          .    _________________/\_______________________________      .            .
          .   / Dr Expenses       Cr Beginning Retained Earnings \     .            .
          .     Dr Dividends      Cr Revenue                           .            .
      \________________________/  \______________________________________________________/
       increased by debits           increased by credits

          Crediting a credit                         
Thus -------------------------> account increases its absolute value (balance)
           Debiting a debit                             

          Debiting a credit                         
Thus -------------------------> account decreases its absolute value (balance)
          Crediting a debit

When the same thing is done to an account as its normal balance it increases; when the opposite is done, it will decrease.  Much like signs in math: two positive numbers are added and two negative numbers are also added.  It is only when there is one positive and one negative (opposites) that you will subtract.
However, it is important to note that there are instances of accounts, known as contra-accounts, which have a normal balance opposite that listed above. Examples include:

Contra-asset accounts (such as accumulated depreciation and allowances for bad debt or obsolete inventory)
Contra-revenue accounts (such as sales allowances)
Contra-equity accounts (such as treasury stock)

Financial accounting versus cost accounting
Financial accounting aims at finding out results of accounting year in the form of Profit and Loss Account and Balance Sheet. Cost Accounting aims at computing cost of production/service in a scientific manner and facilitate cost control and cost reduction.
Financial accounting reports the results and position of business to government, creditors, investors, and external parties.
Cost Accounting is an internal reporting system for an organisation's own management for decision making.
In financial accounting, cost classification based on type of transactions, e.g. salaries, repairs, insurance, stores etc. In cost accounting, classification is basically on the basis of functions, activities, products, process and on internal planning and control and information needs of the organization.
Financial accounting aims at presenting 'true and fair' view of transactions, profit and loss for a period and Statement of financial position (Balance Sheet) on a given date.  It aims at computing 'true and fair' view of the cost of production/services offered by the firm.

Related qualification
Many professional accountancy qualifications cover the field of financial accountancy, including Certified Public Accountant CPA, Chartered Accountant (CA or other national designations, American Institute of Certified Public Accountants AICPA and Chartered Certified Accountant (ACCA).

See also
Constant item purchasing power accounting
DIRTI 5
Historical cost accounting
Philosophy of accounting
Accounting analyst, whose job involves evaluating public company financial statements
Management accounting, the other main division of accounting
Bookkeeping

References
Further reading
David Annand, Introduction to Financial Accounting, Athabasca University, ISBN 978-0-9953266-4-4
Financial Accounting (2015) doi:10.24926/8668.0701 ISBN 978-1-946135-10-0
Johnny Jackson, Introduction to Financial Accounting, Thomas Edison State University.
Alexander, D., Britton, A., Jorissen, A., "International Financial Reporting and Analysis", Second Edition, 2005, ISBN 978-1-84480-201-2

Financial law

Financial law is the law and regulation of the commercial banking, capital markets, insurance, derivatives and investment management sectors. Understanding financial law is crucial to appreciating the creation and formation of banking and financial regulation, as well as the legal framework for finance generally. Financial law forms a substantial portion of commercial law, and notably a substantial proportion of the global economy, and legal billables are dependent on sound and clear legal policy pertaining to financial transactions. Therefore financial law as the law for financial industries involves public and private law matters. Understanding the legal implications of transactions and structures such as an indemnity, or overdraft is crucial to appreciating their effect in financial transactions. This is the core of financial law. Thus, financial law draws a narrower distinction than commercial or corporate law by focusing primarily on financial transactions, the financial market, and its participants; for example, the sale of goods may be part of commercial law but is not financial law. Financial law may be understood as being formed of three overarching methods, or pillars of law formation and categorised into five transaction silos which form the various financial positions prevalent in finance.
Financial regulation can be distinguished from financial law in that regulation sets out the guidelines, framework and participatory rules of the financial markets, their stability and protection of consumers, whereas financial law describes the law pertaining to all aspects of finance, including the law which controls party behaviour in which financial regulation forms an aspect of that law.Financial law is understood as consisting of three pillars of law formation, these serve as the operating mechanisms on which the law interacts with the financial system and financial transactions generally. These three components, being market practices, case law, and regulation; work collectively to set a framework upon which financial markets operate. Whilst regulation experienced a resurgence following the financial crisis of 2007–2008, the role of case law and market practices cannot be understated. Further, whilst regulation is often formulated through legislative practices; market norms and case law serve as primary architects to the current financial system and provide the pillars upon which the markets depend. It is crucial for strong markets to be capable of utilising both self-regulation and conventions as well as commercially mined case law. This must be in addition to regulation. An improper balance of the three pillars is likely to result in instability and rigidity within the market contributing to illiquidity. For example, the soft law of the Potts QC Opinion in 1997 reshaped the derivatives market and helped expand the prevalence of derivatives. 
These three pillars are underpinned by several legal concepts upon which financial law depends, notably, legal personality, set-off, and payment which allows legal scholars to categorise financial instruments and financial market structures into five legal silos; those being (1) simple positions, (2) funded positions, (3) asset-backed positions, (4) net positions, and (5) combined positions. These are used by academic Joanna Benjamin to highlight the distinctions between various groupings of transaction structures based on common underpinnings of treatment under the law. The five position types are used as a framework to understand the legal treatment and corresponding constraints of instruments used in finance (such as, for example, a guarantee or asset-backed security).

Three pillars of financial law formation
Three different (and indeed inconsistent) regulatory projects exist which form the law within financial law. These are based on three different views of the proper nature of financial market relationships.

Market practices
The market practices in the financial field constitute a core aspect of the source of law of the financial markets, primarily within England & Wales. The actions and norms of parties in creating standard practices creates a fundamental aspect of how those parties self-regulate. These market practices create internal norms which parties abide by, correspondingly influencing legal rules which result when the market norms are either broken or are disputed through formal, court, judgments.The principle role is to form soft-law; as a source of rules of conduct which in principle have no legally binding force but have practical effects. This has created standard form of contracts for various financial trade associations such as Loan Market Association, which seeks to set guidance, codes of practice, and legal opinions. It is these norms, particularly those provided by Financial Market Law Committees, and City of London Law Societies which the financial market operates and therefore the courts are often quick to uphold their validity. Oftentimes "soft law" defines the nature and incidents of the relationships that participants of particular types of transactions expect.The implementation and value of soft law within the system, is particularly notable in its relationship with globalisation, consumer rights, and regulation. The FCA plays a central role in regulating the financial markets but soft law, voluntary or practice created legal schemes play a vital role. Soft law can fill market uncertainties what are produced by common law schemes. Obvious risk that that participants become lulled into believing statements of soft law is the law. However, the perception that an opinion constitutes ipso facto a clear and widely held opinion is wrong. For example, the consumer relationship in the case of Office of Fair Trading v Abbey National [2009] UKSC 6 where the bank was fined by the FSA for failing to handle complaints set out in soft law principle practices on broadly worded business principles which state that the bank must pay due regard to the interests of its customers and treat them fairly. Oftentimes the self-regulation of soft law can be problematic for consumer protection policies.
Another example of the expansiveness of soft law in the financial market is the explosion of Credit Derivatives in London, which has flourished on the back of the characteristically robust opinion of Potts for Allen & Overy regarding the ISDA Master Agreement in 1990 which helped the industry separate itself from current market restrictions. A the time, it was unclear whether Credit Derivatives were to be categorised as insurance contracts under English legislation of the Insurance Companies Act 1982. ISDA was firm in rejecting a statutory definition of insurance, stating that 

In practice market participants have had few concerns as to the impacts of boundary issues between CD's and contracts of insurance. This was crucial as Insurance companies were restricted from participating in other financial market activities and a licence needed to be granted to participate in the financial market. As a result of the Potts Opinion, credit derivatives were categorised as outside of insurance contracts, which allowed them to expand without the limitations set in place by insurance legislation.
Soft law has practical effects in that it is liable in many cases to be turned into "hard law", but with verified and experienced practice evidence. In the case Vanheath  Turner (1622) the court remarked that custom of merchants is part of the common law of the United Kingdom. This highlights a long history of incorporating and accounting for the lex mercatoria into the English law in order to facilitate financial markets. Law merchant had been so absorbed by the 18th century that the Bills of Exchange Act 1882 could provide common law rules and merchant law in tandem. We might consider Tidal Energy Ltd v Bank of Scotland, where Lord dyson held that "a many who employee a banker is bound by the usages of bankers" meaning that if a sort code and account number was correct, it did not matter if the name did not match.
There are risks on over-reliance on soft law sources, however. English law makes it difficult to create a type of security and reliance on rules may result in established views which reinforce errors. This could result in unacceptable security even if legally valid.

Case law
The second category which financial law draws most of its pragmatism with regard to the standards of the markets originates in litigation. Often, courts seek to reverse engineer matters to make commercially beneficial outcomes and so case law operates in a similar manner to market practice in producing efficient results.
There are two exceptions, attempting to limit the expectations to reasonable commercial men and uphold the freedom of contract. Autonomy is at the heart of commercial law and there is the strong case for autonomy in complex financial instruments. Re Bank of Credit and Commerce International SA (No 8) highlights the striking effect a commercially beneficial practice can have on financial law. Lord Hoffman upheld the validity of a security charge over a chose in action the bank held which it owed to a client. Despite the formidable conceptual problems in allowing a bank to place a charge over a debt the bank itself owed to another party, the courts have been driven to facilitate market practices as best as possible. Thus, they are careful to declare practices as conceptually impossible. In BCCI, the court held that a charge was no more than labels to self-consistent rules of law, an opinion shared Lord Goff in Clough Mill v Martin where he wrote concepts such as bailment and fiduciary duty must not be allowed to be our masters, but tools of the trade fashioning to aspects of life
Unfortunately, case coverage is unsystematic. Wholesale and international finance is patchy as a result of a preference to settle disputes through arbitration rather than through the courts. This has the potential to be detrimental to advancing the law regulating finance. Market participants generally prefer to settle disputes than litigate, this places a greater level of importance onto the "soft law" of market practices. However, in face of disaster, litigation is essential, especially surrounding major insolvencies, market collapse, wars, and frauds. The collapse of Lehman Brothers provides a good example, with 50 judgments from the English Court of Appeal and 5 from the Supreme Court of the United Kingdom. Despite these problems, there is a new breed of litigious lenders, primarily hedge funds, which has helped propel the pragmatic nature of financial case law past the 2008 crisis.

Regulation and legislation
The third category of law formation within the financial markets are those deriving from national and international regulatory and legislative regimes, which operate to regulate the practice of financial services. Three regulatory lenses ought to be highlighted namely arm's length, fiduciary, and consumerist approaches to financial relationships.
In the EU these might be exampled by MiFiD II, payment services directive, Securities settlement regulations and others which have resulted from the financial crisis or regulate financial trade. Regulatory control by the Financial Conduct Authority and Office of Fair trading set out clear rules replacing extra-statutory codes of conduct and has seen recent resurgence following the 2008 financial crisis. The regulatory policies have not all been rectified in regard to how they the new rules will be coherent with current market practices. We may consider In Re Lehman Brothers [2012] EWHC(Extended liens case) where Briggs J struggled to determine the legislative intent of the Financial Collateral Directive.

Financial collateral regulations
In addition to national and cross-national regulations on finance, additional rules are put into place in order to stabilise the financial markets by reinforcing the utility of collateral. In Europe, two regimes of collateral carve-outs exist; the Financial Collateral Directive, and the Financial Collateral Arrangement (No 2) Regulations 2003. The EU's development of the Financial Collateral Directive is curious if we view it through the lens of only a regulatory matter. It is clear that the law here developed through market practice and private law statutory reform. The EU has played a substantial role in this field to induce and encourage the ease of transfer & realisation of assets and liquidity within markets. The provisions are well adapted to short term transactions such as repos or derivatives.
Further harmonisation rules pertaining to commercial conflict of laws matters were clarified. The additional Geneva Securities Convention set by UNIDROIT provides a basic framework for minimum harmonised provisions governing rights conferred by the credit of securities to an account with an intermediary. However, this international project has as of late been ineffective with only Bangladesh signing.

Legal concepts prevalent in financial law
Several legal concepts underpin the law of finance. Of these, perhaps the most central concept is that of legal personality, the idea that the law can create non-natural persons is one of the most important common myths and among the most ingenious inventions for financial practice because it facilitates the ability to limit risk by creating legal persons which are separate. Other legal concepts, such as set-off and payment are crucial to preventing systemic risk by lessening the level of gross exposure of credit risk a financial participant might be exposed to on any given transaction. This is often mitigated through the use of collateral. If financial law is centrally concerned with the law pertaining to financial instruments or transactions, then it can be said that the legal effect of those transactions is to allocate risk.

Limited liability and legal personality
A limited liability company is an artificial creation of legislature which operates to limit the level of credit risk and exposure a person, natural or legal, will participate within. Lord Sumption summarised the position by stating Subject to very limited exceptions, most of which are statutory, a company is a legal entity distinct from its shareholders.  It has rights and liabilities of its own which are distinct from those of its shareholders. Its property is its own, and not that of its shareholders [...] [T]hese principles appl[y] as much to a company that  [i]s wholly owned and controlled by one man as to any other company For financial markets, the allocation of financial risk through separate legal personality allows for parties to participate in financial contracts and transfer credit risk between parties. The ambition of measuring the likelihood of future loss, that is of identifying risk, is a central part of the role legal liability plays in economics. Risk is a crucial part of financial market sectors:
[I]t is not just legally but economically fundamental, since limited companies have been the principal unit of commercial life for more than a century. Their separate personality and property are the basis on which third parties are entitled to deal with them and commonly do deal with them.

Financial collateral
Financial markets have developed particular methods for taking security in relation to transactions, this is because collateral operates as a central method for parties to mitigate the credit risk of transacting with others. Derivatives frequently utilise collateral to secure transactions. Large notional exposures can be reduced to smaller, single net amounts.  Often, these are designed to mitigate the credit risk one party is exposed to. Two forms of financial collateralization have been developed from the Lex Mercatoria;

Title transfer; or
By granting a security interestA security interest may be granted with a right of use, conferring disposal powers. There is an increasing reliance on collateral in financial markets, and in particular, this is driven by regulatory margin requirements set out for derivatives transactions and financial institution borrowing from the European Central Bank. The higher the collateral requirements, the greater demand for quality exists. For lending, it is generally regarded that there are three criteria for determining high-quality collateral. Those being assets which are or can be:

Liquid; and
Easily priced; and
Of Low credit riskThere are several benefits to having financial collateral provisions. Namely, financial reduces credit risk, meaning the cost of credit and the cost of transacting will be lowered. The reduced insolvency risk of the counter-party, combined with more credit being available to the collateral taker will mean the collateral taker can take additional risk without having to rely on a counter-party. Systemic risk will be reduced by increased liquidity, This produces "knock-on effects" by increasing the number of transactions a collateral taker can safely enter, freeing up capital for other uses. However, there is a need for balance; the removal of limitations on insolvency rules and security registration requirements, as observed in the FCARs, is dangerous as it degrades powers and protections which have been conferred deliberately by the law.

Financial collateral regulations
The primary objective of the Financial Collateral Directive was to reduce systemic risk, harmonise transactions and reduce legal uncertainty. It achieved this by exempting qualified "Financial collateral arrangements" from the performance of formal legal requirements; notably registration and notification. Second, the collateral taker is provided effective right of use and said arrangements are exempted from being re-characterised as different security arrangements. Perhaps most significantly, traditional insolvency rules which may invalidate a financial collateral arrangement; such as freezing assets upon entering into insolvency, are suspended. This allows a collateral taker to act without the limitation which may arise from a collateral provider entering bankruptcy. The FCARs focus on outlining when a financial collateral arrangement will be exempted from national insolvency and registration rules. In England, the requirements that a financial collateral arrangements only applies between non-natural persons with one being a financial institution, central bank, or public body; the FCAR has been "gold-platted" by allowing any non-natural person to benefit. Thus, to qualify as a "financial collateral arrangement" under the FCARs, a transaction must be in writing and regard "relevant financial obligations". The criteria for a "relevant financial obligations" is set out in Part I Paragraph 3

The purpose of the provision is to increase the efficiency of markets and lower the transaction costs. The disapplied formal and perfection requirements accelerates the effectiveness of security through FCAR Reg 4(1),(2),(3) and 4(4). Two things might be said of this. Firstly, academics have highlighted the risk of dappling statute of frauds and other requirements. It runs real risk of repealing substantial protections which were developed, at least in English common law, because of  real risks of exploitation. Other forms of protection which has been repealed includes the ability to allow parties to implement Appropriation if expressly agreed is permitted.
Extensive litigation has resulted from the determination of the FCAR regulations, specifically the meaning of "possession or control" as set out in paragraph 3. Recital 10 states that possession or control is for the safety of third parties, however, the type of mischief this is seeking to remove is unclear. In C-156/15 Swedbank, the CJEU enforced the requirement that practical control was that of legal negative control. second sentence of Article 2(2) provides that any right of substation or to withdraw excess financial collateral in favour of the collateral provider must not prejudice the FC having been provided to the collateral taker. That right would lack any force if the taker of collateral consisting in monies deposited in a bank account were also to be regarded as having acquired "possession or control" of the monies where the account holder may freely dispose of them […] it follows that the taker of collateral in the form of money lodged in an ordinary bank account may be regarded as having acquired 'possession or control' of the monies only if the collateral provider is prevented from disposing of them What is clear is that (1) possession is more than merely custodial and dispossession is mandatory. Some legal control is also crucial, meaning practical or administrative control is insufficient.

Possession
Requirement that collateral must be in possession is unclear. Is it one, two things? Does possession apply to intangibles? We do know that you cant. Is the requirement of control the same as the test for fixed charges. The scope of the regime is not clear. There are several unanswered questions. Only the collateral providers can have is right of substitution and right to withdraw surplus. Possession applies to intigble if it is credited to an account. Gullifer suggests that this is a redundant definition. The directive drafted with English and Irish laws not being centrally in mind. It was about disposition. To some extent, ownership discourages transactions for the risk of ostensible wealth.
It was held that the phrase was to be construed in a manner consistent with meaning and purpose. This is not merely a matter of English law, Lord Briggs' judgment in Client Money [2009] EWHC 3228 held that to interpret the meaning of the directive a court ought to 1. Interpret the directive. We can look at different language texts and cases if any. 2. Interpret domestic legislation in light of the directive (as interpreted through stage 1)  This is not restricted by conventional rules. Meaning that the court can and will depart from literal meaning and may imply words as necessary however, one cannot go against domestic legislation, nor require the court to make decisions it is not prepared to make. Repercussions must be and are considered by the court.

Control
By contrast, Control has been shown to not be that of practical (Administrative) control. It is clear that FCARs require a standard of negative legal control. Practical control, is the Collateral Taker's exclusive ability to dispose and it is suggested this will additionally be required if the parties are to avoid fraud. It is established by the rights and prohibitions in the security agreement but there is limited case-law on the matter Scholars identify two forms of control:  

positive
negative (Collateral Provider has no rights in relation to dealing with or disposal of collateral)Positive and Negative control differ where one either has the right to dispose without reference to the collateral provider, or where collateral provider is able to do so without collateral taker. What is undeniable however, is that dispossession is central to both possession and control. Rights of the Collateral taker must be beyond merely custodial; he must be able to refuse to hand collateral back.
There are a handful of risks to these arrangements - as previously outlined - the ill definition of what constitutes the activation of the FCAR arrangements creates a danger. However, within the context of appropriation, a provider only has a personal right against a taker for the surplus. There is no proprietary right. Should a taker (like Lehman) become insolvent, a provider may well be at a loss for the excess. It encourages the party to reclaim excess value whenever possible/reasonably practical. This is not always possible due to the variation of the markets. Further, the risk of appropriation is that these can be used for ulterior purposes. Which as created the Cukurova problem; there parties had constructed a scheme to capture shares with a clause preventing the collateral taker from selling large securities at once and spooking the market, but valuation is not linear which made it difficult, if not impossible to determine what a commercially reasonable price for securities would be in an illiquid market.

Set off
Other concepts, crucial to financial markets include contingent obligations, the fact that bank debts operate as money; and set-off designed to mitigate the net exposure of transactions. Set-off as a legal concept is crucial part of reducing credit risk and reducing the knock-on effects of insolvency. 
Collectively, these concepts operate to underpin financial transactions by further dividing risk. Various combinations of these legal methods are used to produce various allocations of risk. For example, the ISDA 2002 master agreement utilises contingent obligations, set-off, and legal personality to reduce the liabilities of non-defaulting parties in the event of default. The effect of Clause 2(a)(iii) of the ISDA agreement is to suspend the payment obligations of parties until the event of default has been cured. Such a cure may not ever occur. There is substantial academic caution that such a suspension acts to circumvent insolvency pari passu objectives. However, there is equal evidence that the clause provides substantial market stability as a result of the standardisation and universality that the ISDA Master Agreement has within the derivatives market. It further provides the involved parties to suspend the swap (and any other transactions within the master agreement), providing them the time to understand the overall effect the event of default has had on the agreement and the market. In other words, it provides a breather.

Payment
Payment operates as another core legal concept which underpins financial law. It is crucial because it determines the point at which a party discharges their obligation to another party. In finance, particularly relating to set-off, guarantees, or other simple and funded positions; the definition of payment is crucial to determining the legal exposure of parties. Several of the cases derive predominately from English and U.S. law, pertaining to the Lex mercatoria, and was developed when finical law historically focused on maritime trade.

In English and U.S. law, payment is consensual, requiring acceptance from both payee and payer. Roy Goode suggests that Payment is a; consensual act and thus requires the accord of both creditor and debtor. Payment as a legal concept is underpinned by the law of contract. In most common law jurisdictions, a valid contract requires sufficient consideration.
Payment plays a crucial role in financial law because it determines when parties are able to discharge duties. In Lomas v JFB Firth Rixson Inc [2012] EWCA Civ 419, the issue concerned when a debtor was able to discharge the duty to pay under the ISDA Master Agreement (1992). The requirement for payment arises in English law from a duty in performance of a money obligation. Whilst normally described and fulfilled in monetary terms, payment need only satisfy the creditor and does not necessarily involve the delivery of money, but it cannot constitute payment unless money is involved, even if performance is fulfilled by some other act. a gift or loan of money or any act offered and accepted in performance of a money obligation. Obligation to pay or tender the debt is balanced by the obligation on the part of the seller not to refuse the whole or part of the debt. This is underpinned by limitations on part-payment. This traditionally operates in order to proffering money to fulfil obligations within a contract. In taking it, it is an affirmation of said contract and the debtor is discharged of his obligation to the creditor. This is crucial. In contracts where A ('the debtor') owes money to B ('the creditor'), payment operates as the terminus for A's obligation to B. It was crucially held in Societe des Hotel Le Touquet Paris-Plage v Cummings that the bilateral contractual process did not require "accord and satisfaction" to achieve discharge of a debt by payment. The operation of payment therefore requires mutual compliance from "both creditor and debtor".

Two conceptual points of mutual consent
Mutual consent must thus occur at two points, ex ante and ex post of the contract between parties and at what we might call "point Z" for situations where an obligation to pay does not result from contractual duties. (such as a debt owed to a non-adjusting creditor, cf Bebchuk and Fried). At both points, mutual consent required from both parties.
First, ex ante consent occurs at the time where parties agree on the obligation. If a party has specified a method for discharging an obligation through a specific means, then the parties must have contemplated the sufficiency of the tender to discharge the debt and therefore consensually agreed to payment of a specified way. This will likely provide specification on when tender may be rejected. Chen-Wishart's discussion of the importance of consideration within the bargain theory of contracts enunciates the emphasis the English law has placed on benefit and deliberateness when contracting. Contracting parties must have contemplated, negotiated, and reached mutual agreement in regard to how the obligation would be discharged. This does not, however, prevent or impede the occurrence of "point Z". Parties may duly agree on payment in principle prior to the execution of the contract and subsequently still fail to effect payment. Functionally agreement results in questioning whether or not payment has been made by the debtor. The debtor must commit a certain level of formality to proffering the obligation. This formality may take the form of complying with a contract. Failure to comply, is not payment.
Second, ex post, regardless of whether parties have mutually agreed and specified a method, or a money of payment, the parties (notably the creditor) must consent to the debtor's tender in order to crystallise payment and sever the demand for payment. Discharge of a debt is automatic. In other words, a payment of a contractual obligation requires mutual consent of payment at both the stage of formation and at the conclusion/distribution to be recognised as 'payment', but upon acceptance of payment the debt is discharged. In Colley v Overseas Exporters it was shown that even where tender complies with the contract, it is not payment until the creditor (or Payer) accepts. This is regardless of whether the creditor's rejection frustrates the contract and is a breach of their duty. The law does not allow the debtor to coerce the creditor into accepting a tender. This is the case, even when the debtor has forwarded valid tender. It is the subsequent acceptance or non-acceptance of the tender from the creditor which crystallises payment and effects discharge. Mere receipt will not suffice. However, mutual consent is of a lower standard than that in contractual formation. In TSB Bank of Scotland plc v Welwyn Hatfield District Council [1993] Bank LR 267, Hobhouse J held that acceptance of payment need not be communicated and his judgment provides a clear, two-stage test for determining whether payment has been made. If A;

places the money unconditionally at the disposal of his creditor; and
the conduct of the creditor, viewed objectively, amounts to acceptance, then payment has passed.Thus, in Libyan Arab Bank v Bankers Trust Co the court held that when the collecting bank decided unconditionally to credit the creditor's account, the payment is completed. Presentation and subsequent rejection of payment provides an absolute defence for to an action brought by the creditor, but without the action (and opportunity to pay into the court) and with exceptions, the debtor's proffering of payment does not discharge the money obligation nor does it constitute as payment. In the case of The Laconia, the English House of Lords set out clear conditions on timing of payment in relation to the debtor proffering payment. The charterers had procured a vessel for 3 months, 15 days with a payment due on April 12, a Sunday. The charterers delivered payment on Monday. The vessel owners rejected the payment, which was sent back the following day. Primarily, The Laconia regards the requirement for a tender to be congruent with the conditions in order to amount to a tendering of payment. However, the case might also be used to highlight the necessity for the creditor to accept such tendering. Had the vessel owners merely taken receipt of the payment and not instructed their bank to return the money, then it seems likely that payment was accepted.
The consensual nature of payment thus derives from the requirement that both debtor must offer, and creditor must accept, the medium of payment; and secondly from the fact that creditor rejection of procurement, even if his agent is in receipt of the payment, results in a failure to effect payment. Goode discusses two forms where receipt does not take effect as acceptance that fall into the second aforementioned stage of mutual consent; 

Conditional acceptance. Where a cheque is accepted it is conditional on such a cheque being met. Here, letters of Credit come to mind in that their conditional nature is dependent on the bank effecting payment. In The Chikuma and The Brimnes the court examined whether payment was fulfilled on the side of the payer. From that perspective, it was necessary for the court to analyse whether the payer had fulfilled the conditions in order to effect discharge.
Receipt by creditor's agent. The Laconia falls within this category. This is primarily because it is not always clear whether the agent lacked the authority to accept the payment.The fact that rejection of tender is sufficient to prevent 'payment' derives from the fact that payment is the conferral of property to fulfil the obligation. Property and obligation aspects of the transaction cannot be separated without the transaction ceasing to be "payment".

Financial law transactional categories
As well as being fragmented, financial law is often muddled. Historical segregation of the industry into sectors has meant each has been regulated and conducted by different institutions. The approach to financial law is unique depending on the structure of the financial instrument. The historical development of various financial instruments explains the legal protections which differ between, say, guarantees and indemnities. Due to the limited cross-sectoral legal awareness, innovations in finance have been associated with varying levels of risk. Several different legal "wrappers" provide different structured products, each with differing levels of risk allocation, for example, funded positions consist of bank loans, capital market securities, and managed funds.
The primary purpose of financial law is to allocate risk from one person to another and change the nature of risk being run by the protection buyer into the 'credit risk' of the risk taker. Five categories of market structures are divided according to how the contract deals with the credit risk of the risk taker.

Simple financial positions
Guarantees, insurance, standby letters of credit and performance bonds. The terms Simple can often be misleading, as often the transactions which fall within this category are often complicated. They are termed simple not because of the lack of sophistication but because the transactions do not address the credit exposure of the protection buyer. Rather, as with a guarantee, the protection buyer simply takes the risk of protection seller. Derivatives often fall within this regulatory category because they transfer risk from one party to another.

Derivatives law
The second portion of simple transactions are derivatives, specifically unfunded derivatives of which, four basic types exist. At law, the primary risk of a derivative is the risk of a transaction being re-characterised as another legal structure. Thus, the courts have been cautious to make clear definitions of what amounts to a derivative at law. Fundamentally, a derivative is a contract for difference, it utilises netting to set obligations between parties. Rarely does delivery of the asset occur. In English law, the judgment of Lomas v JFB Firth Rixson [2012] EWCA Civ, quotes the leading test Firth on Derivatives, characterising a derivative as a transaction under which the future obligations are linked to another asset or index. Delivery of the asset is calculated by reference to said asset. Derives from the value of the underlying asset. This creates another asset out of the first type. It is a chose in action with reference to the value of the underlying asset. They are separate and can be traded accordingly 
As legal instruments, derivatives are bilateral contracts which rights and obligations of the parties are derived from, or defined by, reference to a specified asset type, entity, or benchmark and the performance of which is agreed to take place on a date significantly later than the date in which the contract is concluded.Types
Various types of derivatives exist with even greater variance of reference assets. English law in particular has been clear to distinguish between two types of basic derivatives: Forwards and Options. Often parties will place limits on the interest rate differentials when engaging in trades. At law, these are known as "Caps & Collars", these reduce the cost of the transaction. Regulation has been a key component in making the market more transparent, this has been particularly useful in protecting small and medium sized businesses.
Swaps and Credit derivatives also differ in legal function. A credit derivative describes various contracts designed to assume or distribute credit risk on loans or other financial instruments. Payment obligations of a seller is triggered by specified credit events affective defined assets or entities. In a swap, it was held in Hazell v Hammersmith and Fulham London Borough Council by Woolf LJ that equity swaps were developed under ISDA's guidance and might be defined as A transaction in which one party pays periodic amounts of a currency based on a fixed price/rate and the other party pays periodic amounts of the same currency or different currency based on the performance of a share of an issuer, an index or a basket of several issuers. These are differentiated from credit derivatives, which reference the credit risk of specified credit event; usually a bankruptcy, failure to make payment, or a breach of a condition such as a debt-to-equity ratio. Payment as a core concept in finance is crucial to the operation of derivatives. Credit derivatives which are "self-referenced", i.e. referencing the parties own credit worthiness have been considered by the courts as capable of involving fraud.

Legal issues
A swap derivative with negative interest rates highlights particularly issues  at law. It is unclear how a party pays a negative number. Does it reverse the obligations? According to the ISDA Master Agreement variation in 2006, a swap has a "zero floor" which means that if interest rates reverse, the obligations do not reverse. Without the 2006 variation, the negative interest rate is a deduction off what is owed. 
An additional area of relevant derivatives law is shown in the cases of Dharmala and Peekay, both of which involved arguments of mis-selling derivative transactions. This is closely related with the argument that parties, particularly government bodies lack the power to enter into derivative contracts. In Dharmala, the claimant argued unsuccessfully that the bank misrepresented the transaction. It was held that they did misrepresent but for the misrepresentation to effect a claim, it was necessary to induce someone to enter into the contract, which was unable to be proven. In Peekay, the Court of Appeal rejected the suit for misrepresentation when the defendant mis-sold a synthetic credit derivative to Peekay which had its reference assets in Russian investments. The Peekay director ought to have read the documents rather than relying on the defendant's oral representation. This is a pro-market approach with marked Judaical disinclination to not strike down transactions, substantial problems exist with enforcing a contract against a party which argues it lacks the power to enter into an agreement, it has been likened to pulling oneself up by the bootstraps as the party cannot warrant that it has the power if it truly does not.Documentation of derivatives often utilises standard forms to increase liquidity, this is particularly the case in exchange traded, or "over the counter" derivatives which are predominately documented using the ISDA Master Agreement. These agreements operate to create a singular transaction which lasts the duration of the trading relationship. Confirmation of trades can be codified by oral contracts made over the phone. This is only possible because interpretation of the standard form documentation is done in a manner so that the terms of art used within the documents have their own autonomous meaning separate from the law of the forum. Flexibility within the contract, and a court appreciation for the commercial objectives of the master agreements is a crucial aspect of the long-term operation of the financial markets which they support.
The ISDA Master Agreement is dependent on market practices, which attach to interpretations of intention within a context of long term relationships. The aim is to differentiate relational contracts from one-off contracts.  The concept of a single agreement is not new. It is an artificial line to sum-off and default netting practices.  Payment of a derivative contract, particularly those of standardised forms, use netting. This minimises credit risk.

Recharacterisation
In being similar to one another in terms of economic market effects, simple positions are particularly susceptible to being re-characterised. When this happens, substantial legal consequences can result, as each legal instrument has different consequences. Whilst a guarantee and an indemnity have, in substance, the same economic result; the law characterises each differently because it affords an indemnifier less protection than a guarantor. Similarly, a derivative or guarantee must not be recharacterised as an insurance contract, as such contracts are strictly regulated by government regulation. A re-characterisation into an insurance contract would be fatal to the contract, as only licensed parties can issue such terms. The characterisation of financial transactions by the court takes the form of two stages; examining the legal substance, not the form of the agreement. Thus, stating that a contract is a derivative, does not make it a derivative. As held by Lord Millet in Agnew v Commissioners of Inland Revenue (Re Brumark Investments Ltd, characterisation interprets the document and then categorises it within one of the existing legal doctrines. Intention is not relevant, however, there are sensitivities to this matter, primarily concerning the insurance markets. Three key types of recharacterisation can occur to simple positions

Guarantees or Indemnities: In Yeoman Credit Ltd v Latter the court held perhaps the most important distinction. The distinction between the two is that a guarantee is a secondary obligation to pay whilst an indemnity is a primary obligation.
Guarantees or Performance bonds: Performance bonds are similar to a promissory note, this turns again upon the primacy of the obligations. The courts have been extremely hesitant to implement a performance bond onto parties which are not banks.
Guarantees or Insurance: Both are protecting creditors from loss, however a guarantee is narrower. Romer LJ set out three variables to differentiate the two: (1) the motives of the parties differ, insurance is a business contract and cover is provided in consideration for a premium. Guarantees are provided without payment. (2) The manner of dealings differ; an insurer usually deals with the insured and not the reference entity. (3) The means of knowledge which is disclosed. The insured must disclose material facts, why a guarantor is left to himself to determine facts. A guarantee thus is traditionally drafted to stand behind the debtor rather than be payable on the occurrence of an event. In England, prior to the Gambling Act 2005, the courts often interpreted contracts as gambling and avoided them. Any contract which exists under the purview of the Financial Services and Markets Act 2000 is not avoided by the Gambling Act 1845 provisions. This is, in part, due to the Potts Opinion which argued the legal distinctiveness of derivatives from gambling and insurance contracts. This was argued by stating that the payment obligation was not conditional on loss and rights were not dependent on an insurable interest.

Funded positions
Lending is perhaps the most central aspect of the financial system. As discussed by Benjamin, the law attempts to allocate risk in ways which is acceptable to the parties involved. Bank loans and capital market transactions fall within this category. It may be defined as situations where the risk-taker is the provider of capital to another party. If the risk materialise, the exposure is not merely an obligation to pay, but rather the exposure of the risk-taker is the risk of losing its previously committed capital. That is to say, a funded position is the risk of repayment. When a bank makes a loan, it pays money and runs the risk of a lack of repayment.

Difference between funded positions and other positions
One might ask what the difference between an asset-backed security and funded positions. The answer is that funded positions are positions which are acquired without backing of other assets.
The true difference is that of funded positions and simple positions. Simple positions, such as guarantees, insurance, standby credits and derivatives. Funded positions differ from simple positions in that simple positions expose risk as a form of a promise. The risk taker agrees to pay the beneficiary upon certain events. This relies upon exposure to credit risk. Funded positions have the risk exposure has the form of a payment, which is to be restored. The risk exists in that it may not be repaid. It is funding a party with the risk being a lack of repayment. This includes the bank and non-bank lending including syndicated loans.
Two overarching forms of funded positions exist between debt and equity, and there are several ways to raise capital. This might be broken down into Bank loans (debt financing) and equity issuing (capital markets). Alternatively, a company may retain profits internally. This may be summarised as:

Equity shares
Debt financing
Retained profitsFew companies can use equity and retained profits entirely. It would not be good business to do so either; debt is a crucial aspect for corporate finance. This relates to the gearing advantages of taking on debt and maximising the value of debt-to-equity to allow equity to gain maximum returns. Debt is repayable in accordance with the terms; whereas equity instruments, typically includes rights of shareholders, rights to receive reports, accounts, pre-emptions (where the company proposes issuing new shares), and the right to vote on strategic decisions affecting the company.

Debt financing
Bank lending may be categorised according to a large number of variables including the type of borrower, the purpose and the form of the loan facility. Where a bank makes a loan it will typically require a business plan and require security where it has credit concerns. A commitment letter may be produced during the negotiations for a loan. In general these are not legally binding.
A loan facility is an agreement where a bank agrees to lend. It is distinct from the loan itself. Using a loan facility it writes to the bank and the bank makes the loan. LMA syndicated single currency term facility distinguishes between 1. commitment to lend to each lender, 2. average of each; and 3. the loan made under the agreement and the draw down. Three important forms of these are:

Overdraft facilities
term loan facilities
revolving facilitiesThese may be further categorised into two overarching forms of bank lending, organised based on the term/repayment criteria of the loan. These are:

on-demand lending (overdraft and other short term) and;
committed lending (revolving facility or a term loan)Economist and finance lawyers categories these and further categorise syndication separately but within committed lending. This has been a traditional driver for lending within the debt financing market.

On-demand lending
Where express terms state that it is repayable one demand, it will be so repayable even if both bank and borrower envisaged that it would be last for some time. This must be an express term. In England & Wales, because of S6 Limitation Act 1980, time for repayment does not start running until the demand is made. This means that the debt, for example an overdraft, is not repayable without demand but will become repayable if requested; even if the parties thought it would not be repayable for some time.In Sheppard & Cooper Ltd v TSB Bank Plc (No 2) [1996] BCC 965; [1996] 2 ALL ER 654, the plaintiff granted a fixed and floating charge over its assets. He then covenanted to pay or discharge indebtedness on-demand. At any time after indebtedness should become immediately payable, the debtor was authorised to appoint administrative receivers. Soon after a demand was made by the defendant. The plaintiff said that the best that could be done was repayment of half. The defendant appointed administrative receivers to recover the debt as outlined by the charge. The plaintiff sued and claimed claim the time was insufficient. The court held that; "It is physically impossible in most cases for a person to keep the money required to discharge the debt about his person. Must have had reasonable opportunity of implementing reasonable mechanics of payment he may need to employ to discharge the debt." But a "reasonable time" overarching doctrine was found to be too commercially difficult. The courts have held short timelines as being more than sufficient to satisfy the request of on-demand. Walton J only accepted 45 minutes as being a reasonable period of time and in Cripps it was 60 minutes. Therefore the timing of repayment depends on circumstances but is, in commercial matters, extremely quick. If the sum demanded is of an amount which the debtor has, the time must be reasonable to enable the debtor to contact his bank and make necessary arrangements. However, if the party, as in Sheppard admits his inability to pay, Kelly CB believed that seizure was justified immediately stating "If personal service is made and the defendants may have seized immediately afterwards."Parties will want to avoid insolvency consequences. A bank will normally freeze a customers account when a winding up petition occurs to avoid dispositions within insolvency. A payment into an overdrawn account is probably a disposition of the company's property in favour of the bank. This is crucial differentiation as the money of an overdrawn account is going directly to a creditor. Payment into an account in credit is not a disposition of the company's property in favour of the bank, however.The bank makes a payment out of the company's account in accordance with a valid payment instruction - there is no disposition in favour of the bank. As a result, banks traditionally freeze accounts and force insolvent parties to open new accounts.

Overdraft
An overdraft constitutes a loan, traditionally repayable on demand. It is a running account facility (categorising alongside revolving loans) where its on-demand nature of repayment meant immediately. A bank is only obliged to provide an overdraft if the bank has expressly or impliedly agreed to do so. Legally, where a client overdraws his account, the client is not in breach of contract with the bank; if it did constitute a breach, then the fees charged by the bank would be penalties and correspondingly not allowed. If requesting payment when there is no money in the bank account, the customer is merely requesting an overdraft. This should be noted that this is separate and distinct from credit cards; as credit cards invariably say a client must not go over the credit limit. With overdraft requests, the bank has the option not to comply with the request, although this is rare, as the bankss reputations are built upon a willingness and ability to pay on behalf of clients. Often however, the bank complies and then charges a fee to 'create a loan'.OFT v Abbey National held that "if a bank does pay, customer has taken to have agreed to accept the bank's standards", which means that they have asked and the bank has provided a loan.  Banks may charge interest on an overdraft and may compound that interest The point of an overdraft at law is that it is repayable on demand, however, payment instructions within the agreed overdraft limit must be honoured until notice has been given that the facility (the overdraft) is withdrawn.

Committed lending
A committed facility is where the bank is committed to lend throughout a certain period.

Term loan;  all at once or in successive tranches. Can be repayable at once (bullet); or according to a payment scheme (amortising)
Revolving facility; borrow repay and reborrow.
swingline facility; Which is a committed facility providing for short term advancesMost committed lending facilities will be documented, either by:

A facility letter or
A loan agreementThese may be more or less complex, depending on the size of the loan.
Oral assurance can give rise to an obligation to lend prior to any documentation being signed. 'A statement made by a bank employee over the telephone that approval' had been given. Most facility letters and loan agreements will contain contractual provisions designed to protect the lender against the credit risk of the borrower. This requires several aspects. Normally it will require conditions precedent, restrictions on the borrower's activities, information covenants, set-off provisions, stipulations for events of default. Lenders will also traditionally take real or personal security. These are designed to protect the lender against:

Non Payment of both interest and capital; and
InsolvencyThese two objectives are achieved by providing for events that make non-payment or insolvency unappealing or transfer the risk associated with said events to third party. This highlights the difference between risk as assessed and actual risk.

Material adverse change clauses
A common provision relates to material adverse change clauses. The borrow represents/warrants that there has been no material adverse change in its financial condition since the date of the loan agreement. This is a clause which is not often invoked or litigated and therefore the interpretation is uncertain and proof of breach is difficult. Consequences of wrongful invocation by the lender are severe.
Interpretation depends on the terms of the particular clause and is up to lender to prove breach. Cannot be triggered on basis of things lender knew when making the agreement. Normally done by comparing borrower's accounts or other financial information then and now. Other compelling evidence may be enough. Will be material if it significantly affects the borrower's ability to repay the loan in question. We may examine one of the leading authorities on material adverse change clauses in committed lending, Grupo Hotelero Urvasco SA v Carey Value Added [2013] EWHC 1039 (Comm), per Blair J

Therefore, a change will be material if it significantly affects the borrower's ability to repay the loan in question. Normally this is done by comparing borrower's accounts or other financial information then and now.

Net positions
A net position represents a financial position in which a debtor may "off-set" his obligation to the creditor with a mutual obligation which has arisen and is owed from the creditor to the debtor. In financial law, this may often take the form of a simple or funded position such as a securities lending transaction where mutual obligations set-off one another. Three crucial types of netting exists:

Novation Netting
Settlement Netting
Transaction NettingEach party can use its own claim against the other to discharge. Each party bears credit risk which may be offset. For example, a guarantor who is a depositor with a banking institution can set-off obligations he may owe to the bank under the guarantee against the bank's obligation to repay his deposited assets.

Asset-backed positions
Propriety securities like mortgages, charges, liens, pledges and retention of title clauses are financial positions which are collateralised using proprietary assets to mitigate the risk exposure of the collateral-taker. The core purpose it to Manage credit risk by identifying certain assets and ear-marking claims to those assets.

Combined positions
Combined positions use multiple facets of the other four positions, assembling them in various combinations to produce large, often complex, transactional structures. Examples of this category are primarily CDOs and other structured products. For example, a synthetic collateralised debt obligation will draw upon derivatives, syndicated lending, and asset-backed positions to distinguish the risk of the reference asset from other risks. The law pertaining to CDOs is particularly noteworthy, primarily for its use of legal concepts such as legal personality, and risk transfer to develop new products.
The prevalence and importance of combined positions within the financial markets, has meant that the legal underpinnings of the transactional structures are highly relevant to their enforcement and effectiveness.

References
Further reading
Benjamin, Financial Law (OUP, 2007)
Chitty on Contracts (Sweet and Maxwell, 32nd ed 2015) Vols I (General Principles) and II (Specific Contracts)
Goode on Commercial Law (Penguin, 5th ed 2016 by Ewan McKendrick)
Goode & Gullifer on Legal Problems of Credit and Security (Sweet & Maxwell, 7th ed 2017)
Gullifer and Payne, Corporate Finance Law: Principles and Policy (Hart Publishing, 2nd ed 2015)
Hudson, The Law of Finance (Sweet & Maxwell, 2nd ed 2013)
Gullifer and Payne Corporate Finance Law (Hart publishing, 2nd Ed, 2016)

External links
 Media related to Financial law at Wikimedia Commons

Financial market

A financial market is a market in which people trade financial securities and derivatives at low transaction costs. Some of the securities include stocks and bonds, raw materials and precious metals, which are known in the financial markets as commodities.
The term "market" is sometimes used for what are more strictly exchanges, organizations that facilitate the trade in financial securities, e.g., a stock exchange or commodity exchange. This may be a physical location (such as the New York Stock Exchange (NYSE), London Stock Exchange (LSE), JSE Limited (JSE), Bombay Stock Exchange (BSE)) or an electronic system such as NASDAQ. Much trading of stocks takes place on an exchange; still, corporate actions (merger, spinoff) are outside an exchange, while any two companies or people, for whatever reason, may agree to sell the stock from the one to the other without using an exchange.
Trading of currencies and bonds is largely on a bilateral basis, although some bonds trade on a stock exchange, and people are building electronic systems for these as well, to stock exchanges. There are also global initiatives such as the United Nations Sustainable Development Goal 10 which has a target to improve regulation and monitoring of global financial markets.

Types of financial markets
Within the financial sector, the term "financial markets" is often used to refer just to the markets that are used to raise finances. For long term finance, they are usually called the capital markets; for short term finance, they are usually called money markets. The money market deals in short-term loans, generally for a period of a year or less. Another common use of the term is as a catchall for all the markets in the financial sector, as per examples in the breakdown below.

Capital markets which consist of:
Stock markets, which provide financing through the issuance of shares or common stock, and enable the subsequent trading thereof.
Bond markets, which provide financing through the issuance of bonds, and enable the subsequent trading thereof.
Commodity markets, The commodity market is a market that trades in the primary economic sector rather than manufactured products, Soft commodities is a term generally referred as to commodities that are grown, rather than mined such as crops (corn, wheat, soybean, fruit and vegetable), livestock, cocoa, coffee and sugar and Hard commodities is a term generally referred as to commodities that are mined such as gold, gemstones and other metals and generally drilled such as oil and gas.
Money markets, which provide short term debt financing and investment.
Derivatives markets, which provide instruments for the management of financial risk.
Futures markets, which provide standardized forward contracts for trading products at some future date; see also forward market.
Foreign exchange markets, which facilitate the trading of foreign exchange.
Cryptocurrency market which facilitate the trading of digital assets and financial technologies.
Spot market
Interbank lending marketThe capital markets may also be divided into primary markets and secondary markets. Newly formed (issued) securities are bought or sold in primary markets, such as during initial public offerings. Secondary markets allow investors to buy and sell existing securities. The transactions in primary markets exist between issuers and investors, while secondary market transactions exist among investors.
Liquidity is a crucial aspect of securities that are traded in secondary markets. Liquidity refers to the ease with which a security can be sold without a loss of value. Securities with an active secondary market mean that there are many buyers and sellers at a given point in time. Investors benefit from liquid securities because they can sell their assets whenever they want; an illiquid security may force the seller to get rid of their asset at a large discount.

Raising capital
Financial markets attract funds from investors and channels them to corporations—they thus allow corporations to finance their operations and achieve growth. Money markets allow firms to borrow funds on a short-term basis, while capital markets allow corporations to gain long-term funding to support expansion (known as maturity transformation).
Without financial markets, borrowers would have difficulty finding lenders themselves. Intermediaries such as banks, Investment Banks, and Boutique Investment Banks can help in this process. Banks take deposits from those who have money to save on the form of savings a/c. They can then lend money from this pool of deposited money to those who seek to borrow. Banks popularly lend money in the form of loans and mortgages.
More complex transactions than a simple bank deposit require markets where lenders and their agents can meet borrowers and their agents, and where existing borrowing or lending commitments can be sold on to other parties. A good example of a financial market is a stock exchange. A company can raise money by selling shares to investors and its existing shares can be bought or sold.
The following table illustrates where financial markets fit in the relationship between lenders and borrowers:

Lenders
The lender temporarily gives money to somebody else, on the condition of getting back the principal amount together with some interest or profit or charge.

Individuals and doubles
Many individuals are not aware that they are lenders, but almost everybody does lend money in many ways. A person lends money when he or she:

Puts money in a savings account at a bank
Contributes to a pension plan
Pays premiums to an insurance company
Invests in government bonds

Companies
Companies tend to be lenders of capital. When companies have surplus cash that is not needed for a short period of time, they may seek to make money from their cash surplus by lending it via short term markets called money markets. Alternatively, such companies may decide to return the cash surplus to their shareholders (e.g. via a share repurchase or dividend payment).

Banks
Banks can be lenders themselves as they are able to create new debt money in the form of deposits.

Borrowers
Individuals borrow money via bankers' loans for short term needs or longer term mortgages to help finance a house purchase.
Companies borrow money to aid short term or long term cash flows. They also borrow to fund modernization or future business expansion. It is common for companies to use mixed packages of different types of funding for different purposes – especially where large complex projects such as company management buyouts are concerned.
Governments often find their spending requirements exceed their tax revenues. To make up this difference, they need to borrow. Governments also borrow on behalf of nationalized industries, municipalities, local authorities and other public sector bodies. In the UK, the total borrowing requirement is often referred to as the Public sector net cash requirement (PSNCR).Governments borrow by issuing bonds. In the UK, the government also borrows from individuals by offering bank accounts and Premium Bonds. Government debt seems to be permanent. Indeed, the debt seemingly expands rather than being paid off. One strategy used by governments to reduce the value of the debt is to influence inflation.
Municipalities and local authorities may borrow in their own name as well as receiving funding from national governments. In the UK, this would cover an authority like Hampshire County Council.
Public Corporations typically include nationalized industries. These may include the postal services, railway companies and utility companies.
Many borrowers have difficulty raising money locally. They need to borrow internationally with the aid of Foreign exchange markets.
Borrowers having similar needs can form into a group of borrowers. They can also take an organizational form like Mutual Funds. They can provide mortgage on weight basis. The main advantage is that this lowers the cost of their borrowings.

Derivative products
During the 1980s and 1990s, a major growth sector in financial markets was the trade in so called derivatives.
In the financial markets, stock prices, share prices, bond prices, currency rates, interest rates and dividends go up and down, creating risk. Derivative products are financial products that are used to control risk or paradoxically exploit risk. It is also called financial economics.
Derivative products or instruments help the issuers to gain an unusual profit from issuing the instruments. For using the help of these products a contract has to be made. Derivative contracts are mainly four types:
Future
Forward
Option
SwapSeemingly, the most obvious buyers and sellers of currency are importers and exporters of goods. While this may have been true in the distant past, when international trade created the demand for currency markets, importers and exporters now represent only 1/32 of foreign exchange dealing, according to the Bank for International Settlements.The picture of foreign currency transactions today shows:

Banks/Institutions
Speculators
Government spending (for example, military bases abroad)
Importers/Exporters
Tourists

Analysis of financial markets
See Statistical analysis of financial markets, statistical financeMuch effort has gone into the study of financial markets and how prices vary with time. Charles Dow, one of the founders of Dow Jones & Company and The Wall Street Journal, enunciated a set of ideas on the subject which are now called Dow theory. This is the basis of the so-called technical analysis method of attempting to predict future changes. One of the tenets of "technical analysis" is that market trends give an indication of the future, at least in the short term. The claims of the technical analysts are disputed by many academics, who claim that the evidence points rather to the random walk hypothesis, which states that the next change is not correlated to the last change. The role of human psychology in price variations also plays a significant factor. Large amounts of volatility often indicate the presence of strong emotional factors playing into the price. Fear can cause excessive drops in price and greed can create bubbles. In recent years the rise of algorithmic and high-frequency program trading has seen the adoption of momentum, ultra-short term moving average and other similar strategies which are based on technical as opposed to fundamental or theoretical concepts of market behaviour. For instance, according to a study published by the European Central Bank, high frequency trading has a substantial correlation with news announcements and other relevant public information that are able to create wide price movements (e.g., interest rates decisions, trade of balances etc.)
The scale of changes in price over some unit of time is called the volatility.
It was discovered by Benoit Mandelbrot that changes in prices do not follow a normal distribution, but are rather modeled better by Lévy stable distributions. The scale of change, or volatility, depends on the length of the time unit to a power a bit more than 1/2. Large changes up or down are more likely than what one would calculate using a normal distribution with an estimated standard deviation.

Financial market slang
Poison pill, when a company issues more shares to prevent being bought out by another company, thereby increasing the number of outstanding shares to be bought by the hostile company making the bid to establish majority.
Bips, meaning "bps" or basis points. A basis point is a financial unit of measurement used to describe the magnitude of percent change in a variable. One basis point is the equivalent of one hundredth of a percent. For example, if a stock price were to rise 100bit/s, it means it would increase 1%.
Quant, a quantitative analyst with advanced training in mathematics and statistical methods.
Rocket scientist, a financial consultant at the zenith of mathematical and computer programming skill. They are able to invent derivatives of high complexity and construct sophisticated pricing models. They generally handle the most advanced computing techniques adopted by the financial markets since the early 1980s. Typically, they are physicists and engineers by training.
IPO, stands for initial public offering, which is the process a new private company goes through to "go public" or become a publicly traded company on some index.
White Knight, a friendly party in a takeover bid. Used to describe a party that buys the shares of one organization to help prevent against a hostile takeover of that organization by another party.
Round-tripping
Smurfing, a deliberate structuring of payments or transactions to conceal it from regulators or other parties, a type of money laundering that is often illegal.
Bid–ask spread, the difference between the highest bid and the lowest offer.
Pip, smallest price move that a given exchange rate makes based on market convention.
Pegging, when a country wants to obtain price stability, it can use pegging to fix their exchange rate relative to another currency.
Bearish, this phrase is used to refer to the fact that the market has a downward trend.
Bullish, this term is used to refer to the fact that the market has an upward trend.

Functions of financial markets
Intermediary functions: The intermediary functions of financial markets include the following:
Transfer of resources: Financial markets facilitate the transfer of real economic resources from lenders to ultimate borrowers.
Enhancing income: Financial markets allow lenders to earn interest or dividend on their surplus invisible funds, thus contributing to the enhancement of the individual and the national income.
Productive usage: Financial markets allow for the productive use of the funds borrowed. The enhancing the income and the gross national production.
Capital formation: Financial markets provide a channel through which new savings flow to aid capital formation of a country.
Price determination: Financial markets allow for the determination of price of the traded financial assets through the interaction of buyers and sellers. They provide a sign for the allocation of funds in the economy based on the demand and to the supply through the mechanism called price discovery process.
Sale mechanism: Financial markets provide a mechanism for selling of a financial asset by an investor so as to offer the benefit of marketability and liquidity of such assets.
Information: The activities of the participants in the financial market result in the generation and the consequent dissemination of information to the various segments of the market. So as to reduce the cost of transaction of financial assets.
Financial Functions
Providing the borrower with funds so as to enable them to carry out their investment plans.
Providing the lenders with earning assets so as to enable them to earn wealth by deploying the assets in production debentures.
Providing liquidity in the market so as to facilitate trading of funds.
Providing liquidity to commercial bank
Facilitating credit creation
Promoting savings
Promoting investment
Facilitating balanced economic growth
Improving trading floors

Components of financial market
Based on market levels
Primary market: A primary market is a market for new issues or new financial claims. Therefore, it is also called new issue market. The primary market deals with those securities which are issued to the public for the first time.
Secondary market: A market for secondary sale of securities. In other words, securities which have already passed through the new issue market are traded in this market. Generally, such securities are quoted in the stock exchange and it provides a continuous and regular market for buying and selling of securities.Simply put, primary market is the market where the newly started company issued shares to the public for the first time through IPO (initial public offering). Secondary market is the market where the second hand securities are sold (security Commodity Markets).

Based on security types
Money market: Money market is a market for dealing with the financial assets and securities which have a maturity period of up to one year. In other words, it is a market for purely short-term funds.
Capital market: A capital market is a market for financial assets that have a long or indefinite maturity. Generally, it deals with long-term securities that have a maturity period of above one year. The capital market may be further divided into (a) industrial securities market (b) Govt. securities market and (c) long-term loans market.
Equity markets: A market where ownership of securities are issued and subscribed is known as equity market. An example of a secondary equity market for shares is the New York (NYSE) stock exchange.
Debt market: The market where funds are borrowed and lent is known as debt market. Arrangements are made in such a way that the borrowers agree to pay the lender the original amount of the loan plus some specified amount of interest.
Derivative markets: A market where financial instruments are derived and traded based on an underlying asset such as commodities or stocks.
Financial service market: A market that comprises participants such as commercial banks that provide various financial services like ATM. Credit cards. Credit rating, stock broking etc. is known as financial service market. Individuals and firms use financial services markets, to purchase services that enhance the workings of debt and equity markets.
Depository markets: A depository market consists of depository institutions (such as banks) that accept deposits from individuals and firms and uses these funds to participate in the debt market, by giving loans or purchasing other debt instruments such as treasury bills.
Non-depository market: Non-depository market carry out various functions in financial markets ranging from financial intermediary to selling, insurance etc. The various constituencies in non-depositary markets are mutual funds, insurance companies, pension funds, brokerage firms etc.
Relation between Bonds and Commodity Prices: With the increase in commodity prices, the cost of goods for companies increases. This increase in commodity prices level causes a rise in inflation.
Relation between Commodities and Equities: Due to the production cost remaining same, and revenues rising (due to high commodity prices), the operating profit (revenue minus cost) increases, which in turn drives up equity prices.

See also
References
Further reading
External links

Financial Markets with Yale Professor Robert Shiller (Archived 2010-11-03 at the Wayback Machine)

Financial market participants

There are two basic financial market participant distinctions, investors versus speculators and institutional versus retail. Action in financial markets by central banks is usually regarded as intervention rather than participation.

Supply side versus demand side
A market participant may either be coming from the supply side, hence supplying excess money (in the form of investments) in favor of the demand side; or coming from the demand side, hence demanding excess money (in the form of borrowed equity) in favor of the supply side. This equation originated from Keynesian advocates. The theory explains that a given market may have excess cash; hence the supplier of funds may lend it; and those in need of cash may borrow the funds supplied. Hence, the equation: aggregate savings equals aggregate investments.
The demand side consists of: those in need of cash flows (daily operational needs); those in need of interim financing (bridge financing); those in need of long-term funds for special projects (capital funds for venture financing).
The supply side consists of: those who have aggregate savings (retirement funds, pension funds, insurance funds) that can be used in favor of demand side. The origin of the savings (funds) can be local savings or foreign savings. So much pensions or savings can be invested for school buildings; orphanages; (but not earning) or for road network (toll ways) or port development (capable of earnings).
The earnings go to owner (Savers or Lenders) and the margin goes to the banks. When the principal and interest are added up, it will reflect the amount paid for the user (borrower) of the funds.  Thus, an interest percentage for the cost of using the funds.

Investor versus speculator
Investor
An investor is any party that makes an investment. However, the term has taken on a specific meaning in finance to describe the particular types of people and companies that regularly purchase equity or debt securities for financial gain in exchange for funding an expanding company.  Less frequently the term is applied to parties who purchase real estate, currency, commodity derivatives, personal property, or other assets.

Speculation
Speculation, in the narrow sense of financial speculation, involves the buying, holding, selling, and short-selling of stocks, bonds, commodities, currencies, collectibles, real estate, derivatives or any valuable financial instrument to profit from fluctuations in its price as opposed to buying it for use or for income via methods such as dividends or interest. Speculation represents one of three market roles in western financial markets, distinct from hedging, long term investing and arbitrage. Speculators in an asset may have no intention to have long term exposure to that asset.

Institutional versus retail
Institutional investor
An institutional investor is an investor, such as a bank, insurance company, retirement fund, hedge fund, or mutual fund, that is financially sophisticated and makes large investments, often held in very large portfolios of investments. Because of their sophistication, institutional investors may often participate in private placements of securities, in which certain aspects of the securities laws may be inapplicable.

Retail investor
A retail investor is an individual investor possessing shares of a given security. Retail investors can be further divided into two categories of share ownership:

A Beneficial Shareholder is a retail investor who holds shares of their securities in the account of a bank or broker, also known as "in street name". The broker is in possession of the securities on behalf of the underlying shareholder.
A Registered Shareholder is a retail investor who holds shares of their securities directly through the issuer or its transfer agent. Many registered shareholders have physical copies of their stock certificates.In the United States, as of 2005 about 57 million households owned stocks, and in total, individual investors owned 26% of equities.

See also
Do-it-yourself investing
Financial market efficiency
Securities market participants (United States)


== References ==

Financial regulation

Financial regulation is a broad set of policies that apply to the financial sector in most jurisdictions, justified by two main features of finance: systemic risk, which implies that the failure of financial firms involves public interest considerations; and information asymmetry , which justifies curbs on freedom of contract in selected areas of financial services, particularly those that involve retail clients and/or Principal–agent problems. An integral part of financial regulation is the supervision of designated financial firms and markets by specialized authorities such as securities commissions and bank supervisors. 
In some jurisdictions, certain aspects of financial supervision are delegated to self-regulatory organizations. Financial regulation forms one of three legal categories which constitutes the content of financial law, the other two being market practices and case law.

History
In the early modern period, the Dutch were the pioneers in financial regulation. The first recorded ban (regulation) on short selling was enacted by the Dutch authorities as early as 1610.

Aims of regulation
The objectives of financial regulators are usually:
market confidence – to maintain confidence in the financial system
financial stability – contributing to the protection and enhancement of stability of the financial system
consumer protection – securing the appropriate degree of protection for consumers.
reduce financial crime
regulate foreign participation

Structure of supervision
Acts empower organizations, government or non-government, to monitor activities and enforce actions. There are various setups and combinations in place for the financial regulatory structure around the globe.

Securities market regulation
Exchange acts ensure that trading on the floor of exchanges is conducted in a proper manner. Most prominent the pricing process, execution and settlement of  trades, direct and efficient trade monitoring.Financial regulators ensure that listed companies and market participants comply with various regulations under the trading acts. The trading acts demands that listed companies publish regular financial reports, ad hoc notifications or directors' dealings. Whereas market participants are required to publish major shareholder notifications. The objective of monitoring compliance by listed companies with their disclosure requirements is to ensure that investors have access to essential and adequate information for making an informed assessment of listed companies and their securities.Asset management supervision or investment acts ensures the frictionless operation of those vehicles.

Supervision of banks and financial services providers
Banking acts lay down rules for banks which they have to observe when they are being established and when they are carrying on their business. These rules are designed to prevent unwelcome developments that might disrupt the smooth functioning of the banking system. Thus ensuring a strong and efficient banking system.

Financial regulatory authorities
See also
References
Further reading
Labonte, Marc. (2017). Who Regulates Whom? An Overview of the U.S. Financial Regulatory Framework. Washington, D.C.: Congressional Research Service. Archived 2017-12-05 at the Wayback Machine
Reinhart, Carmen; Rogoff, Rogoff (2009), This Time is Different: Eight Centuries of Financial Folly, Princeton U. Pr., ISBN 978-0-691-15264-6
Simpson, D., Meeks, G., Klumpes, P., & Andrews, P. (2000). Some cost-benefit issues in financial regulation. London: Financial Services Authority.

External links
Securities Lawyer's Deskbook from the University of Cincinnati College of Law
Ana Carvajal, Jennifer Elliott: IMF Study Points to Gaps in Securities Market Regulation
IOSCO: Objectives and Principles of Securities Regulation (PDF-Datei 67 Seiten)

Five whys

Five whys (or 5 whys) is an iterative interrogative technique used to explore the cause-and-effect relationships underlying a particular problem. The primary goal of the technique is to determine the root cause of a defect or problem by repeating the question "Why?" five times. The answer to the fifth why should reveal the root cause of the problem.The technique was described by Taiichi Ohno at Toyota Motor Corporation. Others at Toyota and elsewhere have criticized the five whys technique for various reasons (see § Criticism).

Example
An example of a problem is: the vehicle will not start.

Why? – The battery is dead.
Why? – The alternator is not functioning.
Why? – The alternator belt has broken.
Why? – The alternator belt was well beyond its useful service life and not replaced.
Why? – The vehicle was not maintained according to the recommended service schedule. (A root cause)The questioning for this example could be taken further to a sixth, seventh, or higher level, but five iterations of asking why is generally sufficient to get to a root cause. The key is to encourage the troubleshooter to avoid assumptions and logic traps and instead trace the chain of causality in direct increments from the effect through any layers of abstraction to a root cause that still has some connection to the original problem. In this example, the fifth "why" suggests a broken process or an alterable behavior, which is indicative of reaching the root-cause level.
The last answer points to a process. This is one of the most important aspects in the five why approach – the real root cause should point toward a process that is not working well or does not exist. Untrained facilitators will often observe that answers seem to point towards classical answers such as not enough time, not enough investments, or not enough resources. These answers may be true, but they are out of our control. Therefore, instead of asking why?, ask why did the process fail?

History
The technique was originally developed by Sakichi Toyoda and was used within the Toyota Motor Corporation during the evolution of its manufacturing methodologies. It is a critical component of problem-solving training, delivered as part of the induction into the Toyota Production System. The architect of the Toyota Production System, Taiichi Ohno, described the five whys method as "the basis of Toyota's scientific approach by repeating why five times the nature of the problem as well as its solution becomes clear." The tool has seen widespread use beyond Toyota, and is now used within Kaizen, lean manufacturing, lean construction and Six Sigma. The five whys were initially developed to understand why new product features or manufacturing techniques were needed, and was not developed for root cause analysis.
In other companies, it appears in other forms. Under Ricardo Semler, Semco practices "three whys" and broadens the practice to cover goal setting and decision-making.

Techniques
Two primary techniques are used to perform a five whys analysis: the fishbone (or Ishikawa) diagram and a tabular format.
These tools allow for analysis to be branched in order to provide multiple root causes.

Criticism
The five whys have been criticized as a poor tool for root cause analysis. Teruyuki Minoura, former managing director of global purchasing for Toyota, criticized them as being too basic a tool to analyze root causes to the depth that is needed to ensure that they are fixed. Reasons for this criticism include:

Tendency for investigators to stop at symptoms rather than going on to lower-level root causes.
Inability to go beyond the investigator's current knowledge – the investigator cannot find causes that they do not already know.
Lack of support to help the investigator provide the right answer to "why" questions.
Results are not repeatable – different people using five whys come up with different causes for the same problem.
Tendency to isolate a single root cause, whereas each question could elicit many different root causes.Medical professor Alan J. Card also criticized the five whys as a poor root cause analysis tool and suggested that it be abandoned entirely. His reasoning also includes:

The artificial depth of the fifth why is unlikely to correlate with the root cause.
The five whys is based on a misguided reuse of a strategy to understand why new features should be added to products, not a root cause analysis.To avoid these issues, Card suggested abandoning the five whys and instead use other root cause analysis tools such as fishbone or lovebug diagrams.

See also
Eight disciplines problem solving
Five Ws (information-gathering)
Four causes
Issue map
Issue tree
Root cause analysis
Socratic method
Why–because analysis

References
External links
"Dauerspezial" commercial of Deutsche Bahn, where the question "why" is posed 5 times (in German)

Frederick Winslow Taylor

Frederick Winslow Taylor (March 20, 1856 – March 21, 1915) was an American mechanical engineer. He was widely known for his methods to improve industrial efficiency. He was one of the first management consultants. In 1909, Taylor summed up his efficiency techniques in his book The Principles of Scientific Management which, in 2001, Fellows of the Academy of Management voted the most influential management book of the twentieth century. His pioneering work in applying engineering principles to the work done on the factory floor was instrumental in the creation and development of the branch of engineering that is now known as industrial engineering. Taylor made his name, and was most proud of his work, in scientific management; however, he made his fortune patenting steel-process improvements. As a result, scientific management is sometimes referred to as Taylorism.

Biography
Taylor was born in 1856 to a Quaker family in Germantown, Philadelphia, Pennsylvania. Taylor's father, Franklin Taylor, a Princeton-educated lawyer, built his wealth on mortgages. Taylor's mother, Emily Annette Taylor (née Winslow), was an ardent abolitionist and a coworker with Lucretia Mott. His father's ancestor, Samuel Taylor, settled in Burlington, New Jersey, in 1677. His mother's ancestor, Edward Winslow, was one of the fifteen original Mayflower Pilgrims who brought servants or children, and one of eight who had the honorable distinction of Mister. Winslow served for many years as the Governor of the Plymouth colony.
The Taylor family had inherited wealth and property, and the family's assets were maintained by Franklin's older brother, Caleb Newbold Taylor.
Educated early by his mother, Taylor studied for two years in France and Germany and traveled Europe for 18 months. In 1872, he entered Phillips Exeter Academy in Exeter, New Hampshire, with the plan of eventually going to Harvard and becoming a lawyer like his father. In 1874, Taylor passed the Harvard entrance examinations with honors. However, due allegedly to rapidly deteriorating eyesight caused by night study, Taylor chose quite a different path.
Instead of attending Harvard University, Taylor became an apprentice patternmaker and machinist, gaining shop-floor experience at Enterprise Hydraulic Works in Philadelphia (a pump-manufacturing company whose proprietors were friends of the Taylor family). During this time, his eyesight recovered. He left his apprenticeship for six months and represented a group of New England machine-tool manufacturers at Philadelphia's centennial exposition. Taylor finished his four-year apprenticeship and in 1878 became a machine-shop laborer at Midvale Steel Works. At Midvale, he was quickly promoted to time clerk, journeyman machinist, machine shop foreman, research director, and finally chief engineer of the works (while maintaining his position as machine shop foreman). Taylor's fast promotions reflected both his talent and his family's relationship with Edward Clark, part owner of Midvale Steel. (Edward Clark's son Clarence Clark, who was also a manager at Midvale Steel, married Taylor's sister.)

Early on at Midvale, working as a laborer and machinist, Taylor recognized that workmen were working their machines, or themselves, not nearly as hard as they could (a practice that at the time was called "soldiering") and that this resulted in high labor costs for the company. When he became a foreman he expected more output from the workmen. In order to determine how much work should properly be expected, he began to study and analyze the productivity of both the men and the machines (although the word "productivity" was not used at the time, and the applied science of productivity had not yet been developed). His focus on the human component of production Taylor labeled scientific management.While Taylor worked at Midvale, he and Clarence Clark won the first tennis doubles tournament in the 1881 US National Championships, the precursor of the US Open. Taylor became a student of Stevens Institute of Technology, studying via correspondence and obtaining a bachelor's degree in mechanical engineering in 1883. On May 3, 1884, he married Louise M. Spooner of Philadelphia.

From 1890 until 1893 Taylor worked as a general manager and a consulting engineer to management for the Manufacturing Investment Company of Philadelphia, a company that operated large paper mills in Maine and Wisconsin. He was a plant manager in Maine. In 1893, Taylor opened an independent consulting practice in Philadelphia. His business card read "Consulting Engineer - Systematizing Shop Management and Manufacturing Costs a Specialty". Through these consulting experiences, Taylor perfected his management system. His first paper, A Piece Rate System, was presented to the American Society of Mechanical Engineers (ASME) in June 1895.In 1898 he joined Bethlehem Steel to solve an expensive machine-shop capacity problem. While at Bethlehem, he discovered the best known and most profitable of his many patents: between 1898 and 1900 Taylor and Maunsel White (né Maunsel White III; 1856–1912; grandson of Maunsel White; 1783–1863) conducted comprehensive empirical tests, and concluded that tungsten alloyed steel doubled or quadrupled cutting speeds. The inventors received US$100,000 (equivalent to about $3,500,000 in 2022) for the English patents alone, although the U.S. patent was eventually nullified.Taylor was forced to leave Bethlehem Steel in 1901 after discord with other managers. Now a wealthy man, Taylor focused the remainder of his career promoting his management and machining methods through lecturing, writing, and consulting. From 1904 - 1914, Taylor lived in Philadelphia with his wife and three adopted children. In 1910, owing to the Eastern Rate Case, Frederick Winslow Taylor and his Scientific Management methodologies became famous worldwide. In 1911, Taylor introduced his The Principles of Scientific Management paper to the ASME, eight years after his Shop Management paper.
On October 19, 1906, Taylor was awarded an honorary degree of Doctor of Science by the University of Pennsylvania. In the same year, he was elected president of the American Society of Mechanical Engineers. In 1912, Taylor gave testimony to a special committee of the US House of Representatives regarding his own and other systems of management. Taylor eventually became a professor at the Tuck School of Business at Dartmouth College. In early spring of 1915 Taylor caught pneumonia and died, one day after his fifty-ninth birthday, on March 21, 1915. He was buried in West Laurel Hill Cemetery, in Bala Cynwyd, Pennsylvania.

Work
Taylor was a mechanical engineer who sought to improve industrial efficiency. He is regarded as the father of scientific management, and was one of the first management consultants and director of a famous firm. In Peter Drucker's description,
Frederick W. Taylor was the first man in recorded history who deemed work deserving of systematic observation and study. On Taylor's 'scientific management' rests, above all, the tremendous surge of affluence in the last seventy-five years which has lifted the working masses in the developed countries well above any level recorded before, even for the well-to-do. Taylor, though the Isaac Newton (or perhaps the Archimedes) of the science of work, laid only first foundations, however. Not much has been added to them since—even though he has been dead all of sixty years. 
Taylor's scientific management consisted of four principles:

Replace rule-of-thumb work methods with methods based on a scientific study of the tasks.
Scientifically select, train, and develop each employee rather than passively leaving them to train themselves.
Provide "Detailed instruction and supervision of each worker in the performance of that worker's discrete task"
Divide work nearly equally between managers and workers, so that the managers apply scientific management principles to planning the work and the workers actually perform the tasks.Future US Supreme Court justice Louis Brandeis coined the term scientific management in the course of his argument for the Eastern Rate Case before the Interstate Commerce Commission in 1910. Brandeis argued that railroads, when governed according to Taylor's principles, did not need to raise rates to increase wages. Taylor used Brandeis's term in the title of his monograph The Principles of Scientific Management, published in 1911. The Eastern Rate Case propelled Taylor's ideas to the forefront of the management agenda. Taylor wrote to Brandeis, "I have rarely seen a new movement started with such great momentum as you have given this one." Taylor's approach is also often referred to as Taylor's Principles, or, frequently disparagingly, as Taylorism.

Managers and workers
Taylor had very precise ideas about how to introduce his system:

It is only through enforced standardization of methods, enforced adoption of the best implements and working conditions, and enforced cooperation that this faster work can be assured. And the duty of enforcing the adoption of standards and enforcing this cooperation rests with the management alone.
Workers were to be selected appropriately for each task.

One of the very first requirements for a man who is fit to handle pig iron as a regular occupation is that he shall be so stupid and so phlegmatic that he more nearly resembles in his mental make-up the ox than any other type. The man who is mentally alert and intelligent is for this very reason entirely unsuited to what would, for him, be the grinding monotony of work of this character.

Taylor believed in transferring control from workers to management. He set out to increase the distinction between mental (planning work) and manual labor (executing work). Detailed plans, specifying the job and how it was to be done, were to be formulated by management and communicated to the workers.The introduction of his system was often resented by workers and provoked numerous strikes. The strike at Watertown Arsenal led to the congressional investigation in 1912.
Taylor believed the laborer was worthy of his hire, and pay was linked to productivity. His workers were able to earn substantially more than those under conventional management, and this earned him enemies among the owners of factories where scientific management was not in use.

Rhetorical techniques
Taylor promised to reconcile labor and capital. With the triumph of scientific management, unions would have nothing left to do, and they would have been cleansed of their most evil feature: the restriction of output. To underscore this idea, Taylor fashioned the myth that 'there has never been a strike of men working under scientific management', trying to give it credibility by constant repetition. In similar fashion he incessantly linked his proposals to shorter hours of work, without bothering to produce evidence of "Taylorized" firms that reduced working hours, and he revised his famous tale of Schmidt carrying pig iron at Bethlehem Steel at least three times, obscuring some aspects of his study and stressing others, so that each successive version made Schmidt's exertions more impressive, more voluntary and more rewarding to him than the last. Unlike [Harrington] Emerson, Taylor was not a charlatan, but his ideological message required the suppression of all evidence of worker's dissent, of coercion, or of any human motives or aspirations other than those his vision of progress could encompass.
For the stories about Schmidt Montgomery refers to Charles D. Wrege and Amadeo G. Perroni, "Taylor's Pig Tale: A Historical Analysis of Frederick W. Taylor's Pig-Iron experiments" in: Academy of Management Journal, 17 (March 1974), 6-27</ref>

Scholarly debate about increased efficiency moving pig iron at Bethlehem's Iron and Steel
Debate about Taylor's Bethlehem study of workers, particularly the stereotypical laborer "Schmidt", continues to this day. One 2009 study supports assertions Taylor made about the quite substantial increase in productivity, for even the most basic task of picking up, carrying and dropping pigs of iron.

Management theory
Taylor thought that by analysing work, the "one best way" to do it would be found. He is most remembered for developing the stopwatch time study, which, combined with Frank Gilbreth's motion study methods, later became the field of time and motion study. He broke a job into its component parts and measured each to the hundredth of a minute. One of his most famous studies involved shovels. He noticed that workers used the same shovel for all materials. He determined that the most effective load was 21½ pounds, and found or designed shovels that for each material would scoop up that amount. He was generally unsuccessful in getting his concepts applied, and was dismissed from Bethlehem Iron Company/Bethlehem Steel Company. Nevertheless, Taylor was able to convince workers who used shovels and whose compensation was tied to how much they produced to adopt his advice about the optimum way to shovel by breaking the movements down into their component elements and recommending better ways to perform these movements. It was largely through his disciples' efforts (most notably Henry Gantt's) that industry came to implement his ideas. Moreover, the book he wrote after parting company with the Bethlehem company, Shop Management, sold well.

Relations with ASME
Taylor's written works were designed for presentation to the American Society of Mechanical Engineers (ASME). These include Notes on Belting (1894), A Piece-Rate System (1895), Shop Management (1903), Art of Cutting Metals (1906), and The Principles of Scientific Management (1911).
Taylor was president of the ASME from 1906 to 1907. While president, he tried to implement his system into the management of the ASME but met with much resistance. He was able to reorganize only the publications department and that only partially. He also forced out the ASME's longtime secretary, Morris Llewellyn Cooke, and replaced him with Calvin W. Rice. His tenure as president was trouble-ridden and marked the beginning of a period of internal dissension within the ASME during the Progressive Age.In 1911, Taylor collected a number of his articles into a book-length manuscript, which he submitted to the ASME for publication. The ASME formed an ad hoc committee to review the text. The committee included Taylor allies such as James Mapes Dodge and Henry R. Towne. The committee delegated the report to the editor of the American Machinist, Leon P. Alford. Alford was a critic of the Taylor system and his report was negative. The committee modified the report slightly, but accepted Alford's recommendation not to publish Taylor's book. Taylor angrily withdrew the book and published Principles without ASME approval. Taylor published the trade book himself in 1912.

Taylor's influence
United States
Carl G. Barth helped Taylor to develop speed-and-feed-calculating slide rules to a previously unknown level of usefulness. Similar aids are still used in machine shops today. Barth became an early consultant on scientific management and later taught at Harvard.
H. L. Gantt developed the Gantt chart, a visual aid for scheduling tasks and displaying the flow of work.
Harrington Emerson introduced scientific management to the railroad industry, and proposed the dichotomy of staff versus line employees, with the former advising the latter.
Morris Cooke adapted scientific management to educational and municipal organizations.
Hugo Münsterberg created industrial psychology.
Lillian Gilbreth introduced psychology to management studies.
Frank Gilbreth (husband of Lillian) discovered scientific management while working in the construction industry, eventually developing motion studies independently of Taylor. These logically complemented Taylor's time studies, as time and motion are two sides of the efficiency improvement coin. The two fields eventually became time and motion study.
Harvard University, one of the first American universities to offer a graduate degree in business management in 1908, based its first-year curriculum on Taylor's scientific management.
Harlow S. Person, as dean of Dartmouth's Amos Tuck School of Administration and Finance, promoted the teaching of scientific management.
James O. McKinsey, professor of accounting at the University of Chicago and founder of the consulting firm bearing his name, advocated budgets as a means of assuring accountability and of measuring performance.

France
In France, Le Chatelier translated Taylor's work and introduced scientific management throughout government owned plants during World War I. This influenced the French theorist Henri Fayol, whose 1916 Administration Industrielle et Générale emphasized organizational structure in management. In the classic General and Industrial Management, Fayol wrote that "Taylor's approach differs from the one we have outlined in that he examines the firm from the 'bottom up.' He starts with the most elemental units of activity – the workers' actions – then studies the effects of their actions on productivity, devises new methods for making them more efficient, and applies what he learns at lower levels to the hierarchy  ... " He suggests that Taylor has staff analysts and advisors working with individuals at lower levels of the organization to identify the ways to improve efficiency. According to Fayol, the approach results in a "negation of the principle of unity of command." Fayol criticized Taylor's functional management in this way: In Shop Management,  Taylor said « ... the most marked outward characteristics of functional management lies in the fact that each workman, instead of coming in direct contact with the management at one point only, ... receives his daily orders and help from eight different bosses... these eight were (1) route clerks, (2) instruction card men, (3) cost and time clerks, (4) gang bosses, (5) speed bosses, (6) inspectors, (7) repair bosses, and the (8) shop disciplinarian. » Fayol said that this was an unworkable situation and that Taylor must have reconciled the differences in some way not described in Taylor's works.
Around 1922 the journalist Paulette Bernège became interested in Taylor's theories, which were popular in France in the post-war period.
Bernège became the faithful disciple of the Domestic Sciences Movement that Christine Frederick had launched earlier in the United States, which Bernège adapted to French homes.
Frederick had transferred the concepts of Taylorism from the factory to domestic work. These included suitable tools, rational study of movements and timing of tasks. Scientific standards for housework were derived from scientific standards for workshops, intended to streamline the work of a housewife.
The Comité national de l'organisation française (CNOF) was founded in 1925 by a group of journalists and consulting engineers who saw Taylorism as a way to expand their client base.
Founders included prominent engineers such as Henry Louis Le Châtelier and Léon Guillet. Bernège's Institute of Housekeeping Organization participated in various congresses on the scientific organization of work that led up to the founding of the CNOF, and in 1929 led to a section in CNOF on domestic economy.

Great Britain
Older historical accounts used to suggest that British industry had less interest in Taylor's teachings than in similarly sized countries. More recent research has revealed that British engineers and managers were as interested as in other countries. This disparity was largely due to what historians have been analysing: recent research has revealed that Taylor's practices diffused to Britain more through consultancies, in particular the Bedaux consultancy, than through institutions, as in Germany and to a lesser extent France, where a mixture was most effective.Particularly enthusiastic were the Cadbury family, Seebohm Rowntree, Oliver Sheldon and Lyndall Urwick. In addition to establishing a consultancy to implement Taylor's system, Urwick, Orr & Partners, Urwick was also a key historian of F.W. Taylor and scientific management, publishing The Making of Scientific Management trilogy in the 1940s and The Golden Book of Management in 1956.

Switzerland
In Switzerland, the American Edward Albert Filene established the International Management Institute to spread information about management techniques. Lyndall Urwick was its director until the IMI closed in 1933.Charles D. Wrege, Ronald G. Greenwood, and Sakae Hata, 'The International Management Institute and Political Opposition to its Efforts in Europe, 1925-1934' Business and Economic History (1987)PDF link</ref>

USSR
In the Soviet Union, Vladimir Lenin was very impressed by Taylorism, which he and other Bolshevik leaders tried to incorporate into Soviet manufacturing. When Joseph Stalin took power in the 1920s, he championed the theory of "Socialism in one country" which denied that the Soviet economy needed foreign help to develop, and open advocates of Western management techniques fell into disfavor. No longer celebrated by Soviet leadership, Taylorism and the mass production methods of Henry Ford remained silent influences during the industrialization of the Soviet Union. Nevertheless, "[...] Frederick Taylor's methods have never really taken root in the Soviet Union." The voluntaristic approach of Stalin's Stakhanovite movement in the 1930s, fixated on setting individual records, was intrinsically opposed to Taylor's systematic approach and proved to be counter-productive. The stop-and-go of the production process – workers having nothing to do at the beginning of a month and 'storming' during illegal extra shifts at the end of the month – which prevailed even in the 1980s had nothing to do with the successfully taylorized plants e.g., of Toyota which are characterized by continuous production processes (heijunka) which are continuously improved (kaizen)."The easy availability of replacement labor, which allowed Taylor to choose only 'first-class men,' was an important condition for his system's success."  The situation in the Soviet Union was very different.  "Because work is so unrhythmic, the rational manager will hire more workers than he would need if supplies were even in order to have enough for storming. Because of the continuing labor shortage, managers are happy to pay needed workers more than the norm, either by issuing false job orders, assigning them to higher skill grades than they deserve on merit criteria, giving them 'loose' piece rates, or making what is supposed to be 'incentive' pay, premia for good work, effectively part of the normal wage. As Mary McAuley has suggested under these circumstances piece rates are not an incentive wage, but a way of justifying giving workers whatever they 'should' be getting, no matter what their pay is supposed to be according to the official norms."Taylor and his theories are also referenced (and put to practice) in the 1921 dystopian novel We by Yevgeny Zamyatin.

Canada
In the early 1920s, the Canadian textile industry was re-organized according to scientific management principles. In 1928, workers at Canada Cotton Ltd. in Hamilton, Ontario went on strike against newly introduced Taylorist work methods. Also, Henry Gantt, who was a close associate of Taylor, re-organized the Canadian Pacific Railway.With the prevalence of US branch plants in Canada and close economic and cultural ties between the two countries, the sharing of business practices, including Taylorism, has been common.

The Taylor Society and its legacy
The Taylor Society was founded in 1912 by Taylor's allies to promote his values and influence. A decade after Taylor's death in 1915 the Taylor Society had 800 members, including many leading U.S. industrialists and managers. In 1936 the Society merged with the Society of Industrial Engineers, forming the Society for Advancement of Management, which still exists today.

Criticism of Taylor
Many of the critiques of Taylor come from Marxists. The earliest was by Antonio Gramsci, an Italian Communist, in his Prison Notebooks (1937). Gramsci argued that Taylorism subordinates the  workers to management. He also argued that the repetitive work produced by Taylorism might actually give rise to revolutionary thoughts in workers' minds.Harry Braverman's work Labor and Monopoly Capital: The Degradation of Work in the Twentieth Century, published in 1974, was critical of scientific management and of Taylor in particular. This work pioneered the field of Labor Process Theory as well as contributing to the historiography of the workplace.
Management theorist Henry Mintzberg is highly critical of Taylor's methods. Mintzberg states that an obsession with efficiency allows measurable benefits to overshadow less quantifiable social benefits completely, and social values get left behind.Taylor's methods have also been challenged by socialists. Their arguments relate to progressive defanging of workers in the workplace and the subsequent degradation of work as management, powered by capital, uses Taylor's methods to render work repeatable and precise yet monotonous and skill-reducing. James W. Rinehart argued that Taylor's methods of transferring control over production from workers to management, and the division of labor into simple tasks, intensified the alienation of workers that had begun with the factory system of production around the period 1870 to 1890.Criticism of Taylor and the Japanese model, according to Kōnosuke Matsushita:
"We are going to win and the industrial west is going to lose out; ... the reasons for failure are within yourselves. Your firms are built on the Taylor model. Even worse, so are your heads. With your bosses doing the thinking while workers wield the screwdrivers, you're convinced deep down that it is the right way to run a business. For the essence of management is getting ideas out of the heads of the bosses and into the heads of labour. We are beyond your mindset. Business, we know, is now so complex and difficult, the survival of firms so hazardous in an environment increasingly unpredictable, competitive and fraught with danger, that their continued existence depends on the day-to-day mobilisation of every ounce of intelligence."

Tennis and golf accomplishments
Taylor was an accomplished tennis and golf player. He and Clarence Clark won the inaugural United States National tennis doubles championship at Newport Casino in 1881, defeating Alexander Van Rensselaer and Arthur Newbold (né Arthur Emlen Newbold; 1859–1920) in straight sets. In the 1900 Summer Olympics, Taylor finished fourth in golf.

Publications
Books

Selected articles

Bibliography
Notes
References
Further reading
See also
C. Bertrand Thompson

External links

Works by Frederick Winslow Taylor at Project Gutenberg
Works by or about Frederick Winslow Taylor at Internet Archive
Special Collections: Frederick Winslow Taylor. The Samuel C. Williams Library at the Stevens Institute of Technology has an extensive collection. OCLC 123905137.
"Frederick W. Taylor, 1856–1915". Collection of Historical Scientific Instruments, Harvard University. OCLC 77066758.
Charles D. Wrege Research Papers; Collection Number: 6395 (see Charles D. Wrege; 1924–2014). Kheel Center for Labor-Management Documentation and Archives, Cornell University Library. OCLC 826068268

General Electric

General Electric Company (GE) is an American multinational conglomerate founded in 1892 and incorporated in the state of New York and headquartered in Boston. The company has several divisions, including aerospace, power, renewable energy, digital industry, additive manufacturing, and venture capital and finance.In 2020, GE ranked among the Fortune 500 as the 33rd largest firm in the United States by gross revenue. In 2011, GE ranked among the Fortune 20 as the 14th most profitable company, but later very severely underperformed the market (by about 75%) as its profitability collapsed. Two employees of GE – Irving Langmuir (1932) and Ivar Giaever (1973) – have been awarded the Nobel Prize.On November 9, 2021, the company announced it would divide itself into three public companies. On July 18, 2022, GE unveiled the brand names of the companies it will create through its planned separation: GE Aerospace, GE HealthCare and GE Vernova. The new companies will be focused on aerospace, healthcare, and energy (renewable energy, power, and digital). The first spin-off of GE HealthCare was finalized on January 4, 2023; GE continues to hold 13.5% of shares and intends to sell the remaining over time. This will be followed by the spin-off of GE's portfolio of energy businesses which plan to become GE Vernova in 2024. Following these transactions, GE will be an aviation-focused company; GE Aerospace will be the legal successor of the original GE.

History
Formation
During 1889, Thomas Edison (1847–1931) had business interests in many electricity-related companies, including Edison Lamp Company, a lamp manufacturer in East Newark, New Jersey; Edison Machine Works, a manufacturer of dynamos and large electric motors in Schenectady, New York; Bergmann & Company, a manufacturer of electric lighting fixtures, sockets, and other electric lighting devices; and Edison Electric Light Company, the patent-holding company and financial arm for Edison's lighting experiments, backed by J. P. Morgan (1837–1913) and the Vanderbilt family.In 1889, Drexel, Morgan & Co., a company founded by J.P. Morgan and Anthony J. Drexel financed Edison's research and helped merge several of Edison's separate companies under one corporation forming Edison General Electric Company, which was incorporated in New York on April 24, 1889. The new company acquired Sprague Electric Railway & Motor Company in the same year. The consolidation did not involve all of the companies established by Edison; notably, the Edison Illuminating Company, which would later become Consolidated Edison, was not part of the merger.
In 1880, Gerald Waldo Hart formed the American Electric Company of New Britain, Connecticut, which merged a few years later with Thomson-Houston Electric Company, led by Charles Coffin. In 1887, Hart left to become superintendent of the Edison Electric Company. General Electric was formed through the 1892 merger of Edison General Electric Company and Thomson-Houston Electric Company with the support of Drexel, Morgan & Co. The original plants of both companies continue to operate under the GE banner to this day.The General Electric business was incorporated in New York, with the Schenectady plant used as headquarters for many years thereafter. Around the same time, General Electric's Canadian counterpart, Canadian General Electric, was formed.In 1893, General Electric bought the business of Rudolf Eickemeyer in Yonkers, New York, along with all of its patents and designs. Eickemeyer's firm had developed transformers for use in the transmission of electrical power.

Public company
In 1896, General Electric was one of the original 12 companies listed on the newly formed Dow Jones Industrial Average, where it remained a part of the index for 122 years, though not continuously.
In 1911, General Electric absorbed the National Electric Lamp Association (NELA) into its lighting business. GE established its lighting division headquarters at Nela Park in East Cleveland, Ohio. The lighting division has since remained in the same location.

RCA and NBC
Owen D. Young, through GE, founded the Radio Corporation of America (RCA) in 1919, after purchasing the Marconi Wireless Telegraph Company of America. He aimed to expand international radio communications. GE used RCA as its retail arm for radio sales. In 1926, RCA co-founded the National Broadcasting Company (NBC), which built two radio broadcasting networks. In 1930, General Electric was charged with antitrust violations and was ordered to divest itself of RCA.

Television
In 1927, Ernst Alexanderson of GE made the first demonstration of television broadcast reception at his General Electric Realty Plot home at 1132 Adams Road in Schenectady, New York. On January 13, 1928, he made what was said to be the first broadcast to the public in the United States on GE's W2XAD: the pictures were picked up on 1.5 square inch (9.7 square centimeter) screens in the homes of four GE executives. The sound was broadcast on GE's WGY (AM).Experimental television station W2XAD evolved into the station WRGB which, along with WGY and WGFM (now WRVE), was owned and operated by General Electric until 1983. In 1965, the company expanded into cable with the launch of a franchise, which was awarded to a non-exclusive franchise in Schenectady through subsidiary General Electric Cablevision Corporation. On February 15, 1965, General Electric expanded its holdings in order to acquire more television stations to meet the maximum limit of the FCC, and more cable holdings through subsidiaries General Electric Broadcasting Company and General Electric Cablevision Corporation.The company also owned television stations such as KOA-TV (now KCNC-TV) in Denver and WSIX-TV (later WNGE-TV, now WKRN) in Nashville, but like WRGB, General Electric sold off most of its broadcasting holdings, but held on to the Denver television station until in 1986, when General Electric bought out RCA and made it into an owned-and-operated station by NBC. It even stayed on until 1995 when it was transferred to a joint venture between CBS and Group W in a swap deal, alongside KUTV in Salt Lake City for longtime CBS O&O in Philadelphia, WCAU-TV.

Former General Electric-owned stations
Stations are arranged in alphabetical order by state and city of license.

Radio stations
Power generation
Led by Sanford Alexander Moss, GE moved into the new field of aircraft turbo superchargers. This technology also led to the development of industrial gas turbine engines used for power production. GE introduced the first set of superchargers during World War I, and continued to develop them during the interwar period. Superchargers became indispensable in the years immediately prior to World War II. GE supplied 300,000 turbo superchargers for use in fighter and bomber engines. This work led the U.S. Army Air Corps to select GE to develop the nation's first jet engine during the war. This experience, in turn, made GE a natural selection to develop the Whittle W.1 jet engine that was demonstrated in the United States in 1941. GE was ranked ninth among United States corporations in the value of wartime production contracts. Although, their early work with Whittle's designs was later handed to Allison Engine Company. GE Aviation then emerged as one of the world's largest engine manufacturers, bypassing the British company, Rolls-Royce plc.
Some consumers boycotted GE light bulbs, refrigerators and other products during the 1980s and 1990s. The purpose of the boycott was to protest against GE's role in nuclear weapons production.In 2002, GE acquired the wind power assets of Enron during its bankruptcy proceedings. Enron Wind was the only surviving U.S. manufacturer of large wind turbines at the time, and GE increased engineering and supplies for the Wind Division and doubled the annual sales to $1.2 billion in 2003. It acquired ScanWind in 2009.In 2018, GE Power garnered press attention when a model 7HA gas turbine in Texas was shut down for two months due to the break of a turbine blade. This model uses similar blade technology to GE's newest and most efficient model, the 9HA. After the break, GE developed new protective coatings and heat treatment methods. Gas turbines represent a significant portion of GE Power's revenue, and also represent a significant portion of the power generation fleet of several utility companies in the United States. Chubu Electric of Japan and Électricité de France also had units that were impacted. Initially, GE did not realize the turbine blade issue of the 9FB unit would impact the new HA units.

Computing
GE was one of the eight major computer companies of the 1960s along with IBM, Burroughs, NCR, Control Data Corporation, Honeywell, RCA, and UNIVAC. GE had a line of general purpose and special purpose computers, including the GE 200, GE 400, and GE 600 series general purpose computers, the GE 4010, GE 4020, and GE 4060 real-time process control computers, and the DATANET-30 and Datanet 355 message switching computers (DATANET-30 and 355 were also used as front end processors for GE mainframe computers). A Datanet 500 computer was designed, but never sold.In 1956 Homer Oldfield had been promoted to General Manager of GE's Computer Department. He facilitated the invention and construction of the Bank of America ERMA system, the first computerized system designed to read magnetized numbers on checks. But he was fired from GE in 1958 by Ralph J. Cordiner for overstepping his bounds and successfully gaining the ERMA contract. Cordiner was strongly against GE entering the computer business because he did not see the potential in it.
In 1962, GE started developing its GECOS (later renamed GCOS) operating system, originally for batch processing, but later extended to time-sharing and transaction processing. Versions of GCOS are still in use today. From 1964 to 1969, GE and Bell Laboratories (which soon dropped out) joined with MIT to develop the Multics operating system on the GE 645 mainframe computer. The project took longer than expected and was not a major commercial success, but it demonstrated concepts such as single-level storage, dynamic linking, hierarchical file system, and ring-oriented security. Active development of Multics continued until 1985.
GE got into computer manufacturing because in the 1950s they were the largest user of computers outside the United States federal government, aside from being the first business in the world to own a computer. Its major appliance manufacturing plant "Appliance Park" was the first non-governmental site to host one. However, in 1970, GE sold its computer division to Honeywell, exiting the computer manufacturing industry, though it retained its timesharing operations for some years afterwards. GE was a major provider of computer time-sharing services, through General Electric Information Services (GEIS, now GXS), offering online computing services that included GEnie.
In 2000, when United Technologies Corp. planned to buy Honeywell, GE made a counter-offer that was approved by Honeywell. On July 3, 2001, the European Union issued a statement that "prohibit the proposed acquisition by General Electric Co. of Honeywell Inc.". The reasons given were it "would create or strengthen dominant positions on several markets and that the remedies proposed by GE were insufficient to resolve the competition concerns resulting from the proposed acquisition of Honeywell".On June 27, 2014, GE partnered with collaborative design company Quirky to announce its connected LED bulb called Link. The Link bulb is designed to communicate with smartphones and tablets using a mobile app called Wink.

Acquisitions and divestments
In December 1985, GE reacquired the RCA Corporation, primarily to gain ownership of the NBC television network (also parent of Telemundo Communications Group) for $6.28 billion; this merger surpassed the Capital Cities/ABC merger that happened earlier that year as the largest non-oil merger in world business history. The remainder of RCA was sold to various companies, including Bertelsmann which absorbed RCA Records and Thomson SA, which licensed the manufacture of RCA branded electronics, traced its roots to Thomson-Houston, one of the original components of GE. Also in 1986, Kidder, Peabody & Co., a U.S.-based securities firm, was sold to GE and following heavy losses was sold to PaineWebber in 1994.In 2002, Francisco Partners and Norwest Venture Partners acquired a division of GE called GE Information Systems (GEIS). The new company, named GXS, is based in Gaithersburg, Maryland. GXS is a provider of business-to-business e-commerce solutions. GE maintains a minority stake in GXS. Also in 2002, GE Wind Energy was formed when GE bought the wind turbine manufacturing assets of Enron Wind after the Enron scandals.In 2004, GE bought 80% of Vivendi Universal Entertainment, the parent of Universal Pictures from Vivendi. Vivendi bought 20% of NBC forming the company NBCUniversal. GE then owned 80% of NBCUniversal and Vivendi owned 20%. In 2004, GE completed the spin-off of most of its mortgage and life insurance assets into an independent company, Genworth Financial, based in Richmond, Virginia.Genpact formerly known as GE Capital International Services (GECIS) was established by GE in late 1997 as its captive India-based BPO. GE sold 60% stake in Genpact to General Atlantic and Oak Hill Capital Partners in 2005 and hived off Genpact into an independent business. GE is still a major client to Genpact today, for services in customer service, finance, information technology, and analytics.In May 2007, GE acquired Smiths Aerospace for $4.8 billion. Also in 2007, GE Oil & Gas acquired Vetco Gray for $1.9 billion, followed by the acquisition of Hydril Pressure & Control in 2008 for $1.1 billion.GE Plastics was sold in 2008 to SABIC (Saudi Arabia Basic Industries Corporation). In May 2008, GE announced it was exploring options for divesting the bulk of its consumer and industrial business.On December 3, 2009, it was announced that NBCUniversal would become a joint venture between GE and cable television operator Comcast. Comcast would hold a controlling interest in the company, while GE would retain a 49% stake and would buy out shares owned by Vivendi.Vivendi would sell its 20% stake in NBCUniversal to GE for US$5.8 billion. Vivendi would sell 7.66% of NBCUniversal to GE for US$2 billion if the GE/Comcast deal was not completed by September 2010 and then sell the remaining 12.34% stake of NBCUniversal to GE for US$3.8 billion when the deal was completed or to the public via an IPO if the deal was not completed.On March 1, 2010, GE announced plans to sell its 20.85% stake in Turkey-based Garanti Bank. In August 2010, GE Healthcare signed a strategic partnership to bring cardiovascular Computed Tomography (CT) technology from start-up Arineta Ltd. of Israel to the hospital market. In October 2010, GE acquired gas engines manufacturer Dresser Industries in a $3 billion deal and also bought a $1.6 billion portfolio of retail credit cards from Citigroup Inc. On October 14, 2010, GE announced the acquisition of data migration & SCADA simulation specialists Opal Software. In December 2010, for the second time that year (after the Dresser acquisition), GE bought the oil sector company Wellstream, an oil pipe maker, for 800 million pounds ($1.3 billion).In March 2011, GE announced that it had completed the acquisition of privately held Lineage Power Holdings from The Gores Group. In April 2011, GE announced it had completed its purchase of John Wood plc's Well Support Division for $2.8 billion.In 2011, GE Capital sold its $2 billion Mexican assets to Santander for $162 million and exited the business in Mexico. Santander additionally assumed the portfolio debts of GE Capital in the country. Following this, GE Capital focused in its core business and shed its non-core assets.In June 2012, CEO and President of GE Jeff Immelt said that the company would invest ₹3 billion  to accelerate its businesses in Karnataka. In October 2012, GE acquired $7 billion worth of bank deposits from MetLife Inc.On March 19, 2013, Comcast bought GE's shares in NBCU for $16.7 billion, ending the company's longtime stake in television and film media.In April 2013, GE acquired oilfield pump maker Lufkin Industries for $2.98 billion.In April 2014, it was announced that GE was in talks to acquire the global power division of French engineering group Alstom for a figure of around $13 billion. A rival joint bid was submitted in June 2014 by Siemens and Mitsubishi Heavy Industries (MHI) with Siemens seeking to acquire Alstom's gas turbine business for €3.9 billion, and MHI proposing a joint venture in steam turbines, plus a €3.1 billion cash investment. In June 2014 a formal offer from GE worth $17 billion was agreed by the Alstom board. Part of the transaction involved the French government taking a 20% stake in Alstom to help secure France's energy and transport interests and French jobs. A rival offer from Siemens-Mitsubishi Heavy Industries was rejected. The acquisition was expected to be completed in 2015. In October 2014, GE announced it was considering the sale of its Polish banking business Bank BPH.Later in 2014, General Electric announced plans to open its global operations center in Cincinnati, Ohio. The Global Operations Center opened in October 2016 as home to GE's multifunctional shared services organization. It supports the company's finance/accounting, human resources, information technology, supply chain, legal and commercial operations, and is one of GE's four multifunctional shared services centers worldwide in Pudong, China; Budapest, Hungary; and Monterrey, Mexico.In April 2015, GE announced its intention to sell off its property portfolio, worth $26.5 billion, to Wells Fargo and The Blackstone Group. It was announced in April 2015 that GE would sell most of its finance unit and return around $90 billion to shareholders as the firm looked to trim down on its holdings and rid itself of its image of a "hybrid" company, working in both banking and manufacturing. In August 2015, GE Capital agreed to sell its Healthcare Financial Services business to Capital One for US$9 billion. The transaction involved US$8.5 billion of loans made to a wide array of sectors including senior housing, hospitals, medical offices, outpatient services, pharmaceuticals and medical devices. Also in August 2015, GE Capital agreed to sell GE Capital Bank's on-line deposit platform to Goldman Sachs. Terms of the transaction were not disclosed, but the sale included US$8 billion of on-line deposits and another US$8 billion of brokered certificates of deposit. The sale was part of GE's strategic plan to exit the U.S. banking sector and to free itself from tightening banking regulations. GE also aimed to shed its status as a "systematically important financial institution".In September 2015, GE Capital agreed to sell its transportation-finance unit to Canada's Bank of Montreal. The unit sold had US$8.7 billion (CA$11.5 billion) of assets, 600 employees and 15 offices in the U.S. and Canada. Exact terms of the sale were not disclosed, but the final price would be based on the value of the assets at closing, plus a premium according to the parties. In October 2015, activist investor Nelson Peltz's fund Trian bought a $2.5 billion stake in the company.In January 2016, Haier acquired GE's appliance division for $5.4 billion. In October 2016, GE Renewable Energy agreed to pay €1.5 billion to Doughty Hanson & Co for LM Wind Power during 2017.At the end of October 2016, it was announced that GE was under negotiations for a deal valued at about $30 billion to combine GE Oil & Gas with Baker Hughes. The transaction would create a publicly traded entity controlled by GE. It was announced that GE Oil & Gas would sell off its water treatment business, GE Water & Process Technologies, as part of its divestment agreement with Baker Hughes. The deal was cleared by the EU in May 2017, and by the United States Department of Justice in June 2017. The merger agreement was approved by shareholders at the end of June 2017. On July 3, 2017, the transaction was completed and Baker Hughes became a GE company and was renamed Baker Hughes, a GE Company (BHGE). In November 2018, GE reduced its stake in Baker Hughes to 50.4%. On October 18, 2019, GE reduced its stake to 36.8% and the company was renamed back to Baker Hughes.In May 2017, GE had signed $15 billion of business deals with Saudi Arabia. Saudi Arabia is one of GE's largest customers. In September 2017, GE announced the sale of its Industrial Solutions Business to ABB. The deal closed on June 30, 2018.

Fraud allegations and notice of possible SEC civil action
On August 15, 2019, Harry Markopolos, a financial fraud investigator known for his discovery of a Ponzi Scheme run by Bernard Madoff, accused General Electric of being a "bigger fraud than Enron", alleging $38 billion in accounting fraud. GE denied wrongdoing.On October 6, 2020, General Electric reported it received a Wells notice from the Securities and Exchange Commission stating the SEC may take civil action for possible violations of securities laws.

Insufficient reserves for long-term care policies
It is alleged that GE is "hiding" (i.e. under-reserved) $29 billion in losses related to its long-term care business.According to an August 2019 Fitch Ratings report, there are concerns that GE has not set aside enough money to cover its long-term care liabilities.In 2018, a lawsuit (the Bezio case) was filed in New York state court on behalf of participants in GE's 401(k) plan and shareowners alleging violations of Section 11 of the Securities Act of 1933 based on alleged misstatements and omissions related to insurance reserves and performance of GE's business segments.The Kansas Insurance Department (KID) is requiring General Electric to make $14.5 billion of capital contributions for its insurance contracts during the 7-year period ending in 2024.GE reported the total liability related to its insurance contracts increased significantly from 2016 to 2019:

December 31, 2016 $26.1 billion
December 31, 2017 $38.6 billion
December 31, 2018 $35.6 billion
December 31, 2019 $39.6 billionIn 2018, GE announced the issuance of the new standard by the Financial Accounting Standards Board (FASB) regarding Financial Services - Insurance (Topic 944) will materially affect its financial statements.  Mr. Markopolos estimated there will be a $US 10.5 billion charge when the new accounting standard is adopted in the first quarter of 2021.

Anticipated $8 billion loss upon disposition of Baker Hughes
In 2017, GE acquired a 62.5% interest in Baker Hughes (BHGE) when it combined its oil & gas business with Baker Hughes Incorporated.
In 2018, GE reduced its interest to 50.4%, resulting in the realization of a $2.1 billion loss.  GE is planning to divest its remaining interest and has warned that the divestment will result in an additional loss of $8.4 billion (assuming a BHGE share price of $23.57 per share). In response to the fraud allegations, GE noted the amount of the loss would be $7.4 billion if the divestment occurred on July 26, 2019.  Mr. Markopolos noted that BHGE is an asset available for sale and therefore mark-to-market accounting is required.Markopolos noted GE's current ratio was only 0.67. He expressed concerns that GE may file for bankruptcy if there is a recession.

Other
In 2018, the GE Pension Plan reported losses of US$3.3 billion on plan assets.In 2018, General Electric changed the discount rate used to calculate the actuarial liabilities of its pension plans.  The rate was increased from 3.64% to 4.34%.  Consequently, the reported liability for the underfunded pension plans decreased by $7 billion year-over-year, from $34.2 billion in 2017 to $27.2 billion in 2018.In October 2018, General Electric announced it would "freeze pensions" for about 20,000 salaried U.S. employees.  The employees will be moved to a defined-contribution retirement plan in 2021.On March 30, 2020, General Electric factory workers protested to convert jet engine factories to make ventilators during the COVID-19 crisis.In June 2020, GE made an agreement to sell its Lighting business to Savant Systems, Inc. Financial details of the transaction were not disclosed.In November 2020, General Electric warned it would be cutting jobs waiting for a recovery due to the COVID-19 pandemic.

Financial performance
Dividends
General Electric was a longtime "dividend aristocrat" (a company with a long history of maintaining dividend payments to shareholders). Until 2017, the company had never cut dividends for 119 years before a 50% dividend reduction from 24 cents per share to 12 cents per share. In 2018, GE further reduced its quarterly dividend from 12 cents to 1 cent per share.

Stock
As a publicly traded company on the New York Stock Exchange, GE stock was one of the 30 components of the Dow Jones Industrial Average from 1907 to 2018, the longest continuous presence of any company on the index, and during this time the only company which was part of the original Dow Jones Industrial Index created in 1896. In August 2000, the company had a market capitalization of $601 billion, and was the most valuable company in the world. On June 26, 2018, the stock was removed from the index and replaced with Walgreens Boots Alliance. In the years leading to its removal, GE was the worst performing stock in the Dow, falling more than 55 percent year on year and more than 25 percent year to date. The company continued to lose value after being removed from the index.

			
			
		
		
			
			
		
General Electric Co. announced on July 30, 2021 (the completion of) a reverse stock split of GE common stock at a ratio of 1-for-8 and trading on a split-adjusted basis with a new ISIN number (US3696043013) starting on August 2, 2021.

Corporate affairs
In 1959, General Electric was accused of promoting the largest illegal cartel in the United States since the adoption of the Sherman Antitrust Act of 1890 in order to maintain artificially high prices. In total, 29 companies and 45 executives would be convicted. Subsequent parliamentary inquiries revealed that "white-collar crime" was by far the most costly form of crime for the United States' finances.GE is a multinational conglomerate headquartered in Boston, Massachusetts. However its main offices are located at 30 Rockefeller Plaza at Rockefeller Center in New York City, known now as the Comcast Building. It was formerly known as the GE Building for the prominent GE logo on the roof; NBC's headquarters and main studios are also located in the building. Through its RCA subsidiary, it has been associated with the center since its construction in the 1930s. GE moved its corporate headquarters from the GE Building on Lexington Avenue to Fairfield, Connecticut in 1974. In 2016, GE announced a move to the South Boston Waterfront neighborhood of Boston, Massachusetts, partly as a result of an incentive package provide by state and city governments. The first group of workers arrived in the summer of 2016, and the full move will be completed by 2018. Due to poor financial performance and corporate downsizing, GE sold the land it planned to build its new headquarters building on, instead choosing to occupy neighboring leased buildings.GE's tax return is the largest return filed in the United States; the 2005 return was approximately 24,000 pages when printed out, and 237 megabytes when submitted electronically. As of 2011, the company spent more on U.S. lobbying than any other company.In 2005, GE launched its "Ecomagination" initiative in an attempt to position itself as a "green" company.
GE is one of the biggest players in the wind power industry and is developing environment-friendly products such as hybrid locomotives, desalination and water reuse solutions, and photovoltaic cells. The company "plans to build the largest solar-panel-making factory in the U.S.", and has set goals for its subsidiaries to lower their greenhouse gas emissions.On May 21, 2007, GE announced it would sell its GE Plastics division to petrochemicals manufacturer SABIC for net proceeds of $11.6 billion. The transaction took place on August 31, 2007, and the company name changed to SABIC Innovative Plastics, with Brian Gladden as CEO.In July 2010, GE agreed to pay $23.4 million to settle an SEC complaint without admitting or denying the allegations that two of its subsidiaries bribed Iraqi government officials to win contracts under the U.N. oil-for-food program between 2002 and 2003.In February 2017, GE announced that the company intends to close the gender gap by promising to hire and place 20,000 women in technical roles by 2020. The company is also seeking to have a 50:50 male to female gender representation in all entry-level technical programs.In October 2017, GE announced they would be closing research and development centers in Shanghai, Munich and Rio de Janeiro. The company spent $5 billion on R&D in the last year.On February 25, 2019, GE sold its diesel locomotive business to Wabtec.

CEO
As of October 2018, John L. Flannery was replaced by H. Lawrence Culp Jr. as chairman and CEO in a unanimous vote of the GE Board of Directors.
Charles A. Coffin (1913–1922)
Owen D. Young (1922–1939, 1942–1945)
Philip D. Reed (1940–1942, 1945–1958)
Ralph J. Cordiner (1958–1963)
Gerald L. Phillippe (1963–1972)
Fred J. Borch (1967–1972)
Reginald H. Jones (1972–1981)
Jack Welch (1981–2001)
Jeff Immelt (2001–2017)
John L. Flannery (2017–2018)
H. Lawrence Culp Jr. (2018–present)

Corporate recognition and rankings
In 2011, Fortune ranked GE the sixth-largest firm in the U.S., and the 14th-most profitable. Other rankings for 2011–2012 include the following:
#18 company for leaders (Fortune)
#82 green company (Newsweek)
#91 most admired company (Fortune)
#19 most innovative company (Fast Company).In 2012, GE's brand was valued at $28.8 billion. CEO Jeff Immelt had a set of changes in the presentation of the brand commissioned in 2004, after he took the reins as chairman, to unify the diversified businesses of GE.Tom Geismar later stated that looking back at the logos of the 1910s, 1920s, and 1930s, one can clearly judge that they are old-fashioned. Chermayeff & Geismar, along with colleagues Bill Brown and Ivan Chermaev, created the modern 1980 logo. They, in turn, argued that even now the old logos look out of date, earlier they were good. The changes included a new corporate color palette, small modifications to the GE logo, a new customized font (GE Inspira) and a new slogan, "Imagination at work", composed by David Lucas, to replace the slogan "We Bring Good Things to Life" used since 1979. The standard requires many headlines to be lowercased and adds visual "white space" to documents and advertising. The changes were designed by Wolff Olins and are used on GE's marketing, literature, and website. In 2014, a second typeface family was introduced: GE Sans and Serif by Bold Monday created under art direction by Wolff Olins.As of 2016, GE had appeared on the Fortune 500 list for 22 years and held the 11th rank. GE was removed from the Dow Jones Industrial Average on June 28, 2018, after the value had dropped below 1% of the index's weight.

Businesses
GE's primary business divisions are:

GE Additive
GE Aerospace
GE Capital
GE Digital
GE Healthcare
GE Power
GE Renewable Energy
GE ResearchThrough these businesses, GE participates in markets that include the generation, transmission and distribution of electricity (e.g. nuclear, gas and solar), industrial automation, medical imaging equipment, motors, aircraft jet engines, and aviation services. Through GE Commercial Finance, GE Consumer Finance, GE Equipment Services, and GE Insurance it offers a range of financial services. It has a presence in over 100 countries.
General Imaging manufacturers GE digital cameras.Even though the first wave of conglomerates (such as ITT Corporation, Ling-Temco-Vought, Tenneco, etc.) fell by the wayside by the mid-1980s, in the late 1990s, another wave (consisting of Westinghouse, Tyco, and others) tried and failed to emulate GE's success.As of August 2015 GE is planning to set up a silicon carbide chip packaging R&D center in coalition with SUNY Polytechnic Institute in Utica, New York. The project will create 470 jobs with the potential to grow to 820 jobs within 10 years.On September 14, 2015, GE announced the creation of a new unit: GE Digital, which will bring together its software and IT capabilities. The new business unit will be headed by Bill Ruh, who joined GE in 2011 from Cisco Systems and has since worked on GE's software efforts.

Former divisions
GE Industrial was a division providing appliances, lighting and industrial products; factory automation systems; plastics, silicones and quartz products; security and sensors technology, and equipment financing, management and operating services. As of 2007 it had 70,000 employees generating $17.7 billion in revenue. After some major realignments in late 2007, GE Industrial was organized in two main sub businesses:

GE Consumer & Industrial
Appliances
Electrical Distribution
Lighting
GE Enterprise Solutions
Digital Energy
GE Fanuc Intelligent Platforms
Security
Sensing & Inspection TechnologiesThe former GE Plastics division was sold in August 2007 and is now SABIC Innovative Plastics.
On May 4, 2008, it was announced that GE would auction off its appliances business for an expected sale of $5–8 billion. However, this plan fell through as a result of the recession.The former GE Appliances and Lighting segment was dissolved in 2014 when GE's appliance division was attempted to be sold to Electrolux for $5.4 billion, but eventually sold it to Haier in June 2016 due to antitrust filing against Electrolux. GE Lighting (consumer lighting) and the newly created Current, powered by GE, which deals in commercial LED, solar, EV, and energy storage, became stand-alone businesses within the company, until the sale of the latter to American Industrial Partners in April 2019.The former GE Transportation division merged with Wabtec on February 25, 2019, leaving GE with a 24.9% holding in Wabtec.On July 1, 2020, GE Lighting was acquired by Savant Systems and remains headquartered at Nela Park in East Cleveland, Ohio.

Environmental record
Carbon footprint
General Electric Company reported Total CO2e emissions (Direct + Indirect) for the twelve months ending 31 December 2020 at 2,080 Kt (-310 /-13% y-o-y). There has been a consistent declining trend in reported emissions since 2016.

Pollution
Some of GE's activities have given rise to large-scale air and water pollution. Based on data from 2000, researchers at the Political Economy Research Institute listed the corporation as the fourth-largest corporate producer of air pollution in the United States (behind only E. I. Du Pont de Nemours & Co., United States Steel Corp., and ConocoPhillips), with more than 4.4 million pounds per year (2,000 tons) of toxic chemicals released into the air. GE has also been implicated in the creation of toxic waste. According to United States Environmental Protection Agency (EPA) documents, only the United States Government, Honeywell, and Chevron Corporation are responsible for producing more Superfund toxic waste sites.In 1983, New York State Attorney General Robert Abrams filed suit in the United States District Court for the Northern District of New York to compel GE to pay for the clean-up of what was claimed to be more than 100,000 tons of chemicals dumped from their plant in Waterford, New York, which polluted nearby groundwater and the Hudson River. In 1999, the company agreed to pay a $250 million settlement in connection with claims it polluted the Housatonic River (at Pittsfield, Massachusetts) and other sites with polychlorinated biphenyls (PCBs) and other hazardous substances.In 2003, acting on concerns that the plan proposed by GE did not "provide for adequate protection of public health and the environment", EPA issued an administrative order for the company to "address cleanup at the GE site" in Rome, Georgia, also contaminated with PCBs.The nuclear reactors involved in the 2011 crisis at Fukushima I in Japan were GE designs, and the architectural designs were done by Ebasco, formerly owned by GE. Concerns over the design and safety of these reactors were raised as early as 1972, but tsunami danger was not discussed at that time. As of 2014, the same model nuclear reactors designed by GE are operating in the US; however, as of May 31, 2019, the controversial Pilgrim Nuclear Generating Station, in Plymouth, Massachusetts, has been shut down and is in the process of decommission.

Pollution of the Hudson River
GE heavily contaminated the Hudson River with PCBs between 1947 and 1977. This pollution caused a range of harmful effects to wildlife and people who eat fish from the river. In 1983 EPA declared a 200-mile (320 km) stretch of the river, from Hudson Falls to New York City, to be a Superfund site requiring cleanup. This Superfund site is considered to be one of the largest in the nation. In addition to receiving extensive fines, GE is continuing its sediment removal operations, pursuant to the Superfund orders, in the 21st century.

Pollution of the Housatonic River
From c. 1932 until 1977, GE polluted the Housatonic River with PCB discharges from its plant at Pittsfield, Massachusetts. EPA designated the Pittsfield plant and several miles of the Housatonic to be a Superfund site in 1997, and ordered GE to remediate the site. Aroclor 1254 and Aroclor 1260, products manufactured by Monsanto, were the principal contaminants that were discharged to the river. The highest concentrations of PCBs in the Housatonic River are found in Woods Pond in Lenox, Massachusetts, just south of Pittsfield, where they have been measured up to 110 mg/kg in the sediment. About 50% of all the PCBs currently in the river are estimated to be retained in the sediment behind Woods Pond dam. This is estimated to be about 11,000 pounds (5,000 kg) of PCBs. Former filled oxbows are also polluted. Waterfowl and fish who live in and around the river contain significant levels of PCBs and can present health risks if consumed. In 2020 GE completed remediation and restoration of its 10 manufacturing plant areas within the city of Pittsfield. As of 2023 plans for cleanup of the river south of the city are not finalized.

Social responsibility
Environmental initiatives
The environmental work and research of GE can be seen as early as 1968 with the experimental Delta electric car built by the GE Research and Development Center led by Bruce Laumeister. The electric car led to the production shortly after of the cutting-edge technology of the first commercially produced all-electric Elec-Trak garden tractor, which was manufactured from around 1969 until 1975.On June 6, 2011, GE announced that it has licensed solar thermal technology from California-based eSolar for use in power plants that use both solar and natural gas.On May 26, 2011, GE unveiled its EV Solar Carport, a carport that incorporates solar panels on its roof, with electric vehicle charging stations under its cover.In May 2005, GE announced the launch of a program called "Ecomagination", intended, in the words of CEO Jeff Immelt, "to develop tomorrow's solutions such as solar energy, hybrid locomotives, fuel cells, lower-emission aircraft engines, lighter and stronger durable materials, efficient lighting, and water purification technology". The announcement prompted an op-ed piece in The New York Times to observe that, "while General Electric's increased emphasis on clean technology will probably result in improved products and benefit its bottom line, Mr. Immelt's credibility as a spokesman on national environmental policy is fatally flawed because of his company's intransigence in cleaning up its own toxic legacy."GE has said that it will invest $1.4 billion in clean technology research and development in 2008 as part of its Ecomagination initiative. As of October 2008, the scheme had resulted in 70 green products being brought to market, ranging from halogen lamps to biogas engines. In 2007, GE raised the annual revenue target for its Ecomagination initiative from $20 billion in 2010 to $25 billion following positive market response to its new product lines. In 2010, GE continued to raise its investment by adding $10 billion into Ecomagination over the next five years.GE Energy's renewable energy business has expanded greatly, to keep up with growing U.S. and global demand for clean energy. Since entering the renewable energy industry in 2002, GE has invested more than $850 million in renewable energy commercialization. In August 2008, it acquired Kelman Ltd, a Northern Ireland-based company specializing in advanced monitoring and diagnostics technologies for transformers used in renewable energy generation and announced an expansion of its business in Northern Ireland in May 2010. In 2009, GE's renewable energy initiatives, which include solar power, wind power and GE Jenbacher gas engines using renewable and non-renewable methane-based gases, employ more than 4,900 people globally and have created more than 10,000 supporting jobs.GE Energy and Orion New Zealand (Orion) have announced the implementation of the first phase of a GE network management system to help improve power reliability for customers. GE's ENMAC Distribution Management System is the foundation of Orion's initiative. The system of smart grid technologies will significantly improve the network company's ability to manage big network emergencies and help it to restore power faster when outages occur.
In June 2018, GE Volunteers, an internal group of GE employees, along with Malaysian Nature Society, transplanted more than 270 plants from the Taman Tugu forest reserve so that they may be replanted in a forest trail which is under construction.

Educational initiatives
GE Healthcare is collaborating with the Wayne State University School of Medicine and the Medical University of South Carolina to offer an integrated radiology curriculum during their respective MD Programs led by investigators of the Advanced Diagnostic Ultrasound in Microgravity study. GE has donated over one million dollars of Logiq E Ultrasound equipment to these two institutions.

Marketing initiatives
Between September 2011 and April 2013, GE ran a content marketing campaign dedicated to telling the stories of "innovators—people who are reshaping the world through act or invention". The initiative included 30 3-minute films from leading documentary film directors (Albert Maysles, Jessica Yu, Leslie Iwerks, Steve James, Alex Gibney, Lixin Fan, Gary Hustwit and others), and a user-generated competition that received over 600 submissions, out of which 20 finalists were chosen.Short Films, Big Ideas was launched at the 2011 Toronto International Film Festival in partnership with cinelan. Stories included breakthroughs in Slingshot (water vapor distillation system), cancer research, energy production, pain management and food access. Each of the 30 films received world premiere screenings at a major international film festival, including the Sundance Film Festival and the Tribeca Film Festival. The winning amateur director film, The Cyborg Foundation, was awarded a US$100,000 prize at the 2013 Sundance Film Festival. According to GE, the campaign garnered more than 1.5 billion total media impressions, 14 million online views, and was seen in 156 countries.In January 2017, GE signed an estimated $7 million deal with the Boston Celtics to have its corporate logo put on the NBA team's jersey.

Charity
On March 3, 2022, GE published an international memo pledging to donate $4.5 million to Ukraine amid Russian invasion. According to the memo, $4 million will be used for medical equipment, $400,000 for emergency cash for refugees and $100,000 will go to Airlink, a NGO that helps communities in crisis.

Political affiliation
In the 1950s, GE sponsored Ronald Reagan's TV career and launched him on the lecture circuit. GE has also designed social programs, supported civil rights organizations, and funded minority education programs.

Notable appearances in media
In the early 1950s, Kurt Vonnegut was a writer for GE. A number of his novels and stories (notably Cat's Cradle and Player Piano) refer to the fictional city of Ilium, which appears to be loosely based on Schenectady, New York. The Ilium Works is the setting for the short story "Deer in the Works".
In 1981, GE won a Clio award for its :30 Soft White Light Bulbs commercial, We Bring Good Things to Life. The slogan "We Bring Good Things to Life" was created by Phil Dusenberry at the ad agency BBDO.GE was the primary focus of a 1991 short subject Academy Award-winning documentary, Deadly Deception: General Electric, Nuclear Weapons, and Our Environment, that juxtaposed GE's "We Bring Good Things To Life" commercials with the true stories of workers and neighbors whose lives have been affected by the company's activities involving nuclear weapons.GE was frequently mentioned and parodied in the NBC comedy sitcom 30 Rock from 2006 to 2013. Former General Electric CEO Jack Welch even cameoed as himself appearing in the season four episode "Future Husband". The episode is a satirical reference to the real-world acquisition of NBC Universal from General Electric by Comcast in November 2009.In 2013, GE received a National Jefferson Award for Outstanding Service by a Major Corporation.

See also
GE Technology Infrastructure
Knolls Atomic Power Laboratory
List of assets owned by General Electric
Phoebus cartel
Top 100 US Federal Contractors

References
Further reading
Carlson, W. Bernard.  Innovation as a Social Process: Elihu Thomson and the Rise of General Electric, 1870–1900 (Cambridge: Cambridge University Press, 1991).
Woodbury, David O.  Elihu Thomson, Beloved Scientist (Boston: Museum of Science, 1944)
Haney, John L.  The Elihu Thomson Collection American Philosophical Society Yearbook 1944.
Hammond, John W. Men and Volts: The Story of General Electric, published 1941, 436 pages.
Mill, John M. Men and Volts at War: The Story of General Electric in World War II, published 1947.
Irmer, Thomas. Gerard Swope. In Immigrant Entrepreneurship: German-American Business Biographies, 1720 to the Present, vol. 4, edited by Jeffrey Fear. German Historical Institute.

External links

Official website 
Business data for General Electric:

General Motors

General Motors Company (GM) is an American multinational automotive manufacturing company headquartered in Detroit, Michigan, United States. The company is most known for owning and manufacturing its four core automobile brands of Chevrolet, GMC, Cadillac and Buick. By sales, it was the largest automaker in the United States in 2022, and was the largest in the world for 77 years before losing the top spot to Toyota in 2008.General Motors operates manufacturing plants in eight countries. In addition to its four core brands, GM also holds interests in Chinese brands Baojun and Wuling via SAIC-GM-Wuling Automobile. GM further owns the BrightDrop delivery vehicle manufacturer, a namesake defense vehicles division which produces military vehicles for the United States government and military, the vehicle safety, security, and information services provider OnStar, the auto parts company ACDelco, a namesake financial lending service, and majority ownership in the self-driving cars enterprise Cruise LLC.
The company traces itself to a holding company for Buick established on September 16, 1908, by William C. Durant, the largest seller of horse-drawn vehicles at the time. The first half of the 20th century saw the company grow into an automotive behemoth through acquisitions; going into the second half, the company pursued innovation and new offerings to consumers as well as collaborations with NASA to develop the earliest electric vehicles. The current entity was established in 2009 after the General Motors Chapter 11 reorganization.Today, General Motors remains a successful company, ranking 25th by total revenue out of all American companies on the Fortune 500 and 50th on the Fortune Global 500. The company is presently heavily pursuing electric vehicles, as GM announced plans in January 2021 to end production and sales of vehicles using internal combustion engines, including hybrid vehicles and plug-in hybrids, by 2035, as part of its plan to achieve carbon neutrality by 2040.

History
Founding and consolidation
By 1900, William C. Durant's Durant-Dort Carriage Company of Flint, Michigan had become the largest manufacturer of horse-drawn vehicles in the United States. Durant was averse to automobiles, but fellow Flint businessman James H. Whiting, owner of Flint Wagon Works, sold him the Buick Motor Company in 1904. Durant formed the General Motors Company in 1908 as a holding company, with partner Charles Stewart Mott, borrowing a naming convention from General Electric. GM's first acquisition was Buick, which Durant already owned, then Olds Motor Works on November 12, 1908. Under Durant, GM went on to acquire Cadillac, Elmore, Welch, Cartercar, Oakland (the predecessor of Pontiac), the Rapid Motor Vehicle Company of Pontiac, Michigan, and the Reliance Motor Car Company of Detroit, Michigan (predecessors of GMC) in 1909.
Durant, with the board's approval, also tried acquiring Ford Motor Company, but needed an additional $2 million. Durant over-leveraged GM in making acquisitions, and was removed by the board of directors in 1910 at the order of the bankers who backed the loans to keep GM in business. The action of the bankers was partially influenced by the Panic of 1910–1911 that followed the earlier enforcement of the Sherman Antitrust Act of 1890. In 1911, Charles F. Kettering of Dayton Engineering Laboratories Company (DELCO) and Henry M. Leland invented and patented the first electric starter in America. In November 1911, Durant co-founded Chevrolet with Swiss race car driver Louis Chevrolet, who left the company in 1915 after a disagreement with Durant.
GM was reincorporated in Detroit in 1916 as General Motors Corporation and became a public company via an initial public offering. By 1917, Chevrolet had become successful enough that Durant, with the backing of Samuel McLaughlin and Pierre S. du Pont, reacquired a controlling interest in GM. The same year, GM acquired Samson Tractor. Chevrolet Motor Company was consolidated into GM on May 2, 1918, and the same year GM acquired United Motors, a parts supplier founded by Durant and headed by Alfred P. Sloan for $45 million, and the McLaughlin Motor Car Company, founded by R. S. McLaughlin, became General Motors of Canada Limited. In 1919, GM acquired Guardian Frigerator Company, part-owned by Durant, which was renamed Frigidaire. Also in 1919, the General Motors Acceptance Corporation (GMAC), which provides financing to automotive customers, was formed.In 1920, du Pont orchestrated the removal of Durant once again and replaced him with Alfred P. Sloan. At a time when GM was competing heavily with Ford Motor Company, Sloan established annual model changes, making previous years' models "dated" and created a market for used cars. He also implemented the pricing strategy used by car companies today. The pricing strategy had Chevrolet, Pontiac, Oldsmobile, Buick, and Cadillac priced from least expensive to most, respectively.In 1921, Thomas Midgley Jr., an engineer for GM, discovered tetraethyllead (leaded gasoline) as an antiknock agent, and GM patented the compound because ethanol could not be patented. This led to the development of higher compression engines resulting in more power and efficiency. The public later realized that lead contained in the gasoline was harmful to various biological organisms including humans. Evidence shows that corporate executives understood the health implications of tetraethyllead from the beginning. As an engineer for GM, Midgley also developed chlorofluorocarbons, which have now been banned due to their contribution to climate change.Under the encouragement of GM President Alfred P. Sloan Jr., GM acquired Vauxhall Motors for $2.5 million in 1925. The company also acquired an interest in the Yellow Cab Manufacturing Company the same year, and its president, John D. Hertz, joined the board of directors of GM; it acquired the remainder of the company in 1943.

Growth and acquisitions
In 1926, the company introduced the Pontiac brand and established the General Motors Group Insurance Program to provide life insurance to its employees. The following year, after the success of the 1927 model of the Cadillac LaSalle designed by Harley Earl, Sloan created the "Art and Color Section" of GM and named Earl as its first director. Earl was the first design executive to be appointed to leadership at a major American corporation. Earl created a system of automobile design that is still practiced today. At the age of 24, Bill Mitchell was recruited by Harley Earl to the design team at GM, and he was later appointed as Chief Designer of Cadillac. After Earl retired in December 1958, Mitchell took over automotive design for GM.  Also in 1926 the company acquired Fisher Body, its supplier of automobile bodies.GM acquired Allison Engine Company and began developing a 1,000 horsepower liquid-cooled aircraft engine in 1929. The same year, GM acquired 80% of Opel, which at that time had a 37.5% market share in Europe, for $26 million. It acquired the remaining 20% in 1931.In the late-1920s, Charles Kettering embarked on a program to develop a lightweight two-stroke diesel engine for possible usage in automobiles. Soon after, GM acquired Electro-Motive Company and the Winton Engine Co., and in 1941, it expanded EMC's realm to locomotive engine manufacturing.In 1932, GM acquired Packard Electric (not the Packard car company, which merged with Studebaker years later). The following year, GM acquired a controlling interest in North American Aviation and merged it with the General Aviation Manufacturing Corporation.The GM labor force participated in the formation of the United Auto Workers labor union in 1935, and in 1936 the UAW organized the Flint Sit-Down Strike, which initially idled two key plants in Flint, Michigan, and later spread to 6 other plants including those in Janesville, Wisconsin and Fort Wayne, Indiana. In Flint, police attempted to enter the plant to arrest strikers, leading to violence; in other cities, the plants were shuttered peacefully. The strike was resolved on February 11, 1937, when GM recognized the UAW as the exclusive bargaining representative for its workers and gave workers a 5% raise and permission to speak in the lunchroom.Walter E. Jominy and A.L. Boegehold of GM invented the Jominy end-quench test for hardenability of carbon steel in 1937, a breakthrough in heat treating still in use today as ASTM A255. GM established Detroit Diesel the next year.In 1939, the company founded Motors Insurance Corporation and entered the vehicle insurance market. The same year, GM introduced the Hydramatic, the first affordable and successful automatic transmission, for the 1940 Oldsmobile.

During World War II, GM produced vast quantities of armaments, vehicles, and aircraft for the Allies of World War II. In 1940, GM's William S. Knudsen served as head of U.S. wartime production for President Franklin Roosevelt, and by 1942, all of GM's production was to support the war. GM's Vauxhall Motors manufactured the Churchill tank series for the Allies, instrumental in the North African campaign. However, its Opel division, based in Germany, supplied the Nazi Party with vehicles. Sloan, head of GM at the time, was an ardent opponent of the New Deal, which bolstered labor unions and public transport, and Sloan admired and supported Adolf Hitler. Nazi armaments chief Albert Speer allegedly said in 1977 that Hitler "would never have considered invading Poland" without synthetic fuel technology provided by General Motors. GM was compensated $32 million by the U.S. government because its German factories were bombed by U.S. forces during the war.Effective January 28, 1953, Charles Erwin Wilson, then GM president, was named by Dwight D. Eisenhower as United States Secretary of Defense.In December 1953, GM acquired Euclid Trucks, a manufacturer of heavy equipment for earthmoving, including dump trucks, loaders and wheel tractor-scrapers, which later spawned the Terex brand.

Periods of innovation
Alfred P. Sloan retired as chairman and was succeeded by Albert Bradley in April 1956.In 1962, GM introduced the first turbo charged engine in the world for a car in the Oldsmobile Cutlass Turbo-Jetfire. Two years later, the company introduced its "Mark of Excellence" logo and trademark at the 1964 New York World's Fair. The company used the mark as their main corporate identifier until 2021.GM released the Electrovan in 1966, the first hydrogen fuel cell car ever produced. Though fuel cells have existed since the early 1800s, General Motors was the first to use a fuel cell, supplied by Union Carbide, to power the wheels of a vehicle with a budget of "millions of dollars".
In the 1960s, GM was the first to use turbochargers and was an early proponent of V6 engines, but quickly lost interest as the popularity of muscle cars increased. GM demonstrated gas turbine vehicles powered by kerosene, an area of interest throughout the industry, but abandoned the alternative engine configuration due to the 1973 oil crisis.In partnership with Boeing, GM's Delco Defense Electronics Division designed the Lunar Roving Vehicle, which traversed the surface of the Moon, in 1971. The following year, GM produced the first rear wheel anti-lock braking system for two models: the Toronado and Eldorado.In 1973, the Oldsmobile Toronado was the first retail car sold with a passenger airbag.Thomas Murphy became CEO of the company, succeeding Richard C. Gerstenberg in November 1974.GM installed its first catalytic converters in its 1975 models.From 1978 to 1985, GM pushed the benefits of diesel engines and cylinder deactivation technologies. However, it had disastrous results due to poor durability in the Oldsmobile diesels and drivability issues in the Cadillac V8-6-4 variable-cylinder engines.GM sold Frigidaire in 1979. Although Frigidaire had between $450 million and $500 million in annual revenues, it was losing money.
Robert Lee of GM invented the Fe14Nd2B the Neodymium magnet, which was fabricated by rapid solidification, in 1984. This magnet is commonly used in products like a computer hard disk. The same year, GM acquired Electronic Data Systems for $2.5 billion from Ross Perot as part of a strategy by CEO Roger Smith to derive at least 10% of its annual worldwide revenue from non-automotive sources. GM also intended to have EDS handle its bookkeeping, help computerize factories, and integrate GM's computer systems. The transaction made Ross Perot the largest shareholder of GM; however, disagreements with Roger Smith led the company to buy all shares held by Ross Perot for $750 million in 1986.In a continuation of its diversification plans, GMAC formed GMAC Mortgage and acquired Colonial Mortgage as well as the servicing arm of Norwest Mortgage in 1985. This acquisition included an $11 billion mortgage portfolio. The same year, GM acquired the Hughes Aircraft Company for $5 billion in cash and stock and merged it into Delco Electronics. The following year, GM acquired 59.7% of Lotus Cars, a British producer of high-performance sports cars.In 1987, in conjunction with AeroVironment, GM built the Sunraycer, which won the inaugural World Solar Challenge and was a showcase of advanced technology. Much of the technology from Sunraycer found its way into the Impact prototype electric vehicle (also built by Aerovironment) and was the predecessor to the General Motors EV1.In 1988, GM acquired a 15% stake in AeroVironment.In 1989, GM acquired half of Saab Automobile's car operations for $600 million.

Sales of assets
In August 1990, Robert Stempel became CEO of the company, succeeding Roger Smith. GM cut output significantly and suffered losses that year due to the early 1990s recession.In 1990, GM debuted the General Motors EV1 (Impact) concept, a battery electric vehicle, at the LA Auto Show. It was the first car with zero emissions marketed in the US in over three decades. The Impact was produced as the EV1 for the 1996 model year and was available only via lease from certain dealers in California and Arizona. In 1999–2002, GM ceased production of the vehicles and started to not renew the leases, disappointing many people, allegedly because the program would not be profitable and would cannibalize its existing business. All of the EV1s were eventually returned to General Motors, and except for around 40 which were donated to museums with their electric powertrains deactivated, all were destroyed. The documentary film Who Killed the Electric Car? covered the EV1 story.In November 1992, John F. Smith Jr. became CEO of the company.In 1993, GM sold Lotus Cars to Bugatti.In 1996, in a return to its automotive basics, GM completed the corporate spin-off of Electronic Data Systems.In 1997, GM sold the military businesses of Hughes Aircraft Company to Raytheon Company for $9.5 billion in stock and the assumption of debt.In February 2000, Rick Wagoner was named CEO, succeeding John F. Smith Jr. The next month, GM gave 5.1% of its common stock, worth $2.4 billion, to acquire a 20% share of Fiat.In December 2000, GM announced that it would begin phasing out Oldsmobile. The brand was eventually discontinued in 2004, seven years after it had become the first American car brand to turn 100.

In May 2004, GM delivered the first full-sized pickup truck hybrid vehicles, the 1/2-ton Chevrolet Silverado/GMC Sierra trucks. These mild hybrids did not use electrical energy for propulsion, like GM's later designs. Later, the company debuted another hybrid technology, co-developed with DaimlerChrysler and BMW, in diesel-electric hybrid powertrain manufactured by Allison Transmission for transit buses. Continuing to target the diesel-hybrid market, the Opel Astra diesel engine hybrid concept vehicle was rolled out in January 2005. Later that year, GM sold its Electro-Motive Diesel locomotive division to private equity firms Berkshire Partners and Greenbriar Equity Group.GM paid $2 billion to sever its ties with Fiat in 2005, severing ties with the company due to an increasingly contentious dispute.GM began adding its "Mark of Excellence" emblem on all new vehicles produced and sold in North America in mid-2005. However, after the reorganization in 2009, the company no longer added the logo, saying that emphasis on its four core divisions would downplay the GM logo.In 2005, Edward T. Welburn was promoted to the newly created position of vice president, GM Global Design, making him the first African American to lead a global automotive design organization and the highest-ranking African American in the US motor industry at that time. On July 1, 2016, he retired from General Motors after 44 years. He was replaced by Michael Simcoe.In 2006, GM introduced a bright yellow fuel cap on its vehicles to remind drivers that cars can operate using E85 ethanol fuel. They also introduced another hybrid vehicle that year, the Saturn Vue Green Line.In 2008, General Motors committed to engineering half of its manufacturing plants to be landfill-free by recycling or reusing waste in the manufacturing process. Continuing their environmental-conscious development, GM started to offer the 2-mode hybrid system in the Chevrolet Tahoe, GMC Yukon, Cadillac Escalade, and pickup trucks.In late 2008, the world's largest rooftop solar power installation was installed at GM's manufacturing plant in Zaragoza. The Zaragoza solar installation has about 2,000,000 square feet (190,000 m2) of roof at the plant and contains about 85,000 solar panels. The installation was created, owned and operated by Veolia Environment and Clairvoyant Energy, which leases the rooftop area from GM.

Chapter 11 bankruptcy and bailout
In March 2009, after the company had received $17.4 billion in bailouts but was not effective in a turnaround, President Barack Obama forced the resignation of CEO Rick Wagoner.General Motors filed for a government-backed Chapter 11 reorganization on June 8, 2009. On July 10, 2009, the original General Motors sold assets and some subsidiaries to an entirely new company, including the trademark "General Motors". Liabilities were left with the original GM, renamed Motors Liquidation Company, freeing the companies of many liabilities and resulting in a new GM.Through the Troubled Asset Relief Program, the United States Department of the Treasury invested $49.5 billion in General Motors and recovered $39 billion when it sold its shares on December 9, 2013, resulting in a loss of $10.3 billion. The Treasury invested an additional $17.2 billion into GM's former financing company, GMAC (now Ally Financial). The shares in Ally were sold on December 18, 2014, for $19.6 billion netting the government $2.4 billion in profit, including dividends. A study by the Center for Automotive Research found that the GM bailout saved 1.2 million jobs and preserved $34.9 billion in tax revenue.General Motors Canada was not part of the General Motors Chapter 11 bankruptcy.

Post-reorganization
In June 2009, at the request of Steven Rattner, lead adviser to President Barack Obama on the Presidential Task Force on the Auto Industry, Edward Whitacre Jr., who had led a restructuring of AT&T, was appointed as chairman of General Motors. Whitacre was tasked with overseeing GM's emergence from bankruptcy and downsizing its sizable number of brand marques, many of which had produced chronic losses even before the recession began. In July 2009, after 40 days of bankruptcy protection, the company emerged from the government-backed General Motors Chapter 11 reorganization.As mandated by its bailout agreement, GM began the process of shedding its poorest-performing brands in June 2009: Hummer, Saab, Saturn, and Pontiac. An October 2009 agreement to sell the Hummer brand to China-based Sichuan Tengzhong Heavy Industrial Machinery Company Ltd. and a group of private investors fell through three months later, resulting in GM seeking a new suitor. American company Raser Technologies, along with several others, expressed interest in buying the company, but none of the proposed acquisitions came to fruition, and in April 2010 GM said it was officially shutting down the Hummer brand. Similarly, GM's efforts to sell its Saturn division yielded an early suitor. In June 2009, GM announced that the Saturn brand would be sold to the Penske Automotive Group. The deal fell through, however, and GM declared the brand defunct in October 2010. While GM agreed to shed its underperforming Pontiac brand as part of its bailout agreement, the company explicitly opted not to sell it to another company. The last Pontiac was built in January 2010.GM was more successful in its attempts to sell Saab Automobile: the company closed a sale to Dutch automaker Spyker Cars in February 2010. Saab continued to perform poorly under Spyker's management, however, and in 2012 the Saab division declared bankruptcy.
In December 2009, the "new" GM's board of directors asked CEO Fritz Henderson to resign, and its chairman, Ed Whitacre, was named interim CEO. GM opted to appoint Whitacre as its permanent CEO the following month, though Whitacre ultimately stepped down as CEO in September 2010, relinquishing the position to fellow GM board member Daniel Akerson but agreeing to continue on as GM chairman until the end of the year. Akerson replaced him as chairman, while continuing as CEO, in January 2011.In 2010, GM introduced the Chevrolet Volt as an extended-range electric vehicle (EREV), an electric vehicle with backup generators powered by gasoline, or series plug-in hybrid. GM delivered the first Volt in December 2010. The Chevrolet Volt was a plug-in hybrid electric vehicle with back-up generators powered by gasoline (range-extended electric vehicle). GM built a prototype two-seat electric vehicle with Segway Inc. An early prototype of the Personal Urban Mobility and Accessibility vehicle—dubbed Project P.U.M.A. – was presented in New York at the 2009 New York International Auto Show.

On January 15, 2014, Mary Barra was named chief executive officer, succeeding Daniel Akerson. Barra also joined the GM board. Only three weeks later, the company announced its 2014 General Motors recall, which was due to faulty ignition switches, and was linked to at least 124 deaths. The resulting settlements with family members of those killed were estimated to cost the company $1.5 billion.On January 4, 2016, in its first investment in a ridesharing company, GM invested $500 million in Lyft. The company does not directly supply Lyft drivers with vehicles, however – and has no plans to do so in the future – and Lyft ultimately partnered with Motional for production of its autonomous vehicles.
In March 2016, GM acquired Cruise, a San Francisco self-driving vehicle start-up, to develop self-driving cars that could be used in ride-sharing fleets. In June 2022, Cruise received California's first Driverless Deployment Permit, allowing it to both charge fees for its service as well as offer fully autonomous rides in a major public city. The Verge reported that the company lost $561 million in Q1 2023, but said it remains on the path to reach $1 billion in revenue by 2025 and $50 billion by 2030.In October 2016, GM began production of the Chevrolet Bolt EV, the first-ever mass market all-electric car with a range of more than 200 miles (320 km). The battery pack and most drivetrain components were built by LG Corporation and assembled in GM's plant in Lake Orion, Michigan. GM chose to employ the Bolt EV and similar Bolt EUV for its Cruise ride-share service.On January 8, 2021, GM introduced a new logo alongside the tagline "EVerybody in", with the capitalized "EV" as a nod to the company's commitment to electric vehicles. GM's new logo used negative space to create the idea of an electric plug in the "M" of the logo.At the January 2021 Consumer Electronics Show, GM launched BrightDrop, its brand for all-electric commercial vehicles.On January 28, 2021, GM announced that it will end production and sales of fossil-fuel vehicles (including hybrids and plug-in hybrids) by 2035 as part of its plan to reach carbon neutrality by 2040.In 2021, GM announced plans to establish an automotive battery and battery pack laboratory in Michigan. GM will be responsible for battery management systems and power electronics, thermal management, as well as the pack assembly. An existing GM facility at Brownstown Township was chosen to be upgraded as a battery pack plant. LG Chem's U.S. subsidiary, Compact Power of Troy, Michigan, has been building the prototype packs for the development vehicles and will continue to provide integration support and acting as a liaison for the program.

Motorsports history
GM participated in the World Touring Car Championship (WTCC) from 2004 to 2012, and has also participated in other motorsport championships, including 24 Hours of Le Mans, NASCAR, SCCA and Supercars Championship.GM's engines were successful in the Indy Racing League (IRL) throughout the 1990s, winning many races in the small V8 class. GM has also done much work in the development of electronics for GM auto racing. An unmodified Aurora V8 in the Aerotech captured 47 world records, including the record for speed endurance in the Motorsports Hall of Fame of America. Recently, the Cadillac V-Series has entered motorsports racing.
GM has also designed cars specifically for use in NASCAR auto racing. The Chevrolet Camaro ZL1 is the only entry in the series. In the past, the Pontiac Grand Prix, Buick Regal, Oldsmobile Cutlass, Chevrolet Lumina, Chevrolet Malibu, Chevrolet Monte Carlo, Chevrolet Impala, and the Chevrolet SS were also used. GM has won many NASCAR Cup Series manufacturer's championships, including 40 with Chevrolet, the most of any make in NASCAR history, 3 with Oldsmobile, 2 with Buick, and 1 with Pontiac. In 2021, Chevrolet became the first brand to reach 800 wins.In Australia, Holden cars based on the Monaro, Torana and Commodore platforms raced in the Australian Touring Car Championship until 2022. Holden won the Bathurst 1000, a record 36 times between 1968 and 2022 and the Australian Touring Car Championship 23 times. From 2023, the Chevrolet Camaro will be raced.

Logo evolution
Evolution of the GM logo through the years:

Brands
Current
Former
Financial results
Vehicle sales
General Motors was the largest global automaker by annual vehicle sales for 77 consecutive years, from 1931, when it overtook Ford Motor Company, until 2008 when it was overtaken by Toyota. This reign was longer than any other automaker, and GM is still among the world's largest automakers by vehicle unit sales.In 2008, the third-largest individual country by sales was Brazil, with some 550,000 GM vehicles sold. In that year, Argentina, Colombia, and Venezuela sold another 300,000 GM vehicles, suggesting that the total GM sales in South America (including sales in other South American countries such as Chile, Peru, Ecuador, Bolivia, etc.) in that year were at a similar level to sales in China.In 2009, General Motors sold 6.5 million cars and trucks globally; in 2010, it sold 8.39 million. Sales in China rose 66.9% in 2009 to 1,830,000 vehicles and accounting for 13.4% of the market.In 2010, General Motors ranked second worldwide with 8.5 million vehicles produced. In 2011, GM returned to the first place with 9.025 million units sold worldwide, corresponding to 11.9% market share of the global motor vehicle industry. In 2010, vehicle sales in China by GM rose 28.8% to a record 2,351,610 units. The top two markets in 2011 were China, with 2,547,203 units, and the United States, with 2,503,820 vehicles sold. The Chevrolet brand was the main contributor to GM performance, with 4.76 million vehicles sold around the world in 2011, a global sales record.Based on global sales in 2012, General Motors was ranked among the world's largest automakers.In May 2012, GM recorded an 18.4% market share in the U.S. with stock imported.Annual worldwide sales volume reached 10 million vehicles in 2016. Sales in India for April 2016 – March 2017 declined to 25,823 units from 32,540 the previous year and market share contracted from 1.17% to 0.85% for the same period. However, exports surged 89% during the same period to 70,969 units. GMTC-I, GM's technical center in Bangalore, India continued in operation. Weak product line-up and below par service quality were the reasons for the poor showing by GM in India that year.Global Volt/Ampera family sales totalled about 177,000 units from its inception in December 2010 through 2018. including over 10,000 Opel/Vauxhall Amperas sold in Europe up to December 2015. The Volt family of vehicles ranked as the world's all-time top-selling plug-in hybrid as of September 2018, and it is also the third best selling plug-in electric car in history after the Nissan Leaf (375,000) and the Tesla Model S (253,000), as of October 2018. The Chevrolet Volt is also the U.S. all-time top-selling plug-in electric car with 148,556 units delivered through October 2018.

Management
Current board of directors
Notable members of the board of directors of the company are as follows:

Chairmen of the Board of General Motors
Thomas Neal—November 19, 1912 – November 16, 1915
Pierre S. du Pont—November 16, 1915 – February 7, 1929
Lammot du Pont II—February 7, 1929 – May 3, 1937
Alfred P. Sloan Jr.—May 3, 1937 – April 2, 1956
Albert Bradley—April 2, 1956 – August 31, 1958
Frederic G. Donner—September 1, 1958 – October 31, 1967
James M. Roche—November 1, 1967 – December 31, 1971
Richard C. Gerstenberg—January 1, 1972 – November 30, 1974
Thomas A. Murphy—December 1, 1974 – December 31, 1980
Roger B. Smith—January 1, 1981 – July 31, 1990
Robert C. Stempel—August 1, 1990 – November 1, 1992
John G. Smale—November 2, 1992 – December 31, 1995
John F. Smith Jr.—January 1, 1996 – April 30, 2003
Rick Wagoner—May 1, 2003 – March 30, 2009
Kent Kresa—March 30, 2009 – July 10, 2009
Edward Whitacre Jr.—July 10, 2009 – December 31, 2010
Daniel Akerson—December 31, 2010 – January 15, 2014
Tim Solso—January 15, 2014 – January 4, 2016
Mary Barra—January 4, 2016 – Present

Chief Executive Officers of General Motors
Chief Executive Officers of General Motors

Alfred P. Sloan Jr.—May 10, 1923 – June 3, 1946
Charles Erwin Wilson—June 3, 1946 – January 26, 1953
Harlow H. Curtice—February 2, 1953 – August 31, 1958
James M. Roche—November 1, 1967 – December 31, 1971
Richard C. Gerstenberg—January 1, 1972 – November 30, 1974
Thomas A. Murphy—December 1, 1974 – December 31, 1980
Roger B. Smith—January 1, 1981 – July 31, 1990
Robert C. Stempel—August 1, 1990 – November 1, 1992
John F. Smith Jr.—November 2, 1992 – May 31, 2000
Rick Wagoner—June 1, 2000 – March 30, 2009
Frederick Henderson—March 30, 2009 – December 1, 2009
Edward Whitacre Jr.—December 1, 2009 – September 1, 2010
Daniel Akerson—September 1, 2010 – January 15, 2014
Mary Barra—January 15, 2014 – Present

Philanthropy
GM publishes an annual Social Impact Report detailing its contributions to charity; in 2020 it provided nearly $35 million in funding to 357 U.S.-based non-profits as well as in-kind assets (primarily donations of vehicles) to non-profits valued at more than $9.8 million. From 1976 until 2017, philanthropic activity was carried out via the General Motors Foundation, a 501(c)(3) foundation.General Motors has a close relationship with the Nature Conservancy and has fundraised for and donated cash and vehicles to the charity.In 1996, GM commissioned five designer-original vehicles, sold in a silent auction for Concept: Cure, to benefit the Nina Hyde Center for breast cancer research, founded by Ralph Lauren. The program involved five designers, each lending their artistic talents to customize five different vehicles. Nicole Miller, Richard Tyler, Anna Sui, Todd Oldham, and Mark Eisen were tasked with transforming a Cadillac STS, Buick Riviera, GMC Yukon, Oldsmobile Bravada and Chevrolet Camaro Z28, respectively. The cars were then auctioned with the proceeds presented to the Nina Hyde Center at the Greater LA Auto Show in 1997.Since 1997, GM has been a source of funding for Safe Kids Worldwide's "Safe Kids Buckle Up" program, an initiative to ensure child automobile safety through education and inspection.

Labor conflicts
General Motors' American workers are unionized generally under the United Auto Workers (UAW), which is the primary auto workers union in the United States.

Flint sit-down strike
The 1936–1937 Flint sit-down strike against General Motors changed the UAW from a collection of isolated local unions on the fringes of the industry into a major labor union and led to the unionization of the domestic United States automobile industry.
After the first convention of UAW in 1936, the union decided that it could not survive by piecemeal organizing campaigns at smaller plants, as it had in the past, but that it could organize the automobile industry only by going after its biggest and most powerful employer, General Motors, focusing on GM's production complex in Flint, Michigan.
Organizing in Flint was a difficult and dangerous plan. GM controlled city politics in Flint and kept a close eye on outsiders. According to Wyndham Mortimer, the UAW officer put in charge of the organizing campaign in Flint, he received a death threat by an anonymous caller when he visited Flint in 1936. GM also maintained an extensive network of spies throughout its plants. This forced UAW members to keep the names of new members secret and meeting workers at their homes.
As the UAW studied its target, it discovered that GM had only two factories that produced the dies from which car body components were stamped: one in Flint that produced the parts for Buicks, Pontiacs, and Oldsmobiles, and another in Cleveland that produced Chevrolet parts.

While the UAW called for a sit-down strike in Flint, the police, armed with guns and tear gas, attempted to enter the Fisher Body 2 plant on January 11, 1937. The strikers inside the plant pelted them with hinges, bottles, and bolts. At the time, Vice President John Nance Garner supported federal intervention to break up the Flint Strike, but this idea was rejected by President Franklin D. Roosevelt. The president urged GM to distinguish a union so the plants could re-open. The strike ended after 44 days.
That development forced GM to bargain with the union. John L. Lewis, President of the United Mine Workers and founder and leader of the Congress of Industrial Organizations, spoke for the UAW in those negotiations; UAW President Homer Martin was sent on a speaking tour to keep him out of the way. GM's representatives refused to be in the same room as the UAW, so Governor Frank Murphy acted as a courier and intermediary between the two groups. Governor Murphy sent in the U.S. National Guard not to evict the strikers but rather to protect them from the police and corporate strike-breakers. The two parties finally reached an agreement on February 11, 1937, on a one-page agreement that recognized the UAW as the exclusive bargaining representative for GM's employees, who were union members for the next six months.

Tool and die strike of 1939
The tool and die strike of 1939, also known as the "strategy strike", was an ultimately successful attempt by the UAW to be recognized as the sole representative for General Motors workers. In addition to representation rights, the UAW, working jointly with the Congress of Industrial Organizations (CIO), sought to resolve existing grievances of skilled workers.

United Auto Workers (UAW) strike of 1945–1946
From November 21, 1945, until March 13, 1946, (113 days) the UAW organized "320,000 hourly workers" to form a US-wide strike against the General Motors Corporation, workers used the tactic of the sit down strike. It was "the longest strike against a major manufacturer" that the UAW had yet seen, and it was also "the longest national GM strike in its history". As director of the UAW's General Motors Department (coordinator of union relations with GM), Walter Reuther suggested to his colleagues the idea of striking the GM manufacturing plants with a 'one-at-a-time' strategy, which was "intended to maximize pressure on the target company". Reuther also put forth the demands of the strikers: a 30 percent increase in wages and a hold on product prices. However, the strike ended with the dissatisfaction of Walter Reuther and the UAW, and the workers received only a 17.5-percent increase in wages.

2007 General Motors strike
The 2007 General Motors strike was a strike from September 24 to 26, 2007, by the UAW against General Motors.
On September 24, 2007, General Motors workers represented by the UAW union went on strike against the company. The first US-wide strike against GM since 1970 was expected to idle 59 plants and facilities for an indefinite period of time. Talks broke down after more than 20 straight days of bargaining failed to produce a new contract. Major issues that proved to be stumbling blocks for an agreement included wages, benefits, job security and investments in US facilities.Two car assembly plants in Oshawa, Ontario and a transmission facility in Windsor closed on September 25. However, on September 26, a tentative agreement was reached, and the strike's end was announced by UAW officials in a news conference at 4 a.m. By the following day, all GM workers in both countries were back to work.

2019 General Motors strike
On the morning of September 15, 2019, after talks broke down to renew their contract, which expired earlier that day, the UAW announced that GM employees would begin striking at 11:59 pm. This strike shut down operations in nine states, including 33 manufacturing plants and 22 parts distribution warehouses. After 40 days, on October 25, 2019, the "longest strike by autoworkers in a decade" and the longest against GM since 1970 came to an end when United Auto Workers members voted to approve a new contract with GM. Striking labor union members received a $275 a week strike pay salary for the duration of the strike. The strike cost GM more than $2 billion.

2023 United Auto Workers strike
The ongoing strike launched by the UAW is the first strike against all three major American automakers in history. Then-recently elected UAW president Shawn Fain stated that he was "fed up" with the current situation between workers and automakers; Fain specifically blasted the tiered workers system at automakers, failure for automakers to keep wages up with inflation, pensions, as well as the introduction of a four-day workweek as opposed to the five-day workweek. GM CEO Mary Barra protested that her company offered an "unprecedented deal" which gave workers 20% raises as well as "world-class" healthcare. Barra further stated that meeting all 1,000 plus demands would bankrupt the company and cost over $100 billion.

Controversies
Streetcar conspiracy
Between 1938 and 1950, GM allegedly deliberately monopolized the sale of buses and supplies to National City Lines (NCL) and its subsidiaries, in violation of the Sherman Antitrust Act of 1890, intending to dismantle streetcar systems in many cities in the United States and make buses, sold by GM, the dominant form of public transport.

Ralph Nader and the Corvair
Unsafe at Any Speed: The Designed-In Dangers of the American Automobile by Ralph Nader, published in 1965, is a book accusing car manufacturers of being slow to introduce safety features and reluctant to spend money on improving safety. It relates to the first models of the Chevrolet Corvair (1960–1964) that had a swing axle suspension design that was prone to 'tuck under' in certain circumstances. To compensate for the removal of a front stabilizer bar (anti-roll bar) as a cost-cutting measure, Corvairs required tire pressures that were outside of the tire manufacturer's recommended tolerances. The Corvair relied on an unusually high front to rear pressure differential (15 psi front, 26 psi rear, when cold; 18 psi and 30 psi hot), and if one inflated the tires equally, as was standard practice for all other cars at the time, the result was dangerous over-steer.In early March 1966, several media outlets, including The New Republic and The New York Times, alleged that GM had tried to discredit Ralph Nader, hiring private detectives to tap his phones and investigate his past, and hiring prostitutes to trap him in compromising situations. Nader sued the company for invasion of privacy and settled the case for $425,000. Nader's lawsuit against GM was ultimately decided by the New York Court of Appeals, whose opinion in the case expanded tort law to cover "overzealous surveillance". Nader used the proceeds from the lawsuit to start the pro-consumer Center for Study of Responsive Law.
A 1972 safety commission report conducted by Texas A&M University concluded that the 1960–1963 Corvair possessed no greater potential for loss of control than its contemporary competitors in extreme situations. The United States Department of Transportation (DOT) issued a press release in 1972 describing the findings of NHTSA testing from the previous year. NHTSA conducted a series of comparative tests in 1971 studying the handling of the 1963 Corvair and four contemporary cars — a Ford Falcon, Plymouth Valiant, Volkswagen Beetle, and Renault Dauphine — along with a second-generation Corvair (with its completely redesigned, independent rear suspension). The 143-page report reviewed NHTSA's extreme-condition handling tests, national crash-involvement data for the cars in the test as well as General Motors' internal documentation regarding the Corvair's handling. NHTSA went on to contract an independent advisory panel of engineers to review the tests. This review panel concluded that 'the 1960–63 Corvair compares favorably with contemporary vehicles used in the tests ... the handling and stability performance of the 1960–63 Corvair does not result in an abnormal potential for loss of control or rollover, and it is at least as good as the performance of some contemporary vehicles both foreign and domestic'.
Former GM executive John DeLorean asserted, in his book On a Clear Day You Can See General Motors, that Nader's criticisms were valid.Journalist David E. Davis noted that despite Nader's claim that swing-axle rear suspension were dangerous, Porsche, Mercedes-Benz, and Volkswagen all used similar swing-axle concepts during that era.

Apartheid
In 2002, GM (along with other multinational corporations) was sued by a group of South Africans represented by the Khulumani Support Group. The plaintiffs alleged that the company provided vehicles to the South African security forces during the Apartheid. The company settled with the plaintiffs in 2012, agreeing to pay a sum of up to $1.5 million.

Ignition switch recall
In May 2014, the National Highway Traffic Safety Administration fined the company $35 million for failing to recall cars with faulty ignition switches for a decade, despite knowing there was a problem with the switches. General Motors paid compensation for 124 deaths linked to the faulty switches. The $35 million fine was the maximum the regulator could impose. The total cost of the recall was estimated to be $1.5 billion. As well as the Cobalts, the switches of concern had been installed in many other cars, such as the Pontiac G5, the Saturn Ion, the Chevrolet HHR, the Saturn Sky, and Pontiac Solstice. The recall involved about 2.6 million GM cars worldwide.

Xinjiang region
In 2020, the Australian Strategic Policy Institute accused at least 82 major brands, including General Motors, of being connected to forced Uyghur labor in Xinjiang.

See also
Alliance of Automobile Manufacturers
ASOTRECOL
Crucible Industries
EcoCAR
General Motors Hy-wire
General Motors proving grounds
General Motors Technical Center
GM people
GM vehicles by brand
List of automobile manufacturers of the United States
List of GM engines
List of General Motors factories
List of General Motors platforms
List of GM transmissions
United States Council for Automotive Research
VIA Motors
Freon
Global Climate Coalition
Ethyl Corporation
Durant Motors

References
Further reading
External links

Official website 
Business data for General Motors Company:

Geomagnetic storm

A geomagnetic storm, also known as a magnetic storm, is temporary disturbance of the Earth's magnetosphere caused by a solar wind shock wave.
The disturbance that drives the magnetic storm may be a solar coronal mass ejection (CME) or (much less severely) a co-rotating interaction region (CIR), a high-speed stream of solar wind originating from a coronal hole. The frequency of geomagnetic storms increases and decreases with the sunspot cycle. During solar maximum, geomagnetic storms occur more often, with the majority driven by CMEs.
The increase in the solar wind pressure initially compresses the magnetosphere. The solar wind's magnetic field interacts with the Earth's magnetic field and transfers an increased energy into the magnetosphere. Both interactions cause an increase in plasma movement through the magnetosphere (driven by increased electric fields inside the magnetosphere) and an increase in electric current in the magnetosphere and ionosphere. During the main phase of a geomagnetic storm, electric current in the magnetosphere creates a magnetic force that pushes out the boundary between the magnetosphere and the solar wind.
Several space weather phenomena tend to be associated with or are caused by a geomagnetic storm. These include solar energetic particle (SEP) events, geomagnetically induced currents (GIC), ionospheric storms and its disturbances that cause radio and radar scintillation, disruption of navigation by magnetic compass and auroral displays at much lower latitudes than normal.
The largest recorded geomagnetic storm, the Carrington Event in September 1859, took down parts of the recently created US telegraph network, starting fires and electrically shocking telegraph operators. In 1989, a geomagnetic storm energized ground induced currents that disrupted electric power distribution throughout most of Quebec and caused aurorae as far south as Texas. The Carrington Event was mild compared with very rare extreme geomagnetic storms called Miyake events, which cause spikes in radioactive carbon-14 in tree rings.

Definition
A geomagnetic storm is defined by changes in the Dst (disturbance – storm time) index. The Dst index estimates the globally averaged change of the horizontal component of the Earth's magnetic field at the magnetic equator based on measurements from a few magnetometer stations. Dst is computed once per hour and reported in near-real-time. During quiet times, Dst is between +20 and −20 nano-Tesla (nT).A geomagnetic storm has three phases: initial, main and recovery. The initial phase is characterized by Dst (or its one-minute component SYM-H) increasing by 20 to 50 nT in tens of minutes. The initial phase is also referred to as a storm sudden commencement (SSC). However, not all geomagnetic storms have an initial phase and not all sudden increases in Dst or SYM-H are followed by a geomagnetic storm. The main phase of a geomagnetic storm is defined by Dst decreasing to less than −50 nT. The selection of −50 nT to define a storm is somewhat arbitrary. The minimum value during a storm will be between −50 and approximately −600 nT. The duration of the main phase is typically 2–8 hours. The recovery phase is when Dst changes from its minimum value to its quiet time value. The recovery phase may last as short as 8 hours or as long as 7 days.
The size of a geomagnetic storm is classified as moderate (−50 nT > minimum of Dst > −100 nT), intense (−100 nT > minimum Dst > −250 nT) or super-storm (minimum of Dst < −250 nT).

Measuring intensity
Geomagnetic storm intensity is reported in several different ways, including:

K-index
A-index
The G-scale used by the U.S. National Oceanic and Atmospheric Administration, which rates the storm from G1 to G5 (i.e. G1, G2, G3, G4, G5 in order), where G1 is the weakest storm classification (corresponding to a Kp value of 5), and G5 is the strongest (corresponding to a Kp value of 9).

History of the theory
In 1931, Sydney Chapman and Vincenzo C. A. Ferraro wrote an article, A New Theory of Magnetic Storms, that sought to explain the phenomenon. They argued that whenever the Sun emits a solar flare it also emits a plasma cloud, now known as a coronal mass ejection. They postulated that this plasma travels at a velocity such that it reaches Earth within 113 days, though we now know this journey takes 1 to 5 days. They wrote that the cloud then compresses the Earth's magnetic field and thus increases this field at the Earth's surface. Chapman and Ferraro's work drew on that of, among others, Kristian Birkeland, who had used recently-discovered cathode ray tubes to show that the rays were deflected towards the poles of a magnetic sphere. He theorised that a similar phenomenon was responsible for auroras, explaining why they are more frequent in polar regions.

Occurrences
The first scientific observation of the effects of a geomagnetic storm occurred early in the 19th century: from May 1806 until June 1807, Alexander von Humboldt recorded the bearing of a magnetic compass in Berlin. On 21 December 1806, he noticed that his compass had become erratic during a bright auroral event.On September 1–2, 1859, the largest recorded geomagnetic storm occurred. From August 28 until September 2, 1859, numerous sunspots and solar flares were observed on the Sun, with the largest flare on September 1. This is referred to as the Solar storm of 1859 or the Carrington Event. It can be assumed that a massive coronal mass ejection (CME) was launched from the Sun and reached the Earth within eighteen hours—a trip that normally takes three to four days. The horizontal field was reduced by 1600 nT as recorded by the Colaba Observatory. It is estimated that Dst would have been approximately −1760 nT. Telegraph wires in both the United States and Europe experienced induced voltage increases (emf), in some cases even delivering shocks to telegraph operators and igniting fires. Aurorae were seen as far south as Hawaii, Mexico, Cuba and Italy—phenomena that are usually only visible in polar regions. Ice cores show evidence that events of similar intensity recur at an average rate of approximately once per 500 years.
Since 1859, less severe storms have occurred, notably the aurora of November 17, 1882 and the May 1921 geomagnetic storm, both with disruption of telegraph service and initiation of fires, and 1960, when widespread radio disruption was reported.
In early August 1972, a series of flares and solar storms peaks with a flare estimated around X20 producing the fastest CME transit ever recorded and a severe geomagnetic and proton storm that disrupted terrestrial electrical and communications networks, as well as satellites (at least one made permanently inoperative), and spontaneously detonated numerous U.S. Navy magnetic-influence sea mines in North Vietnam.The March 1989 geomagnetic storm caused the collapse of the Hydro-Québec power grid in seconds as equipment protection relays tripped in a cascading sequence. Six million people were left without power for nine hours. The storm caused auroras as far south as Texas and Florida. The storm causing this event was the result of a coronal mass ejected from the Sun on March 9, 1989. The minimum Dst was −589 nT.
On July 14, 2000, an X5 class flare erupted (known as the Bastille Day event) and a coronal mass was launched directly at the Earth. A geomagnetic super storm occurred on July 15–17; the minimum of the Dst index was −301 nT. Despite the storm's strength, no power distribution failures were reported. The Bastille Day event was observed by Voyager 1 and Voyager 2, thus it is the farthest out in the Solar System that a solar storm has been observed.
Seventeen major flares erupted on the Sun between 19 October and 5 November 2003, including perhaps the most intense flare ever measured on the GOES XRS sensor—a huge X28 flare, resulting in an extreme radio blackout, on 4 November. These flares were associated with CME events that caused three geomagnetic storms between 29 October and 2 November, during which the second and third storms were initiated before the previous storm period had fully recovered. The minimum Dst values were −151, −353 and −383 nT. Another storm in this sequence occurred on 4–5 November with a minimum Dst of −69 nT. The last geomagnetic storm was weaker than the preceding storms, because the active region on the Sun had rotated beyond the meridian where the central portion CME created during the flare event passed to the side of the Earth. The whole sequence became known as the Halloween Solar Storm. The Wide Area Augmentation System (WAAS) operated by the Federal Aviation Administration (FAA) was offline for approximately 30 hours due to the storm. The Japanese ADEOS-2 satellite was severely damaged and the operation of many other satellites were interrupted due to the storm.

Interactions with planetary processes
The solar wind also carries with it the Sun's magnetic field. This field will have either a North or South orientation. If the solar wind has energetic bursts, contracting and expanding the magnetosphere, or if the solar wind takes a southward polarization, geomagnetic storms can be expected. The southward field causes magnetic reconnection of the dayside magnetopause, rapidly injecting magnetic and particle energy into the Earth's magnetosphere.
During a geomagnetic storm, the ionosphere's F2 layer becomes unstable, fragments, and may even disappear. In the northern and southern pole regions of the Earth, auroras are observable.

Instruments
Magnetometers monitor the auroral zone as well as the equatorial region. Two types of radar, coherent scatter and incoherent scatter, are used to probe the auroral ionosphere. By bouncing signals off ionospheric irregularities, which move with the field lines, one can trace their motion and infer magnetospheric convection.
Spacecraft instruments include:

Magnetometers, usually of the flux gate type. Usually these are at the end of booms, to keep them away from magnetic interference by the spacecraft and its electric circuits.
Electric sensors at the ends of opposing booms are used to measure potential differences between separated points, to derive electric fields associated with convection. The method works best at high plasma densities in low Earth orbit; far from Earth long booms are needed, to avoid shielding-out of electric forces.
Radio sounders from the ground can bounce radio waves of varying frequency off the ionosphere, and by timing their return determine the electron density profile—up to its peak, past which radio waves no longer return. Radio sounders in low Earth orbit aboard the Canadian Alouette 1 (1962) and Alouette 2 (1965), beamed radio waves earthward and observed the electron density profile of the "topside ionosphere". Other radio sounding methods were also tried in the ionosphere (e.g. on IMAGE).
Particle detectors include a Geiger counter, as was used for the original observations of the Van Allen radiation belt. Scintillator detectors came later, and still later "channeltron" electron multipliers found particularly wide use. To derive charge and mass composition, as well as energies, a variety of mass spectrograph designs were used. For energies up to about 50 keV (which constitute most of the magnetospheric plasma) time-of-flight spectrometers (e.g. "top-hat" design) are widely used.Computers have made it possible to bring together decades of isolated magnetic observations and extract average patterns of electrical currents and average responses to interplanetary variations. They also run simulations of the global magnetosphere and its responses, by solving the equations of magnetohydrodynamics (MHD) on a numerical grid. Appropriate extensions must be added to cover the inner magnetosphere, where magnetic drifts and ionospheric conduction need to be taken into account. At polar regions, directly linked to the solar wind, large-scale ionospheric anomalies can be successfully modeled, even during geomagnetic super-storms.  At smaller scales (comparable to a degree of latitude/longitude) the results are difficult to interpret, and certain assumptions about the high-latitude forcing uncertainty are needed.

Geomagnetic storm effects
Disruption of electrical systems
It has been suggested that a geomagnetic storm on the scale of the solar storm of 1859 today would cause billions or even trillions of dollars of damage to satellites, power grids and radio communications, and could cause electrical blackouts on a massive scale that might not be repaired for weeks, months, or even years. Such sudden electrical blackouts may threaten food production.

Main electrical grid
When magnetic fields move about in the vicinity of a conductor such as a wire, a geomagnetically induced current is produced in the conductor. This happens on a grand scale during geomagnetic storms (the same mechanism also influenced telephone and telegraph lines before fiber optics, see above) on all long transmission lines. Long transmission lines (many kilometers in length) are thus subject to damage by this effect. Notably, this chiefly includes operators in China, North America, and Australia, especially in modern high-voltage, low-resistance lines. The European grid consists mainly of shorter transmission circuits, which are less vulnerable to damage.The (nearly direct) currents induced in these lines from geomagnetic storms are harmful to electrical transmission equipment, especially transformers—inducing core saturation, constraining their performance (as well as tripping various safety devices), and causing coils and cores to heat up. In extreme cases, this heat can disable or destroy them, even inducing a chain reaction that can overload transformers. Most generators are connected to the grid via transformers, isolating them from the induced currents on the grid, making them much less susceptible to damage due to geomagnetically induced current. However, a transformer that is subjected to this will act as an unbalanced load to the generator, causing negative sequence current in the stator and consequently rotor heating.
According to a study by Metatech corporation, a storm with a strength comparable to that of 1921 would destroy more than 300 transformers and leave over 130 million people without power in the United States, costing several trillion dollars. The extent of the disruption is debated, with some congressional testimony indicating a potentially indefinite outage until transformers can be replaced or repaired. These predictions are contradicted by a North American Electric Reliability Corporation report that concludes that a geomagnetic storm would cause temporary grid instability but no widespread destruction of high-voltage transformers. The report points out that the widely quoted Quebec grid collapse was not caused by overheating transformers but by the near-simultaneous tripping of seven relays.Besides the transformers being vulnerable to the effects of a geomagnetic storm, electricity companies can also be affected indirectly by the geomagnetic storm. For instance, internet service providers may go down during geomagnetic storms (and/or remain non-operational long after). Electricity companies may have equipment requiring a working internet connection to function, so during the period the internet service provider is down, the electricity too may not be distributed.By receiving geomagnetic storm alerts and warnings (e.g. by the Space Weather Prediction Center; via Space Weather satellites as SOHO or ACE), power companies can minimize damage to power transmission equipment, by momentarily disconnecting transformers or by inducing temporary blackouts. Preventive measures also exist, including preventing the inflow of GICs into the grid through the neutral-to-ground connection.

Communications
High frequency (3–30 MHz) communication systems use the ionosphere to reflect radio signals over long distances. Ionospheric storms can affect radio communication at all latitudes. Some frequencies are absorbed and others are reflected, leading to rapidly fluctuating signals and unexpected propagation paths. TV and commercial radio stations are little affected by solar activity, but ground-to-air, ship-to-shore, shortwave broadcast and amateur radio (mostly the bands below 30 MHz) are frequently disrupted. Radio operators using HF bands rely upon solar and geomagnetic alerts to keep their communication circuits up and running.
Military detection or early warning systems operating in the high frequency range are also affected by solar activity. The over-the-horizon radar bounces signals off the ionosphere to monitor the launch of aircraft and missiles from long distances. During geomagnetic storms, this system can be severely hampered by radio clutter. Also some submarine detection systems use the magnetic signatures of submarines as one input to their locating schemes. Geomagnetic storms can mask and distort these signals.
The Federal Aviation Administration routinely receives alerts of solar radio bursts so that they can recognize communication problems and avoid unnecessary maintenance. When an aircraft and a ground station are aligned with the Sun, high levels of noise can occur on air-control radio frequencies. This can also happen on UHF and SHF satellite communications, when an Earth station, a satellite and the Sun are in alignment. In order to prevent unnecessary maintenance on satellite communications systems aboard aircraft AirSatOne provides a live feed for geophysical events from NOAA's Space Weather Prediction Center. allows users to view observed and predicted space storms. Geophysical Alerts are important to flight crews and maintenance personnel to determine if any upcoming activity or history has or will have an effect on satellite communications, GPS navigation and HF Communications.
Telegraph lines in the past were affected by geomagnetic storms. Telegraphs used a single long wire for the data line, stretching for many miles, using the ground as the return wire and fed with DC power from a battery; this made them (together with the power lines mentioned below) susceptible to being influenced by the fluctuations caused by the ring current. The voltage/current induced by the geomagnetic storm could have diminished the signal, when subtracted from the battery polarity, or to overly strong and spurious signals when added to it; some operators learned to disconnect the battery and rely on the induced current as their power source. In extreme cases the induced current was so high the coils at the receiving side burst in flames, or the operators received electric shocks. Geomagnetic storms affect also long-haul telephone lines, including undersea cables unless they are fiber optic.Damage to communications satellites can disrupt non-terrestrial telephone, television, radio and Internet links. The National Academy of Sciences reported in 2008 on possible scenarios of widespread disruption in the 2012–2013 solar peak. A solar superstorm could cause large-scale global months-long Internet outages. A study describes potential mitigation measures and exceptions – such as user-powered mesh networks, related peer-to-peer applications and new protocols – and analyzes the robustness of the current Internet infrastructure.

Navigation systems
Global navigation satellite systems (GNSS), and other navigation systems such as LORAN and the now-defunct OMEGA are adversely affected when solar activity disrupts their signal propagation. The OMEGA system consisted of eight transmitters located throughout the world. Airplanes and ships used the very low frequency signals from these transmitters to determine their positions. During solar events and geomagnetic storms, the system gave navigators information that was inaccurate by as much as several miles. If navigators had been alerted that a proton event or geomagnetic storm was in progress, they could have switched to a backup system.
GNSS signals are affected when solar activity causes sudden variations in the density of the ionosphere, causing the satellite signals to scintillate (like a twinkling star). The scintillation of satellite signals during ionospheric disturbances is studied at HAARP during ionospheric modification experiments. It has also been studied at the Jicamarca Radio Observatory.
One technology used to allow GNSS receivers to continue to operate in the presence of some confusing signals is Receiver Autonomous Integrity Monitoring (RAIM), used by GPS. However, RAIM is predicated on the assumption that a majority of the GPS constellation is operating properly, and so it is much less useful when the entire constellation is perturbed by global influences such as geomagnetic storms. Even if RAIM detects a loss of integrity in these cases, it may not be able to provide a useful, reliable signal.

Satellite hardware damage
Geomagnetic storms and increased solar ultraviolet emission heat Earth's upper atmosphere, causing it to expand. The heated air rises, and the density at the orbit of satellites up to about 1,000 km (600 mi) increases significantly. This results in increased drag, causing satellites to slow and change orbit slightly. Low Earth orbit satellites that are not repeatedly boosted to higher orbits slowly fall and eventually burn up. Skylab's 1979 destruction is an example of a spacecraft reentering Earth's atmosphere prematurely as a result of higher-than-expected solar activity. During the great geomagnetic storm of March 1989, four of the U.S. Navy's navigational satellites had to be taken out of service for up to a week, the U.S. Space Command had to post new orbital elements for over 1000 objects affected, and the Solar Maximum Mission satellite fell out of orbit in December the same year.The vulnerability of the satellites depends on their position as well. The South Atlantic Anomaly is a perilous place for a satellite to pass through, due to the unusually weak geomagnetic field at low Earth orbit.

Pipelines
Rapidly fluctuating geomagnetic fields can produce geomagnetically induced currents in pipelines. This can cause multiple problems for pipeline engineers. Pipeline flow meters can transmit erroneous flow information and the corrosion rate of the pipeline can be dramatically increased.

Radiation hazards to humans
Earth's atmosphere and magnetosphere allow adequate protection at ground level, but astronauts are subject to potentially lethal radiation poisoning. The penetration of high-energy particles into living cells can cause chromosome damage, cancer and other health problems. Large doses can be immediately fatal. Solar protons with energies greater than 30 MeV are particularly hazardous.Solar proton events can also produce elevated radiation aboard aircraft flying at high altitudes. Although these risks are small, flight crews may be exposed repeatedly, and monitoring of solar proton events by satellite instrumentation allows exposure to be monitored and evaluated, and eventually flight paths and altitudes to be adjusted to lower the absorbed dose.Ground level enhancements, also known as ground level events or GLEs, occur when a solar particle event contains particles with sufficient energy to have effects at ground level, mainly detected as an increase in the number of neutrons measured at ground level. These events have been shown to have an impact on radiation dosage, but they do not significantly increase the risk of cancer.

Effect on animals
There is a large but controversial body of scientific literature on connections between geomagnetic storms and human health. This began with Russian papers, and the subject was subsequently studied by Western scientists. Theories for the cause include the involvement of cryptochrome, melatonin, the pineal gland, and the circadian rhythm.Some scientists suggest that solar storms induce whales to beach themselves. Some have speculated that migrating animals which use magnetoreception to navigate, such as birds and honey bees, might also be affected.

See also
References
Further reading
External links
Live solar and geomagnetic activity data at Spaceweather
NOAA Space Weather Prediction Center
Real time magnetograms
Aurora Watch at Lancaster University
USGS Geomagnetism programLinks related to power grids:

Geomagnetic Storm Induced HVAC Transformer Failure is Avoidable Archived 2013-05-17 at the Wayback Machine
NOAA Economics – Geomagnetic Storm datasets and Economic Research
Geomagnetic Storms Can Threaten Electric Power Grid

Goods

In economics, goods are items that satisfy human wants and provide utility, for example, to a consumer making a purchase of a satisfying product. A common distinction is made between goods which are transferable, and services, which are not transferable.A good is an "economic good" if it is useful to people but scarce in relation to its demand so that human effort is required to obtain it.
In contrast, free goods, such as air, are naturally in abundant supply and need no conscious effort to obtain them.
Private goods are things owned by people, such as televisions, living room furniture, wallets, cellular telephones, almost anything owned or used on a daily basis that is not food-related. 
A consumer good or "final good" is any item that is ultimately consumed, rather than used in the production of another good. For example, a microwave oven or a bicycle that is sold to a consumer is a final good or consumer good, but the components that are sold to be used in those goods are intermediate goods. For example, textiles or transistors can be used to make some further goods.
Commercial goods are construed as tangible products that are manufactured and then made available for supply to be used in an industry of commerce. Commercial goods could be tractors, commercial vehicles, mobile structures, airplanes, and even roofing materials. Commercial and personal goods as categories are very broad and cover almost everything a person sees from the time they wake up in their home, on their commute to work to their arrival at the workplace.
Commodities may be used as a synonym for economic goods but often refer to marketable raw materials and primary products.Although common goods are tangible, certain classes of goods, such as information, only take intangible forms. For example, among other goods an apple is a tangible object, while news belongs to an intangible class of goods and can be perceived only by means of an instrument such as printers or television.

Utility and characteristics of goods
Goods may increase or decrease their utility directly or indirectly and may be described as having marginal utility. Some things are useful, but not scarce enough to have monetary value, such as the Earth's atmosphere, these are referred to as 'free goods'.
In normal parlance, "goods" is always a plural word, but economists have long termed a single item of goods "a good".
In economics, a bad is the opposite of a good. Ultimately, whether an object is a good or a bad depends on each individual consumer and therefore, not all goods are goods to all people.

Types of goods
Goods' diversity allows for their classification into different categories based on distinctive characteristics, such as tangibility and (ordinal) relative elasticity. A tangible good like an apple differs from an intangible good like information due to the impossibility of a person to physically hold the latter, whereas the former occupies physical space.  Intangible goods differ from services in that final (intangible) goods are transferable and can be traded, whereas a service cannot.
Price elasticity also differentiates types of goods.  An elastic good is one for which there is a relatively large change in quantity due to a relatively small change in price, and therefore is likely to be part of a family of substitute goods; for example, as pen prices rise, consumers might buy more pencils instead. An inelastic good is one for which there are few or no substitutes, such as tickets to major sporting events, original works by famous artists, and prescription medicine such as insulin. Complementary goods are generally more inelastic than goods in a family of substitutes. For example, if a rise in the price of beef results in a decrease in the quantity of beef demanded, it is likely that the quantity of hamburger buns demanded will also drop, despite no change in buns' prices. This is because hamburger buns and beef (in Western culture) are complementary goods. Goods considered complements or substitutes are relative associations and should not be understood in a vacuum. The degree to which a good is a substitute or a complement depends on its relationship to other goods, rather than an intrinsic characteristic, and can be measured as cross elasticity of demand by employing statistical techniques such as covariance and correlation.

Goods classified by exclusivity and competitiveness
Fourfold model of goods
Goods can be classified based on their degree of excludability and rivalry (competitiveness). Considering excludability can be measured on a continuous scale, some goods  would not be able to fall into one of the four common categories used.
There are four types of goods based on the characteristics of rival in consumption and excludability: Public Goods, Private Goods, Common Resources, and Club Goods.  These four types plus examples for anti-rivalry appear in the accompanying table.

Public goods
Goods that are both non-rival and non-excludable are called public goods. In many cases, renewable resources, such as land, are common commodities but some of them are contained in public goods. Public goods are non-exclusive and non-competitive, meaning that individuals cannot be stopped from using them and anyone can consume this good without hindering the ability of others to consume them. Examples in addition to the ones in the matrix are national parks, or firework displays. It is generally accepted by mainstream economists that the market mechanism will under-provide public goods, so these goods have to be produced by other means, including government provision. Public goods can also suffer from the Free-Rider problem.

Private goods
Private goods are excludable goods, which prevent other consumers from consuming them. Private goods are also rivalrous because one good in private ownership cannot be used by someone else. That is to say, consuming some goods will deprive another consumer of the ability to consume the goods. Private goods are the most common type of goods.  They include what you have to get from the store. For examples food, clothing, cars, parking spaces,etc. An individual who consumes an apple denies another individual from consuming the same one. It is excludable because consumption is only offered to those willing to pay the price.

Common-pool resources
Common-pool resources are rival in consumption and non-excludable. An example is that of fisheries, which harvest fish from a shared common resource pool of fish stock. Fish caught by one group of fishermen are no longer accessible to another group, thus being rivalrous. However, oftentimes, due to an absence of well-defined property rights, it is difficult to restrict access to fishermen who may overfish.

Club goods
Club goods are excludable but not rivalrous in the consumption. That is, not everyone can use the good, but when one individual has claim to use it, they do not reduce the amount or the ability for others to consume the good. By joining a specific club or organization we can obtain club goods; As a result, some people are excluded because they are not members. Examples in addition to the ones in the matrix are cable television, golf courses, and any merchandise provided to club members. A large television service provider would already have infrastructure in place which would allow for the addition of new customers without infringing on existing customers viewing abilities. This would also mean that marginal cost would be close to zero, which satisfies the criteria for a good to be considered non-rival. However, access to cable TV services is only available to consumers willing to pay the price, demonstrating the excludability aspect.Economists set these categories for these goods and their impact on consumers. The government is usually responsible for public goods and common goods, and enterprises are generally responsible for the production of private and club goods. But this pattern does not fit for all the goods as they can intermingle.

History of the fourfold model of goods
In 1977, Nobel winner Elinor Ostrom and her husband Vincent Ostrom proposed additional modifications to the existing classification of goods so to identify fundamental differences that affect the incentives facing individuals. Their definitions are presented on the matrix.Elinor Ostrom proposed additional modifications to the classification of goods to identify fundamental differences that affect the incentives facing individuals
Replacing the term "rivalry of consumption" with "subtractability of use".
Conceptualizing subtractability of use and excludability to vary from low to high rather than characterizing them as either present or absent.
Overtly adding a very important fourth type of good—common-pool resources—that shares the attribute of subtractability with private goods and difficulty of exclusion with public goods. Forests, water systems, fisheries, and the global atmosphere are all common-pool resources of immense importance for the survival of humans on this earth.
Changing the name of a "club" good to a "toll" good since goods that share these characteristics are provided by small scale public as well as private associations.

Expansion of Fourfold model: Anti-rivalrous
Consumption can be extended to include "Anti-rivalrous" consumption.

Expansion of Fourfold model: Semi-Excludable
The additional definition matrix shows the four common categories alongside providing some examples of fully excludable goods, Semi-excludable goods and fully non-excludeable goods. Semi-excludable goods can be considered goods or services that a mostly successful in excluding non-paying customer, but are still able to be consumed by non-paying consumers.  An example of this is movies, books or video games that could be easily pirated and shared for free.

Trading of goods
Goods are capable of being physically delivered to a consumer. Goods that are economic intangibles can only be stored, delivered, and consumed by means of media.
Goods, both tangibles and intangibles, may involve the transfer of product ownership to the consumer. Services do not normally involve transfer of ownership of the service itself, but may involve transfer of ownership of goods developed or marketed by a service provider in the course of the service. For example, sale of storage related goods, which could consist of storage sheds, storage containers, storage buildings as tangibles or storage supplies such as boxes, bubble wrap, tape, bags and the like which are consumables, or distributing electricity among consumers is a service provided by an electric utility company. This service can only be experienced through the consumption of electrical energy, which is available in a variety of voltages and, in this case, is the economic goods produced by the electric utility company. While the service (namely, distribution of electrical energy) is a process that remains in its entirety in the ownership of the electric service provider, the goods (namely, electric energy) is the object of ownership transfer. The consumer becomes an electric energy owner by purchase and may use it for any lawful purposes just like any other goods.

See also
Notes
References
Bannock, Graham et al. (1997). Dictionary of Economics, Penguin Books.
Milgate, Murray (1987), "goods and commodities," The New Palgrave: A Dictionary of Economics, v. 2, pp. 546–48. Includes historical and contemporary uses of the terms in economics.
Vuaridel, R. (1968). Une définition des biens économiques. (A definition of economic goods). L'Année sociologique (1940/1948-), 19, 133-170. Stable JStor URL: [1]

External links
 Media related to Goods (economics) at Wikimedia Commons

Harley-Davidson

Harley-Davidson, Inc. (H-D, or simply Harley) is an American motorcycle manufacturer headquartered in Milwaukee, Wisconsin, United States. Founded in 1903, it is one of two major American motorcycle manufacturers to survive the Great Depression along with its historical rival, Indian Motorcycles. The company has survived numerous ownership arrangements, subsidiary arrangements, periods of poor economic health and product quality, and intense global competition to become one of the world's largest motorcycle manufacturers and an iconic brand widely known for its loyal following. There are owner clubs and events worldwide, as well as a company-sponsored, brand-focused museum.
Harley-Davidson is noted for a style of customization that gave rise to the chopper motorcycle style. The company traditionally marketed heavyweight, air-cooled cruiser motorcycles with engine displacements greater than 700 cc, but it has broadened its offerings to include more contemporary VRSC (2002) and middle-weight Street (2015) platforms.
Harley-Davidson manufactures its motorcycles at factories in York, Pennsylvania; Menomonee Falls, Wisconsin; Tomahawk, Wisconsin; Manaus, Brazil; and Rayong, Thailand. The company markets its products worldwide, and also licenses and markets merchandise under the Harley-Davidson brand, among them apparel, home décor and ornaments, accessories, toys, scale models of its motorcycles, and video games based on its motorcycle line and the community.

History
In 1901, 20-year-old William S. Harley drew up plans for a small engine with a displacement of 7.07 cubic inches (116 cc) and four-inch (102 mm) flywheels designed for use in a regular pedal-bicycle frame. Over the next two years, he and his childhood friend Arthur Davidson worked on their motor-bicycle using the northside Milwaukee machine shop at the home of their friend Henry Melk. It was finished in 1903 with the help of Arthur's brother Walter Davidson. Upon testing their power-cycle, Harley and the Davidson brothers found it unable to climb the hills around Milwaukee without pedal assistance, and they wrote off their first motor-bicycle as a valuable learning experiment.The three began work on a new and improved machine with an engine of 24.74 cubic inches (405 cc) with 9.75 in (24.8 cm) flywheels weighing 28 lb (13 kg). Its advanced loop-frame pattern was similar to the 1903 Milwaukee Merkel motorcycle designed by Joseph Merkel, later of Flying Merkel fame. The bigger engine and loop-frame design took it out of the motorized bicycle category and marked the path to future motorcycle designs. They also received help with their bigger engine from outboard motor pioneer Ole Evinrude, who was then building gas engines of his own design for automotive use on Milwaukee's Lake Street.

The prototype of the new loop-frame Harley-Davidson was assembled in a 10 ft × 15 ft (3.0 m × 4.6 m) shed in the Davidson family backyard. Most of the major parts, however, were made elsewhere, including some probably fabricated at the West Milwaukee railshops where oldest brother William A. Davidson was toolroom foreman. This prototype machine was functional by September 8, 1904, when it competed in a Milwaukee motorcycle race held at State Fair Park. Edward Hildebrand rode it and placed fourth in the race.In January 1905, the company placed small advertisements in the Automobile and Cycle Trade Journal offering bare Harley-Davidson engines to the do-it-yourself trade. By April, they were producing complete motorcycles on a very limited basis. That year, Harley-Davidson dealer Carl H. Lang of Chicago sold three bikes from the five built in the Davidson backyard shed. Years later, the company moved the original shed to the Juneau Avenue factory where it stood for many decades as a tribute.
In 1906, Harley and the Davidson brothers built their first factory on Chestnut Street (later Juneau Avenue), at the current location of Harley-Davidson's corporate headquarters. The first Juneau Avenue plant was a 40 ft × 60 ft (12 m × 18 m) single-story wooden structure. The company produced about 50 motorcycles that year.

In 1907, William S. Harley graduated from the University of Wisconsin–Madison with a degree in mechanical engineering. That year, they expanded the factory with a second floor and later with facings and additions of Milwaukee pale yellow ("cream") brick. With the new facilities, production increased to 150 motorcycles in 1907. The company was officially incorporated that September. They also began selling their motorcycles to police departments around this time, a market that has been important to them ever since. In 1907, William A. Davidson quit his job as tool foreman for the Milwaukee Road railroad and joined the Motor Company.
Production in 1905 and 1906 were all single-cylinder models with 26.84-cubic-inch (440 cc) engines. In February 1907, they displayed a prototype model at the Chicago Automobile Show with a 45-degree V-Twin engine. Very few V-Twin models were built between 1907 and 1910. These first V-Twins displaced 53.68 cubic inches (880 cc) and produced about 7 horsepower (5.2 kW). This gave about double the power of the first singles, and top speed was about 60 mph (100 km/h). Production jumped from 450 motorcycles in 1908 to 1,149 machines in 1909.
In 1911, the company introduced an improved V-Twin model with a displacement of 49.48 cubic inches (811 cc) and mechanically operated intake valves, as opposed to the "automatic" intake valves used on earlier V-Twins that opened by engine vacuum. It was smaller than earlier twins but gave better performance. After 1913, the majority of bikes produced by Harley-Davidson were V-Twin models.
In 1912, Harley-Davidson introduced their patented "Ful-Floteing Seat", which was suspended by a coil spring inside the seat tube. The spring tension could be adjusted to suit the rider's weight, and more than 3 inches (76 mm) of travel was available. Harley-Davidson used seats of this type until 1958.By 1913, the yellow brick factory had been demolished and a new five-story structure had been built on the site which took up two blocks along Juneau Avenue and around the corner on 38th Street. Despite the competition, Harley-Davidson was already pulling ahead of Indian and dominated motorcycle racing after 1914. Production that year swelled to 16,284 machines.

World War I
In 1917, the United States entered World War I and the military demanded motorcycles for the war effort. Harleys had already been used by the military in the Pancho Villa Expedition but World War I was the first time that it was adopted for military issue, first with the British Model H produced by Triumph Motorcycles Ltd in 1915. The U.S. military purchased over 20,000 motorcycles from Harley-Davidson.Harley-Davidson launched a line of bicycles in 1917 in hopes of recruiting more domestic customers for its motorcycles. Models included the traditional diamond frame men's bicycle, a step-through frame 3–18 "Ladies Standard", and a 5–17 "Boy Scout" for youth. The effort was discontinued in 1923 because of disappointing sales. The bicycles were built for Harley-Davidson in Dayton, Ohio by the Davis Machine Company from 1917 to 1921, when Davis stopped manufacturing bicycles.

1920s
By 1920 Harley-Davidson was the largest motorcycle manufacturer in the world, with 28,189 machines produced and dealers in 67 countries. In 1921, Otto Walker set a record on a Harley-Davidson as the first motorcycle to win a race at an average speed greater than 100 mph (160 km/h).Harley-Davidson put several improvements in place during the 1920s, such as a new 74 cubic inch (1,212.6  cc) V-Twin introduced in 1921, and the "teardrop" gas tank in 1925. They added a front brake in 1928, although only on the J/JD models. In the late summer of 1929, Harley-Davidson introduced its 45-cubic-inch (737 cc) flathead V-Twin to compete with the Indian 101 Scout and the Excelsior Super X. This was the "D" model produced from 1929 to 1931. Riders of Indian motorcycles derisively referred to it as the "three cylinder Harley" because the generator was upright and parallel to the front cylinder. In 1929, Vivian Bales drove a record 5,000 miles across the United States and Canada on a D-model.

Great Depression
The Great Depression began a few months after the introduction of their 45 cu in (740 cm3) model. Harley-Davidson's sales fell from 21,000 in 1929 to 3,703 in 1933. Despite this, Harley-Davidson unveiled a new lineup for 1934, which included a flathead engine and Art Deco styling.In order to survive the remainder of the Depression, the company manufactured industrial powerplants based on their motorcycle engines. They also designed and built a three-wheeled delivery vehicle called the Servi-Car, which remained in production until 1973.
Alfred Rich Child opened a production line in Japan in the mid-1930s with the 74 cu in (1,210 cm3) VL. The Japanese license-holder, Sankyo Seiyaku Corporation, severed its business relations with Harley-Davidson in 1936 and continued manufacturing the VL under the Rikuo name.
An 80 cubic inches (1,300 cm3) flathead engine was added to the line in 1935, by which time the single-cylinder motorcycles had been discontinued.In 1936, the 61E and 61EL models with the "Knucklehead" OHV engines were introduced. Valvetrain problems in early Knucklehead engines required a redesign halfway through its first year of production and retrofitting of the new valvetrain on earlier engines.By 1937, all Harley-Davidson flathead engines were equipped with dry-sump oil recirculation systems similar to the one introduced in the "Knucklehead" OHV engine. The revised 74 cubic inches (1,210 cm3) V and VL models were renamed U and UL, the 80 cu in (1,300 cm3) VH and VLH to be renamed UH and ULH, and the 45 cu in (740 cm3) R to be renamed W.In 1941, the 74-cubic-inch "Knucklehead" was introduced as the F and the FL. The 80 cu in (1,300 cm3) flathead UH and ULH models were discontinued after 1941, while the 74-cubic-inchU & UL flathead models were produced up to 1948.

World War II
One of only two American motorcycle manufacturers to survive the Great Depression (the other being the Indian Motorcycle Manufacturing Company), Harley-Davidson again produced large numbers of motorcycles for the US Army in World War II and resumed civilian production afterwards, producing a range of large V-twin motorcycles that were successful both on racetracks and for private buyers.
Harley-Davidson, on the eve of World War II, was already supplying the Army with a military-specific version of its 45 cubic inches (740 cm3) WL line, called the WLA. The A in this case stood for "Army". Upon the outbreak of war, the company, along with most other manufacturing enterprises, shifted to war work. More than 90,000 military motorcycles, mostly WLAs and WLCs (the Canadian version) were produced, many to be provided to allies. Harley-Davidson received two Army-Navy "E" Awards, one in 1943 and the other in 1945, which were awarded for Excellence in Production.

Shipments to the Soviet Union under the Lend-Lease program numbered at least 30,000. The WLAs produced during all four years of war production generally have 1942 serial numbers. Production of the WLA stopped at the end of World War II, but was resumed from 1950 to 1952 for use in the Korean War.
The U.S. Army also asked Harley-Davidson to produce a new motorcycle with many of the features of BMW's side-valve and shaft-driven R71. Harley-Davidson largely copied the BMW engine and drive train and produced the shaft-driven 750 cc 1942 Harley-Davidson XA. This shared no dimensions, no parts or no design concepts (except side valves) with any prior Harley-Davidson engine. Due to the superior cooling of the flat-twin engine with the cylinders across the frame, Harley's XA cylinder heads ran 100 °F (56 °C) cooler than its V-twins. The XA never entered full production: the motorcycle by that time had been eclipsed by the Jeep as the Army's general-purpose vehicle, and the WLA – already in production – was sufficient for its limited police, escort, and courier roles. Only 1,000 were made and the XA never went into full production. It remains the only shaft-driven Harley-Davidson ever made.

Small: Hummer, Sportcycle and Aermacchi
As part of war reparations, Harley-Davidson acquired the design of a small German motorcycle, the DKW RT 125, which they adapted, manufactured, and sold from 1948 to 1966. Various models were made, including the Hummer from 1955 to 1959, but they are all colloquially referred to as "Hummers" at present. BSA in the United Kingdom took the same design as the foundation of their BSA Bantam.
In 1960, Harley-Davidson consolidated the Model 165 and Hummer lines into the Super-10, introduced the Topper scooter, and bought fifty percent of Aermacchi's motorcycle division. Importation of Aermacchi's 250 cc horizontal single began the following year. The bike bore Harley-Davidson badges and was marketed as the Harley-Davidson Sprint. The engine of the Sprint was increased to 350 cc in 1969 and would remain that size until 1974, when the four-stroke Sprint was discontinued.After the Pacer and Scat models were discontinued at the end of 1965, the Bobcat became the last of Harley-Davidson's American-made two-stroke motorcycles. The Bobcat was manufactured only in the 1966 model year.Harley-Davidson replaced their American-made lightweight two-stroke motorcycles with the Italian Aermacchi-built two-stroke powered M-65, M-65S, and Rapido. The M-65 had a semi-step-through frame and tank. The M-65S was a M-65 with a larger tank that eliminated the step-through feature. The Rapido was a larger bike with a 125 cc engine. The Aermacchi-built Harley-Davidsons became entirely two-stroke powered when the 250 cc two-stroke SS-250 replaced the four-stroke 350 cc Sprint in 1974.Harley-Davidson purchased full control of Aermacchi's motorcycle production in 1974 and continued making two-stroke motorcycles there until 1978, when they sold the facility to Cagiva, owned by the Castiglioni family.

Tarnished reputation
In 1952, following their application to the U.S. Tariff Commission for a 40 percent tax on imported motorcycles, Harley-Davidson was charged with restrictive practices.
In 1969, American Machine and Foundry (AMF) bought the company, streamlined production, and slashed the workforce. This tactic resulted in a labor strike and cost-cutting produced lower-quality bikes. Simultaneously, the Japanese "big four" manufacturers (Honda, Kawasaki, Suzuki, and Yamaha) revolutionized the North American market by introducing what the motoring press would call the Universal Japanese Motorcycle. In comparison, Harley-Davidson's bikes were expensive and inferior in performance, handling, and quality. Sales and quality declined, and the company almost went bankrupt. The "Harley-Davidson" name was mocked as "Hardly Ableson", "Hardly Driveable", and "Hogly Ferguson",
and the nickname "Hog" became pejorative.In 1977, following the successful manufacture of the Liberty Edition to commemorate America's bicentennial in 1976, Harley-Davidson produced what has become one of its most controversial models, the Harley-Davidson Confederate Edition. The bike was essentially a stock Harley-Davidson with Confederate-specific paint and details.

Restructuring and revival
In 1981, AMF sold the company to a group of 13 investors led by Vaughn Beals and Willie G. Davidson for $80 million. The new management team improved product quality, introduced new technologies, and adopted just-in-time inventory management. These operational and product improvements were matched with a strategy of seeking tariff protection for large-displacement motorcycles in the face of intense competition with Japanese manufacturers. These protections were granted by the Reagan administration in 1983, giving Harley-Davidson time to implement their new strategies.Revising stagnated product designs was a crucial centerpiece of Harley-Davidson's turnaround strategy. Rather than trying to mimic popular Japanese designs, the new management deliberately exploited the "retro" appeal of Harley motorcycles, building machines that deliberately adopted the look and feel of their earlier bikes and the subsequent customizations of owners of that era. Many components such as brakes, forks, shocks, carburetors, electrics and wheels were outsourced from foreign manufacturers and quality increased, technical improvements were made, and buyers slowly returned.
Harley-Davidson bought the "Sub Shock" cantilever-swingarm rear suspension design from Missouri engineer Bill Davis and developed it into its Softail series of motorcycles, introduced in 1984 with the FXST Softail.In response to possible motorcycle market loss due to the aging of baby-boomers, Harley-Davidson bought luxury motorhome manufacturer Holiday Rambler in 1986. In 1996, the company sold Holiday Rambler to the Monaco Coach Corporation.The "Sturgis" model, boasting a dual belt-drive, was introduced initially in 1980 and was made for three years. This bike was then brought back as a commemorative model in 1991.

Fat Boy, Dyna, and Harley-Davidson museum
By 1990, with the introduction of the "Fat Boy", Harley-Davidson once again became the sales leader in the heavyweight (over 750 cc) market. At the time of the Fat Boy model introduction, a false etymology spread that "Fat Boy" was a combination of the names of the atomic bombs Fat Man and Little Boy. This has been debunked, as the name "Fat Boy" actually comes from the observation that the motorcycle is somewhat wider than other bikes when viewed head-on.1993 and 1994 saw the replacement of FXR models with the Dyna (FXD), which became the sole rubber mount FX Big Twin frame in 1994. The FXR was revived briefly from 1999 to 2000 for special limited editions (FXR2, FXR3 & FXR4).Harley-Davidson celebrated their 100th anniversary on September 1, 2003 with a large event and concert featuring performances from Elton John, The Doobie Brothers, Kid Rock, and Tim McGraw.Construction started on the $75 million, 130,000 square-foot (12,000 m2) Harley-Davidson Museum in the Menomonee Valley of Milwaukee, Wisconsin on June 1, 2006. It opened in 2008 and houses the company's vast collection of historic motorcycles and corporate archives, along with a restaurant, café and meeting space.

Overseas operations
Established in 1918, the oldest continuously operating Harley-Davidson dealership outside of the United States is in Australia. Sales in Japan started in 1912 then in 1929, Harley-Davidsons were produced in Japan under license to the company Rikuo (Rikuo Internal Combustion Company) under the name of Harley-Davidson and using the company's tooling, and later under the name Rikuo. Production continued until 1958.In 1998, the first Harley-Davidson factory outside the US opened in Manaus, Brazil, taking advantage of the free economic zone there. The location was positioned to sell motorcycles in the southern hemisphere market.In August 2009, Harley-Davidson launched Harley-Davidson India and started selling motorcycles there in 2010.  The company established the subsidiary in Gurgaon, near Delhi, in 2011 and created an Indian dealer network. On September 24, 2020, Harley Davidson announced that it would discontinue its sales and manufacturing operations in India due to weak demand and sales. The move involves $75 million in restructuring costs, 70 layoffs and the closure of its Bawal plant in northern India.

Buell Motorcycle Company
Harley-Davidson's association with sportbike manufacturer Buell Motorcycle Company began in 1987 when they supplied Buell with fifty surplus XR1000 engines. Buell continued to buy engines from Harley-Davidson until 1993, when Harley-Davidson bought 49 percent of the Buell Motorcycle Company. Harley-Davidson increased its share in Buell to ninety-eight percent in 1998, and to complete ownership in 2003.In an attempt to attract newcomers to motorcycling in general and to Harley-Davidson in particular, Buell developed a low-cost, low-maintenance motorcycle. The resulting single-cylinder Buell Blast was introduced in 2000, and was made through 2009, which, according to Buell, was to be the final year of production. The Buell Blast was the training vehicle for the Harley-Davidson Rider's Edge New Rider Course from 2000 until May 2014, when the company re-branded the training academy and started using the Harley-Davidson Street 500 motorcycles. In those 14 years, more than 350,000 participants in the course learned to ride on the Buell Blast.On October 15, 2009, Harley-Davidson Inc. issued an official statement that it would be discontinuing the Buell line and ceasing production immediately, in order to focus on the Harley-Davidson brand. The company refused to consider selling Buell. Founder Erik Buell subsequently established Erik Buell Racing and continued to manufacture and develop the company's 1125RR racing motorcycle.

Claims of stock price manipulation
During its period of peak demand, during the late 1990s and early first decade of the 21st century, Harley-Davidson embarked on a program of expanding the number of dealerships throughout the country. At the same time, its current dealers typically had waiting lists that extended up to a year for some of the most popular models. Harley-Davidson, like the auto manufacturers, records a sale not when a consumer buys their product, but rather when it is delivered to a dealer. Therefore, it is possible for the manufacturer to inflate sales numbers by requiring dealers to accept more inventory than desired in a practice called channel stuffing. When demand softened following the unique 2003 model year, this news led to a dramatic decline in the stock price. In April 2004 alone, the price of HOG shares dropped from more than $60 to less than $40. Immediately prior to this decline, retiring CEO Jeffrey Bleustein profited $42 million on the exercise of employee stock options. Harley-Davidson was named as a defendant in numerous class action suits filed by investors who claimed they were intentionally defrauded by Harley-Davidson's management and directors. By January 2007, the price of Harley-Davidson shares reached $70.

Problems with Police Touring models
Starting around 2000, several police departments started reporting problems with high-speed instability on the Harley-Davidson Touring motorcycles.  A Raleigh, North Carolina police officer, Charles Paul, was killed when his 2002 police touring motorcycle crashed after reportedly experiencing a high-speed wobble. The California Highway Patrol conducted testing of the Police Touring motorcycles in 2006.  The CHP test riders reported experiencing wobble or weave instability while operating the motorcycles on the test track.

2007 strike
On February 2, 2007, upon the expiration of their union contract, about 2,700 employees at Harley-Davidson Inc.'s largest manufacturing plant in York, Pennsylvania, went on strike after failing to agree on wages and health benefits. During the pendency of the strike, the company refused to pay for any portion of the striking employees' health care.The day before the strike, after the union voted against the proposed contract and to authorize the strike, the company shut down all production at the plant. The York facility employs more than 3,200 workers, both union and non-union.Harley-Davidson announced on February 16, 2007, that it had reached a labor agreement with union workers at its largest manufacturing plant, a breakthrough in the two-week-old strike. The strike disrupted Harley-Davidson's national production and was felt in Wisconsin, where 440 employees were laid off, and many Harley suppliers also laid off workers because of the strike.

MV Agusta Group
On July 11, 2008, Harley-Davidson announced they had signed a definitive agreement to acquire the MV Agusta Group for US$109 million (€70M). MV Agusta Group contains two lines of motorcycles: the high-performance MV Agusta brand and the lightweight Cagiva brand. The acquisition was completed on August 8.On October 15, 2009, Harley-Davidson announced that it would divest its interest in MV Agusta.  Harley-Davidson Inc. sold Italian motorcycle maker MV Agusta to Claudio Castiglioni – a member of the family that had purchased Aermacchi from H-D in 1978 – for a reported 3 euros, ending the transaction in the first week of August 2010. Castiglioni was MV Agusta's former owner, and had been MV Agusta's chairman since Harley-Davidson bought it in 2008.  As part of the deal, Harley-Davidson put $26M into MV Agusta's accounts, essentially giving Castiglioni $26M to take the brand.

Financial crisis
The 2007–2008 financial crisis and 2008–2010 automotive industry crisis affected also the motorcycle industry. According to Interbrand, the value of the Harley-Davidson brand fell by 43 percent to $4.34 billion in 2009. The fall in value is believed to be connected to the 66 percent drop in the company profits in two-quarters of the previous year. On April 29, 2010, Harley-Davidson stated that they must cut $54 million in manufacturing costs from its production facilities in Wisconsin, and that they would explore alternative U.S. sites to accomplish this. The announcement came in the wake of a massive company-wide restructuring, which began in early 2009 and involved the closing of two factories, one distribution center, and the planned elimination of nearly 25 percent of its total workforce (around 3,500 employees). The company announced on September 14, 2010, that it would remain in Wisconsin.

Motorcycle engines
The classic Harley-Davidson engines are V-twin engines, with a 45° angle between the cylinders. The crankshaft has a single pin, and both pistons are connected to this pin through their connecting rods.This 45° angle is covered under several United States patents and is an engineering tradeoff that allows a large, high-torque engine in a relatively small space. It causes the cylinders to fire at uneven intervals and produces the choppy "potato-potato" sound so strongly linked to the Harley-Davidson brand.
To simplify the engine and reduce costs, the V-twin ignition was designed to operate with a single set of points and no distributor. This is known as a dual fire ignition system, causing both spark plugs to fire regardless of which cylinder was on its compression stroke, with the other spark plug firing on its cylinder's exhaust stroke, effectively "wasting a spark". The exhaust note is basically a throaty growling sound with some popping.
The 45° design of the engine thus creates a plug firing sequencing as such: The first cylinder fires, the second (rear) cylinder fires 315° later, then there is a 405° gap until the first cylinder fires again, giving the engine its unique sound.Harley-Davidson has used various ignition systems, including the early points and condenser system on Big Twins and Sportsters up to 1978, a magneto ignition system used on some 1958 to 1969 Sportsters, an early electronic with centrifugal mechanical advance weights on all models from mid-1978 until 1979, and a later electronic with a transistorized ignition control module (more familiarly known as a black box or a brain) on all models 1980 to present.
Starting in 1995, the company introduced Electronic Fuel Injection (EFI) as an option for the 30th anniversary edition Electra Glide. EFI became standard on all Harley-Davidson motorcycles, including Sportsters, upon the introduction of the 2007 product line.In 1991, Harley-Davidson began to participate in the Sound Quality Working Group, founded by Orfield Labs, Bruel and Kjaer, TEAC, Yamaha, Sennheiser, SMS and Cortex. This was the nation's first group to share research on psychological acoustics. Later that year, Harley-Davidson participated in a series of sound quality studies at Orfield Labs, based on recordings taken at the Talladega Superspeedway, with the objective to lower the sound level for EU standards while analytically capturing the "Harley Sound". This research resulted in the bikes that were introduced in compliance with EU standards for 1998.
On February 1, 1994, the company filed a sound trademark application for the distinctive sound of the Harley-Davidson motorcycle engine: "The mark consists of the exhaust sound of applicant's motorcycles, produced by V-twin, common crankpin motorcycle engines when the goods are in use". Nine of Harley-Davidson's competitors filed comments opposing the application, arguing that cruiser-style motorcycles of various brands use a single-crankpin V-twin engine which produce a similar sound. These objections were followed by litigation. In June 2000, the company dropped efforts to register a sound trademark.

Big V-twins
F-head, also known as JD, pocket valve and IOE (intake over exhaust), 1914–1929 (1,000 cc), and 1922–1929 (1,200 cc)
Flathead, 1930–1949 (1,200 cc) and 1935–1941 (1,300 cc).
Knucklehead, 1936–1947 61 cubic inch (1,000 cc), and 1941–1947 74 cubic inch (1,200 cc)
Panhead, 1948–1965 61 cubic inch (1,000 cc), and 1948–1965, 74 cubic inch (1,200 cc)
Shovelhead, 1966–1984, 74 cubic inch (1,200 cc) and 80 cubic inch (1,338 cc) since late 1978
Evolution (a.k.a. "Evo" and "Blockhead"), 1984–1999, 80 cubic inch (1,340 cc)
Twin Cam (a.k.a. "Fathead" as named by American Iron Magazine) 1999–2017, in the following versions:
Twin Cam 88, 1999–2006, 88 cubic inch (1,450 cc)
Twin Cam 88B, counterbalanced version of the Twin Cam 88, 2000–2006, 88 cubic inch (1,450 cc)
Twin Cam 95, since 2000, 95 cubic inch (1,550 cc) (engines for early C.V.O. models)
Twin Cam 96, since 2007.
Twin Cam 103, 2003–2006, 2009, 103 cubic inch (1,690 cc) (engines for C.V.O. models), Standard on 2011 Touring models: Ultra Limited, Road King Classic and Road Glide Ultra and optional on the Road Glide Custom and Street Glide. Standard on most 2012 models excluding Sportsters and 2 Dynas (Street Bob and Super Glide Custom). Standard on all 2014 dyna models.
Twin Cam 110, 2007–2017, 110 cubic inch (1,800 cc) (engines for C.V.O. models, 2016 Soft Tail Slim S; FatBoy S, Low Rider S, and Pro-Street Breakout)
Milwaukee-Eight
Standard 107 cu in (1,746 cc): Standard on touring model year 2017+ and Softail models 2018+.
Twin-cooled 107 cu in (1,746 cc): Optional on some touring and trike model year 2017+.
Twin-cooled 114 cu in (1,868 cc): Optional on touring and trike model year 2017+, standard on 2017 CVO models.
Twin-cooled 117 cu in (1,923 cc): Standard on 2018 CVO models

Small V-twins
D Model, 1929–1931, 750 cc
R Model, 1932–1936, 750 cc
Flathead 750 cc
1937–1952 W Model solo 2 wheel
1932–1973 G Model Servi-Car three-wheeler
K Model, 1952–1953, 750 cc
KH Model, 1954–1956, 900 cc
Ironhead, 1957–1971, 883 cc; 1972–1985, 1,000 cc
Evolution, since 1986, 883 cc, 1,100 cc and 1,200 cc

Revolution engine
The Revolution engine is based on the VR-1000 Superbike race program, developed by Harley-Davidson's Powertrain Engineering with Porsche helping to make the engine suitable for street use. It is a liquid cooled, dual overhead cam, internally counterbalanced 60 degree V-twin engine with a displacement of 69 cubic inch (1,130 cc), producing 115 hp (86 kW) at 8,250 rpm at the crank, with a redline of 9,000 rpm. It was introduced for the new VRSC (V-Rod) line in 2001 for the 2002 model year, starting with the single VRSCA (V-Twin Racing Street Custom) model. The Revolution marks Harley's first collaboration with Porsche since the V4 Nova project, which, like the V-Rod, was a radical departure from Harley's traditional lineup until it was cancelled by AMF in 1981 in favor of the Evolution engine.A 1,250 cc Screamin' Eagle version of the Revolution engine was made available for 2005 and 2006, and was present thereafter in a single production model from 2005 to 2007. In 2008, the 1,250 cc Revolution Engine became standard for the entire VRSC line. Harley-Davidson claims 123 hp (92 kW) at the crank for the 2008 VRSCAW model. The VRXSE Destroyer dragbike is equipped with a stroker (75 mm crank) Screamin' Eagle 79 cubic inch (1,300 cc) Revolution Engine, producing 97 pound-feet (132 N⋅m), and more than 165 hp (123 kW).
750 cc and 500 cc versions of the Revolution engine are used in Harley-Davidson's Street line of light cruisers. These motors, named the Revolution X, use a single overhead cam, screw and locknut valve adjustment, a single internal counterbalancer, and vertically split crankcases; all of these changes making it different from the original Revolution design.

Düsseldorf-Test
An extreme endurance test of the Revolution engine was performed in a dynamometer installation at the Harley-Davidson factory in Milwaukee, simulating the German Autobahn (highways without general speed limit) between the Porsche research and development center in Weissach, near Stuttgart to Düsseldorf. An undisclosed number of samples of engines failed, until an engine successfully passed the 500-hour nonstop run. This was the benchmark for the engineers to approve the start of production for the Revolution engine, which was documented in the Discovery channel special Harley-Davidson: Birth of the V-Rod, October 14, 2001.

Single-cylinder engines
IOE singlesThe first Harley-Davidson motorcycles were powered by single-cylinder IOE engines with the inlet valve operated by engine vacuum, based on the DeDion-Bouton pattern. Singles of this type continued to be made until 1913, when a pushrod and rocker system was used to operate the overhead inlet valve on the single, a similar system having been used on their V-twins since 1911. Single-cylinder motorcycle engines were discontinued in 1918.
Flathead and OHV singlesSingle-cylinder engines were reintroduced in 1925 as 1926 models. These singles were available either as flathead engines or as overhead valve engines until 1930, after which they were only available as flatheads. The flathead single-cylinder motorcycles were designated Model A for engines with magneto systems only and Model B for engines with battery and coil systems, while overhead valve versions were designated Model AA and Model BA respectively, and a magneto-only racing version was designated Model S. This line of single-cylinder motorcycles ended production in 1934.
Two-stroke singles

Model families
Modern Harley-branded motorcycles fall into one of seven model families: Touring, Softail, Dyna, Sportster, Vrod, Street and LiveWire. These model families are distinguished by the frame, engine, suspension, and other characteristics.

Touring
Touring models use Big-Twin engines and large-diameter telescopic forks. All Touring designations begin with the letters FL, e.g., FLHR (Road King) and FLTR (Road Glide).
The touring family, also known as "dressers" or "baggers", includes Road King, Road Glide, Electra Glide and Street Glide models offered in various trims. The Road Kings have a "retro cruiser" appearance and are equipped with a large clear windshield. Road Kings are reminiscent of big-twin models from the 1940s and 1950s. Electra Glides can be identified by their full front fairings. Most Electra Glides sport a fork-mounted fairing referred to as the "Batwing" due to its unmistakable shape. The Road Glide and Road Glide Ultra Classic have a frame-mounted fairing, referred to as the "Sharknose". The Sharknose includes a unique, dual front headlight.
Touring models are distinguishable by their large saddlebags, rear coil-over air suspension and are the only models to offer full fairings with radios and CBs. All touring models use the same frame, first introduced with a Shovelhead motor in 1980, and carried forward with only modest upgrades until 2009, when it was extensively redesigned. The frame is distinguished by the location of the steering head in front of the forks and was the first H-D frame to rubber mount the drivetrain to isolate the rider from the vibration of the big V-twin.

The frame was modified for the 1993 model year when the oil tank went under the transmission and the battery was moved inboard from under the right saddlebag to under the seat. In 1997, the frame was again modified to allow for a larger battery under the seat and to lower seat height. In 2007, Harley-Davidson introduced the 96 cubic inches (1,570 cubic centimetres) Twin Cam 96 engine, as well the six-speed transmission to give the rider better speeds on the highway.
In 2006, Harley introduced the FLHX Street Glide, a bike designed by Willie G. Davidson to be his personal ride, to its touring line.In 2008, Harley added anti-lock braking systems and cruise control as a factory installed option on all touring models (standard on CVO and Anniversary models). Also new for 2008 is the 6-US-gallon (23 L; 5.0 imp gal) fuel tank for all touring models. 2008 also brought throttle-by-wire to all touring models.
For the 2009 model year, Harley-Davidson redesigned the entire touring range with several changes, including a new frame, new swingarm, a completely revised engine-mounting system, 17-inch (430 mm) front wheels for all but the FLHRC Road King Classic, and a 2–1–2 exhaust. The changes result in greater load carrying capacity, better handling, a smoother engine, longer range and less exhaust heat transmitted to the rider and passenger.
Also released for the 2009 model year is the FLHTCUTG Tri-Glide Ultra Classic, the first three-wheeled Harley since the Servi-Car was discontinued in 1973. The model features a unique frame and a 103-cubic-inch (1,690 cc) engine exclusive to the trike.In 2014, Harley-Davidson released a redesign for specific touring bikes and called it "Project Rushmore". Changes include a new 103CI High Output engine, one handed easy open saddlebags and compartments, a new Boom! Box Infotainment system with either 4.3-inch (10 cm) or 6.5-inch (16.5 cm) screens featuring touchscreen functionality [6.5-inch (16.5 cm) models only], Bluetooth (media and phone with approved compatible devices), available GPS and SiriusXM, Text-to-Speech functionality (with approved compatible devices) and USB connectivity with charging. Other features include ABS with Reflex linked brakes, improved styling, Halogen or LED lighting and upgraded passenger comfort.

Softail
These big-twin motorcycles capitalize on Harley's strong value on tradition. With the rear-wheel suspension hidden under the transmission, they are visually similar to the "hardtail" choppers popular in the 1960s and 1970s, as well as from their own earlier history. In keeping with that tradition, Harley offers Softail models with "Heritage" styling that incorporate design cues from throughout their history and used to offer "Springer" front ends on these Softail models from the factory.

DesignationSoftail models utilize the big-twin engine (F) and the Softail chassis (ST).

Softail models that use 21-inch (530 mm) Front Wheels have designations that begin with FX, e.g., FXSTB (Night Train), FXSTD (Deuce), and FXSTS (Springer).
Softail models that use 16-inch (410 mm) Front Wheels have designations beginning with FL, e.g., FLSTF (Fat Boy), FLSTC (Heritage Softail Classic), FLSTN (Softail Deluxe) and FLS (Softail Slim).
Softail models that use Springer forks with a 21-inch (530 mm) wheel have designations that begin with FXSTS, e.g., FXSTS (Springer Softail) and FXSTSB (Bad Boy).
Softail models that use Springer forks with a 16-inch (410 mm) wheel have designations that begin with FLSTS, e.g., FLSTSC (Springer Classic) and FLSTSB (Cross Bones).

Dyna
Dyna-frame motorcycles were developed in the 1980s and early 1990s and debuted in the 1991 model year with the FXDB Sturgis offered in limited edition quantities. In 1992 the line continued with the limited edition FXDB Daytona and a production model FXD Super Glide. The new DYNA frame featured big-twin engines and traditional styling. They can be distinguished from the Softail by the traditional coil-over suspension that connects the swingarm to the frame, and from the Sportster by their larger engines. On these models, the transmission also houses the engine's oil reservoir.
Prior to 2006, Dyna models typically featured a narrow, XL-style 39mm front fork and front wheel, as well as footpegs which the manufacturer indicated with the letter "X" in the model designation. This lineup traditionally included the Super Glide (FXD), Super Glide Custom (FXDC), Street Bob (FXDB), and Low Rider (FXDL). One exception was the Wide Glide (FXDWG), which featured thicker 41mm forks and a narrow front wheel, but positioned the forks on wider triple-trees that give a beefier appearance. In 2008, the Dyna Fat Bob (FXDF) was introduced to the Dyna lineup, featuring aggressive styling like a new 2–1–2 exhaust, twin headlamps, a 180 mm rear tire, and, for the first time in the Dyna lineup, a 130 mm front tire. For the 2012 model year, the Dyna Switchback (FLD) became the first Dyna to break the tradition of having an FX model designation with floorboards, detachable painted hard saddlebags, touring windshield, headlight nacelle and a wide front tire with full fender.  The new front end resembled the big-twin FL models from 1968 to 1971.
The Dyna family used the 88-cubic-inch (1,440 cc) twin cam from 1999 to 2006. In 2007, the displacement was increased to 96 cubic inches (1,570 cc) as the factory increased the stroke to 4.375 inches (111.1 mm). For the 2012 model year, the manufacturer began to offer Dyna models with the 103-cubic-inch (1,690 cc) upgrade.  All Dyna models use a rubber-mounted engine to isolate engine vibration. Harley discontinued the Dyna platform in 2017 for the 2018 model year, having been replaced by a completely-redesigned Softail chassis; some of the existing models previously released by the company under the Dyna nameplate have since been carried over to the new Softail line.

DesignationDyna models utilize the big-twin engine (F), footpegs noted as (X) with the exception of the 2012 FLD Switchback, a Dyna model which used floorboards as featured on the Touring (L) models, and the Dyna chassis (D). Therefore, except for the FLD from 2012 to 2016, all Dyna models have designations that begin with FXD, e.g., FXDWG (Dyna Wide Glide) and FXDL (Dyna Low Rider).

Sportster
Introduced in 1957, the Sportster family were conceived as racing motorcycles, and were popular on dirt and flat-track race courses through the 1960s and 1970s. Smaller and lighter than the other Harley models, contemporary Sportsters make use of 883 cc or 1,200 cc Evolution engines and, though often modified, remain similar in appearance to their racing ancestors.Up until the 2003 model year, the engine on the Sportster was rigidly mounted to the frame. The 2004 Sportster received a new frame accommodating a rubber-mounted engine. This made the bike heavier and reduced the available lean angle, while it reduced the amount of vibration transmitted to the frame and the rider, providing a smoother ride for rider and passenger.In the 2007 model year, Harley-Davidson celebrated the 50th anniversary of the Sportster and produced a limited edition called the XL50, of which only 2000 were made for sale worldwide. Each motorcycle was individually numbered and came in one of two colors, Mirage Pearl Orange or Vivid Black. Also in 2007, electronic fuel injection was introduced to the Sportster family, and the Nightster model was introduced in mid-year. In 2009, Harley-Davidson added the Iron 883 to the Sportster line, as part of the Dark Custom series.
In the 2008 model year, Harley-Davidson released the XR1200 Sportster in Europe, Africa, and the Middle East. The XR1200 had an Evolution engine tuned to produce 91 bhp (68 kW), four-piston dual front disc brakes, and an aluminum swing arm.  Motorcyclist featured the XR1200 on the cover of its July 2008 issue and was generally positive about it in their "First Ride" story, in which Harley-Davidson was repeatedly asked to sell it in the United States.
One possible reason for the delayed availability in the United States was that Harley-Davidson had to obtain the "XR1200" naming rights from Storz Performance, a Harley customizing shop in Ventura, Calif. The XR1200 was released in the United States in 2009 in a special color scheme including Mirage Orange highlighting its dirt-tracker heritage. The first 750 XR1200 models in 2009 were pre-ordered and came with a number 1 tag for the front of the bike, autographed by Kenny Coolbeth and Scott Parker and a thank you/welcome letter from the company, signed by Bill Davidson. The XR1200 was discontinued in model year 2013.
In 2021, Harley-Davidson launched the Sportster S model, with a 121 hp engine and 228 Kg ready-to-ride weight. The Sportster S was one of the first Harleys to come with cornering-ABS and lean-sensitive traction control. The Sportster S is also the first model under the Sportster nameplate since 1957 to receive a completely new engine.

DesignationExcept for the street-going XR1000 of the 1980s and the XR1200, most Sportsters made for street use have the prefix XL in their model designation. For the Sportster Evolution engines used since the mid-1980s, there have been two engine sizes. Motorcycles with the smaller engine are designated XL883, while those with the larger engine were initially designated XL1100. When the size of the larger engine was increased from 1,100 cc to 1,200 cc, the designation was changed accordingly from XL1100 to XL1200. Subsequent letters in the designation refer to model variations within the Sportster range, e.g. the XL883C refers to an 883 cc Sportster Custom, while the XL1200S designates the now-discontinued 1200 Sportster Sport.

VRSC
Introduced in 2001 and produced until 2017, the VRSC muscle bike family bears little resemblance to Harley's more traditional lineup. Competing against Japanese and American muscle bikes in the upcoming muscle bike/power cruiser segment, the "V-Rod" makes use of the revolution engine that, for the first time in Harley history, incorporates overhead cams and liquid cooling. The V-Rod is visually distinctive, easily identified by the 60-degree V-Twin engine, the radiator and the hydroformed frame members that support the round-topped air cleaner cover. The VRSC platform was also used for factory drag-racing motorcycles.
In 2008, Harley added the anti-lock braking system as a factory-installed option on all VRSC models. Harley also increased the displacement of the stock engine from 1,130 to 1,250 cc (69 to 76 cu in), which had only previously been available from Screamin' Eagle, and added a slipper clutch as standard equipment.
VRSC models include:

VRSCA: V-Rod (2002–2006), VRSCAW: V-Rod (2007–2010), VRSCB: V-Rod (2004–2005), VRSCD: Night Rod (2006–2008), VRSCDX: Night Rod Special (2007–2014), VRSCSE: Screamin' Eagle CVO V-Rod (2005), VRSCSE2: Screamin' Eagle CVO V-Rod (2006), VRSCR: Street Rod (2006–2007), VRSCX: Screamin' Eagle Tribute V-Rod (2007), VRSCF: V-Rod Muscle (2009–2014).VRSC models utilize the Revolution engine (VR), and the street versions are designated Street Custom (SC). After the VRSC prefix common to all street Revolution bikes, the next letter denotes the model, either A (base V-Rod: discontinued), AW (base V-Rod + W for Wide with a 240 mm rear tire), B (discontinued), D (Night Rod: discontinued), R (Street Rod: discontinued), SE and SEII (CVO Special Edition), or X (Special edition). Further differentiation within models are made with an additional letter, e.g., VRSCDX denotes the Night Rod Special.

VRXSE
The VRXSE V-Rod Destroyer is Harley-Davidson's production drag racing motorcycle, constructed to run the quarter mile in less than ten seconds. It is based on the same revolution engine that powers the VRSC line, but the VRXSE uses the Screamin' Eagle 1,300 cc "stroked" incarnation, featuring a 75 mm crankshaft, 105 mm Pistons, and 58 mm throttle bodies.
The V-Rod Destroyer is not a street-legal motorcycle. As such, it uses "X" instead of "SC" to denote a non-street bike.  "SE" denotes a CVO Special Edition.

Street
The Street, Harley-Davidson's newest platform and their first all new platform in thirteen years, was designed to appeal to younger riders looking for a lighter bike at a cheaper price. The Street 750 model was launched in India at the 2014 Indian Auto Expo, Delhi-NCR on February 5, 2014. The Street 750 weighs 218 kg and has a ground clearance of 144 mm giving it the lowest weight and the highest ground clearance of Harley-Davidson motorcycles currently available.The Street 750 uses an all-new, liquid-cooled, 60° V-twin engine called the Revolution X. In the Street 750, the engine displaces 749 cc (45.7 cu in) and produces 65 Nm at 4,000 rpm. A six speed transmission is used.The Street 750 and the smaller-displacement Street 500 have been available since late 2014. Street series motorcycles for the North American market will be built in Harley-Davidson's Kansas City, Missouri plant, while those for other markets around the world will be built completely in their plant in Bawal, India.

LiveWire
Harley-Davidson's LiveWire, released in 2019, is their first electric vehicle. The high-voltage battery provides a minimum city range of 98 miles (158 km). The LiveWire targets a different type of customer than their classic V-twin powered motorcycles.In March 2020, a Harley-Davidson LiveWire was used to break the 24-hour distance record for an electric motorcycle. The bike traveled a reported 1,723 km (1,079 miles) in 23 hours and 48 minutes. The LiveWire offers a Level 1 slow recharge, which uses a regular wall outlet to refill an empty battery overnight, or a quick Level 3 DC Fast Charge. The Fast Charge fills the battery most of the way in about 40 minutes. Swiss rider Michel von Tell used the Level 3 charging to make the 24-hour ride.In December 2021, the company announced that that LiveWire was to be spun-off from parent Harley Davidson, set to go public in the first half of 2022 as a special-purpose acquisition company (SPAC) with the value estimated to be $1.77 billion.

Custom Vehicle Operations
Custom Vehicle Operations (CVO) is a team within Harley-Davidson that produces limited-edition customizations of Harley's stock models. Every year since 1999, the team has selected two to five of the company's base models and added higher-displacement engines, performance upgrades, special-edition paint jobs, more chromed or accented components, audio system upgrades, and electronic accessories to create high-dollar, premium-quality customizations for the factory custom market. The models most commonly upgraded in such a fashion are the Ultra Classic Electra Glide, which has been selected for CVO treatment every year from 2006 to the present, and the Road King, which was selected in 2002, 2003, 2007, and 2008. The Dyna, Softail, and VRSC families have also been selected for CVO customization.

Environmental record
The Environmental Protection Agency conducted emissions-certification and representative emissions test in Ann Arbor, Michigan, in 2005. Subsequently, Harley-Davidson produced an "environmental warranty". The warranty ensures each owner that the vehicle is designed and built free of any defects in materials and workmanship that would cause the vehicle to not meet EPA standards. In 2005, the EPA and the Pennsylvania Department of Environmental Protection (PADEP) confirmed Harley-Davidson to be the first corporation to voluntarily enroll in the One Clean-Up Program. This program is designed for the clean-up of the affected soil and groundwater at the former York Naval Ordnance Plant. The program is backed by the state and local government along with participating organizations and corporations.Paul Gotthold, Director of Operations for the EPA, congratulated the motor company:

Harley-Davidson has taken their environmental responsibilities very seriously and has already made substantial progress in the investigation and cleanup of past contamination. Proof of Harley's efforts can be found in the recent EPA determination that designates the Harley property as 'under control' for cleanup purposes. This determination means that there are no serious contamination problems at the facility. Under the new One Cleanup Program, Harley, EPA, and PADEP will expedite the completion of the property investigation and reach a final solution that will permanently protect human health and the environment.
Harley-Davidson also purchased most of Castalloy, a South Australian producer of cast motorcycle wheels and hubs. The South Australian government has set forth "protection to the purchaser (Harley-Davidson) against environmental risks".In August 2016, Harley-Davidson settled with the EPA for $12 million, without admitting wrongdoing, over the sale of after-market "super tuners". Super tuners were devices, marketed for competition, which enabled increased performance of Harley-Davidson products. However, the devices also modified the emission control systems, producing increased hydrocarbon and nitrogen oxide. Harley-Davidson is required to buy back and destroy any super tuners which do not meet Clean Air Act requirements and spend $3 million on air pollution mitigation.

Brand culture
According to a recent Harley-Davidson study, in 1987 half of all Harley riders were under age 35. However, by 2006, only 15 percent of Harley buyers were under 35, and as of 2005, the median age had risen to 46.7. In 2008, Harley-Davidson stopped disclosing the average age of riders; at this point it was 48 years old.In 1987, the median household income of a Harley-Davidson rider was $38,000. By 1997, the median household income for those riders had more than doubled, to $83,000.Many Harley-Davidson Clubs exist nowadays around the world; the oldest one, founded in 1928, is in Prague.Harley-Davidson attracts a loyal brand community, with licensing of the Harley-Davidson logo accounting for almost 5 percent of the company's net revenue ($41 million in 2004). Harley-Davidson supplies many American police forces with their motorcycle fleets.From its founding, Harley-Davidson had worked to brand its motorcycles as respectable and refined products, with ads that showed what motorcycling writer Fred Rau called "refined-looking ladies with parasols, and men in conservative suits as the target market". The 1906 Harley-Davidson's effective, and polite, muffler was emphasized in advertisements with the nickname "The Silent Gray Fellow". That began to shift in the 1960s, partially in response to the clean-cut motorcyclist portrayed in Honda's "You meet the nicest people on a Honda" campaign, when Harley-Davidson sought to draw a contrast with Honda by underscoring the more working-class, macho, and even a little anti-social attitude associated with motorcycling's dark side. With the 1971 FX Super Glide, the company embraced, rather than distanced itself from, chopper style and the counterculture custom Harley scene. Their marketing cultivated the "bad boy" image of biker and motorcycle clubs, and to a point, even outlaw or one-percenter motorcycle clubs.

Origin of "Hog" nickname
Beginning in 1920, a team of farm boys, including Ray Weishaar, who became known as the "hog boys", consistently won races. The group had a live hog as their mascot. Following a win, they would put the hog on their Harley and take a victory lap. In 1983, the Motor Company formed a club for owners of its product, taking advantage of the long-standing nickname by turning "hog" into the acronym HOG, for Harley Owners Group. Harley-Davidson attempted to trademark "hog", but lost a case against an independent Harley-Davidson specialist, The Hog Farm of West Seneca, New York, in 1999, when the appellate panel ruled that "hog" had become a generic term for large motorcycles and was therefore unprotectable as a trademark.On August 15, 2006, Harley-Davidson Inc. had its NYSE ticker symbol changed from HDI to HOG.

Bobbers
Harley-Davidson FL "big twins" normally had heavy steel fenders, chrome trim, and other ornate and heavy accessories. After World War II, riders wanting more speed would often shorten the fenders or take them off completely to reduce the weight of the motorcycle. These bikes were called "bobbers" or sometimes "choppers", because parts considered unnecessary were chopped off. Those who made or rode choppers and bobbers, especially members of motorcycle clubs like the Hells Angels, referred to stock FLs as "garbage wagons".

Harley Owners Group
Harley-Davidson established the Harley Owners Group (HOG) in 1983 to build on the loyalty of Harley-Davidson enthusiasts as a means to promote a lifestyle alongside its products. The HOG also opened new revenue streams for the company, with the production of tie-in merchandise offered to club members, numbering more than one million. Other motorcycle brands,
and other and consumer brands outside motorcycling, have also tried to create factory-sponsored community marketing clubs of their own.
HOG members typically spend 30 percent more than other Harley owners on such items as clothing and Harley-Davidson-sponsored events.In 1991, HOG went international, with the first official European HOG Rally in Cheltenham, England.
Today, more than one million members and more than 1400 chapters worldwide make HOG the largest factory-sponsored motorcycle organization in the world.HOG benefits include organized group rides, exclusive products and product discounts, insurance discounts, and the Hog Tales newsletter. A one-year full membership is included with the purchase of a new, unregistered Harley-Davidson.In 2008, HOG celebrated its 25th anniversary in conjunction with the Harley 105th in Milwaukee, Wisconsin.
3rd Southern HOG Rally set to bring together largest gathering of Harley-Davidson owners in South India. More than 600 Harley-Davidson Owners expected to ride to Hyderabad from across 13 HOG Chapters.

Factory tours and museum
Harley-Davidson offers factory tours at four of its manufacturing sites, and the Harley-Davidson Museum, which opened in 2008, exhibits Harley-Davidson's history, culture, and vehicles, including the motor company's corporate archives.
York, Pennsylvania – Vehicle Operations: Manufacturing site for Touring class, Softail, and custom vehicles.
Tomahawk, Wisconsin – Tomahawk Operations: Facility that makes sidecars, saddlebags, windshields, and more.
Kansas City, Missouri – Vehicle and Powertrain Operations: Manufacturing site of Sportster, VRSC, and other vehicles.
Menomonee Falls, Wisconsin – Pilgrim Road Powertrain Operations plant, two types of tours.
Milwaukee, Wisconsin – Harley-Davidson Museum: Archive; exhibits of people, products, culture and history; restaurant & café; and museum store.Due to the consolidation of operations, the Capitol Drive Tour Center in Wauwatosa, Wisconsin, was closed in 2009.

Historic register designations
Some of the company's buildings have been listed on state and national historic registers, including:

Harley-Davidson Motorcycle Factory Building – added to National Register of Historic Places on November 9, 1994.
Factory No. 7 – added to Wisconsin State Register of Historic Places on August 14, 2020.

Anniversary celebrations
Beginning with Harley-Davidson's 90th anniversary in 1993, Harley-Davidson has had celebratory rides to Milwaukee called the "Ride Home". This new tradition has continued every five years, and is referred to unofficially as "Harleyfest", in line with Milwaukee's other festivals (Summerfest, German fest, Festa Italiana, etc.). This event brings Harley riders from all around the world. The 105th anniversary celebration was held on August 28–31, 2008, and included events in Milwaukee, Waukesha, Racine, and Kenosha counties, in Southeast Wisconsin. The 110th-anniversary celebration was held on August 29–31, 2013. The 115th anniversary was held in Prague, Czech Republic, the home country of the oldest existing Harley Davidson Club, on July 5–8, 2018 and attracted more than 100,000 visitors and 60,000 bikes.
The 120th anniversary was held in Budapest, Hungary, with the parade on June 24.

Labor Hall of Fame
William S. Harley, Arthur Davidson, William A. Davidson and Walter Davidson Sr were, in 2004, inducted into the Labor Hall of Fame for their accomplishments for the H-D company and its workforce.

Television drama
The company's origins were dramatized in a 2016 miniseries entitled Harley and the Davidsons, starring Robert Aramayo as William Harley, Bug Hall as Arthur Davidson and Michiel Huisman as Walter Davidson, and premiered on the Discovery Channel as a "three-night event series" on September 5, 2016.

See also
List of Harley-Davidson motorcycles
Category:Harley-Davidson engines
Harley-Davidson (Bally pinball)
Harley-Davidson (Sega/Stern pinball)
Harley-Davidson & L.A. Riders
Harley-Davidson: Race Across America
List of motor scooter manufacturers and brands

References
Further reading
Videos
"Why Harley-Davidson Is Struggling In India". CNBC. May 28, 2019. Archived from the original on December 11, 2021.

External links

Official website 
Business data for Harley-Davidson:

Henry Ford

Henry Ford (July 30, 1863 – April 7, 1947) was an American industrialist and business magnate. He was the founder of Ford Motor Company, and chief developer of the assembly line technique of mass production. Ford created the first automobile that middle-class Americans could afford, and his conversion of the automobile from an expensive luxury into an accessible conveyance profoundly impacted the landscape of the 20th century.
Ford was born on a farm in Michigan's Springwells Township, leaving home at age 16 to work in Detroit. It was a few years before this time that Ford first experienced automobiles, and throughout the later half of the 1880s, Ford began repairing and later constructing engines, and through the 1890s worked with a division of Edison Electric. He officially founded Ford Motor Company in 1903, after prior failures in business but success in constructing automobiles.
Ford's 1908 introduction of the Model T automobile revolutionized both transportation and American industry. As the Ford Motor Company sole owner, he became one of the richest and best-known people in the world. He is credited with "Fordism", the mass production of inexpensive goods coupled with high wages for workers. Ford was also among the pioneers of the five-day workweek. Ford believed that consumerism was a key to global peace. His commitment to systematically lowering costs resulted in many technical and business innovations, including a franchise system that put dealerships throughout North America and major cities on six continents.
In 1911, he was award a patent for the transmission mechanism that would be used in the Model T and other automobiles. 
Ford was known for his pacifism during the first years of World War I, although during the war his company became a major supplier of weapons. He promoted the League of Nations. In the 1920s Ford promoted antisemitism through his newspaper The Dearborn Independent and the book The International Jew. He opposed United States entry into World War II, and served for a time on the America First Committee board. After his son Edsel died in 1943, Ford resumed control of the company but was too frail to make decisions and quickly came under the control of subordinates. He turned over the company to his grandson Henry Ford II in 1945. He died in 1947 after leaving most of his wealth to the Ford Foundation, and control of the company to his family.

Early life
Henry Ford was born July 30, 1863, on a farm in Springwells Township, Michigan. His father, William Ford (1826–1905), was born in County Cork, Ireland, to a family that had emigrated from Somerset, England in the 16th century. His mother, Mary Ford (née Litogot; 1839–1876), was born in Michigan as the youngest child of Belgian immigrants; her parents died when she was a child and she was adopted by neighbors, the O'Herns. Henry Ford's siblings were Margaret Ford (1867–1938); Jane Ford (c. 1868–1945); William Ford (1871–1917) and Robert Ford (1873–1934).  Ford finished eighth grade at a one-room school, Springwells Middle School. He never attended high school; he later took a bookkeeping course at a commercial school.His father gave him a pocket watch when he was 12. At 15, Ford dismantled and reassembled the timepieces of friends and neighbors dozens of times, gaining the reputation of a watch repairman. At twenty, Ford walked four miles to their Episcopal church every Sunday.Ford was devastated when his mother died in 1876. His father expected him to take over the family farm eventually, but he despised farm work. He later wrote, "I never had any particular love for the farm—it was the mother on the farm I loved."In 1879, Ford left home to work as an apprentice machinist in Detroit, first with James F. Flower & Bros., and later with the Detroit Dry Dock Co. In 1882, he returned to Dearborn to work on the family farm, where he became adept at operating the Westinghouse portable steam engine. He was later hired by Westinghouse to service their steam engines.Ford said two significant events occurred in 1875 when he was 12: He received the watch, and he witnessed the operation of a Nichols and Shepard road engine, "...the first vehicle other than horse-drawn that I had ever seen". In his farm workshop, Ford built a "steam wagon or tractor" and a steam car, but thought "steam was not suitable for light vehicles," as "the boiler was dangerous." Ford also said that he "did not see the use of experimenting with electricity, due to the expense of trolley wires, and "no storage battery was in sight of a weight that was practical." In 1885, Ford repaired an Otto engine, and in 1887 he built a four-cycle model with a one-inch bore and a three-inch stroke. In 1890, Ford started work on a two-cylinder engine.
Ford said, "In 1892, I completed my first motor car, powered by a two-cylinder four horsepower motor, with a two-and-half-inch bore and a six-inch stroke, which was connected to a countershaft by a belt and then to the rear wheel by a chain. The belt was shifted by a clutch lever to control speeds at 10 or 20 miles per hour, augmented by a throttle. Other features included 28-inch wire bicycle wheels with rubber tires, a foot brake, a 3-gallon gasoline tank, and later, a water jacket around the cylinders for cooling. Ford added that "in the spring of 1893 the machine was running to my partial satisfaction and giving an opportunity further to test out the design and material on the road." Between 1895 and 1896, Ford drove that machine about 1000 miles. He then started a second car in 1896, eventually building three of them in his home workshop.

Marriage and family
Ford married Clara Jane Bryant (1866–1950) on April 11, 1888, and supported himself by farming and running a sawmill. They had one child, Edsel Ford (1893–1943).

Career
In 1891, Ford became an engineer with the Edison Illuminating Company of Detroit. After his promotion to Chief Engineer in 1893, he had enough time and money to devote attention to his experiments on gasoline engines. These experiments culminated in 1896 with the completion of a self-propelled vehicle, which he named the Ford Quadricycle. He test-drove it on June 4. After various test drives, Ford brainstormed ways to improve the Quadricycle.Also in 1896, Ford attended a meeting of Edison executives, where he was introduced to Thomas Edison. Edison approved of Ford's automobile experimentation. Encouraged by Edison, Ford designed and built a second vehicle, completing it in 1898. Backed by the capital of Detroit lumber baron William H. Murphy, Ford resigned from the Edison Company and founded the Detroit Automobile Company on August 5, 1899. However, the automobiles produced were of a lower quality and higher price than Ford wanted. Ultimately, the company was not successful and was dissolved in January 1901.With the help of C. Harold Wills, Ford designed, built, and successfully raced a 26-horsepower automobile in October 1901. With this success, Murphy and other stockholders in the Detroit Automobile Company formed the Henry Ford Company on November 30, 1901, with Ford as chief engineer. In 1902, Murphy brought in Henry M. Leland as a consultant; Ford, in response, left the company bearing his name. With Ford gone, Leland renamed the company the Cadillac Automobile Company.Teaming up with former racing cyclist Tom Cooper, Ford also produced the 80+ horsepower racer "999," which Barney Oldfield was to drive to victory in a race in October 1902. Ford received the backing of an old acquaintance, Alexander Y. Malcomson, a Detroit-area coal dealer. They formed a partnership, "Ford & Malcomson, Ltd." to manufacture automobiles. Ford went to work designing an inexpensive automobile, and the duo leased a factory and contracted with a machine shop owned by John and Horace E. Dodge to supply over $160,000 in parts. Sales were slow, and a crisis arose when the Dodge brothers demanded payment for their first shipment.

Ford Motor Company
In response, Malcomson brought in another group of investors and convinced the Dodge Brothers to accept a portion of the new company. Ford & Malcomson was reincorporated as the Ford Motor Company on June 16, 1903, with $28,000 capital. The original investors included Ford and Malcomson, the Dodge brothers, Malcomson's uncle John S. Gray, Malcolmson's secretary James Couzens, and two of Malcomson's lawyers, John W. Anderson and Horace Rackham. Because of Ford's volatility, Gray was elected president of the company. Ford then demonstrated a newly designed car on the ice of Lake St. Clair, driving 1 mile (1.6 km) in 39.4 seconds and setting a new land speed record at 91.3 miles per hour (146.9 kilometres per hour). Convinced by this success, race driver Barney Oldfield, who named this new Ford model "999" in honor of the fastest locomotive of the day, took the car around the country, making the Ford brand known throughout the United States. Ford also was one of the early backers of the Indianapolis 500.

Transmission Patent
In 1909, Ford submitted for patent application for his invention for a new transmission mechanism.
It was awarded a patent in 1911.

Model T
The Model T debuted on October 1, 1908. It had the steering wheel on the left, which every other company soon copied. The entire engine and transmission were enclosed; the four cylinders were cast in a solid block; the suspension used two semi-elliptic springs. The car was very simple to drive, and easy and cheap to repair. It was so cheap at $825 in 1908 ($26,870 today), with the price falling every year, that by the 1920s, a majority of American drivers had learned to drive on the Model T, despite the fact that drivers who were only familiar with the Model T's unique foot-operated planetary transmission and steering-column operated throttle-cum-accelerator had to learn a completely different set of skills to drive any other gasoline-powered automobile of the time.
Ford created a huge publicity machine in Detroit to ensure every newspaper carried stories and ads about the new product. Ford's network of local dealers made the car ubiquitous in almost every city in North America. As independent dealers, the franchises grew rich and publicized not just the Ford but also the concept of automobiling; local motor clubs sprang up to help new drivers and encourage them to explore the countryside. Ford was always eager to sell to farmers, who looked at the vehicle as a commercial device to help their business. Sales skyrocketed—several years posted 100% gains on the previous year. In 1913, Ford introduced moving assembly belts into his plants, which enabled an enormous increase in production. Although Ford is often credited with the idea, contemporary sources indicate that the concept and development came from employees Clarence Avery, Peter E. Martin, Charles E. Sorensen, and C. Harold Wills. (See Ford Piquette Avenue Plant)
Sales passed 250,000 in 1914. By 1916, as the price dropped to $360 for the basic touring car, sales reached 472,000.By 1918, half of all cars in the United States were Model Ts. All new cars were black; as Ford wrote in his autobiography, "Any customer can have a car painted any color that he wants so long as it is black." Until the development of the assembly line, which mandated black because of its quicker drying time, Model Ts were available in other colors, including red. The design was fervently promoted and defended by Ford, and production continued as late as 1927; the final total production was 15,007,034. This record stood for the next 45 years, and was achieved in 19 years from the introduction of the first Model T (1908).Henry Ford turned the presidency of Ford Motor Company over to his son Edsel Ford in December 1918. Henry retained final decision authority and sometimes reversed the decisions of his son. Ford started another company, Henry Ford and Son, and made a show of taking himself and his best employees to the new company; the goal was to scare the remaining holdout stockholders of the Ford Motor Company to sell their stakes to him before they lost most of their value. (He was determined to have full control over strategic decisions.) The ruse worked, and Ford and Edsel purchased all remaining stock from the other investors, thus giving the family sole ownership of the company.In 1922, Ford also purchased Lincoln Motor Co., founded by Cadillac founder Henry Leland and his son Wilfred during World War I. The Lelands briefly stayed to manage the company, but were soon expelled from it. Despite this acquisition of a premium car maker, Henry displayed relatively little enthusiasm for luxury automobiles in contrast to Edsel, who actively sought to expand Ford into the upscale market. The original Lincoln Model L that the Lelands had introduced in 1920 was also kept in production, untouched for a decade until it became too outdated. It was replaced by the modernized Model K in 1931.
By the mid-1920s, General Motors was rapidly rising as the leading American automobile manufacturer. GM president Alfred Sloan established the company's "price ladder" whereby GM would offer an automobile for "every purse and purpose" in contrast to Ford's lack of interest in anything outside the low-end market. Although Henry Ford was against replacing the Model T, now 16 years old, Chevrolet was mounting a bold new challenge as GM's entry-level division in the company's price ladder. Ford also resisted the increasingly popular idea of payment plans for cars. With Model T sales starting to slide, Ford was forced to relent and approve work on a successor model, shutting down production for 18 months. During this time, Ford constructed a massive new assembly plant at River Rouge for the new Model A, which launched in 1927.In addition to its price ladder, GM also quickly established itself at the forefront of automotive styling under Harley Earl's Arts & Color Department, another area of automobile design that Henry Ford did not entirely appreciate or understand. Ford would not have a true equivalent of the GM styling department for many years.

Model A and Ford's later career
By 1926, flagging sales of the Model T finally convinced Ford to make a new model. He pursued the project with a great deal of interest in the design of the engine, chassis, and other mechanical necessities, while leaving the body design to his son. Although Ford fancied himself an engineering genius, he had little formal training in mechanical engineering and could not even read a blueprint. A talented team of engineers performed most of the actual work of designing the Model A (and later the flathead V8) with Ford supervising them closely and giving them overall direction. Edsel also managed to prevail over his father's initial objections in the inclusion of a sliding-shift transmission.The result was the Ford Model A, introduced in December 1927 and produced through 1931, with a total output of more than four million. Subsequently, the Ford company adopted an annual model change system similar to that recently pioneered by its competitor General Motors (and still in use by automakers today). Not until the 1930s did Ford overcome his objection to finance companies, and the Ford-owned Universal Credit Corporation became a major car-financing operation. Henry Ford still resisted many technological innovations such as hydraulic brakes and all-metal roofs, which Ford vehicles did not adopt until 1935–36. For 1932 however, Ford dropped a bombshell with the flathead Ford V8, the first low-price eight-cylinder engine. The flathead V8, variants of which were used in Ford vehicles for 20 years, was the result of a secret project launched in 1930 and Henry had initially considered a radical X-8 engine before agreeing to a conventional design. It gave Ford a reputation as a performance make well-suited for hot-rodding.Ford did not believe in accountants; he amassed one of the world's largest fortunes without ever having his company audited under his administration. Without an accounting department, Ford had no way of knowing exactly how much money was being taken in and spent each month, and the company's bills and invoices were reportedly guessed at by weighing them on a scale. Not until 1956 would Ford be a publicly-traded company.Also, at Edsel's insistence, Ford launched Mercury in 1939 as a mid-range make to challenge Dodge and Buick, although Henry also displayed relatively little enthusiasm for it.

Labor philosophy
Five-dollar wage
Ford was a pioneer of "welfare capitalism", designed to improve the lot of his workers and especially to reduce the heavy turnover that had many departments hiring 300 men per year to fill 100 slots. Efficiency meant hiring and keeping the best workers.Ford astonished the world in 1914 by offering a $5 per day wage ($146 in 2022), which more than doubled the rate of most of his workers. A Cleveland, Ohio, newspaper editorialized that the announcement "shot like a blinding rocket through the dark clouds of the present industrial depression". The move proved extremely profitable; instead of constant employee turnover, the best mechanics in Detroit flocked to Ford, bringing their human capital and expertise, raising productivity, and lowering training costs. Ford announced his $5-per-day program on January 5, 1914, raising the minimum daily pay from $2.34 to $5 for qualifying male workers.Detroit was already a high-wage city, but competitors were forced to raise wages or lose their best workers. Ford's policy proved that paying employees more would enable them to afford the cars they were producing and thus boost the local economy. He viewed the increased wages as profit-sharing linked with rewarding those who were most productive and of good character. It may have been Couzens who convinced Ford to adopt the $5-day wage.Real profit-sharing was offered to employees who had worked at the company for six months or more, and, importantly, conducted their lives in a manner of which Ford's "Social Department" approved. They frowned on heavy drinking, gambling, and on what are now called deadbeat dads. The Social Department used 50 investigators and support staff to maintain employee standards; a large percentage of workers were able to qualify for this "profit-sharing".Ford's incursion into his employees' private lives was highly controversial, and he soon backed off from the most intrusive aspects. By the time he wrote his 1922 memoir, he had spoken of the Social Department and the private conditions for profit-sharing in the past tense. He admitted that "paternalism has no place in the industry. Welfare work that consists in prying into employees' private concerns is out of date. Men need counsel and men need help, often special help; and all this ought to be rendered for decency's sake. But the broad workable plan of investment and participation will do more to solidify the industry and strengthen the organization than will any social work on the outside. Without changing the principle we have changed the method of payment."

Five-day workweek
In addition to raising his workers' wages, Ford also introduced a new, reduced workweek in 1926. The decision was made in 1922, when Ford and Crowther described it as six 8-hour days, giving a 48-hour week, but in 1926 it was announced as five 8-hour days, giving a 40-hour week. The program apparently started with Saturday being designated a workday, before becoming a day off sometime later. On May 1, 1926, the Ford Motor Company's factory workers switched to a five-day, 40-hour workweek, with the company's office workers making the transition the following August.Ford had decided to boost productivity, as workers were expected to put more effort into their work in exchange for more leisure time. Ford also believed decent leisure time was good for business, giving workers additional time to purchase and consume more goods. However, charitable concerns also played a role. Ford explained, "It is high time to rid ourselves of the notion that leisure for workmen is either 'lost time' or a class privilege."

Labor unions
Ford was adamantly against labor unions. He explained his views on unions in Chapter 18 of My Life and Work. He thought they were too heavily influenced by leaders who would end up doing more harm than good for workers despite their ostensible good motives.  Most wanted to restrict productivity as a means to foster employment, but Ford saw this as self-defeating because, in his view, productivity was necessary for economic prosperity to exist.He believed that productivity gains that obviated certain jobs would nevertheless stimulate the broader economy and grow new jobs elsewhere, whether within the same corporation or in others. Ford also believed that union leaders had a perverse incentive to foment perpetual socio-economic crises to maintain their power. Meanwhile, he believed that smart managers had an incentive to do right by their workers, because doing so would maximize their profits. However, Ford did acknowledge that many managers were basically too bad at managing to understand this fact. But Ford believed that eventually, if good managers such as he could fend off the attacks of misguided people from both left and right (i.e., both socialists and bad-manager reactionaries), the good managers would create a socio-economic system wherein neither bad management nor bad unions could find enough support to continue existing.To forestall union activity, Ford promoted Harry Bennett, a former Navy boxer, to head the Service Department. Bennett employed various intimidation tactics to quash union organizing. On March 7, 1932, during the Great Depression, unemployed Detroit auto workers staged the Ford Hunger March to the Ford River Rouge Complex to present 14 demands to Henry Ford. The Dearborn police department and Ford security guards opened fire on workers leading to over sixty injuries and five deaths. On May 26, 1937, Bennett's security men beat members of the United Automobile Workers (UAW), including Walter Reuther, with clubs. While Bennett's men were beating the UAW representatives, the supervising police chief on the scene was Carl Brooks, an alumnus of Bennett's Service Department, and [Brooks] "did not give orders to intervene".: 311 The following day photographs of the injured UAW members appeared in newspapers, later becoming known as The Battle of the Overpass.In the late 1930s and early 1940s, Edsel—who was president of the company—thought Ford had to come to a collective bargaining agreement with the unions because the violence, work disruptions, and bitter stalemates could not go on forever. But Ford, who still had the final veto in the company on a de facto basis even if not an official one, refused to cooperate. For several years, he kept Bennett in charge of talking to the unions trying to organize the Ford Motor Company. Sorensen's memoir makes clear that Ford's purpose in putting Bennett in charge was to make sure no agreements were ever reached.The Ford Motor Company was the last Detroit automaker to recognize the UAW, despite pressure from the rest of the U.S. automotive industry and even the U.S. government. A sit-down strike by the UAW union in April 1941 closed the River Rouge Plant. Sorensen recounted that a distraught Henry Ford was very close to following through with a threat to break up the company rather than cooperate. Still, his wife Clara told him she would leave him if he destroyed the family business. In her view, it would not be worth the chaos it would create. Ford complied with his wife's ultimatum and even agreed with her in retrospect. Overnight, the Ford Motor Company went from the most stubborn holdout among automakers to the one with the most favorable UAW contract terms. The contract was signed in June 1941. About a year later, Ford told Walter Reuther, "It was one of the most sensible things Harry Bennett ever did when he got the UAW into this plant." Reuther inquired, "What do you mean?" Ford replied, "Well, you've been fighting General Motors and the Wall Street crowd. Now you're in here and we've given you a union shop and more than you got out of them. That puts you on our side, doesn't it? We can fight General Motors and Wall Street together, eh?"

Ford Airplane Company
Like other automobile companies, Ford entered the aviation business during World War I, building Liberty engines. After the war, it returned to auto manufacturing until 1925, when Ford acquired the Stout Metal Airplane Company.
Ford's most successful aircraft was the Ford 4AT Trimotor, often called the "Tin Goose" because of its corrugated metal construction. It used a new alloy called Alclad that combined the corrosion resistance of aluminum with the strength of duralumin. The plane was similar to Fokker's V.VII–3m, and some say that Ford's engineers surreptitiously measured the Fokker plane and then copied it. The Trimotor first flew on June 11, 1926, and was the first successful U.S. passenger airliner, accommodating about 12 passengers in a rather uncomfortable fashion. Several variants were also used by the U.S. Army. The Smithsonian Institution has honored Ford for changing the aviation industry. 199 Trimotors were built before it was discontinued in 1933, when the Ford Airplane Division shut down because of poor sales during the Great Depression.
In 1985, Ford was posthumously inducted into the National Aviation Hall of Fame for his impact on the industry.

World War I era and peace activism
Ford opposed war, which he viewed as a terrible waste, and supported causes that opposed military intervention. Ford became highly critical of those who he felt financed war, and he tried to stop them. In 1915, the pacifist Rosika Schwimmer gained favor with Ford, who agreed to fund a Peace Ship to Europe, where World War I was raging. He led 170 other peace activists. Ford's Episcopalian pastor, Reverend Samuel S. Marquis, accompanied him on the mission. Marquis headed Ford's Sociology Department from 1913 to 1921. Ford talked to President Woodrow Wilson about the mission but had no government support. His group went to neutral Sweden and the Netherlands to meet with peace activists. A target of much ridicule, Ford left the ship as soon as it reached Sweden. In 1915, Ford blamed "German-Jewish bankers" for instigating the war.According to biographer Steven Watts, Ford's status as a leading industrialist gave him a worldview that warfare was wasteful folly that retarded long-term economic growth. The losing side in the war typically suffered heavy damage. Small business were especially hurt, for it takes years to recuperate. He argued in many newspaper articles that a focus on business efficiency would discourage warfare because,  “If every man who manufactures an article would make the very best he can in the very best way at the very lowest possible price the world would be kept out of war, for commercialists would not have to search for outside markets which the other fellow covets.”  Ford admitted that munitions makers enjoyed wars, but he argued the most businesses wanted to avoid wars and instead work to manufacture and sell useful goods, hire workers, and generate steady long-term profits.Ford's British factories produced Fordson tractors to increase the British food supply, as well as trucks and warplane engines. When the U.S. entered the war in 1917, Ford went quiet on foreign policy. His company became a major supplier of weapons, especially the Liberty engine for warplanes and anti-submarine boats.: 95–100, 119 In 1918, with the war on and the League of Nations a growing issue in global politics, President Woodrow Wilson, a Democrat, encouraged Ford to run for a Michigan seat in the U.S. Senate. Wilson believed that Ford could tip the scales in Congress in favor of Wilson's proposed League. "You are the only man in Michigan who can be elected and help bring about the peace you so desire," the president wrote Ford. Ford wrote back: "If they want to elect me let them do so, but I won't make a penny's investment." Ford did run, however, and came within 7,000 votes of winning, out of more than 400,000 cast statewide. He was defeated in a close election by the Republican candidate, Truman Newberry, a former United States Secretary of the Navy. Ford remained a staunch Wilsonian and supporter of the League. When Wilson made a major speaking tour in the summer of 1919 to promote the League, Ford helped fund the attendant publicity.

World War II era and controversies
Ford opposed the United States' entry into World War II and continued to believe that international business could generate the prosperity that would head off wars. Ford "insisted that war was the product of greedy financiers who sought profit in human destruction". In 1939, he went so far as to claim that the torpedoing of U.S. merchant ships by German submarines was the result of conspiratorial activities undertaken by financier war-makers. The financiers to whom he was referring was Ford's code for Jews; he had also accused Jews of fomenting the First World War.In the run-up to World War II and when the war erupted in 1939, he reported that he did not want to trade with belligerents. Like many other businessmen of the Great Depression era, he never liked or entirely trusted the Franklin Roosevelt Administration, and thought Roosevelt was inching the U.S. closer to war. Ford continued to do business with Nazi Germany, including the manufacture of war materiel. However, he also agreed to build warplane engines for the British government. In early 1940, he boasted that Ford Motor Company would soon be able to produce 1,000 U.S. warplanes a day, even though it did not have an aircraft production facility at that time.: 430  Ford was a prominent early member of the America First Committee against World War II involvement, but was forced to resign from its executive board when his involvement proved too controversial.Beginning in 1940, with the requisitioning of between 100 and 200 French POWs to work as slave laborers, Ford-Werke contravened Article 31 of the 1929 Geneva Convention.When Rolls-Royce sought a U.S. manufacturer as an additional source for the Merlin engine (as fitted to Spitfire and Hurricane fighters), Ford first agreed to do so and then reneged. He "lined up behind the war effort" when the U.S. entered in December 1941.

Willow Run
Before the U.S. entered the war, responding to President Roosevelt's call in December 1940 for the "Great Arsenal of Democracy", Ford directed the Ford Motor Company to construct a vast new purpose-built aircraft factory at Willow Run near Detroit, Michigan. Ford broke ground on Willow Run in the spring of 1941, B-24 component production began in May 1942, and the first complete B-24 came off the line in October 1942. At 3,500,000 sq ft (330,000 m2), it was the largest assembly line in the world at the time. At its peak in 1944, the Willow Run plant produced 650 B-24s per month, and by 1945 Ford was completing each B-24 in eighteen hours, with one rolling off the assembly line every 58 minutes. Ford produced 9,000 B-24s at Willow Run, half of the 18,000 total B-24s produced during the war.: 430

Edsel's death
When Edsel Ford died of cancer in 1943, aged only 49, Henry Ford nominally resumed control of the company, but a series of strokes in the late 1930s had left him increasingly debilitated, and his mental ability was fading. Ford was increasingly sidelined, and others made decisions in his name. The company was controlled by a handful of senior executives led by Charles Sorensen, an important engineer and production executive at Ford; and Harry Bennett, the chief of Ford's Service Unit, Ford's paramilitary force that spied on, and enforced discipline upon, Ford employees. Ford grew jealous of the publicity Sorensen received and forced Sorensen out in 1944. Ford's incompetence led to discussions in Washington about how to restore the company, whether by wartime government fiat, or by instigating a coup among executives and directors.

Forced out
Nothing happened until 1945 when, with bankruptcy a serious risk, Ford's wife Clara and Edsel's widow Eleanor confronted him and demanded he cede control of the company to his grandson Henry Ford II. They threatened to sell off their stock, which amounted to three quarters of the company's total shares, if he refused. Ford was reportedly infuriated, but had no choice but to give in. The young man took over and, as his first act of business, fired Harry Bennett.

Antisemitism and The Dearborn Independent
Ford was a conspiracy theorist who drew on a long tradition of false allegations against Jews. Ford claimed that Jewish internationalism posed a threat to traditional American values, which he deeply believed were at risk in the modern world. Part of his racist and antisemitic legacy includes the funding of square-dancing in American schools because he hated jazz and associated its creation with Jewish people. In 1920 Ford wrote, "If fans wish to know the trouble with American baseball they have it in three words—too much Jew."In 1918, Ford purchased his hometown newspaper, The Dearborn Independent. A year and a half later, Ford began publishing a series of articles in the paper under his own name, claiming a vast Jewish conspiracy was affecting America.  The series ran in 91 issues.  Every Ford dealership nationwide was required to carry the paper and distribute it to its customers.  Ford later bound the articles into four volumes entitled The International Jew: The World's Foremost Problem, which was translated into multiple languages and distributed widely across the US and Europe. The International Jew blamed nearly all the troubles it saw in American society on Jews.  The Independent ran for eight years, from 1920 until 1927.  With around 700,000 readers of his newspaper, Ford emerged as a "spokesman for right-wing extremism and religious prejudice."
In Germany, Ford's The International Jew, the World's Foremost Problem was published by Theodor Fritsch, founder of several antisemitic parties and a member of the Reichstag. In a letter written in 1924, Heinrich Himmler described Ford as "one of our most valuable, important, and witty fighters". Ford is the only American mentioned favorably in Hitler's autobiography Mein Kampf. Adolf Hitler wrote, "only a single great man, Ford, [who], to [the Jews'] fury, still maintains full independence ... [from] the controlling masters of the producers in a nation of one hundred and twenty millions." Speaking in 1931 to a Detroit News reporter, Hitler said "I regard Henry Ford as my inspiration," explaining his reason for keeping a life-size portrait of Ford behind his desk. Steven Watts wrote that Hitler "revered" Ford, proclaiming that "I shall do my best to put his theories into practice in Germany", and modeling the Volkswagen Beetle, the people's car, on the Model T. Max Wallace has stated, "History records that ... Adolf Hitler was an ardent Anti-Semite before he ever read Ford's The International Jew." Ford also paid to print and distribute 500,000 copies of the antisemitic fabricated text The Protocols of the Elders of Zion.  Historians say Hitler distributed Ford’s books and articles throughout Germany, stoking the hatred that helped fuel the Holocaust.On February 1, 1924, Ford received Kurt Ludecke, a representative of Hitler, at home. Ludecke was introduced to Ford by Siegfried Wagner (son of the composer Richard Wagner) and his wife Winifred, both Nazi sympathizers and antisemites. Ludecke asked Ford for a contribution to the Nazi cause, but was apparently refused.Ford's articles were denounced by the Anti-Defamation League (ADL). While these articles explicitly condemned pogroms and violence against Jews, they blamed the Jews themselves for provoking them. According to some trial testimony, none of this work was written by Ford, but he allowed his name to be used as an author. Friends and business associates said they warned Ford about the contents of the Independent and that he probably never read the articles (he claimed he only read the headlines). On the other hand, court testimony in a libel suit, brought by one of the targets of the newspaper, alleged that Ford did know about the contents of the Independent in advance of publication.A libel lawsuit was brought by San Francisco lawyer and Jewish farm cooperative organizer Aaron Sapiro in response to the antisemitic remarks, and led Ford to close the Independent in December 1927. News reports at the time quoted him as saying he was shocked by the content and unaware of its nature. During the trial, the editor of Ford's "Own Page", William Cameron, testified that Ford had nothing to do with the editorials even though they were under his byline. Cameron testified at the libel trial that he never discussed the content of the pages or sent them to Ford for his approval. Investigative journalist Max Wallace noted that "whatever credibility this absurd claim may have had was soon undermined when James M. Miller, a former Dearborn Independent employee, swore under oath that Ford had told him he intended to expose Sapiro."Michael Barkun observed: "That Cameron would have continued to publish such anti-Semitic material without Ford's explicit instructions seemed unthinkable to those who knew both men. Mrs. Stanley Ruddiman, a Ford family intimate, remarked that "I don't think Mr. Cameron ever wrote anything for publication without Mr. Ford's approval." According to Spencer Blakeslee, "[t]he ADL mobilized prominent Jews and non-Jews to publicly oppose Ford's message. They formed a coalition of Jewish groups for the same purpose and raised constant objections in the Detroit press. Before leaving his presidency early in 1921, Woodrow Wilson joined other leading Americans in a statement that rebuked Ford and others for their antisemitic campaign. A boycott against Ford products by Jews and liberal Christians also had an impact, and Ford shut down the paper in 1927, recanting his views in a public letter to Sigmund Livingston, president of the ADL." Wallace also found that Ford's apology was likely, or at least partly, motivated by a business that was slumping as a result of his antisemitism, repelling potential buyers of Ford cars. Up until the apology, a considerable number of dealers, who had been required to make sure that buyers of Ford cars received the Independent, bought up and destroyed copies of the newspaper rather than alienate customers.Ford's 1927 apology was well received. "Four-fifths of the hundreds of letters addressed to Ford in July 1927 were from Jews, and almost without exception they praised the industrialist..." In January 1937, a Ford statement to The Detroit Jewish Chronicle disavowed "any connection whatsoever with the publication in Germany of a book known as the International Jew". Ford, however, allegedly never signed the retraction and apology, which were written by others—rather, his signature was forged by Harry Bennett—and Ford never actually recanted his antisemitic views, stating in 1940: "I hope to republish The International Jew again some time."
In July 1938, the German consul in Cleveland gave Ford, on his 75th birthday, the award of the Grand Cross of the German Eagle, the highest medal Nazi Germany could bestow on a foreigner. James D. Mooney, vice president of overseas operations for General Motors, received a similar medal, the Merit Cross of the German Eagle, First Class.On January 7, 1942, Ford wrote another letter to Sigmund Livingston disclaiming direct or indirect support of "any agitation which would promote antagonism toward my Jewish fellow citizens". He concluded the letter with, "My sincere hope that now in this country and throughout the world when the war is finished, hatred of the Jews and hatred against any other racial or religious groups shall cease for all time."The distribution of The International Jew was halted in 1942 through legal action by Ford, despite complications from a lack of copyright. It is still banned in Germany. Extremist groups often recycle the material; it still appears on antisemitic and neo-Nazi websites. Testifying at Nuremberg, convicted Hitler Youth leader Baldur von Schirach who, in his role as Gauleiter of Vienna, deported 65,000 Jews to camps in Poland, stated: "The decisive anti-Semitic book I was reading and the book that influenced my comrades was ... that book by Henry Ford, The International Jew. I read it and became anti-Semitic. The book made a great influence on myself and my friends because we saw in Henry Ford the representative of success and also the representative of a progressive social policy."Robert Lacey wrote in Ford: The Men and the Machines that a close Willow Run associate of Ford reported that when he was shown newsreel footage of the Nazi concentration camps, he "was confronted with the atrocities which finally and unanswerably laid bare the bestiality of the prejudice to which he contributed, he collapsed with a stroke – his last and most serious." Ford had suffered previous strokes and his final cerebral hemorrhage occurred in 1947 at age 83.

International business
Ford's philosophy was one of economic independence for the United States. His River Rouge Plant became the world's largest industrial complex, pursuing vertical integration to such an extent that it could produce its own steel. Ford's goal was to produce a vehicle from scratch without reliance on foreign trade. He believed in the global expansion of his company. He believed that international trade and cooperation led to international peace, and he used the assembly line process and production of the Model T to demonstrate it.He opened Ford assembly plants in Britain and Canada in 1911, and soon became the biggest automotive producer in those countries. In 1912, Ford cooperated with Giovanni Agnelli of Fiat to launch the first Italian automotive assembly plants. The first plants in Germany were built in the 1920s with the encouragement of Herbert Hoover and the Commerce Department, which agreed with Ford's theory that international trade was essential to world peace and reduced the chance of war. In the 1920s, Ford also opened plants in Australia, France, India, and Mexico, and by 1929, he had successful dealerships on six continents. Ford experimented with a commercial rubber plantation in the Amazon jungle called Fordlândia; it failed.

In 1929, Ford made an agreement with the Soviets to provide technical aid over nine years in building the first Soviet automobile plant (GAZ) near Nizhny Novgorod (Gorky) (an additional contract for construction of the plant was signed with The Austin Company on August 23, 1929). The contract involved the purchase of $30,000,000 worth of knocked-down Ford cars and trucks for assembly during the first four years of the plant's operation, after which the plant would gradually switch to Soviet-made components. Ford sent his engineers and technicians to the Soviet Union to help install the equipment and train the workforce, while over a hundred Soviet engineers and technicians were stationed at Ford's plants in Detroit and Dearborn "for the purpose of learning the methods and practice of manufacture and assembly in the Company's plants". Said Ford: "No matter where industry prospers, whether in India or China, or Russia, the more profit there will be for everyone, including us. All the world is bound to catch some good from it."By 1932, Ford was manufacturing one-third of the world's automobiles. It set up numerous subsidiaries that sold or assembled the Ford cars and trucks:

Ford's image transfixed Europeans, especially the Germans, arousing the "fear of some, the infatuation of others, and the fascination among all". Germans who discussed "Fordism" often believed that it represented something quintessentially American. They saw the size, tempo, standardization, and philosophy of production demonstrated at the Ford Works as a national service—an "American thing" that represented the culture of the United States. Both supporters and critics insisted that Fordism epitomized American capitalist development, and that the auto industry was the key to understanding economic and social relations in the United States. As one German explained, "Automobiles have so completely changed the American's mode of life that today one can hardly imagine being without a car. It is difficult to remember what life was like before Mr. Ford began preaching his doctrine of salvation". For many Germans, Ford embodied the essence of successful Americanism.
In My Life and Work, Ford predicted that if greed, racism, and short-sightedness could be overcome, then economic and technological development throughout the world would progress to the point that international trade would no longer be based on (what today would be called) colonial or neocolonial models and would truly benefit all peoples.

Racing
Ford maintained an interest in auto racing from 1901 to 1913 and began his involvement in the sport as both a constructor and a driver, later turning the wheel over to hired drivers. On October 10, 1901, he defeated Alexander Winton in a race car named "Sweepstakes"; it was through the wins of this car that Ford created the Henry Ford Company. Ford entered stripped-down Model Ts in races, finishing first (although later disqualified) in an "ocean-to-ocean" (across the United States) race in 1909, and setting a one-mile (1.6 km) oval speed record at Detroit Fairgrounds in 1911 with driver Frank Kulick. In 1913, he attempted to enter a reworked Model T in the Indianapolis 500 but was told rules required the addition of another 1,000 pounds (450 kg) to the car before it could qualify. Ford dropped out of the race and soon thereafter exited racing permanently, citing dissatisfaction with the sport's rules, demands on his time by the booming production of the Model T, and his low opinion of racing as a worthwhile activity.
In My Life and Work Ford speaks (briefly) of racing in a rather dismissive tone, as something that is not at all a good measure of automobiles in general. He describes himself as someone who raced only because in the 1890s through 1910s, one had to race because prevailing ignorance held that racing was the way to prove the worth of an automobile. Ford did not agree. But he was determined that as long as this was the definition of success (flawed though the definition was), then his cars would be the best that there were at racing. Throughout the book, he continually returns to ideals such as transportation, production efficiency, affordability, reliability, fuel efficiency, economic prosperity, and the automation of drudgery in farming and industry, but rarely mentions, and rather belittles, the idea of merely going fast from point A to point B.
Nevertheless, Ford did make quite an impact on auto racing during his racing years, and he was inducted into the Motorsports Hall of Fame of America in 1996.

Later career and death
When Edsel Ford, President of Ford Motor Company, died of cancer in May 1943, the elderly and ailing Henry Ford decided to assume the presidency. By this point, Ford, nearing 80 years old, had had several cardiovascular events (variously cited as heart attacks or strokes) and was mentally inconsistent, suspicious, and generally no longer fit for such immense responsibilities.Most of the directors did not want to see him as president. But for the previous 20 years, though he had long been without any official executive title, he had always had de facto control over the company; the board and the management had never seriously defied him, and this time was no different. The directors elected him, and he served until the end of the war. During this period the company began to decline, losing more than $10 million a month ($169,120,000 today). The administration of President Franklin Roosevelt had been considering a government takeover of the company in order to ensure continued war production, but the idea never progressed.

His health failing, Ford ceded the company presidency to his grandson Henry Ford II in September 1945 and retired. He died on April 7, 1947, of a cerebral hemorrhage at Fair Lane, his estate in Dearborn, at the age of 83. A public viewing was held at Greenfield Village where up to 5,000 people per hour filed past the casket. Funeral services were held in Detroit's Cathedral Church of St. Paul and he was buried in the Ford Cemetery in Detroit.

Personal interests
A compendium of short biographies of famous Freemasons, published by a Freemason lodge, lists Ford as a member. The Grand Lodge of New York confirms that Ford was a Freemason, and was raised in Palestine Lodge No. 357, Detroit, in 1894. When he received the 33rd degree of the Scottish Rite in 1940, he said, "Masonry is the best balance wheel the United States has."In 1923, Ford's pastor, and head of his sociology department, Episcopal minister Samuel S. Marquis, claimed that Ford believed, or "once believed," in reincarnation.Ford published an anti-smoking book, circulated to youth in 1914, called The Case Against the Little White Slaver, which documented many dangers of cigarette smoking attested to by many researchers and luminaries. At the time, smoking was ubiquitous and not yet widely associated with health problems, making Ford's opposition to cigarettes unusual.

Interest in materials science and engineering
Henry Ford had a long-held interest in materials science and engineering. He enthusiastically described his company's adoption of vanadium steel alloys and subsequent metallurgic R&D work.Ford also had a long-standing interest in plastics developed from agricultural products, particularly soybeans. He cultivated a relationship with George Washington Carver for this purpose. Soybean-based plastics were used in Ford automobiles throughout the 1930s in plastic parts such as car horns, in paint and other components. The project culminated in 1942, when Ford patented an automobile made almost entirely of plastic, attached to a tubular welded frame. It weighed 30% less than a steel car and was said to be able to withstand blows ten times greater than steel. It ran on grain alcohol (ethanol) instead of gasoline. The design never caught on.Ford was interested in engineered woods ("Better wood can be made than is grown") (at this time plywood and particle board were little more than experimental ideas); corn as a fuel source, via both corn oil and ethanol; and the potential uses of cotton. Ford was instrumental in developing charcoal briquets, under the brand name "Kingsford". His brother-in-law, Edward G. Kingsford, used wood scraps from the Ford factory to make the briquets.
In 1927, Ford partnered with Thomas Edison and Harvey Samuel Firestone (each contributing $25,000) to create the Edison Botanic Research Corp. in Fort Myers, Florida to seek a native source of rubber.
Ford was a prolific inventor and was awarded 161 U.S. patents.

Florida and Georgia residences and community
Ford had a vacation residence in Fort Myers, Florida, next to that of Thomas Edison, which he bought in 1915 and used until c. 1930. It still stands today as a museum.He also had a vacation home (known today as the "Ford Plantation") in Richmond Hill, Georgia, which is now a private community. Ford started buying land in this area and eventually owned 70,000 acres (110 square miles) there. In 1936, Ford broke ground for a beautiful Greek revival style mansion on the banks of the Ogeechee River on the site of a 1730s plantation. The grand house, made of Savannah-gray brick, had marble steps, air conditioning, and an elevator. It sat on 55 acres (22 ha) of manicured lawns and flowering gardens. The house became the center of social gatherings with visitations by the Vanderbilts, Rockefellers, and the DuPonts. It remains the centerpiece of The Ford Plantation today. Ford converted the 1870s-era rice mill into his personal research laboratory and powerhouse and constructed a tunnel from there to the new home, providing it with steam. He contributed substantially to the community, building a chapel and schoolhouse and employing numerous local residents.

Preserving Americana
Ford had an interest in Americana. In the 1920s, he began work to turn Sudbury, Massachusetts, into a themed historical village. He moved the schoolhouse supposedly referred to in the "Mary Had a Little Lamb" nursery rhyme from Sterling, Massachusetts, and purchased the historic Wayside Inn. The historical village plan never came to fruition. He repeated the concept of collecting historic structures with the creation of Greenfield Village in Dearborn, Michigan. It may have inspired the creation of Old Sturbridge Village as well. About the same time, he began collecting materials for his museum, which had a theme of practical technology. It was opened in 1929 as the Edison Institute. The museum has been greatly modernized and is still open today.

In popular culture
In Aldous Huxley's Brave New World (1932), society is organized on "Fordist" lines, the years are dated A.F. or Anno Ford ("In the Year of Ford"), and the expression "My Ford" is used instead of "My Lord". The Christian cross is replaced with a capital "T" for Model-T.
Upton Sinclair created a fictional description of Ford in the 1937 novel The Flivver King.
Symphonic composer Ferde Grofé composed a tone poem in Henry Ford's honor (1938).
Ford appears as a character in several historical novels, notably E. L. Doctorow's Ragtime (1975), and Richard Powers' Three Farmers on Their Way to a Dance (1985).
Ford, his family, and his company were the subjects of a 1987 film starring Cliff Robertson  and  Michael Ironside, based on the 1986 biography Ford: The Man and the Machine by Robert Lacey.
In the 2004 alternative history novel The Plot Against America, Philip Roth features Ford as Secretary of the Interior in a fictional Charles Lindbergh presidential administration after Lindbergh's victory over Roosevelt in the 1940 presidential election. The novel draws heavily on the administration's antisemitism and isolationism as a catalyst for its plot.
In the 2020 HBO adapted miniseries of the same name, Ford is portrayed by actor Ed Moran.
Ford appears as a Great Builder in the 2008 strategy video game Civilization Revolution.
In the fictional history of the Assassin's Creed video game franchise, Ford is portrayed as having been a major Templar influence on the events of the Great Depression, and later World War II.
Ford is featured as an ally of Thomas Edison in the youtube series Super Science Friends.

Honors and recognition
In December 1999, Ford was among 18 included in Gallup's List of Widely Admired People of the 20th Century, from a poll conducted of the American people.
In 1928, Ford was awarded the Franklin Institute's Elliott Cresson Medal.
In 1938, Ford was awarded Nazi Germany's Grand Cross of the German Eagle, a medal given to foreigners sympathetic to Nazism.
The United States Postal Service honored Ford with a Prominent Americans series (1965–1978) 12¢ postage stamp.
He was inducted into the Automotive Hall of Fame in 1946.
In 1975, Ford was posthumously inducted into the Junior Achievement U.S. Business Hall of Fame.
In 1985, he was inducted into the National Aviation Hall of Fame.
He was inducted into the Motorsports Hall of Fame of America in 1996.

See also
References
Bibliography
Further reading
Memoirs by Ford Motor Company principals
Biographies
Specialized studies
External links
 Media related to Henry Ford at Wikimedia Commons
 Quotations related to Henry Ford at Wikiquote
 Works by or about Henry Ford at Wikisource
Full text of My Life and Work from Project Gutenberg
Timeline
The Henry Ford Heritage Association
Henry Ford – an American Experience documentary
Works by Henry Ford at Project Gutenberg
Works by or about Henry Ford at Internet Archive
Works by Henry Ford at LibriVox (public domain audiobooks) 
Newspaper clippings about Henry Ford in the 20th Century Press Archives of the ZBW

Hewlett-Packard

The Hewlett-Packard Company, commonly shortened to Hewlett-Packard ( HYEW-lit PAK-ərd) or HP, was an American multinational information technology company headquartered in Palo Alto, California. HP developed and provided a wide variety of hardware components, as well as software and related services to consumers, small and medium-sized businesses (SMBs), and large enterprises, including customers in the government, health, and education sectors. The company was founded in a one-car garage in Palo Alto by Bill Hewlett and David Packard in 1939, and initially produced a line of electronic test and measurement equipment. The HP Garage at 367 Addison Avenue is now designated an official California Historical Landmark, and is marked with a plaque calling it the "Birthplace of 'Silicon Valley'".
The company won its first big contract in 1938 to provide the HP 200A, a low distortion frequency oscillator for Walt Disney's production of the animated film Fantasia, which allowed Hewlett and Packard to formally establish the Hewlett-Packard Company on July 2, 1939. The company grew into a multinational corporation widely respected for its products. HP was the world's leading PC manufacturer from 2007 until the second quarter of 2013, when Lenovo moved ahead of HP. HP specialized in developing and manufacturing computing, data storage, and networking hardware; designing software; and delivering services. Major product lines included personal computing devices, enterprise and industry standard servers, related storage devices, networking products, software, and a range of printers and other imaging products. The company directly marketed its products to households; small- to medium-sized businesses and enterprises, as well as via online distribution; consumer-electronics and office-supply retailers; software partners; and major technology vendors. It also offered services and a consulting business for its products and partner products.
In 1999, HP spun off its electronic and bio-analytical test and measurement instruments business into Agilent Technologies; HP retained focus on its later products, including computers and printers. It merged with Compaq in 2002, and acquired Electronic Data Systems in 2008, which led to combined revenues of $118.4 billion that year and a Fortune 500 ranking of 9 in 2009. In November 2009, HP announced its acquisition of 3Com, and closed the deal on April 12, 2010. On April 28, 2010, HP announced its buyout of Palm, Inc. for $1.2 billion. On September 2, 2010, HP won its bidding war for 3PAR with a $33 a share offer ($2.07 billion), which Dell declined to match.On November 1, 2015, Hewlett-Packard was split into two separate companies; its enterprise products and services business were spun-off to form Hewlett Packard Enterprise, while its personal computer and printer businesses became HP Inc.

History
Bill Hewlett and David Packard graduated with degrees in electrical engineering from Stanford University in 1935. The company started in a garage in Palo Alto during a fellowship they had with past professor Frederick Terman at Stanford during the Great Depression, whom they considered a mentor in forming the company. In 1938, Packard and Hewlett began part-time work in a rented garage with an initial capital investment of US$538 (equivalent to $11,185 in 2022). In 1939, Hewlett and Packard decided to formalize their partnership. They tossed a coin to decide whether the company they founded would be called Hewlett-Packard (HP) or Packard-Hewlett.Hewlett and Packard's first financially successful product was a precision audio oscillator known as the HP 200A, which used a small incandescent light bulb (known as a "pilot light") as a temperature dependent resistor in a critical portion of the circuit, and a negative feedback loop to stabilize the amplitude of the output sinusoidal waveform. This allowed the HP 200A to be sold for $89.40 when competitors were selling less stable oscillators for over $200. The 200 series of generators continued production until at least 1972 as the 200AB, still tube-based but improved in design through the years.One of the company's earliest customers was Bud Hawkins, chief sound engineer for Walt Disney Studios, who bought eight HP 200B audio oscillators (at $71.50 each) to be used in the animated film Fantasia. HP's profit at the end of 1939, its first full year of business, was $1,563 (equivalent to $32,883 in 2022) on revenues of $5,369.In 1942, they built their first building at 395 Page Mill Road and were awarded the Army-Navy "E" Award in 1943. HP employed 200 people and produced the audio oscillator, a wave analyzer, distortion analyzers, an audio-signal generator, and the Model 400A vacuum-tube voltmeter during the war.: 54–60, 195 Hewlett and Packard worked on counter-radar technology and artillery shell proximity fuzes during World War II; the work exempted Packard from the draft, but Hewlett had to serve as an officer in the Army Signal Corps after being called to active duty.
HP was incorporated on August 18, 1947, with Packard as president. Sales reached $5.5 million in 1951 with 215 employees. The company went public on November 6, 1957.: 35, 40, 64, 70, 196  In 1959, a manufacturing plant was established in Böblingen and a marketing organization in Geneva.: 196  Packard handed the presidency over to Hewlett when he became chairman in 1964, but remained CEO of the company.

1960s
HP is recognized as the symbolic founder of Silicon Valley, though it did not actively investigate semiconductor devices until a few years after the "traitorous eight" abandoned William Shockley to create Fairchild Semiconductor in 1957. Hewlett-Packard's HP Associates division, established around 1960, developed semiconductor devices primarily for internal use. HP Associates was co-founded by another former Bell Labs researcher, MOSFET inventor Mohamed Atalla, who served as Director of Semiconductor Research. Instruments and calculators were some of the products using semiconductor devices from HP Associates.
During the 1960s, HP partnered with Sony and Yokogawa Electric in Japan to develop several high-quality products. The products were not a huge success, as there were high costs involved in building HP-looking products in Japan. In 1963, HP and Yokogawa formed the joint venture Yokogawa-Hewlett-Packard to market HP products in Japan. HP bought Yokogawa Electric's share of Hewlett-Packard Japan in 1999.HP spun off the small company Dynac to specialize in digital equipment. The name was picked so that the HP logo could be turned upside down to be a reflected image of the logo of the new company. Dynac was eventually renamed Dymec and folded back into HP in 1959. HP experimented with using Digital Equipment Corporation (DEC) minicomputers with its instruments, but entered the computer market in 1966 with the HP 2100 / HP 1000 series of minicomputers after it decided that it would be easier to build another small design team than deal with DEC. The minicomputers had a simple accumulator-based design with two accumulator registers and, in the HP 1000 models, two index registers. The series was produced for 20 years in spite of several attempts to replace it, and was a forerunner of the HP 9800 and HP 250 series of desktop and business computers.
At the end of 1968, Packard handed over the duties of CEO to Hewlett to become United States Deputy Secretary of Defense in the incoming Nixon administration. He resumed the chairmanship in 1972 and served until 1993, but Hewlett remained the CEO.

1970s
The HP 3000 was an advanced stack-based design for a business computing server, later redesigned with RISC technology. The HP 2640 series of smart and intelligent terminals introduced forms-based interfaces to ASCII terminals, and also introduced screen labeled function keys, now commonly used on gas pumps and bank ATMs. The HP 2640 series included one of the first bit mapped graphics displays that, when combined with the HP 2100 21MX F-Series microcoded Scientific Instruction Set, enabled the first commercial WYSIWYG presentation program, BRUNO, that later became the program HP-Draw on the HP 3000. Although scoffed at in the formative days of computing, HP surpassed IBM as the world's largest technology vendor in terms of sales.
HP was identified by Wired magazine as the producer of the world's first device to be called a personal computer: the Hewlett-Packard 9100A, introduced in 1968. HP called it a desktop calculator because, as Hewlett said: "If we had called it a computer, it would have been rejected by our customers' computer gurus because it didn't look like an IBM. We therefore decided to call it a calculator, and all such nonsense disappeared." An engineering triumph at the time, the logic circuit was produced without any integrated circuits, and the CPU assembly was entirely executed in discrete components. With CRT display, magnetic-card storage, and printer, the price was around $5,000. The machine's keyboard was a cross between the keyboard of a scientific calculator and the keyboard of an adding machine. There was no alphabetic keyboard.
Apple co-founder Steve Wozniak originally designed the Apple I computer while working at HP and offered it to them under their right of first refusal to his work; they did not take it up as the company wanted to stay in scientific, business, and industrial markets. Wozniak said that HP "turned him down five times", but that his loyalty to HP made him hesitant to start Apple with Steve Jobs.The company earned global respect for a variety of products. They introduced the world's first handheld scientific electronic calculator in 1972 (the HP-35), the first handheld programmable in 1974 (the HP-65), the first alphanumeric, programmable, expandable in 1979 (the HP-41C), and the first symbolic and graphing calculator, the HP-28C. 
Like their scientific and business calculators, HP oscilloscopes, logic analyzers, and other measurement instruments had a reputation for sturdiness and usability. HP introduced the Hewlett-Packard Interface Bus (HPIB) computer peripheral interface (later cloned by National Instruments as GPIB and standardized by the IEEE as IEEE-488) on their relay actuator products in 1973; HPIB was later integrated into most high end test & measurement equipment it produced from 1980 onward. As early as 1977 HP began production of the HP856x spectrum analyzers to complement its RF power meters and sensors capable of measuring signals in excess of 20 GHz. HP also produced configurable chassis based sweep generators capable of generating signals to 20GHz. Other T&M products of the time included lab grade multimeters, microwave frequency counters, RF amplifiers, high accuracy microwave detectors, lab grade power supplies and more. These products were succeeded by modernized versions as well as the introduction of the scalar and vector network analyzer product lines prior to the business being spun off into Agilent Technologies.
The HP 9800 series of technical desktop computers started in 1971 with the 9810A. The HP series 80 started in 1979 with the 85. Some of these machines used a version of the BASIC programming language, which was available immediately after they were switched on, and used a proprietary magnetic tape for storage. HP computers were similar in capabilities to the much later IBM Personal Computer, though the limitations of available technology forced prices to be high.In 1978, Hewlett stepped down as CEO and was succeeded by John A. Young.

1980s
HP expanded into South Africa in the 1980s. Activists supporting divestment from South Africa accused HP of "automating apartheid".Sales reached $6.5 billion in 1985 with 85,000 employees.: 198 In 1984, HP introduced both inkjet and laser printers for the desktop. Along with its scanner product line, the printers have later been developed into successful multifunction products, the most significant being single-unit printer/scanner/copier/fax machines. The print mechanisms in HP's LaserJet line of laser printers depend almost entirely on Canon Inc.'s components (print engines), which in turn use technology developed by Xerox. HP developed the hardware, firmware, and software to convert data into dots for printing.On March 3, 1986, HP registered the HP.com domain name, making it the ninth Internet .com domain to be registered.In 1987, the Palo Alto garage where Hewlett and Packard started their business was designated as a California Historical Landmark.

1990s
In the 1990s, HP expanded their computer product line, which initially had been targeted at university, research, and business users, to reach consumers. HP also grew through acquisitions: it bought Apollo Computer in 1989 and Convex Computer in 1995.
In 1992, Young was succeeded by Lewis E. Platt, and in 1993 and Hewlett and Packard stepped down from the board with Platt succeeding Packard as chairman.
In 1993, HP acquired Advanced Design System from Pathwave.  The ADS suite of RF simulation tools was spun off into Agilent in 1999 along with related T&M business units, all of which were carried forward into the spinoff of Agilent into Keysight.
Later in the decade, HP opened hpshopping.com as an independent subsidiary to sell online, direct to consumers; in 2005, the store was renamed "HP Home & Home Office Store".
From 1995 to 1998, Hewlett-Packard were sponsors of the English football team Tottenham Hotspur.
In 1999, all of the businesses not related to computers, storage, and imaging were spun off from HP to form Agilent Technologies. Agilent's spin-off was the largest initial public offering in the history of Silicon Valley, and it created an $8 billion company with about 30,000 employees, manufacturing scientific instruments, semiconductors, optical networking devices, and electronic test equipment for telecom and wireless, research and development, and production.
In July 1999, HP appointed Carly Fiorina as the first female CEO of a Fortune-20 company in the Dow Jones Industrial Average. Fiorina received a larger signing offer than any of her predecessors. The same year, Fiorina articulated a set of "rules of the garage", an attempt to capture the spirit of the company's founders.

Sales to Iran despite sanctions
In 1997, HP started selling its products in Iran through a European subsidiary and a Dubai-based Middle Eastern distributor, despite U.S. export sanctions prohibiting such deals imposed by Bill Clinton's 1995 executive orders. The story was initially reported by The Boston Globe, and it triggered an inquiry by the U.S. Securities and Exchange Commission (SEC). HP responded that products worth US$120 million were sold in fiscal year 2008 for distribution via Redington Gulf, a company based in the Netherlands, and that as these sales took place through a foreign subsidiary, HP had not violated sanctions.HP named Redington Gulf "Wholesaler of the Year" in 2003, which in turn published a press release stating that "[t]he seeds of the Redington-Hewlett-Packard relationship were sowed six years ago for one market — Iran." At the time, Redington Gulf had only three employees whose sole purpose was to sell HP products to the Iran market. According to former officials who worked on sanctions, HP used a loophole by routing their sales through a foreign subsidiary. HP ended its relationship with Redington Gulf after the SEC inquiry.

2000–2005
On September 3, 2001, HP announced that an agreement had been reached with Compaq to merge the two companies. In May 2002, after passing a shareholder vote, HP officially merged with Compaq. Prior to this, plans had been in place to consolidate the companies' product teams and product lines.As Compaq took over Tandem Computers in 1997 and Digital Equipment Corporation in 1998, HP offers support for the former Tandem NonStop family and Digital Equipment products PDP-11, VAX and AlphaServer.The merger occurred after a proxy fight with Bill Hewlett's son Walter, who objected to the merger. HP became a major producer in desktop computers, laptops, and servers for many different markets. After the merger with Compaq, the new ticker symbol became "HPQ", a combination of the two previous symbols, "HWP" and "CPQ", to show the significance of the alliance and also key letters from the two companies Hewlett-Packard and Compaq (the latter company being famous for its "Q" logo on all of its products).
Mscape was a mobile media gaming platform that could be used to create location-based games originating in 2002.
In 2004, HP released the DV 1000 Series, including the HP Pavilion dv 1658 and 1040. In May 2006, HP began its campaign, "The Computer is Personal Again"; the campaign was designed to bring back the personal computer as a personal product. The campaign utilized viral marketing, sophisticated visuals, and its own website. Some of the ads featured Pharrell, Petra Nemcova, Mark Burnett, Mark Cuban, Alicia Keys, Jay-Z, Gwen Stefani, and Shaun White.In January 2005, following years of underperformance, which included HP's Compaq merger that fell short and disappointing earning reports, the board asked Fiorina to resign as chair and chief executive officer of the company, and she did on February 9, 2005. After her departure, HP's stock jumped 6.9 percent. Robert Wayman, chief financial officer of HP, served as interim CEO while the board undertook a formal search for a replacement.Mark Hurd of NCR Corporation was hired to take over as CEO and president, effective April 1, 2005. Hurd was the board's top choice given the revival of NCR that took place under his leadership.

2006–2009
In 2006, HP unveiled several new products including desktops, enhanced notebooks, a workstation, and software to manage them—OpenView Client Configuration Manager 2.0. In the same year, HP's share price skyrocketed due to consistent results in the last couple quarters of the year with Hurd's plan to cut back HP's workforce and lower costs.In July 2007, HP signed a definitive agreement to acquire Opsware in a cash tender deal that values the company at $14.25 per share, which combined Opsware software with the Oracle enterprise IT management software.In the first few years of Hurd's tenure as CEO, HP's stock price more than doubled. By the end of the 2007 fiscal year, HP reached the $100 billion mark for the first time. The company's annual revenue reached $104 billion, allowing HP to overtake competitor IBM.On May 13, 2008, HP and Electronic Data Systems (EDS) announced that they had signed a definitive agreement under which HP would purchase EDS. On June 30, HP announced that the waiting period under the Hart-Scott-Rodino Antitrust Improvements Act of 1976 had expired. "The transaction still requires EDS stockholder approval and regulatory clearance from the European Commission and other non-U.S. jurisdictions and is subject to the satisfaction or waiver of the other closing conditions specified in the merger agreement." The agreement was finalized on August 26, 2008, at $13 billion, and it was publicly announced that EDS would be re-branded. The first targeted layoff of 24,600 former EDS workers was announced on September 15, 2008. (The company's 2008 annual report gave the number as 24,700, to be completed by end of 2009.) This round was factored into purchase price as a $19.5 billion liability against goodwill. As of September 23, 2009, EDS was known as HP Enterprise Services (now known as DXC Technology).
On November 11, 2009, 3Com and Hewlett-Packard announced that the latter would be acquiring 3Com for $2.7 billion in cash. The acquisition was one of the biggest in size among a series of takeovers and acquisitions by technology giants to push their way to become one-stop shops. Since the beginning of the financial crisis in 2007, tech giants have constantly felt the pressure to expand beyond their current market niches. Dell purchased Perot Systems recently to invade into the technology consulting business area previously dominated by IBM. Hewlett-Packard's latest move marked its incursion into enterprise networking gear market dominated by Cisco.

2010–2012
On April 28, 2010, Palm, Inc. and HP announced that the latter would buy the former for $1.2 billion in cash and debt. Adding Palm handsets to the HP product line created some overlap with the iPAQ series of mobile devices, but was thought to significantly improve HP's mobile presence as iPAQ devices had not been selling well. Buying Palm, Inc. gave HP a library of valuable patents and the mobile operating platform, webOS. On July 1, 2010, the acquisition of Palm, Inc. was finalized. Purchasing its webOS was a big gamble to build HP's own ecosystem. On July 1, 2011, HP launched its first tablet, HP TouchPad, which brought webOS to tablet devices. On September 2, 2010, HP won the bidding war for 3PAR with a $33 a share offer ($2.07 billion) that Dell declined to match. After HP acquired Palm Inc., it phased out the Compaq brand.
On August 6, 2010, Hurd resigned amid controversy and CFO Cathie Lesjak assumed the role of interim CEO. Hurd had turned HP around and was widely regarded as one of Silicon Valley's star CEOs, and under his leadership, HP became the largest computer company in the world when measured by total revenue. He was accused of sexual harassment against a colleague, though the allegations were deemed baseless. The investigation led to questions concerning some of his private expenses and the lack of disclosure related to the friendship. Some observers have argued that Hurd was innocent, but the board asked for his resignation to avoid negative public relations. Public analysis was divided between those who saw it as a commendable tough action by HP in handling expenses irregularities, and those who saw it as an ill-advised, hasty, and expensive reaction in ousting a remarkably capable leader who had turned the business around. At HP, Hurd oversaw a series of acquisitions worth over $20 billion, which allowed the company to expand into services of networking equipment and smartphones. HP shares dropped by 8.4% in after-hours trading, hitting a 52-week low with $9 billion in market capitalization shaved off. Larry Ellison publicly attacked HP's board for Hurd's ousting, stating that the HP board had "made the worst personnel decision since the idiots on the Apple board fired Steve Jobs many years ago".On September 30, 2010, Léo Apotheker was named HP's new CEO and president. His appointment sparked a strong reaction from Ellison, who complained that Apotheker had been in charge of SAP when one of its subsidiaries was systematically stealing software from Oracle. SAP accepted that its subsidiary, which has now closed, illegally accessed Oracle intellectual property. Following Hurd's departure, HP was seen to be problematic by the market, with margins falling and having failed to redirect and establish itself in major new markets such as cloud and mobile services. Apotheker's strategy was to broadly aim at disposing hardware and moving into the more profitable software services sector. On August 18, 2011, HP announced that it would strategically exit the smartphone and tablet computer business, and focus on higher-margin "strategic priorities of Cloud, solutions and software with an emphasis on enterprise, commercial and government markets". It also contemplated selling off its personal computer division or spinning it off into a separate company, and quitting PC development while continuing to sell servers and other equipment to business customers, which was a strategy undertaken by IBM in 2005.HP's stock dropped by about a further 40% after the company abruptly announced a number of decisions: to discontinue its webOS device business (mobile phones and tablet computers), the intent to sell its personal computer division (at the time HP was the largest personal computer manufacturer in the world), and to acquire British big data software firm Autonomy for a 79% premium, seen externally as an "absurdly high" price for a business with known concerns over its accounts. Media analysts described HP's actions as a "botched strategy shift" and a "chaotic" attempt to rapidly reposition HP and enhance earnings. The Autonomy acquisition was objected to by HP's own CFO.: 3–6 HP lost more than $30 billion in market capitalization during Apotheker's tenure, and on September 22, 2011, the HP Board of Directors fired him as chief executive and replaced him with fellow board member and former eBay chief Meg Whitman, with Raymond J. Lane as executive chairman. Although Apotheker served barely ten months, he received over $13 million in compensation. Weeks later, HP announced that a review had concluded their PC division was too integrated and critical to business operations, and the company reaffirmed their commitment to the Personal Systems Group. In November 2012, HP wrote off almost $9 billion related to the Autonomy acquisition, which became the subject of intense litigation, as HP accused Autonomy's previous management of fraudulently exaggerating Autonomy's financial position and called in law enforcement and regulators in both countries, while Autonomy's previous management accused HP of "textbook" obfuscation and finger pointing to protect HP's executives from criticism and conceal HP culpability, their prior knowledge of Autonomy's financial position, and gross mismanagement of Autonomy after acquisition.: 6 On March 21, 2012, HP said its printing and PC divisions would become one unit headed by Todd Bradley from the PC division, and printing chief Vyomesh Joshi left the company.On May 23, 2012, HP announced plans to lay off approximately 27,000 employees, after posting a profit decline of 31% in the second quarter of 2012. Profits declined because of the growing popularity of smart phones, tablets, and other mobile devices, which slowed down personal computer sales.On May 30, 2012, HP unveiled its first net zero energy data center, which used solar energy and other renewable sources instead of traditional power grids.On July 10, 2012, HP's Server Monitoring Software was discovered to have a previously unknown security vulnerability. A security warning was given to customers about two vulnerabilities, and a patch addressing the issues was released. One month later, HP's official training center was hacked and defaced by a Pakistani hacker known as Hitcher to demonstrate a Web vulnerability.On September 10, 2012, HP revised their restructuring figures and started cutting 29,000 jobs.

2013–2015
On December 31, 2013, HP revised the number of jobs cut from 29,000 to 34,000 up to October 2014. The number of jobs cut until the end of 2013 was 24,600. At the end of 2013 the company had 317,500 employees. On May 22, 2014, HP announced it would cut a further 11,000 to 16,000 jobs, in addition to the 34,000 announced in 2013. Whitman said: "We are gradually shaping HP into a more nimble, lower-cost, more customer and partner-centric company that can successfully compete across a rapidly changing IT landscape." During the June 2014 HP Discover customer event in Las Vegas, Whitman and Martin Fink announced a project for a radically new computer architecture called The Machine. Based on memristors and silicon photonics, it was supposed to come into commercialization before the end of the decade, and represented 75% of the research activity in HP Labs at the time.On October 6, 2014, HP announced it was going to split into two separate companies to separate its personal computer and printer businesses from its technology services. The split, which was first reported by The Wall Street Journal and confirmed by other media, resulted in two publicly traded companies on November 1, 2015: Hewlett Packard Enterprise and HP Inc. The split was structured so that Hewlett-Packard changed its name to HP Inc. and spun off Hewlett Packard Enterprise as a new publicly traded company. Whitman became chairman of HP Inc. and CEO of Hewlett Packard Enterprise, Patricia Russo became chairman of the enterprise business, and Dion Weisler became CEO of HP, Inc.On October 29, 2014, Hewlett-Packard announced their new Sprout personal computer.In May 2015, the company announced it would be selling its controlling 51 percent stake in its Chinese data-networking business to Tsinghua Unigroup for a fee of at least $2.4 billion.

Facilities
HP's global operations were directed from its headquarters in Palo Alto, California. Its US operations were directed from its facility in an unincorporated area of Harris County, Texas, near Houston. Its Latin America offices were in unincorporated Miami-Dade County, Florida. Its European offices were in Meyrin, close to Geneva, Switzerland, but it also had a research center in the Paris-Saclay cluster 20 km south of Paris, France. Its Asia-Pacific offices were in Singapore.It also had large operations in Leixlip, Ireland; Austin, Texas; Boise, Idaho; Corvallis, Oregon; Fort Collins, Colorado; Roseville, California; Saint Petersburg, Florida; San Diego, California; Tulsa, Oklahoma; Vancouver, Washington; Conway, Arkansas; and Plano, Texas. In the UK, HP was based at a large site in Bracknell, Berkshire, with offices in various UK locations, including a landmark office tower in London, 88 Wood Street. Its acquisition of 3Com expanded its employee base to Marlborough, Massachusetts, where it has been manufacturing its convertible laptop series since late 2019. The company also had a large workforce and numerous offices in Bucharest, Romania, and at Bangalore, India, to address their back end and IT operations. Mphasis, which is headquartered at Bangalore, also enabled HP to increase their footprint in the city as it was a subsidiary of EDS which the company acquired.

Products and organizational structure
HP produced lines of printers, scanners, digital cameras, calculators, personal digital assistants, servers, workstation computers, and computers for home and small-business use; many of the computers came from the 2002 merger with Compaq. HP as of 2001 promoted itself as supplying not just hardware and software, but also a full range of services to design, implement, and support IT infrastructure.
HP's Imaging and Printing Group (IPG) was described by the company in 2005 as "the leading imaging and printing systems provider in the world for printer hardware, printing supplies and scanning devices, providing solutions across customer segments from individual consumers to small and medium businesses to large enterprises".
Products and technology associated with IPG included the Inkjet and LaserJet printers, the Officejet all-in-one multifunction printer/scanner/faxes, Indigo Digital Press, the HP Photosmart digital cameras and photo printers, and the photo sharing service Snapfish.On December 23, 2008, HP released iPrint Photo for the iPhone.HP's Personal Systems Group (PSG) was claimed by HP in 2005 to be "one of the leading vendors of personal computers ("PCs") in the world based on unit volume shipped and annual revenue". PSG dealt with business and consumer PCs and accessories (such as e.g., HP Pavilion, Compaq Presario, and VoodooPC), handheld computing (e.g., iPAQ Pocket PC), digital "connected" entertainment (e.g., HP MediaSmart TVs, HP MediaSmart Servers, HP MediaVaults, DVD+RW drives) and Apple's iPod (until November 2005).HP Enterprise Business (EB) incorporated HP Technology Services and Enterprise Services (an amalgamation of the former EDS, and what was known as HP Services). HP Enterprise Security Services oversaw professional services such as network security, information security and information assurance/compliancy, HP Software Division, and Enterprise Servers, Storage and Networking Group (ESSN). The Enterprise Servers, Storage and Networking Group (ESSN) oversaw "back end" products like storage and servers. HP Networking (former ProCurve) was responsible for the NW family of products.

HP Software Division was the company's enterprise software unit, which produced and marketed its brand of enterprise-management software, HP OpenView. From September 2005 HP purchased several software companies as part of a publicized, deliberate strategy to augment its software offerings for large business customers. HP Software sold several categories of software, which included business service management software, application lifecycle management software, mobile apps, and enterprise security software (the latter of which included, ArcSight, Fortify Software, Atalla and TippingPoint). HP Software also provided software as a service (SaaS), cloud computing solutions, and software services, including consulting, education, professional services, and support.
HP's Office of Strategy and Technology had four main functions: To steer the company's $3.6 billion research and development investment; foster the development of the company's global technical community; lead the company's strategy and corporate development efforts, and perform worldwide corporate marketing activities.
HP Labs served as the research arm of HP.
HP also offered managed services by which they provide complete IT-support solutions for other companies and organizations. One example of these was offering "Professional Support" and desktop "Premier Support" for Microsoft in the EMEA marketplace. This was done from the Leixlip campus near Dublin, Sofia and Israel. Support was offered for Microsoft Windows, Exchange, SharePoint, and some office applications.

Staff and culture
Notable people
Michael Capellas (Compaq CEO/Chairman – HP President)
Barney Oliver, founder and director of HP Labs
Steve Wozniak
Tom Perkins
Carly Fiorina, 2016 Republican presidential candidate
Matt Shaheen, management consultant executive at HP Enterprise Services in Plano, Texas; Republican member of the Texas House of Representatives
Enrique Lores, CEO HP Inc.

Corporate social responsibility
In July 2007, the company announced that it had met its 2004 target to recycle one billion pounds of electronics, toner, and ink cartridges. It set a new goal of recycling a further two billion pounds of hardware by the end of 2010. In 2006, the company recovered 187 million pounds of electronics.In 2008, HP released its supply chain emissions data.In September 2009, Newsweek ranked HP No. 1 on its 2009 Green Rankings of America's 500 largest corporations. According to Environmental Leader (now Environment + Energy Leader), "Hewlett-Packard earned its number one position due to its greenhouse gas (GHG) emission reduction programs, and was the first major IT company to report GHG emissions associated with its supply chain, according to the ranking. In addition, HP has made an effort to remove toxic substances from its products, though Greenpeace has targeted the company for not doing better."HP took the top spot on Corporate Responsibility Magazine's 100 Best Corporate Citizens List for 2010. HP beat out other Russell 1000 Index companies because of its leadership in seven categories including environment, climate changes and corporate philanthropy. In 2009, HP was ranked fifth.Fortune magazine named HP one of the World's Most Admired Companies in 2010, placing it No. 2 in the computer industry and No. 32 overall in its list of the top 50. This year in the computer industry HP was ranked No. 1 in social responsibility, long-term investment, global competitiveness, and use of corporate assets.In May 2011, HP released a Global Responsibility report covering accomplishments in 2010. It provides a comprehensive view of HP's global citizenship programs, performance, and goals and describes how HP used its technology, influence, and expertise to make a positive impact on the world. The company's 2009 report won best corporate responsibility report of the year, and claims HP decreased its total energy use by 9 percent when compared with 2008. HP recovered a total of 118,000 tonnes of electronic products and supplies for recycling in 2009, including 61 million print cartridges.In an April 2010 San Francisco Chronicle article, HP was one of 12 companies commended for "designing products to be safe from the start, following the principles of green chemistry". The commendations came from Environment California, an environmental advocacy group, who praised select companies in the Golden State and the Bay Area for their conservational efforts.In May 2010, HP was named one of the World's Most Ethical Companies by Ethisphere Institute. It was one of 100 companies to earn the distinction of top winner and was the only computer hardware vendor to be recognized.HP was listed in Greenpeace's Guide to Greener Electronics that ranks electronics manufacturers according to their policies on sustainability, energy and climate, and green products. In November 2011, HP secured first place (out of 15) in this ranking with a score of 5.9. It scored the most points on the new Sustainable Operations criteria, having the best program for measuring and reducing emissions of greenhouse gases from its suppliers and scoring maximum points for its thorough paper procurement policy. In the November 2012 report, HP was ranked second with a score of 5.7.HP earned recognition of its work in data privacy and security. In 2010 the company ranked No. 4 in the Ponemon Institute's annual study of the most trusted companies for privacy. Since 2006, HP has worked directly with the U.S. Congress, the Federal Trade Commission (FTC), and the Department of Commerce to establish a new strategy for federal legislation. HP played a key role in work toward the December 2010 FTC report "Protecting Consumer Privacy in an Era of Rapid Change".After winning nine straight annual "Most Respected Company in China" awards from the Economic Observer and Peking University, HP China added the "10 Year Contribution" award to its list of accolades.In its 2012 rankings of consumer electronics companies on progress relating to conflict minerals, the Enough Project rated HP second out of 24 companies.

Brand
According to a 2009 BusinessWeek study, HP was the world's 11th most valuable brand.HP had many sponsorships, such as Mission: SPACE in Epcot at the Walt Disney World Resort. From 1995 to 1999, and again from 2013, HP had been the shirt sponsor of Premier League club Tottenham Hotspur F.C. From 1997 to 1999 they sponsored Australian Football League club North Melbourne Football Club. They also sponsored the Jordan Grand Prix from 1999 to 2001, Stewart Grand Prix in 1999, Jaguar Racing from 2000 to 2002, BMW Williams Formula 1 team until 2005 (a sponsorship formerly held by Compaq), and since 2010 sponsored Renault F1. HP also had the naming rights arrangement for the HP Pavilion at San Jose, whose naming rights were acquired by SAP AG and consequently renamed SAP Center at San Jose. HP also maintained a number of corporate sponsorships in the business sector, including sponsorships of trade organisations including Fespa (print trade exhibitions), and O'Reilly Media's Velocity (web development) conference.
After the acquisition of Compaq in 2002, HP maintained the Compaq Presario brand on low-end home desktops and laptops, the HP Compaq brand on business desktops and laptops, and the HP ProLiant brand on Intel-architecture servers. The HP Pavilion brand was used on home entertainment laptops and all home desktops.Tandem's "NonStop" servers were rebranded as "HP Integrity NonStop".

Controversies
Restatement
In March 2003, HP restated its first-quarter cash flow from operations, reducing it by 18 percent because of an accounting error. The actual cash flow from operations was $647 million, and not $791 million as reported; HP shifted $144 million to net cash used in investing activities.

Spying scandal
On September 5, 2006, Shawn Cabalfin and David O'Neil of Newsweek wrote that HP's general counsel, at the behest of chairwoman Patricia Dunn, contracted a team of independent security experts to investigate board members and several journalists to identify the source of an information leak. In turn, those security experts recruited private investigators who used pretexting, which involved investigators impersonating HP board members and nine journalists (including reporters for CNET, The New York Times and The Wall Street Journal) in order to obtain their phone records. The information leaked related to HP's long-term strategy and was published as part of a CNET article in January 2006. Most HP employees accused of criminal acts have since been acquitted.

Hardware
In November 2007, HP released a BIOS update covering a wide range of laptops with the intent to speed up the computer fan and have it run constantly while the computer was on or off to prevent the overheating of defective Nvidia graphics processing units (GPUs) that had been shipped to many of the original equipment manufacturers, including HP, Dell, and Apple. The defect concerned the new packaging material used by Nvidia from 2007 onwards in joining the graphics chip onto the motherboard, which did not perform well under thermal cycling and was prone to develop stress cracks – effectively severing the connection between the GPU and the motherboard that led to a blank screen. In July 2008, HP issued an extension to the initial one-year warranty to replace the motherboards of selected models. However this option was not extended to all models with the defective Nvidia chipsets, despite research showing that these computers were also affected by the fault. Furthermore, the replacement of the motherboard was a temporary fix, since the fault was inherent in all units of the affected models from the point of manufacture, including the replacement motherboards offered by HP as a free "repair". Since then, several websites have been documenting the issue. There have been several small-claims lawsuits filed in several states, as well as suits filed in other countries. HP also faced a class-action lawsuit in 2009 over its i7 processor computers: the complainants stated that their systems consistently locked up within 30 minutes of powering on. Even after being replaced with newer i7 systems, the lockups continued.

Lawsuit against Oracle
HP filed a lawsuit in California Superior Court in Santa Clara, claiming that Oracle had breached an agreement to support the Itanium microprocessor used in HP's high-end enterprise servers. On June 15, 2011, HP sent a "formal legal demand" letter to Oracle in an attempt to force them to reverse its decision to discontinue software development on Intel Itanium microprocessors and build its own servers. HP won the lawsuit in 2012, which required Oracle to continue producing software compatible with the Itanium processor. HP was awarded $3 billion in damages against Oracle on June 30, 2016, arguing that Oracle canceling support damaged HP's Itanium server brand. Oracle said it would appeal both the decision and damages.

HP wage and hour lawsuit
Several class action firms filed a class action lawsuit on January 12, 2012, against HP Inc. and Hewlett Packard Enterprise (“HP”), entitled "Jeffrey Wall, etc. v. HP, Inc." (formerly known as Hewlett-Packard Company, et al.), Case No. 30-2012-00537897, pending in the Superior Court of California, County of Orange. According to the lawsuit, HP allegedly failed to pay commission payments and incentive compensation that its California sales employees were owed within the timeframes proscribed by California law (Labor Code §§ 201, 202 and 204). In 2017, FDAzar obtained a settlement of $25 million for class participants and changed the way HP pays incentive compensation and commission payments.

Takeover of Autonomy
In November 2012, HP recorded a write-down of around $8.8 billion related to its acquisition a year earlier of the UK-based Autonomy Corporation PLC. HP accused Autonomy of deliberately inflating the value of the company prior to its takeover, which the former management team of Autonomy denied.
At that time, HP had fired its previous CEO for expenses irregularities a year before, and appointed Apotheker as CEO and president. HP was seen as problematic by the market, with margins falling and having failed to redirect and establish itself in major new markets such as cloud and mobile services.
As part of Apotheker's strategy, Autonomy was acquired by HP in October 2011. HP paid $10.3 billion for 87.3% of the shares, valuing Autonomy at around $11.7 billion (£7.4 billion) overall, a premium of around 79% over market price. The deal was widely criticized as "absurdly high", a "botched strategy shift" and a "chaotic" attempt to rapidly reposition HP and enhance earnings, and had been objected to even by HP's own CFO.: 3–6  Within a year, Apotheker was fired, major culture clashes became apparent, and HP wrote off $8.8 billion of Autonomy's value.HP claimed that this resulted from "accounting improprieties, misrepresentations and disclosure failures" by the previous management, who in turn accused HP of a "textbook example of defensive stalling": 6  to conceal evidence of its own prior knowledge, gross mismanagement, and undermining of the company, noting public awareness since 2009 of its financial reporting issues: 3  and that even HP's CFO disagreed with the price paid.: 3–6  External observers generally stated that only a small part of the write-off appears to be due to accounting mis-statements, and that HP had previously overpaid for businesses.The Serious Fraud Office (SFO) and the SEC joined the FBI in investigating the potential anomalies. HP incurred damage with its stock falling to its lowest in decades. Three lawsuits were brought by shareholders against HP for the fall in value of HP shares. In August 2014, a United States district court judge threw out a proposed settlement, which Autonomy's previous management had argued would be collusive and intended to divert scrutiny of HP's own responsibility and knowledge. It essentially engaged the plaintiff's attorneys from the existing cases and redirected them against the previous Autonomy vendors and management for a fee of up to $48 million, with plaintiffs agreeing to end any claims against HP's management and similarly redirect those claims against the previous Autonomy vendors and management. In January 2015 the SFO closed its investigation as the likelihood of a successful prosecution was low. The dispute continued in the US, and is being investigated by the UK and Ireland Financial Reporting Council. On June 9, 2015, HP agreed to pay $100 million to investors who bought HP shares between August 19, 2011 and November 20, 2012, to settle the lawsuits over the Autonomy purchase.Another term of the shareholder settlement was to sue Autonomy management, which occurred in London in 2019. HP "failed to produce a smoking gun for the fraud it alleges", and its accountants admitted that they "never formally prepared anything to attribute the irregularities to the amount of the fraud".

Israeli settlements
On October 25, 2012, Richard Falk, the United Nations Human Rights Council's Special Rapporteur on the situation of human rights in the Palestinian territories occupied since 1967, called to boycott HP and other businesses that profit from Israeli settlements on occupied Palestinian lands until they brought their operations in line with international human rights and humanitarian law. In 2014, the Presbyterian Church voted to move forward with divestment from HP to pressure Israel in regards to their policies toward Palestinians. In 2015, the Human Rights Commission of Portland, Oregon, requested to place Caterpillar, G4S, HP, and Motorola Solutions on the city's "Do Not Buy" list.

Bribery
On April 9, 2014, an administrative proceeding before the SEC was settled by HP consenting to an order acknowledging that HP had violated the Foreign Corrupt Practices Act (FCPA) when HP subsidiaries in Russia, Poland, and Mexico made improper payments to government officials to obtain or retain lucrative public contracts.The SEC's order found that HP's subsidiary in Russia paid more than $2 million through agents and various shell companies to a Russian government official to retain a multimillion-dollar contract with the federal prosecutor's office; in Poland, HP's subsidiary provided gifts and cash bribes worth more than $600,000 to a Polish government official to obtain contracts with the national police agency; and to win a software sale to Mexico's state-owned petroleum company, HP's subsidiary in Mexico paid more than $1 million in inflated commissions to a consultant with close ties to company officials, one of whom was funneled money. HP agreed to pay $108 million to settle the SEC charges and a parallel criminal case.

See also
ArcSight
Fortify
HP calculators
HP Linux Imaging and Printing
HP Software & Solutions
HP User Group
List of acquisitions by Hewlett-Packard
List of computer system manufacturers
List of Hewlett-Packard products
TippingPoint

References
External links

Hewlett-Packard
The Museum of HP Calculators
HP History Links
Protect 724 CommunityBusiness data for Hewlett-Packard Company:

IBM

The International Business Machines Corporation (doing business as IBM), nicknamed Big Blue, is an American multinational technology corporation headquartered in Armonk, New York and is present in over 175 countries. It specializes in computer hardware, middleware, and software, and provides hosting and consulting services in areas ranging from mainframe computers to nanotechnology. IBM is the largest industrial research organization in the world, with 19 research facilities across a dozen countries, and held the record for most annual U.S. patents generated by a business for 29 consecutive years from 1993 to 2021.IBM was founded in 1911 as the Computing-Tabulating-Recording Company (CTR), a holding company of manufacturers of record-keeping and measuring systems. It was renamed "International Business Machines" in 1924 and soon became the leading manufacturer of punch-card tabulating systems. For the next several decades, IBM would become an industry leader in several emerging technologies, including electric typewriters, electromechanical calculators, and personal computers. During the 1960s and 1970s, the IBM mainframe, exemplified by the System/360, was the dominant computing platform, and the company produced 80 percent of computers in the U.S. and 70 percent of computers worldwide.After pioneering the multipurpose microcomputer in the 1980s, which set the standard for personal computers, IBM began losing its market dominance to emerging competitors. Beginning in the 1990s, the company began downsizing its operations and divesting from commodity production, most notably selling its personal computer division to the Lenovo Group in 2005. IBM has since concentrated on computer services, software, supercomputers, and scientific research. Since 2000, its supercomputers have consistently ranked among the most powerful in the world, and in 2001 it became the first company to generate more than 3,000 patents in one year, beating this record in 2008 with over 4,000 patents. As of 2022, the company held 150,000 patents.As one of the world's oldest and largest technology companies, IBM has been responsible for several technological innovations, including the automated teller machine (ATM), dynamic random-access memory (DRAM), the floppy disk, the hard disk drive, the magnetic stripe card, the relational database, the SQL programming language, and the UPC barcode. The company has made inroads in advanced computer chips, quantum computing, artificial intelligence, and data infrastructure. IBM employees and alumni have won various recognitions for their scientific research and inventions, including six Nobel Prizes and six Turing Awards.IBM is a publicly traded company and one of 30 companies in the Dow Jones Industrial Average. It is among the world's largest employers, with over 297,900 employees worldwide in 2022. Despite its relative decline within the technology sector, IBM remains the seventh largest technology company by revenue, and 49th largest overall, according to the 2022 Fortune 500. It is also consistently ranked among the world's most recognizable, valuable, and admired brands.

History
IBM was founded in 1911 in Endicott, New York; as the Computing-Tabulating-Recording Company (CTR) and was renamed "International Business Machines" in 1924. IBM is incorporated in New York and has operations in over 170 countries.In the 1880s, technologies emerged that would ultimately form the core of International Business Machines (IBM). Julius E. Pitrap patented the computing scale in 1885; Alexander Dey invented the dial recorder (1888); Herman Hollerith (1860–1929) patented the Electric Tabulating Machine; and Willard Bundy invented a time clock to record workers' arrival and departure times on a paper tape in 1889. On June 16, 1911, their four companies were amalgamated in New York State by Charles Ranlett Flint forming a fifth company, the Computing-Tabulating-Recording Company (CTR) based in Endicott, New York. The five companies had 1,300 employees and offices and plants in Endicott and Binghamton, New York; Dayton, Ohio; Detroit, Michigan; Washington, D.C.; and Toronto.They manufactured machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards. Thomas J. Watson, Sr., fired from the National Cash Register Company by John Henry Patterson, called on Flint and, in 1914, was offered a position at CTR. Watson joined CTR as general manager and then, 11 months later, was made President when antitrust cases relating to his time at NCR were resolved. Having learned Patterson's pioneering business practices, Watson proceeded to put the stamp of NCR onto CTR's companies. He implemented sales conventions, "generous sales incentives, a focus on customer service, an insistence on well-groomed, dark-suited salesmen and had an evangelical fervor for instilling company pride and loyalty in every worker". His favorite slogan, "THINK", became a mantra for each company's employees. During Watson's first four years, revenues reached $9 million ($152 million today) and the company's operations expanded to Europe, South America, Asia and Australia. Watson never liked the clumsy hyphenated name "Computing-Tabulating-Recording Company" and on February 14, 1924, chose to replace it with the more expansive title "International Business Machines" which had previously been used as the name of CTR's Canadian Division. By 1933, most of the subsidiaries had been merged into one company, IBM.
The Nazis reportedly made extensive use of Hollerith punch card and alphabetical accounting equipment and IBM's majority-owned German subsidiary, Deutsche Hollerith Maschinen GmbH (Dehomag), supplied this equipment from the early 1930s. This equipment was critical to Nazi efforts to categorize citizens of both Germany and other nations that fell under Nazi control through ongoing censuses. This census data was used to facilitate the round-up of Jews and other targeted groups, and to catalog their movements through the machinery of the Holocaust, including internment in the concentration camps. Nazi concentration camps operate a Hollerith department called Hollerith Abteilung, which had IBM machineries that also included calculating and sorting machines. There is much debate amongst the history community about whether IBM was complicit in the use of these machines, whether the machines used were IBM branded, and even whether tabulating machines were used for this purpose at all.IBM has several leadership development and recognition programs to acknowledge and foster employee potential and achievements. For early-career high potential employees, IBM sponsors leadership development programs by discipline (e.g., general management (GMLDP), human resources (HRLDP), finance (FLDP)). Each year, the company also selects 500 IBM employees for the IBM Corporate Service Corps (CSC), which gives top employees a month to do humanitarian work abroad. For certain interns, IBM also has a program called Extreme Blue that partners top business and technical students to develop high-value technology and compete to present their business case to the company's CEO at internship's end.The company also has various designations for exceptional individual contributors such as Senior Technical Staff Member (STSM), Research Staff Member (RSM), Distinguished Engineer (DE), and Distinguished Designer (DD). Prolific inventors can also achieve patent plateaus and earn the designation of Master Inventor. The company's most prestigious designation is that of IBM Fellow. Since 1963, the company names a handful of Fellows each year based on technical achievement. Other programs recognize years of service such as the Quarter Century Club established in 1924, and sellers are eligible to join the Hundred Percent Club, composed of IBM salesmen who meet their quotas, convened in Atlantic City, New Jersey. Each year, the company also selects 1,000 IBM employees annually to award the Best of IBM Award, which includes an all-expenses-paid trip to the awards ceremony in an exotic location.
IBM built the Automatic Sequence Controlled Calculator, an electromechanical computer, during World War II. It offered its first commercial stored-program computer, the vacuum tube based IBM 701, in 1952. The IBM 305 RAMAC introduced the hard disk drive in 1956. The company switched to transistorized designs with the 7000 and 1400 series, beginning in 1958.
In 1956, the company demonstrated the first practical example of artificial intelligence when Arthur L. Samuel of IBM's Poughkeepsie, New York, laboratory programmed an IBM 704 not merely to play checkers but "learn" from its own experience. In 1957, the FORTRAN scientific programming language was developed. In 1961, IBM developed the SABRE reservation system for American Airlines and introduced the highly successful Selectric typewriter.
In 1963, IBM employees and computers helped NASA track the orbital flights of the Mercury astronauts. A year later, it moved its corporate headquarters from New York City to Armonk, New York. The latter half of the 1960s saw IBM continue its support of space exploration, participating in the 1965 Gemini flights, 1966 Saturn flights, and 1969 lunar mission. IBM also developed and manufactured the Saturn V's Instrument Unit and Apollo spacecraft guidance computers.

On April 7, 1964, IBM launched the first computer system family, the IBM System/360. It spanned the complete range of commercial and scientific applications from large to small, allowing companies for the first time to upgrade to models with greater computing capability without having to rewrite their applications. It was followed by the IBM System/370 in 1970. Together the 360 and 370 made the IBM mainframe the dominant mainframe computer and the dominant computing platform in the industry throughout this period and into the early 1980s. They and the operating systems that ran on them such as OS/VS1 and MVS, and the middleware built on top of those such as the CICS transaction processing monitor, had a near-monopoly-level market share and became the thing IBM was most known for during this period.In 1969, the United States of America alleged that IBM violated the Sherman Antitrust Act by monopolizing or attempting to monopolize the general-purpose electronic digital computer system market, specifically computers designed primarily for business, and subsequently alleged that IBM violated the antitrust laws in IBM's actions directed against leasing companies and plug-compatible peripheral manufacturers. Shortly after, IBM unbundled its software and services in what many observers believed was a direct result of the lawsuit, creating a competitive market for software. In 1982, the Department of Justice dropped the case as "without merit".Also in 1969, IBM engineer Forrest Parry invented the magnetic stripe card that would become ubiquitous for credit/debit/ATM cards, driver's licenses, rapid transit cards, and a multitude of other identity and access control applications. IBM pioneered the manufacture of these cards, and for most of the 1970s, the data processing systems and software for such applications ran exclusively on IBM computers. In 1974, IBM engineer George J. Laurer developed the Universal Product Code. IBM and the World Bank first introduced financial swaps to the public in 1981, when they entered into a swap agreement. The IBM PC, originally designated IBM 5150, was introduced in 1981, and it soon became an industry standard.
In 1991 IBM began spinning off its many divisions into autonomous subsidiaries (so-called "Baby Blues") in an attempt to make the company more manageable and to streamline IBM by having other investors finance those companies. These included AdStar, dedicated to disk drives and other data storage products; IBM Application Business Systems, dedicated to mid-range computers; IBM Enterprise Systems, dedicated to mainframes; Pennant Systems, dedicated to mid-range and large printers; Lexmark, dedicated to small printers; and more. Lexmark was acquired by Clayton & Dubilier in a leveraged buyout shortly after its formation.In September 1992, IBM completed the spin-off of their various non-mainframe and non-midrange, personal computer manufacturing divisions, combining them into an autonomous wholly owned subsidiary known as the IBM Personal Computer Company (IBM PC Co.). This corporate restructuring came after IBM reported a sharp drop in profit margins during the second quarter of fiscal year 1992; market analysts attributed the drop to a fierce price war in the personal computer market over the summer of 1992. The corporate restructuring was one of the largest and most expensive in history up to that point. By the summer of 1993, the IBM PC Co. had divided into multiple business units itself, including Ambra Computer Corporation and the IBM Power Personal Systems Group, the former an attempt to design and market "clone" computers of IBM's own architecture and the latter responsible for IBM's PowerPC-based workstations.In 1993, IBM posted an $8 billion loss – at the time the biggest in American corporate history. Lou Gerstner was hired as CEO from RJR Nabisco to turn the company around. In 2002 IBM acquired PwC Consulting, the consulting arm of PwC which was merged into its IBM Global Services.
In 1998, IBM merged the enterprise-oriented Personal Systems Group of the IBM PC Co. into IBM's own Global Services personal computer consulting and customer service division. The resulting merged business units then became known simply as IBM Personal Systems Group. In 1999, IBM stopped selling their computers at retail outlets after their market share in this sector had fallen considerably behind competitors Compaq and Dell. Immediately afterwards, the IBM PC Co. was dissolved and merged into IBM Personal Systems Group.On September 14, 2004, LG and IBM announced that their business alliance in the South Korean market would end at the end of that year. Both companies stated that it was unrelated to the charges of bribery earlier that year. Xnote was originally part of the joint venture and was sold by LG in 2012.In 2005, the company sold all of its personal computer business to Chinese technology company Lenovo and, in 2009, it acquired software company SPSS Inc. Later in 2009, IBM's Blue Gene supercomputing program was awarded the National Medal of Technology and Innovation by U.S. President Barack Obama. In 2011, IBM gained worldwide attention for its artificial intelligence program Watson, which was exhibited on Jeopardy! where it won against game-show champions Ken Jennings and Brad Rutter. The company also celebrated its 100th anniversary in the same year on June 16. In 2012, IBM announced it had agreed to buy Kenexa and Texas Memory Systems, and a year later it also acquired SoftLayer Technologies, a web hosting service, in a deal worth around $2 billion. Also that year, the company designed a video surveillance system for Davao City.In 2014, IBM announced it would sell its x86 server division to Lenovo for $2.1 billion. while continuing to offer Power ISA-based servers. Also that year, IBM began announcing several major partnerships with other companies, including Apple Inc., Twitter, Facebook, Tencent, Cisco, UnderArmour, Box, Microsoft, VMware, CSC, Macy's, Sesame Workshop, the parent company of Sesame Street, and Salesforce.com.In 2015, IBM announced three major acquisitions: Merge Healthcare for $1 billion, data storage vendor Cleversafe, and all digital assets from The Weather Company, including Weather.com and the Weather Channel mobile app. Also that year, IBM employees created the film A Boy and His Atom, which was the first molecule movie to tell a story. In 2016, IBM acquired video conferencing service Ustream and formed a new cloud video unit. In April 2016, it posted a 14-year low in quarterly sales. The following month, Groupon sued IBM accusing it of patent infringement, two months after IBM accused Groupon of patent infringement in a separate lawsuit.In 2015, IBM bought the digital part of The Weather Company, Truven Health Analytics for $2.6 billion in 2016, and in October 2018, IBM announced its intention to acquire Red Hat for $34 billion, which was completed on July 9, 2019.IBM announced in October 2020 that it would divest the Managed Infrastructure Services unit of its Global Technology Services division into a new public company. The new company, Kyndryl, will have 90,000 employees, 4,600 clients in 115 countries, with a backlog of $60 billion. IBM's spin off was greater than any of its previous divestitures, and welcomed by investors. IBM appointed Martin Schroeter, who had been IBM's CFO from 2014 through the end of 2017, as CEO of Kyndryl.On March 7, 2022, a few days after the start of the Russian invasion of Ukraine, IBM CEO Arvind Krishna published a Ukrainian flag and announced that "we have suspended all business in Russia". All Russian articles were also removed from the IBM website. On June 7, Krishna announced that IBM would carry out an "orderly wind-down" of its operations in Russia.In 2023, IBM acquired Manta Software Inc. to complement its data and AI governance capabilities for an undisclosed amount.

Headquarters and offices
IBM is headquartered in Armonk, New York, a community 37 miles (60 km) north of Midtown Manhattan. A nickname for the company is the "Colossus of Armonk". Its principal building, referred to as CHQ, is a 283,000-square-foot (26,300 m2) glass and stone edifice on a 25-acre (10 ha) parcel amid a 432-acre former apple orchard the company purchased in the mid-1950s. There are two other IBM buildings within walking distance of CHQ: the North Castle office, which previously served as IBM's headquarters; and the Louis V. Gerstner, Jr., Center for Learning (formerly known as IBM Learning Center (ILC)), a resort hotel and training center, which has 182 guest rooms, 31 meeting rooms, and various amenities.IBM operates in 174 countries as of 2016, with mobility centers in smaller market areas and major campuses in the larger ones. In New York City, IBM has several offices besides CHQ, including the IBM Watson headquarters at Astor Place in Manhattan. Outside of New York, major campuses in the United States include Austin, Texas; Research Triangle Park (Raleigh-Durham), North Carolina; Rochester, Minnesota; and Silicon Valley, California.
IBM's real estate holdings are varied and globally diverse. Towers occupied by IBM include 1250 René-Lévesque (Montreal, Canada) and One Atlantic Center (Atlanta, Georgia, US). In Beijing, China, IBM occupies Pangu Plaza, the city's seventh tallest building and overlooking Beijing National Stadium ("Bird's Nest"), home to the 2008 Summer Olympics.
IBM India Private Limited is the Indian subsidiary of IBM, which is headquartered at Bangalore, Karnataka. It has facilities in Coimbatore, Chennai, Kochi, Ahmedabad, Delhi, Kolkata, Mumbai, Pune, Gurugram, Noida, Bhubaneshwar, Surat, Visakhapatnam, Hyderabad, Bangalore and Jamshedpur.
Other notable buildings include:

the IBM Rome Software Lab (Rome, Italy),
Hursley House (Winchester, UK),
330 North Wabash (Chicago, Illinois, United States),
the Cambridge Scientific Center (Cambridge, Massachusetts, United States),
the IBM Toronto Software Lab (Toronto, Canada),
the IBM Building, Johannesburg (Johannesburg, South Africa),
the IBM Building (Seattle) (Seattle, Washington, United States),
the IBM Hakozaki Facility (Tokyo, Japan),
the IBM Yamato Facility (Yamato, Japan),
the IBM Canada Head Office Building (Ontario, Canada)
the Watson IoT Headquarters (Munich, Germany).Defunct IBM campuses include the IBM Somers Office Complex (Somers, New York), Spango Valley (Greenock, Scotland), and Tour Descartes (Paris, France). The company's contributions to industrial architecture and design include works by Marcel Breuer, Eero Saarinen, Ludwig Mies van der Rohe, I.M. Pei and Ricardo Legorreta. Van der Rohe's building in Chicago was recognized with the 1990 Honor Award from the National Building Museum.IBM was recognized as one of the Top 20 Best Workplaces for Commuters by the United States Environmental Protection Agency (EPA) in 2005, which recognized Fortune 500 companies that provided employees with excellent commuter benefits to help reduce traffic and air pollution. In 2004, concerns were raised related to IBM's contribution in its early days to pollution in its original location in Endicott, New York.

Finance
For the fiscal year 2020, IBM reported earnings of $5.6 billion, with an annual revenue of $73.6 billion. IBM's revenue has fallen for 8 of the last 9 years. IBM's market capitalization was valued at over $127 billion as of April 2021. IBM ranked No. 38 on the 2020 Fortune 500 rankings of the largest United States corporations by total revenue. In 2014, IBM was accused of using "financial engineering" to hit its quarterly earnings targets rather than investing for the longer term.

Products and services
IBM has a large and diverse portfolio of products and services. As of 2016, these offerings fall into the categories of cloud computing, artificial intelligence, commerce, data and analytics, Internet of things (IoT), IT infrastructure, mobile, digital workplace and cybersecurity.IBM Cloud includes infrastructure as a service (IaaS), software as a service (SaaS) and platform as a service (PaaS) offered through public, private and hybrid cloud delivery models. For instance, the IBM Bluemix PaaS enables developers to quickly create complex websites on a pay-as-you-go model. IBM SoftLayer is a dedicated server, managed hosting and cloud computing provider, which in 2011 reported hosting more than 81,000 servers for more than 26,000 customers. IBM also provides Cloud Data Encryption Services (ICDES), using cryptographic splitting to secure customer data.IBM also hosts the industry-wide cloud computing and mobile technologies conference InterConnect each year.Hardware designed by IBM for these categories include IBM's Power microprocessors, which are employed inside many console gaming systems, including Xbox 360, PlayStation 3, and Nintendo's Wii U. IBM Secure Blue is encryption hardware that can be built into microprocessors, and in 2014, the company revealed TrueNorth, a neuromorphic CMOS integrated circuit and announced a $3 billion investment over the following five years to design a neural chip that mimics the human brain, with 10 billion neurons and 100 trillion synapses, but that uses just 1 kilowatt of power. In 2016, the company launched all-flash arrays designed for small and midsized companies, which includes software for data compression, provisioning, and snapshots across various systems.IT outsourcing also represents a major service provided by IBM, with more than 60 data centers worldwide. IBM Developer is IBM's source for emerging software technologies, and SPSS is a software package used for statistical analysis. IBM's Kenexa suite provides employment and retention solutions, and includes the BrassRing, an applicant tracking system used by thousands of companies for recruiting. IBM also owns The Weather Company, which provides weather forecasting and includes weather.com and Weather Underground.Smarter Planet is an initiative that seeks to achieve economic growth, near-term efficiency, sustainable development, and societal progress, targeting opportunities such as smart grids, water management systems, solutions to traffic congestion, and greener buildings.Services provisions include Redbooks, which are publicly available online books about best practices with IBM products, and developerWorks, a website for software developers and IT professionals with how-to articles and tutorials, as well as software downloads, code samples, discussion forums, podcasts, blogs, wikis, and other resources for developers and technical professionals.IBM Watson is a technology platform that uses natural language processing and machine learning to reveal insights from large amounts of unstructured data. Watson was debuted in 2011 on the American game show Jeopardy!, where it competed against champions Ken Jennings and Brad Rutter in a three-game tournament and won. Watson has since been applied to business, healthcare, developers, and universities. For example, IBM has partnered with Memorial Sloan Kettering Cancer Center to assist with considering treatment options for oncology patients and for doing melanoma screenings. Several companies use Watson for call centers, either replacing or assisting customer service agents.
In January 2019, IBM introduced its first commercial quantum computer: IBM Q System One.IBM also provides infrastructure for the New York City Police Department through their IBM Cognos Analytics to perform data visualizations of CompStat crime data.In March 2020, it was announced that IBM will build the first quantum computer in Germany. The computer should allow researchers to harness the technology without falling foul of the EU's increasingly assertive stance on data sovereignty.In June 2020, IBM announced that it was exiting the facial recognition business. In a letter to congress, IBM's Chief Executive Officer Arvind Krishna told lawmakers, "now is the time to begin a national dialogue on whether and how facial recognition technology should be employed by domestic law enforcement agencies."In May 2022, IBM announced the company had signed a multi-year Strategic Collaboration Agreement with Amazon Web Services to make a wide variety of IBM software available as a service on AWS Marketplace. Additionally, the deal includes both companies making joint investments that make it easier for companies to consume IBM's offering and integrate them with AWS, including developer training and software development for select markets.In November 2022, the company came out with a chip called the 433-qubit Osprey. Time called it "the world's most powerful quantum processor" and noted that if the processor's speed were represented in bits, the number would be larger than the total number of atoms in the universe.In an effort to streamline its products and services, beginning in the 1990s, IBM has regularly sold off low margin assets while shifting its focus to higher-value, more profitable markets. In 1991, the company spun off its printer and keyboard manufacturing division to Lexmark, in 2005 it sold its personal computer (ThinkPad/ThinkCentre) business to Lenovo, in 2015 it adopted a "fabless" model with semiconductors design and offloaded manufacturing to GlobalFoundries, and in 2021 it spun-off its managed infrastructure services unit into a new public company named Kyndryl. IBM also announced the acquisition of the enterprise software company Turbonomic for $1.5 billion. In 2022, IBM announced it would sell Watson Health to private equity firm Francisco Partners. IBM also started a collaboration with new Japanese manufacturer Rapidus in late 2022, which led GlobalFoundries to file a lawsuit against IBM the following year.

Research
Research has been part of IBM since its founding, and its organized efforts trace their roots back to 1945, when the Watson Scientific Computing Laboratory was founded at Columbia University in New York City, converting a renovated fraternity house on Manhattan's West Side into IBM's first laboratory. Now, IBM Research constitutes the largest industrial research organization in the world, with 12 labs on 6 continents. IBM Research is headquartered at the Thomas J. Watson Research Center in New York, and facilities include the Almaden lab in California, Austin lab in Texas, Australia lab in Melbourne, Brazil lab in São Paulo and Rio de Janeiro, China lab in Beijing and Shanghai, Ireland lab in Dublin, Haifa lab in Israel, India lab in Delhi and Bangalore, Tokyo lab, Zurichlab and Africa lab in Nairobi.
In terms of investment, IBM's R&D expenditure totals several billion dollars each year. In 2012, that expenditure was approximately $6.9 billion. Recent allocations have included $1 billion to create a business unit for Watson in 2014, and $3 billion to create a next-gen semiconductor along with $4 billion towards growing the company's "strategic imperatives" (cloud, analytics, mobile, security, social) in 2015.IBM has been a leading proponent of the Open Source Initiative, and began supporting Linux in 1998. The company invests billions of dollars in services and software based on Linux through the IBM Linux Technology Center, which includes over 300 Linux kernel developers. IBM has also released code under different open-source licenses, such as the platform-independent software framework Eclipse (worth approximately $40 million at the time of the donation), the three-sentence International Components for Unicode (ICU) license, and the Java-based relational database management system (RDBMS) Apache Derby. IBM's open source involvement has not been trouble-free, however (see SCO v. IBM).
Famous inventions and developments by IBM include: the automated teller machine (ATM), dynamic random access memory (DRAM), the electronic keypunch, the financial swap, the floppy disk, the hard disk drive, the magnetic stripe card, the relational database, RISC, the SABRE airline reservation system, SQL, the Universal Product Code (UPC) bar code, and the virtual machine. Additionally, in 1990 company scientists used a scanning tunneling microscope to arrange 35 individual xenon atoms to spell out the company acronym, marking the first structure assembled one atom at a time. A major part of IBM research is the generation of patents. Since its first patent for a traffic signaling device, IBM has been one of the world's most prolific patent sources. In 2021, the company held the record for most patents generated by a business for 29 consecutive years for the achievement.Five IBM employees have received the Nobel Prize: Leo Esaki, of the Thomas J. Watson Research Center in Yorktown Heights, N.Y., in 1973, for work in semiconductors; Gerd Binnig and Heinrich Rohrer, of the Zurich Research Center, in 1986, for the scanning tunneling microscope; and Georg Bednorz and Alex Müller, also of Zurich, in 1987, for research in superconductivity. Six IBM employees have won the Turing Award, including the first female recipient Frances E. Allen. Ten National Medals of Technology (USA) and five National Medals of Science (USA) have been awarded to IBM employees.

Brand and reputation
IBM is nicknamed Big Blue partly due to its blue logo and color scheme, and also in reference to its former de facto dress code of white shirts with blue suits. The company logo has undergone several changes over the years, with its current "8-bar" logo designed in 1972 by graphic designer Paul Rand. It was a general replacement for a 13-bar logo, since period photocopiers did not render narrow (as opposed to tall) stripes well. Aside from the logo, IBM used Helvetica as a corporate typeface for 50 years, until it was replaced in 2017 by the custom-designed IBM Plex.
IBM has a valuable brand as a result of over 100 years of operations and marketing campaigns. Since 1996, IBM has been the exclusive technology partner for the Masters Tournament, one of the four major championships in professional golf, with IBM creating the first Masters.org (1996), the first course cam (1998), the first iPhone app with live streaming (2009), and first-ever live 4K Ultra High Definition feed in the United States for a major sporting event (2016). As a result, IBM CEO Ginni Rometty became the third female member of the Master's governing body, the Augusta National Golf Club. IBM is also a major sponsor in professional tennis, with engagements at the U.S. Open, Wimbledon, the Australian Open, and the French Open. The company also sponsored the Olympic Games from 1960 to 2000, and the National Football League from 2003 to 2012.In 2012, IBM's brand was valued at $75.5 billion and ranked by Interbrand as the third-best brand worldwide. That same year, it was also ranked the top company for leaders (Fortune), the number two green company in the U.S. (Newsweek), the second-most respected company (Barron's), the fifth-most admired company (Fortune), the 18th-most innovative company (Fast Company), and the number one in technology consulting and number two in outsourcing (Vault). In 2015, Forbes ranked IBM as the fifth-most valuable brand, and for 2020, the Drucker Institute named IBM the No. 3 best-managed company. During the 2022 Russian invasion of Ukraine, IBM donated $250,000 to Polish Humanitarian Action and the same amount to People in Need, Czech Republic.In terms of ESG, IBM reported its total CO2e emissions (direct and indirect) for the twelve months ending December 31, 2020 at 621 kilotons (-324 /-34.3% year-on-year). In February 2021, IBM committed to achieve net zero greenhouse gas emissions by the year 2030.

People and culture
Employees
IBM has one of the largest workforces in the world, and employees at Big Blue are referred to as "IBMers". The company pioneered in several employment practices unheard of at the time.  IBM was among the first corporations to provide group life insurance (1934), survivor benefits (1935), training for women (1935), paid vacations (1937), and training for disabled people (1942). IBM hired its first black salesperson in 1946, and in 1952, CEO Thomas J. Watson, Jr. published the company's first written equal opportunity policy letter, one year before the U.S. Supreme Court decision in Brown vs. Board of Education and 11 years before the Civil Rights Act of 1964. The Human Rights Campaign has rated IBM 100% on its index of gay-friendliness every year since 2003, with IBM providing same-sex partners of its employees with health benefits and an anti-discrimination clause. Additionally, in 2005, IBM became the first major company in the world to formally commit to not using genetic information in employment decisions. In 2017, IBM was named to Working Mother's 100 Best Companies List for the 32nd consecutive year.IBM has several leadership development and recognition programs to recognize employee potential and achievements. For early-career high potential employees, IBM sponsors leadership development programs by discipline (e.g., general management (GMLDP), human resources (HRLDP), finance (FLDP)). Each year, the company also selects 500 IBM employees for the IBM Corporate Service Corps (CSC), which gives top employees a month to do humanitarian work abroad. For certain interns, IBM also has a program called Extreme Blue that partners with top business and technical students to develop high-value technology and compete to present their business case to the company's CEO at internship's end.The company also has various designations for exceptional individual contributors such as Senior Technical Staff Member (STSM), Research Staff Member (RSM), Distinguished Engineer (DE), and Distinguished Designer (DD). Prolific inventors can also achieve patent plateaus and earn the designation of Master Inventor. The company's most prestigious designation is that of IBM Fellow. Since 1963, the company names a handful of Fellows each year based on technical achievement. Other programs recognize years of service such as the Quarter Century Club established in 1924, and sellers are eligible to join the Hundred Percent Club, composed of IBM salesmen who meet their quotas, convened in Atlantic City, New Jersey. Each year, the company also selects 1,000 IBM employees annually to award the Best of IBM Award, which includes an all-expenses-paid trip to the awards ceremony in an exotic location.
IBM's culture has evolved significantly over its century of operations. In its early days, a dark (or gray) suit, white shirt, and a "sincere" tie constituted the public uniform for IBM employees. During IBM's management transformation in the 1990s, CEO Louis V. Gerstner Jr. relaxed these codes, normalizing the dress and behavior of IBM employees. The company's culture has also given to different plays on the company acronym (IBM), with some saying it stands for "I've Been Moved" due to relocations and layoffs, others saying it stands for "I'm By Myself" pursuant to a prevalent work-from-anywhere norm, and others saying it stands for "I'm Being Mentored" due to the company's open door policy and encouragement for mentoring at all levels. In terms of labor relations, the company has traditionally resisted labor union organizing, although unions represent some IBM workers outside the United States. In Japan, IBM employees also have an American football team complete with pro stadium, cheerleaders and televised games, competing in the Japanese X-League as the "Big Blue".In 2015, IBM started giving employees the option of choosing Mac as their primary work device, next to the option of a PC or a Linux distribution. In 2016, IBM eliminated forced rankings and changed its annual performance review system to focus more on frequent feedback, coaching, and skills development.

IBM alumni
Many IBM employees have achieved notability outside of work and after leaving IBM. In business, former IBM employees include:

Apple Inc. CEO Tim Cook
former EDS CEO and politician Ross Perot
Microsoft chairman John W. Thompson
SAP co-founder Hasso Plattner
Gartner founder Gideon Gartner
Advanced Micro Devices (AMD) CEO Lisa Su
Cadence Design Systems CEO Anirudh Devgan
former Citizens Financial Group CEO Ellen Alemany
former Yahoo! chairman Alfred Amoroso
former AT&T CEO C. Michael Armstrong
former Xerox Corporation CEOs David T. Kearns and G. Richard Thoman
former Fair Isaac Corporation CEO Mark N. Greene
Citrix Systems co-founder Ed Iacobucci
former Lenovo CEO Steve Ward
former Teradata CEO Kenneth SimondsIn government, former IBM employees include:

Patricia Roberts Harris (United States Secretary of Housing and Urban Development)
Samuel K. Skinner (U.S. Secretary of Transportation and as the White House Chief of Staff)
Mack Mattingly (Diplomat)
Thom Tillis (American politician)
Scott Walker (Former Governor of Wisconsin)
Arthur K. Watson (Former diplomat)
Todd Akin (US politician)
Glenn Andrews (Former US representative from Alabama)
Robert Garcia, (Former US representative)
Katherine Harris (Former US politician),
Amo Houghton (US politician)
Jim Ross Lightfoot (Former US House of Representatives)
Thomas J. Manton (US politician)
Donald W. Riegle Jr. (Former US senator)
Ed Zschau (US politician)Other former IBM employees include:

NASA astronaut Michael J. Massimino
Canadian astronaut and former Governor General Julie Payette
musician Dave Matthews
Harvey Mudd College president Maria Klawe
Western Governors University president emeritus Robert Mendenhall
former University of Kentucky president Lee T. Todd Jr.
NFL referee Bill Carollo
former Rangers F.C. chairman John McClelland
recipient of the Nobel Prize in Literature J. M. Coetzee

Board and shareholders
The company's 15-member board of directors are responsible for overall corporate management and includes the current or former CEOs of Anthem, Dow Chemical, Johnson and Johnson, Royal Dutch Shell, UPS, and Vanguard as well as the president of Cornell University and a retired U.S. Navy admiral.In 2011, IBM became the first technology company Warren Buffett's holding company Berkshire Hathaway invested in. Initially he bought 64 million shares costing $10.5 billion. Over the years, Buffett increased his IBM holdings, but by the end of 2017 had reduced them by 94.5% to 2.05 million shares; by May 2018, he was completely out of IBM.

See also
IBM SkillsBuild
List of electronics brands
List of largest Internet companies
List of largest manufacturing companies by revenue
Tech companies in the New York City metropolitan region
Top 100 US Federal Contractors

References
Further reading
External links

Official website 
IBM companies grouped at OpenCorporates
Business data for IBM:

IEC 62264

IEC 62264 is an international standard for enterprise control system integration. This standard is based upon ANSI/ISA-95.

Current parts of IEC 62264
IEC 62264 consists of the following parts detailed in separate IEC 62264 standard documents:

Part 1:2013 Object Models and Attributes of Manufacturing Operations (Second edition 2013-05) 
Part 2:2013 Object model attributes (Second edition 2013-06) 
Part 3:2016 Activity models of manufacturing operations management (Second edition 2016-12) 
Part 4:2015 Objects models attributes for manufacturing operations management integration 
Part 5:2016 Business to manufacturing transactions
Publicly Available Specification - Pre-standard Part 6:2016 Messaging Service Model

References
External links
"IEC 62264-1", "IEC 62264-2", "IEC 62264-3", "IEC 62264-4", "IEC 62264-5", "IEC 62264-6" at International Electrotechnical Commission

ISA-88

S88, shorthand for ANSI/ISA88, is a standard addressing batch process control. It is a design philosophy for describing equipment, and procedures. It is not a standard for software, it is equally applicable to manual processes. It was approved by the ISA in 1995 and updated in 2010. Its original version was adopted by the IEC in 1997 as IEC 61512-1.
The current parts of the S88 standard include:

Models and terminology
Data structures and guidelines for languages
General and site recipe models and representation
Batch Production Records
Machine and Unit States: An Implementation Example of ISA-88S88 provides a consistent set of standards and terminology for batch control and defines the physical model, procedures, and recipes. The standard sought to address the following problems: lack of a universal model for batch control, difficulty in communicating user requirement, integration among batch automation suppliers, difficulty in batch control configuration.
The standard defines a process model which consists of a process which consists of an ordered set of process stages which consist of an ordered set of process operations which consist of an ordered set of process actions.
The physical model begins with the enterprise which may contain a site which may contain areas which may contain process cells which must contain a unit which may contain equipment modules which may contain control modules. Some of these levels may be excluded, but not the Unit.
The procedural control model consists of recipe procedures which consist of an ordered set of unit procedures which consist of an ordered set of operations which consist of an ordered set of phases. Some of these levels may be excluded.
Recipes can have the following types: general, site, master, control. The contents of the recipe include: header, formula, equipment requirements, procedure, and other information required to make the recipe.

Implemented in other standards
Like in Packml, the Machine and Unit States described by this standard are implemented in other standards.


== References ==

ISBN

The International Standard Book Number (ISBN) is a numeric commercial book identifier that is intended to be unique. Publishers purchase or receive ISBNs from an affiliate of the International ISBN Agency.An ISBN is assigned to each separate edition and variation (except reprintings) of a publication. For example, an e-book, a paperback and a hardcover edition of the same book will each have a different ISBN. The ISBN is ten digits long if assigned before 2007, and thirteen digits long if assigned on or after 1 January 2007. The method of assigning an ISBN is nation-specific and varies between countries, often depending on how large the publishing industry is within a country.
The initial ISBN identification format was devised in 1967, based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the 9-digit SBN code can be converted to a 10-digit ISBN by prefixing it with a zero).
Privately published books sometimes appear without an ISBN. The International ISBN Agency sometimes assigns such books ISBNs on its own initiative.Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines and newspapers. The International Standard Music Number (ISMN) covers musical scores.

History
The Standard Book Number (SBN) is a commercial system using nine-digit code numbers to identify books. In 1965, British bookseller and stationers WHSmith announced plans to implement a standard numbering system for its books. They hired consultants to work on their behalf, and the system was devised by Gordon Foster, emeritus professor of statistics at Trinity College Dublin. The International Organization for Standardization (ISO) Technical Committee on Documentation sought to adapt the British SBN for international use. The ISBN identification format was conceived in 1967 in the United Kingdom by David Whitaker (regarded as the "Father of the ISBN") and in 1968 in the United States by Emery Koltay (who later became director of the U.S. ISBN agency R. R. Bowker).The 10-digit ISBN format was developed by the ISO and was published in 1970 as international standard ISO 2108. The United Kingdom continued to use the nine-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.
An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8", where "340" indicates the publisher, "01381" is the serial number assigned by the publisher, and "8" is the check digit. By prefixing a zero, this can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated. Some publishers, such as Ballantine Books, would sometimes use 12-digit SBNs where the last three digits indicated the price of the book; for example, Woodstock Handmade Houses had a 12-digit Standard Book Number of 345-24223-8-595 (valid SBN: 345-24223-8, ISBN: 0-345-24223-8), and it cost US$5.95.Since 1 January 2007, ISBNs have contained thirteen digits, a format that is compatible with "Bookland" European Article Numbers, which have 13 digits.The United-States, with 3.9 million registered ISBNs in 2020, was by far the biggest user of the ISBN identifier in 2020, followed by the Republic of Korea (329,582), Germany (284,000), China (263,066), the UK (188,553) and Indonesia (144,793). Lifetime ISBNs registered in the United States are over 39 millions in 2020.

Overview
A separate ISBN is assigned to each edition and variation (except reprintings) of a publication. For example, an ebook, audiobook, paperback, and hardcover edition of the same book will each have a different ISBN assigned to it.: 12  The ISBN is thirteen digits long if assigned on or after 1 January 2007, and ten digits long if assigned before 2007. An International Standard Book Number consists of four parts (if it is a 10-digit ISBN) or five parts (for a 13-digit ISBN).
Section 5 of the International ISBN Agency's official user manual: 11  describes the structure of the 13-digit ISBN, as follows:

for a 13-digit ISBN, a prefix element – a GS1 prefix: so far 978 or 979 have been made available by GS1,
the registration group element (language-sharing country group, individual country or territory),
the registrant element,
the publication element, and
a checksum character or check digit.A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.

Issuing process
ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from the government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.A full directory of ISBN agencies is available on the International ISBN Agency website. A list for a few countries is given below:

Australia – Thorpe-Bowker
Brazil – The National Library of Brazil; (Up to 28 February 2020)
Brazil – Câmara Brasileira do Livro (From 1 March 2020)
Canada – English Library and Archives Canada, a government agency; French Bibliothèque et Archives nationales du Québec;
Colombia – Cámara Colombiana del Libro, an NGO
Hong Kong – Books Registration Office (BRO), under the Hong Kong Public Libraries
Iceland – Landsbókasafn (National and University Library of Iceland)
India – The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development
Israel – The Israel Center for Libraries
Italy – EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association)
Kenya – National Library of Kenya
Lebanon – Lebanese ISBN Agency
Maldives – The National Bureau of Classification (NBC)
Malta – The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb)
Morocco – The National Library of Morocco
New Zealand – The National Library of New Zealand
Nigeria – National Library of Nigeria
Pakistan – National Library of Pakistan
Philippines – National Library of the Philippines
South Africa – National Library of South Africa
Spain – Spanish ISBN Agency – Agencia del ISBN
Turkey – General Directorate of Libraries and Publications, a branch of the Ministry of Culture
United Kingdom and Republic of Ireland – Nielsen Book Services Ltd, part of Nielsen Holdings N.V.
United States – R. R. Bowker

Registration group element
The ISBN registration group element is a 1-to-5-digit number that is valid within a single prefix element (i.e. one of 978 or 979),: 11  and can be separated between hyphens, such as "978-1-...". Registration groups have primarily been allocated within the 978 prefix element. The single-digit registration groups within the 978-prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. Example 5-digit registration groups are 99936 and 99980, for Bhutan. The allocated registration groups are: 0–5, 600–631, 65, 7, 80–94, 950–989, 9910–9989, and 99901–99993. Books published in rare languages typically have longer group elements.Within the 979 prefix element, the registration group 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN. The registration groups within prefix element 979 that have been assigned are 8 for the United States of America, 10 for France, 11 for the Republic of Korea, and 12 for Italy.The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero to a 9-digit SBN creates a valid 10-digit ISBN.

Registrant element
The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not legally required to assign an ISBN, although most large bookstores only handle publications that have ISBNs assigned to them.The International ISBN Agency maintains the details of over one million ISBN prefixes and publishers in the Global Register of Publishers. This database is freely searchable over the internet.
Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.
By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements. Here are some sample ISBN-10 codes, illustrating block length variations.

English language pattern
English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:

Check digits
A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the 10-digit ISBN is an extension of that for SBNs, so the two systems are compatible; an SBN prefixed with a zero (the 10-digit ISBN) will give the same check digit as the SBN without the zero. The check digit is base eleven, and can be an integer between 0 and 9, or an 'X'. The system for 13-digit ISBNs is not compatible with SBNs and will, in general, give a different check digit from the corresponding 10-digit ISBN, so does not provide the same protection against transposition. This is because the 13-digit code was required to be compatible with the EAN format, and hence could not contain an 'X'.

ISBN-10 check digits
According to the 2001 edition of the International ISBN Agency's official user manual, the ISBN-10 check digit (which is the last digit of the 10-digit ISBN) must range from 0 to 10 (the symbol 'X' is used for 10), and must be such that the sum of the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11. That is, if xi is the ith digit, then x10 must be chosen such that:

For example, for an ISBN-10 of 0-306-40615-2:

Formally, using modular arithmetic, this is rendered

It is also true for ISBN-10s that the sum of all ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:

Formally, this is rendered

The two most common errors in handling an ISBN (e.g. when typing it or writing it down) are a single altered digit or the transposition of adjacent digits. It can be proven mathematically that all pairs of valid ISBN-10s differ in at least two digits. It can also be proven that there are no pairs of valid ISBN-10s with eight identical digits and two transposed digits. (These proofs are true because the ISBN is less than eleven digits long and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e., if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error were to occur in the publishing house and remain undetected, the book would be issued with an invalid ISBN.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).

ISBN-10 check digit calculation
Each of the first nine digits of the 10-digit ISBN – excluding the check digit itself – is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.
For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:

Adding 2 to 130 gives a multiple of 11 (because 132 = 12×11) – this is the only number between 0 and 10 which does so. Therefore, the check digit has to be 2, and the complete sequence is ISBN 0-306-40615-2. If the value of 
  
    
      
        
          x
          
            10
          
        
      
    
    {\displaystyle x_{10}}
   required to satisfy this condition is 10, then an 'X' should be used.
Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation, the calculation could result in a check digit value of 11 − 0 = 11, which is invalid. (Strictly speaking, the first "modulo 11" is not needed, but it may be considered to simplify the calculation.)
For example, the check digit for the ISBN of 0-306-40615-? is calculated as follows:

Thus the check digit is 2.
It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:

The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.

ISBN-13 check digit calculation
Appendix 1 of the International ISBN Agency's official user manual: 33  describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10. As ISBN-13 is a subset of EAN-13, the algorithm for calculating the check digit is exactly the same for both.
Formally, using modular arithmetic, this is rendered:

The calculation of an ISBN-13 check digit begins with the first twelve digits of the 13-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero replaces a ten, so, in all cases, a single check digit results.
For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:

s = 9×1 + 7×3 + 8×1 + 0×3 + 3×1 + 0×3 + 6×1 + 4×3 + 0×1 + 6×3 + 1×1 + 5×3
  =   9 +  21 +   8 +   0 +   3 +   0 +   6 +  12 +   0 +  18 +   1 +  15
  = 93
93 / 10 = 9 remainder 3
10 –  3 = 7

Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.
In general, the ISBN check digit is calculated as follows.
Let

Then

This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3 × 6 + 1 × 1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3 × 1 + 1 × 6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0–9 to express the check digit.
Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).

ISBN-10 to ISBN-13 conversion
A 10-digit ISBN is converted to a 13-digit ISBN by prepending "978" to the  ISBN-10 and recalculating the final checksum digit using the ISBN-13 algorithm. The reverse process can also be performed, but not for numbers commencing with a prefix other than 978, which have no 10-digit equivalent.

Errors in usage
Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers. For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.
Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN". The International Union Library Catalog (a.k.a., WorldCat OCLC – Online Computer Library Center system) often indexes by invalid ISBNs, if the book is indexed in that way by a member library.

eISBN
Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.

EAN format used in barcodes, and upgrading
The barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits called an EAN-5 for the currency and the recommended retail price. For 10-digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN-13 formula (modulo 10, 1× and 3× weighting on alternating digits).
Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a 13-digit ISBN (ISBN-13). The process began on 1 January 2005 and was planned to conclude on 1 January 2007. As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. The 10-digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero, and for checksum purposes it counted as a 3. All ISMNs are now thirteen digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.
Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the 10-digit ISBN check digit generally is not the same as the 13-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN in North America.

See also
ASIN (Amazon Standard Identification Number)
BICI (Book Item and Component Identifier)
Book sources search – a Wikipedia resource that allows search by ISBNs
CODEN (serial publication identifier currently used by libraries; replaced by the ISSN for new works)
DOI (Digital Object Identifier)
ESTC (English Short Title Catalogue)
ISAN (International Standard Audiovisual Number)
ISRC (International Standard Recording Code)
ISTC (International Standard Text Code)
ISWC (International Standard Musical Work Code)
ISWN (International Standard Wine Number)
LCCN (Library of Congress Control Number)
License number (East German books) (Book identification system used between 1951 and 1990 in the former GDR)
List of group-0 ISBN publisher codes
List of group-1 ISBN publisher codes
List of ISBN registration groups
SICI (Serial Item and Contribution Identifier)
VD 16 (Verzeichnis der im deutschen Sprachbereich erschienenen Drucke des 16. Jahrhunderts, "Bibliography of Books Printed in the German Speaking Countries of the Sixteenth Century")
VD 17 (Verzeichnis der im deutschen Sprachraum erschienenen Drucke des 17. Jahrhunderts, "Bibliography of Books Printed in the German Speaking Countries of the Seventeenth Century")

Explanatory notes
References
External links

ISO 2108:2017 – International Standard Book Number (ISBN)
International ISBN Agency – coordinates and supervises the worldwide use of the ISBN system
Numerical List of Group Identifiers – List of language/region prefixes
Free conversion tool: ISBN-10 to ISBN-13 & ISBN-13 to ISBN-10 from the ISBN agency. Also shows correct hyphenation & verifies if ISBNs are valid or not.
"Guidelines for the Implementation of 13-Digit ISBNs" (PDF). Archived from the original (PDF) on 12 September 2004
RFC 3187 – Using International Standard Book Numbers as Uniform Resource Names (URN)

ISSN

An International Standard Serial Number (ISSN) is an eight-digit serial number used to uniquely identify a serial publication, such as a magazine. The ISSN is especially helpful in distinguishing between serials with the same title. ISSNs are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature.The ISSN system was first drafted as an International Organization for Standardization (ISO) international standard in 1971 and published as ISO 3297 in 1975. ISO subcommittee TC 46/SC 9 is responsible for maintaining the standard.
When a serial with the same content is published in more than one media type, a different ISSN is assigned to each media type. For example, many serials are published both in print and electronic media. The ISSN system refers to these types as print ISSN (p-ISSN) and electronic ISSN (e-ISSN). Consequently, as defined in ISO 3297:2007, every serial in the ISSN system is also assigned a linking ISSN (ISSN-L), typically the same as the ISSN assigned to the serial in its first published medium, which links together all ISSNs assigned to the serial in every medium.

Code format
An ISSN is an eight-digit code, divided by a hyphen into two four-digit numbers. The last digit, which may be zero through nine or an X, is a check digit, so the ISSN is uniquely represented by its first seven digits. Formally, the general form of the ISSN (also named "ISSN structure" or "ISSN syntax") can be expressed as follows:

where N is in the set {0,1,2,...,9}, a decimal digit character, and C is in {0,1,2,...,9,X}; or by a Perl Compatible Regular Expressions (PCRE) regular expression:

For example, the ISSN of the journal Hearing Research, is 0378-5955, where the final 5 is the check digit, that is C=5. To calculate the check digit, the following algorithm may be used:

 
To confirm the check digit, calculate the sum of all eight digits of the ISSN multiplied by their position in the number, counting from the right. (If the check digit is X, add 10 to the sum.) The remainder of the sum modulo 11 must be 0. There is an online ISSN checker that can validate an ISSN, based on the above algorithm.

In EANs
ISSNs can be encoded in EAN-13 bar codes with a 977 "country code" (compare the 978 country code ("bookland") for ISBNs), followed by the 7 main digits of the ISSN (the check digit is not included), followed by 2 publisher-defined digits, followed by the EAN check digit (which need not match the ISSN check digit).

Code assignment, maintenance and look-up
ISSN codes are assigned by a network of ISSN National Centres, usually located at national libraries and coordinated by the ISSN International Centre based in Paris. The International Centre is an intergovernmental organization created in 1974 through an agreement between UNESCO and the French government.

Linking ISSN
ISSN-L is a unique identifier for all versions of the serial containing the same content across different media. As defined by ISO 3297:2007, the "linking ISSN (ISSN-L)" provides a mechanism for collocation or linking among the different media versions of the same continuing resource. The ISSN-L is one of a serial's existing ISSNs, so does not change the use or assignment of "ordinary" ISSNs; it is based on the ISSN of the first published medium version of the publication. If the print and online versions of the publication are published at the same time, the ISSN of the print version is chosen as the basis of the ISSN-L.
With ISSN-L is possible to designate one single ISSN for all those media versions of the title. The use of ISSN-L facilitates search, retrieval and delivery across all media versions for services like OpenURL, library catalogues, search engines or knowledge bases.

Register
The International Centre maintains a database of all ISSNs assigned worldwide, the ISDS Register (International Serials Data System), otherwise known as the ISSN Register. At the end of 2016, the ISSN Register contained records for 1,943,572 items. The Register is not freely available for interrogation on the web, but is available by subscription.

The print version of a serial typically will include the ISSN code as part of the publication information.
Most serial websites contain ISSN code information.
Derivative lists of publications will often contain ISSN codes; these can be found through on-line searches with the ISSN code itself or serial title.
WorldCat permits searching its catalog by ISSN, by entering "issn:" before the code in the query field. One can also go directly to an ISSN's record by appending it to "https://www.worldcat.org/ISSN/", e.g. n2:1021-9749 – Search Results. This does not query the ISSN Register itself, but rather shows whether any WorldCat library holds an item with the given ISSN.

Comparison with other identifiers
ISSN and ISBN codes are similar in concept, where ISBNs are assigned to individual books. An ISBN might be assigned for particular issues of a serial, in addition to the ISSN code for the serial as a whole. An ISSN, unlike the ISBN code, is an anonymous identifier associated with a serial title, containing no information as to the publisher or its location. For this reason a new ISSN is assigned to a serial each time it undergoes a major title change.

Extensions
Since the ISSN applies to an entire serial, other identifiers have been built on top of it to allow references to specific volumes, articles, or other identifiable components (like the table of contents): the Publisher Item Identifier (PII) and the Serial Item and Contribution Identifier (SICI).

Media versus content
Separate ISSNs are needed for serials in different media (except reproduction microforms). Thus, the print and electronic media versions of a serial need separate ISSNs, and CD-ROM versions and web versions require different ISSNs. However, the same ISSN can be used for different file formats (e.g. PDF and HTML) of the same online serial.
This "media-oriented identification" of serials made sense in the 1970s. In the 1990s and onward, with personal computers, better screens, and the Web, it makes sense to consider only content, independent of media. This "content-oriented identification" of serials was a repressed demand during a decade, but no ISSN update or initiative occurred. A natural extension for ISSN, the unique-identification of the articles in the serials, was the main demand application. An alternative serials' contents model arrived with the indecs Content Model and its application, the digital object identifier (DOI), an ISSN-independent initiative, consolidated in the 2000s.
Only later, in 2007, ISSN-L was defined in the new ISSN standard (ISO 3297:2007) as an "ISSN designated by the ISSN Network to enable collocation or versions of a continuing resource linking among the different media".

Use in URNs
An ISSN can be encoded as a uniform resource name (URN) by prefixing it with "urn:ISSN:". For example, Rail could be referred to as "urn:ISSN:0953-4563". URN namespaces are case-sensitive, and the ISSN namespace is all caps. If the checksum digit is "X" then it is always encoded in uppercase in a URN.

Problems
The URNs are content-oriented, but ISSN is media-oriented:

ISSN is not unique when the concept is "a journal is a set of contents, generally copyrighted content": the same journal (same contents and same copyrights) may have two or more ISSN codes. A URN needs to point to "unique content" (a "unique journal" as a "set of contents" reference).Example: Nature has an ISSN for print, 0028-0836, and another for the same content on the Web, 1476-4687; only the oldest (0028-0836) is used as a unique identifier. As the ISSN is not unique, the U.S. National Library of Medicine needed to create, prior to 2007, the NLM Unique ID (JID).ISSN does not offer resolution mechanisms like a digital object identifier (DOI) or a URN does, so the DOI is used as a URN for articles, with (for historical reasons) no need for an ISSN's existence.Example: the DOI name "10.1038/nature13777" can be represented as an HTTP string by https://doi.org/10.1038/nature13777, and is redirected (resolved) to the current article's page; but there is no ISSN online service, like http://dx.issn.org/, to resolve the ISSN of the journal (in this sample 1476-4687).A unique URN for serials simplifies the search, recovery and delivery of data for various services including, in particular, search systems and knowledge databases. ISSN-L (see Linking ISSN above) was created to fill this gap.

Media category labels
The two standard categories of media in which serials are most available are print and electronic. In metadata contexts (e.g., JATS), these may have standard labels.

Print ISSN
p-ISSN is a standard label for "Print ISSN", the ISSN for the print media (paper) version of a serial. Usually it is the "default media" and so the "default ISSN".

Electronic ISSN
e-ISSN (or eISSN) is a standard label for "Electronic ISSN", the ISSN for the electronic media (online) version of a serial.

ROAD
ROAD: Directory of Open Access Scholarly Resources (est. 2013), produced by the ISSN International Centre and UNESCO

See also
CODEN
WorldCat—an ISSN-resolve service

References
External links

ISSN International Centre
ISSN Portal
List of 63800 ISSN numbers and titles
ISSN InterNational Centre (January 2015), ISSN Manual (PDF) (2015 ed.), Paris: ISSN InterNational Centre.
How U.S. publishers can obtain an ISSN, United States: Library of Congress.
ISSN Canada, Ottawa: Library and Archives Canada, 8 January 2020, retrieved 3 April 2020..
Getting an ISSN in the UK, British Library.
Getting an ISSN in France (in French), Bibliothèque nationale de France, 15 June 2023
Getting an ISSN in Germany (in German), Deutsche Nationalbibliothek
Getting an ISSN in South Africa, National Library of South Africa, archived from the original on 24 December 2017, retrieved 7 January 2015

Industrial engineering

Industrial engineering is an engineering profession that is concerned with the optimization of complex processes, systems, or organizations by developing, improving and implementing integrated systems of people, money, knowledge, information and equipment. Industrial engineering is central to manufacturing operations.Industrial engineers use specialized knowledge and skills in the mathematical,  physical, and social sciences, together with engineering analysis and design principles and methods, to specify, predict, and evaluate the results obtained from systems and processes. Several industrial engineering principles are followed in the manufacturing industry to ensure the effective flow of systems, processes, and operations. These include:

Lean Manufacturing
Six Sigma
Information Systems
Process Capability
Define, Measure, Analyze, Improve and Control (DMAIC).These principles allow the creation of new systems, processes or situations for the useful coordination of labor, materials and machines and also improve the quality and productivity of systems, physical or social. Depending on the subspecialties involved, industrial engineering may also overlap with, operations research, systems engineering, manufacturing engineering, production engineering, supply chain engineering, management science,  engineering management, financial engineering, ergonomics or human factors engineering, safety engineering, logistics engineering, quality engineering or other related capabilities or fields.

History
Origins
Industrial engineering
There is a general consensus among historians that the roots of the industrial engineering profession date back to the Industrial Revolution.  The technologies that helped mechanize traditional manual operations in the textile industry including the flying shuttle, the spinning jenny, and perhaps most importantly the steam engine generated economies of scale that made mass production in centralized locations attractive for the first time.  The concept of the production system had its genesis in the factories created by these innovations.  It has also been suggested that perhaps Leonardo da Vinci was the first industrial engineer because there is evidence that he applied science to the analysis of human work by examining the rate at which a man could shovel dirt around the year 1500. Others also state that the industrial engineering profession grew from Charles Babbage’s study of factory operations and specifically his work on the manufacture of straight pins in 1832 . However, it has been generally argued that these early efforts, while valuable, were merely observational and did not attempt to engineer the jobs studied or increase overall output.

Specialization of labour
Adam Smith's concepts of Division of Labour and the "Invisible Hand" of capitalism introduced in his treatise The Wealth of Nations motivated many of the technological innovators of the Industrial Revolution to establish and implement factory systems.  The efforts of James Watt and Matthew Boulton led to the first integrated machine manufacturing facility in the world, including the application of concepts such as cost control systems to reduce waste and increase productivity and the institution of skills training for craftsmen.Charles Babbage became associated with industrial engineering because of the concepts he introduced in his book On the Economy of Machinery and Manufacturers which he wrote as a result of his visits to factories in England and the United States in the early 1800s.  The book includes subjects such as the time required to perform a specific task, the effects of subdividing tasks into smaller and less detailed elements, and the advantages to be gained from repetitive tasks.

Interchangeable parts
Eli Whitney and Simeon North proved the feasibility of the notion of interchangeable parts in the manufacture of muskets and pistols for the US Government.  Under this system, individual parts were mass-produced to tolerances to enable their use in any finished product.  The result was a significant reduction in the need for skill from specialized workers, which eventually led to the industrial environment to be studied later.

Pioneers
Frederick Taylor (1856–1915) is generally credited as being the father of the industrial engineering discipline.  He earned a degree in mechanical engineering from Stevens Institute of Technology and earned several patents from his inventions.  His books, Shop Management and The Principles of Scientific Management, which were published in the early 1900s, were the beginning of industrial engineering.  Improvements in work efficiency under his methods was based on improving work methods, developing of work standards, and reduction in time required to carry out the work.  With an abiding faith in the scientific method, Taylor did many experiments in machine shop work on machines as well as men. Taylor developed "time study" to measure time taken for various elements of a task and then used the study observations to reduce the time further. Time study was done for the improved method once again to provide time standards which are accurate for planning manual tasks and also for providing incentives.The husband-and-wife team of Frank Gilbreth (1868–1924) and Lillian Gilbreth (1878–1972) was the other cornerstone of the industrial engineering movement whose work is housed at Purdue University School of Industrial Engineering.  They categorized the elements of human motion into 18 basic elements called therbligs.  This development permitted analysts to design jobs without knowledge of the time required to do a job. These developments were the beginning of a much broader field known as human factors or ergonomics.In 1908, the first course on industrial engineering was offered as an elective at Pennsylvania State University, which became a separate program in 1909 through the efforts of Hugo Diemer. The first doctoral degree in industrial engineering was awarded in 1933 by Cornell University.
In 1912, Henry Laurence Gantt developed the Gantt chart, which outlines actions the organization along with their relationships. This chart opens later form familiar to us today by Wallace Clark.With the development of assembly lines, the factory of Henry Ford (1913) accounted for a significant leap forward in the field. Ford reduced the assembly time of a car from more than 700 hours to 1.5 hours. In addition, he was a pioneer of the economy of the capitalist welfare ("welfare capitalism") and the flag of providing financial incentives for employees to increase productivity.
In 1927, the then Technische Hochschule Berlin was the first German university to introduce the degree.  The course of studies developed by Willi Prion was then still called Business and Technology and was intended to provide descendants of industrialists with an adequate education. 
Comprehensive quality management system (Total quality management or TQM) developed in the forties was gaining momentum after World War II and was part of the recovery of Japan after the war.
The American Institute of Industrial Engineering was formed in 1948. The early work by F. W. Taylor and the Gilbreths was documented in papers presented to the American Society of Mechanical Engineers as interest grew from merely improving machine performance to the performance of the overall manufacturing process, most notably starting with the presentation by Henry R. Towne (1844–1924) of his paper The Engineer as An Economist (1886).

Modern practice
From 1960 to 1975, with the development of decision support systems in supply such as material requirements planning (MRP), one can emphasize the timing issue (inventory, production, compounding, transportation, etc.) of industrial organization. Israeli scientist Dr. Jacob Rubinovitz installed the CMMS program developed in IAI and Control-Data (Israel) in 1976 in South Africa and worldwide.
In the 1970s, with the penetration of Japanese management theories such as Kaizen and Kanban, Japan realized very high levels of quality and productivity.  These theories improved issues of quality, delivery time, and flexibility.  Companies in the west realized the great impact of Kaizen and started implementing their own continuous improvement programs. W. Edwards Deming made significant contributions in the minimization of variance starting in the 1950s and continuing to the end of his life. 
In the 1990s, following the global industry globalization process, the emphasis was on supply chain management and customer-oriented business process design. The theory of constraints, developed by Israeli scientist Eliyahu M. Goldratt (1985), is also a significant milestone in the field.

Comparison to other engineering disciplines
Engineering is traditionally decompositional. To understand the whole of something, it is first broken down into its parts. One masters the parts, then puts them back together to create a better understanding of how to master the whole. The approach of industrial and systems engineering (ISE) is opposite; any one part cannot be understood without the context of the whole system. Changes in one part of the system affect the entire system, and the role of a single part is to better serve the whole system.
Also, industrial engineering considers the human factor and its relation to the technical aspect of the situation and all of the other factors that influence the entire situation, while other engineering disciplines focus on the design of inanimate objects.
"Industrial Engineers integrate combinations of people, information, materials, and equipment that produce innovative and efficient organizations. In addition to manufacturing, Industrial Engineers work and consult in every industry, including hospitals, communications, e-commerce, entertainment, government, finance, food, pharmaceuticals, semiconductors, sports, insurance, sales, accounting, banking, travel, and transportation.""Industrial Engineering is the branch of Engineering most closely related to human resources in that we apply social skills to work with all types of employees, from engineers to salespeople to top management. One of the main focuses of an Industrial Engineer is to improve the working environments of people – not to change the worker, but to change the workplace.""All engineers, including Industrial Engineers, take mathematics through calculus and differential equations. Industrial Engineering is different in that it is based on discrete variable math, whereas all other engineering is based on continuous variable math. We emphasize the use of linear algebra and difference equations, as opposed to the use of differential equations which are so prevalent in other engineering disciplines. This emphasis becomes evident in optimization of production systems in which we are sequencing orders, scheduling batches, determining the number of materials handling units, arranging factory layouts, finding sequences of motions, etc. As, Industrial Engineers, we deal almost exclusively with systems of discrete components."

Etymology
Etymology
While originally applied to manufacturing, the use of industrial in industrial engineering can be somewhat misleading, since it has grown to encompass any methodical or quantitative approach to optimizing how a process, system, or organization operates. In fact, the industrial in industrial engineering means the industry in its broadest sense. People have changed the term industrial to broader terms such as industrial and manufacturing engineering, industrial and systems engineering, industrial engineering and operations research, industrial engineering and management.

Sub-disciplines
Industrial engineering has many sub-disciplines, the most common of which are listed below. Although there are industrial engineers who focus exclusively on one of these sub-disciplines, many deals with a combination of them such as supply chain and logistics, and facilities and energy management.Methods engineering

Facilities engineering & energy management

Financial engineering

Energy engineering

Human factors & safety engineering

Information systems engineering & management

Manufacturing engineering

Operations engineering & managementOperations research & optimization

Policy planning

Production engineeringQuality & reliability engineering

Supply chain management & logistics

Systems engineering & analysis

Systems simulation

Related disciplines
Organization development & change management
Behavioral economics

Education
Industrial engineers study the interaction of human beings with machines, materials, information, procedures and environments in such developments and in designing a technological system.Industrial engineering degrees accredited within any member country of the Washington Accord enjoy equal accreditation within all other signatory countries, thus allowing engineers from one country to practice engineering professionally in any other.
Universities offer degrees at the bachelor, masters, and doctoral level.

Undergraduate curriculum
In the United States, the undergraduate degree earned is either a bachelor of science (B.S.) or a bachelor of science and engineering (B.S.E.) in industrial engineering (IE). In South Africa, the undergraduate degree is a bachelor of engineering (BEng). Variations of the title include Industrial & Operations Engineering (IOE), and Industrial & Systems Engineering (ISE or ISyE). 
The typical curriculum includes a broad math and science foundation spanning chemistry, physics, mechanics (i.e., statics, kinematics, and dynamics), materials science, computer science, electronics/circuits, engineering design, and the standard range of engineering mathematics (i.e., calculus, linear algebra, differential equations, statistics). For any engineering undergraduate program to be accredited, regardless of concentration, it must cover a largely similar span of such foundational work – which also overlaps heavily with the content tested on one or more engineering licensure exams in most jurisdictions.
The coursework specific to IE entails specialized courses in areas such as optimization, applied probability, stochastic modeling, design of experiments, statistical process control, simulation, manufacturing engineering, ergonomics/safety engineering, and engineering economics. Industrial engineering elective courses typically cover more specialized topics in areas such as manufacturing, supply chains and logistics, analytics and machine learning, production systems, human factors and industrial design, and service systems.Certain business schools may offer programs with some overlapping relevance to IE, but the engineering programs are distinguished by a much more intensely quantitative focus, required engineering science electives, and the core math and science courses required of all engineering programs.

Graduate curriculum
The usual graduate degree earned is the master of science (MS), master of science and engineering (MSE) or master of engineering (MEng) in industrial engineering or various alternative related concentration titles.
Typical MS curricula may cover:

Differences in teaching
While industrial engineering as a formal degree has been around for years, consensus on what topics should be taught and studied differs across countries. For example, Turkey focuses on a very technical degree while Denmark, Finland and the United Kingdom have a management focus degree, thus making it less technical. The United States, meanwhile, focuses on case studies, group problem solving and maintains a balance between the technical and non-technical side.

Practicing engineers
Traditionally, a major aspect of industrial engineering was planning the layouts of factories and designing assembly lines and other manufacturing paradigms. And now, in lean manufacturing systems, industrial engineers work to eliminate wastes of time, money, materials, energy, and other resources.
Examples of where industrial engineering might be used include flow process charting, process mapping, designing an assembly workstation, strategizing for various operational logistics, consulting as an efficiency expert, developing a new financial algorithm or loan system for a bank, streamlining operation and emergency room location or usage in a hospital, planning complex distribution schemes for materials or products (referred to as supply-chain management), and shortening lines (or queues) at a bank, hospital, or a theme park.
Modern industrial engineers typically use predetermined motion time systems, computer simulation (especially discrete event simulation), along with extensive mathematical tools for modeling, such as mathematical optimization and queueing theory, and computational methods for system analysis, evaluation, and optimization. Industrial engineers also use the tools of data science and machine learning in their work owing to the strong relatedness of these disciplines with the field and the similar technical background required of industrial engineers (including a strong foundation in probability theory, linear algebra, and statistics, as well as having coding skills).

See also
Related topics
Associations
Notes
Further reading
Badiru, A. (Ed.) (2005).  Handbook of industrial and systems engineering. CRC Press. ISBN 0-8493-2719-9.
B. S. Blanchard and Fabrycky, W. (2005).  Systems Engineering and Analysis (4th Edition).  Prentice-Hall. ISBN 0-13-186977-9.
Salvendy, G. (Ed.) (2001).  Handbook of industrial engineering: Technology and operations management. Wiley-Interscience. ISBN 0-471-33057-4.
Turner, W. et al. (1992). Introduction to industrial and systems engineering (Third edition). Prentice Hall.  ISBN 0-13-481789-3.
Eliyahu M. Goldratt, Jeff Cox (1984). The Goal  North River Press; 2nd Rev edition (1992). ISBN 0-88427-061-0; 20th Anniversary edition (2004) ISBN 0-88427-178-1
Miller, Doug, Towards Sustainable Labour Costing in UK Fashion Retail (February 5, 2013). doi:10.2139/ssrn.2212100
Malakooti, B. (2013). Operations and Production Systems with Multiple Objectives. John Wiley & Sons.ISBN 978-1-118-58537-5
Systems Engineering Body of Knowledge (SEBoK)
Traditional Engineering
Master of Engineering Administration (MEA)
Kambhampati, Venkata Satya Surya Narayana Rao (2017). "Principles of Industrial Engineering" IIE Annual Conference. Proceedings; Norcross (2017): 890-895.[1]

External links
 Media related to Industrial engineering at Wikimedia Commons

Inventory

Inventory (American English) or stock (British English) refers to the goods and materials that a business holds for the ultimate goal of resale, production or utilisation.Inventory management is a discipline primarily about specifying the shape and placement of stocked goods. It is required at different locations within a facility or within many locations of a supply network to precede the regular and planned course of production and stock of materials.
The concept of inventory, stock or work in process (or work in progress) has been extended from manufacturing systems to service businesses  and projects, by generalizing the definition to be "all work within the process of production—all work that is or has occurred prior to the completion of production". In the context of a manufacturing production system, inventory refers to all work that has occurred—raw materials, partially finished products, finished products prior to sale and departure from the manufacturing system. In the context of services, inventory refers to all work done prior to sale, including partially process information.

Business inventory
Reasons for keeping stock
There are five basic reasons for keeping an inventory:

Time: The time lags present in the supply chain, from supplier to user at every stage, requires that you maintain certain amounts of inventory to use in this lead time. However, in practice, inventory is to be maintained for consumption during 'variations in lead time'. Lead time itself can be addressed by ordering that many days in advance.
Seasonal demand: Demands varies periodically, but producers capacity is fixed. This can lead to stock accumulation, consider for example how goods consumed only in holidays can lead to accumulation of large stocks on the anticipation of future consumption.
Uncertainty: Inventories are maintained as buffers to meet uncertainties in demand, supply and movements of goods.
Economies of scale: Ideal condition of "one unit at a time at a place where a user needs it, when he needs it" principle tends to incur lots of costs in terms of logistics. So bulk buying, movement and storing brings in economies of scale, thus inventory.
Appreciation in value: In some situations, some stock gains the required value when it is kept for some time to allow it reach the desired standard for consumption, or for production. For example, beer in the brewing industry.All these stock reasons can apply to any owner or product.

Special terms used in dealing with inventory management
Stock Keeping Unit (SKU) SKUs are clear, internal identification numbers assigned to each of the products and their variants. SKUs can be any combination of letters and numbers chosen, just as long as the system is consistent and used for all the products in the inventory. An SKU code may also be referred to as product code, barcode, part number or MPN (Manufacturer's Part Number)."Specialinvestor.com". www.specialinvestor.com. Archived from the original on 31 December 2006. Retrieved 8 May 2018.</ref>
"New old stock" (sometimes abbreviated NOS) is a term used in business to refer to merchandise being offered for sale that was manufactured long ago but that has never been used. Such merchandise may not be produced anymore, and the new old stock may represent the only market source of a particular item at the present time.
ABC analysis (also known as Pareto analysis) is a method of classifying inventory items based on their contribution to total sales revenue. This can be used to prioritize inventory management efforts and ensure that businesses are focusing on the most important items.

Typology
Buffer/safety stock: Safety stock is the additional inventory that a company keeps on hand to mitigate the risk of stockouts or delays in supply chain. It is the extra stock that is kept in reserve above and beyond the regular inventory levels. The purpose of safety stock is to provide a buffer against fluctuations in demand or supply that could otherwise result in stockouts.
Reorder level: Reorder level refers to the point when a company place an order to re-fill the stocks. Reorder point depends on the inventory policy of a company. Some companies place orders when the inventory level is lower than a certain quantity. Some companies place orders periodically.
Cycle stock: Used in batch processes, cycle stock is the available inventory, excluding buffer stock.
De-coupling: Buffer stock held between the machines in a single process which serves as a buffer for the next one allowing smooth flow of work instead of waiting the previous or next machine in the same process.
Anticipation stock: Building up extra stock for periods of increased demand—e.g., ice cream for summer.
Pipeline stock: Goods still in transit or in the process of distribution; e.g., they have left the factory but not arrived at the customer yet. Often calculated as: Average Daily / Weekly usage quantity X Lead time in days + Safety stock.

Inventory examples
While accountants often discuss inventory in terms of goods for sale, organizations—manufacturers, service-providers and not-for-profits—also have inventories (fixtures, equipment, furniture, supplies, parts, etc.) that they do not intend to sell. Manufacturers', distributors', and wholesalers' inventory tends to cluster in warehouses. Retailers' inventory may exist in a warehouse or in a shop or store accessible to customers. Inventories not intended for sale to customers or to clients may be held in any premises an organization uses. Stock ties up cash and, if uncontrolled, it will be impossible to know the actual level of stocks and therefore difficult to keep the costs associated with holding too much or too little inventory under control.
While the reasons for holding stock were covered earlier, most manufacturing organizations usually divide their "goods for sale" inventory into:

Raw materials: Materials and components scheduled for use in making a product.
Work in process (WIP): Materials and components that have begun their transformation to finished goods. These are used in process of manufacture and as such these are neither raw material nor finished goods.
Finished goods: Goods ready for sale to customers.
Goods for resale: Returned goods that are salable.
Stocks in transit: The materials which are not at the seller's location or buyers' location but in between are "stocks in transit". Or we could say, the stocks which left the seller's plant but have not reached the buyer, and are in transit.
Consignment stocks: The inventories where goods are with the buyer, but the actual ownership of goods remains with the seller until the goods are sold. Though the goods were transported to the buyer, payment of goods is done once the goods are sold. Hence such stocks are known as consignment stocks.
Maintenance supply.For example:

Manufacturing
A canned food manufacturer's materials inventory includes the ingredients to form the foods to be canned, empty cans and their lids (or coils of steel or aluminum for constructing those components), labels, and anything else (solder, glue, etc.) that will form part of a finished can. The firm's work in process includes those materials from the time of release to the work floor until they become complete and ready for sale to wholesale or retail customers. This may be vats of prepared food, filled cans not yet labeled or sub-assemblies of food components. It may also include finished cans that are not yet packaged into cartons or pallets. Its finished good inventory consists of all the filled and labeled cans of food in its warehouse that it has manufactured and wishes to sell to food distributors (wholesalers), to grocery stores (retailers), and even perhaps to consumers through arrangements like factory stores and outlet centers.

Capital projects
The partially completed work (or work in process) is a measure of inventory built during the work execution of a capital project, such as encountered in civilian infrastructure construction or oil and gas. Inventory may not only reflect physical items (such as materials, parts, partially-finished sub-assemblies) but also knowledge work-in-process (such as partially completed engineering designs of components and assemblies to be fabricated).

Virtual inventory
A "virtual inventory" (also known as a "bank inventory") enables a group of users to share common parts, especially where their availability at short notice may be critical but they are unlikely to required by more than a few bank members at any one time. Virtual inventory also allows distributors and fulfilment houses to ship goods to retailers direct from stock regardless of whether the stock is held in a retail store, stock room or warehouse.

Costs associated with inventory
There are several costs associated with inventory:

Ordering cost
Setup cost
Holding Cost
Shortage Cost

Principle of inventory proportionality
Purpose
Inventory proportionality is the goal of demand-driven inventory management. The primary optimal outcome is to have the same number of days' (or hours', etc.) worth of inventory on hand across all products so that the time of runout of all products would be simultaneous. In such a case, there is no "excess inventory", that is, inventory that would be left over of another product when the first product runs out. Holding excess inventory is sub-optimal because the money spent to obtain and the cost of holding it could have been utilized better elsewhere, i.e. to the product that just ran out.
The secondary goal of inventory proportionality is inventory minimization. By integrating accurate demand forecasting with inventory management, rather than only looking at past averages, a much more accurate and optimal outcome is expected. Integrating demand forecasting into inventory management in this way also allows for the prediction of the "can fit" point when inventory storage is limited on a per-product basis.

Applications
The technique of inventory proportionality is most appropriate for inventories that remain unseen by the consumer, as opposed to "keep full" systems where a retail consumer would like to see full shelves of the product they are buying so as not to think they are buying something old, unwanted or stale; and differentiated from the "trigger point" systems where product is reordered when it hits a certain level; inventory proportionality is used effectively by just-in-time manufacturing processes and retail applications where the product is hidden from view.
One early example of inventory proportionality used in a retail application in the United States was for motor fuel. Motor fuel (e.g. gasoline) is generally stored in underground storage tanks. The motorists do not know whether they are buying gasoline off the top or bottom of the tank, nor need they care. Additionally, these storage tanks have a maximum capacity and cannot be overfilled. Finally, the product is expensive. Inventory proportionality is used to balance the inventories of the different grades of motor fuel, each stored in dedicated tanks, in proportion to the sales of each grade. Excess inventory is not seen or valued by the consumer, so it is simply cash sunk (literally) into the ground. Inventory proportionality minimizes the amount of excess inventory carried in underground storage tanks. This application for motor fuel was first developed and implemented by Petrolsoft Corporation in 1990 for Chevron Products Company. Most major oil companies use such systems today.

Roots
The use of inventory proportionality in the United States is thought to have been inspired by Japanese just-in-time parts inventory management made famous by Toyota Motors in the 1980s.

High-level inventory management
It seems that around 1880 there was a change in manufacturing practice from companies with relatively homogeneous lines of products to horizontally integrated companies with unprecedented diversity in processes and products. Those companies (especially in metalworking) attempted to achieve success through economies of scope—the gains of jointly producing two or more products in one facility. The managers now needed information on the effect of product-mix decisions on overall profits and therefore needed accurate product-cost information. A variety of attempts to achieve this were unsuccessful due to the huge overhead of the information processing of the time. However, the burgeoning need for financial reporting after 1900 created unavoidable pressure for financial accounting of stock and the management need to cost manage products became overshadowed. In particular, it was the need for audited accounts that sealed the fate of managerial cost accounting. The dominance of financial reporting accounting over management accounting remains to this day with few exceptions, and the financial reporting definitions of 'cost' have distorted effective management 'cost' accounting since that time. This is particularly true of inventory.
Hence, high-level financial inventory has these two basic formulas, which relate to the accounting period:

Cost of Beginning Inventory at the start of the period + inventory purchases within the period + cost of production within the period =  cost of goods available
Cost of goods available − cost of ending inventory at the end of the period = cost of goods soldThe benefit of these formulas is that the first absorbs all overheads of production and raw material costs into a value of inventory for reporting. The second formula then creates the new start point for the next period and gives a figure to be subtracted from the sales price to determine some form of sales-margin figure.
Manufacturing management is more interested in inventory turnover ratio or average days to sell inventory since it tells them something about relative inventory levels.

Inventory turnover ratio (also known as inventory turns) = cost of goods sold / Average Inventory = Cost of Goods Sold / ((Beginning Inventory + Ending Inventory) / 2)and its inverse

Average Days to Sell Inventory = Number of Days a Year / Inventory Turnover Ratio = 365 days a year / Inventory Turnover RatioThis ratio estimates how many times the inventory turns over a year. This number tells how much cash/goods are tied up waiting for the process and is a critical measure of process reliability and effectiveness. So a factory with two inventory turns has six months stock on hand, which is generally not a good figure (depending upon the industry), whereas a factory that moves from six turns to twelve turns has probably improved effectiveness by 100%. This improvement will have some negative results in the financial reporting, since the 'value' now stored in the factory as inventory is reduced.
While these accounting measures of inventory are very useful because of their simplicity, they are also fraught with the danger of their own assumptions. There are, in fact, so many things that can vary hidden under this appearance of simplicity that a variety of 'adjusting' assumptions may be used. These include:

Specific Identification
Lower of cost or market
Weighted Average Cost
Moving-Average Cost
FIFO and LIFO.
Queueing theory.Inventory Turn is a financial accounting tool for evaluating inventory and it is not necessarily a management tool. Inventory management should be forward looking. The methodology applied is based on historical cost of goods sold. The ratio may not be able to reflect the usability of future production demand, as well as customer demand.
Business models, including Just in Time (JIT) Inventory, Vendor Managed Inventory (VMI) and Customer Managed Inventory (CMI), attempt to minimize on-hand inventory and increase inventory turns. VMI and CMI have gained considerable attention due to the success of third-party vendors who offer added expertise and knowledge that organizations may not possess.
Inventory management also involves risk which varies depending upon a firm's position in the distribution channel. Some typical measures of inventory exposure are width of commitment, time of duration and depth. Inventory management in modern days is online oriented and more viable in digital. This type of dynamics order management will require end-to-end visibility, collaboration across fulfillment processes, real-time data automation among different companies, and integration among multiple systems.

Accounting for inventory
Each country has its own rules about accounting for inventory that fit with their financial-reporting rules.
For example, organizations in the U.S. define inventory to suit their needs within US Generally Accepted Accounting Practices (GAAP), the rules defined by the Financial Accounting Standards Board (FASB) (and others) and enforced by the U.S. Securities and Exchange Commission (SEC) and other federal and state agencies. Other countries often have similar arrangements but with their own accounting standards and national agencies instead.
It is intentional that financial accounting uses standards that allow the public to compare firms' performance, cost accounting functions internally to an organization and potentially with much greater flexibility. A discussion of inventory from standard and Theory of Constraints-based (throughput) cost accounting perspective follows some examples and a discussion of inventory from a financial accounting perspective.
The internal costing/valuation of inventory can be complex. Whereas in the past most enterprises ran simple, one-process factories, such enterprises are quite probably in the minority in the 21st century. Where 'one process' factories exist, there is a market for the goods created, which establishes an independent market value for the good. Today, with multistage-process companies, there is much inventory that would once have been finished goods which is now held as 'work in process' (WIP). This needs to be valued in the accounts, but the valuation is a management decision since there is no market for the partially finished product. This somewhat arbitrary 'valuation' of WIP combined with the allocation of overheads to it has led to some unintended and undesirable results.

Financial accounting
An organization's inventory can appear a mixed blessing, since it counts as an asset on the balance sheet, but it also ties up money that could serve for other purposes and requires additional expense for its protection. Inventory may also cause significant tax expenses, depending on particular countries' laws regarding depreciation of inventory, as in Thor Power Tool Company v. Commissioner.
Inventory appears as a current asset on an organization's balance sheet because the organization can, in principle, turn it into cash by selling it. Some organizations hold larger inventories than their operations require in order to inflate their apparent asset value and their perceived profitability.
In addition to the money tied up by acquiring inventory, inventory also brings associated costs for warehouse space, for utilities, and for insurance to cover staff to handle and protect it from fire and other disasters, obsolescence, shrinkage (theft and errors), and others. Such holding costs can mount up: between a third and a half of its acquisition value per year. 
Businesses that stock too little inventory cannot take advantage of large orders from customers if they cannot deliver. The conflicting objectives of cost control and customer service often put an organization's financial and operating managers against its sales and marketing departments. Salespeople, in particular, often receive sales-commission payments, so unavailable goods may reduce their potential personal income. This conflict can be minimised by reducing production time to being near or less than customers' expected delivery time. This effort, known as "Lean production" will significantly reduce working capital tied up in inventory and reduce manufacturing costs (See the Toyota Production System).

Role of inventory accounting
By helping the organization to make better decisions, the accountants can help the public sector to change in a very positive way that delivers increased value for the taxpayer's investment. It can also help to incentive's progress and to ensure that reforms are sustainable and effective in the long term, by ensuring that success is appropriately recognized in both the formal and informal reward systems of the organization.
To say that they have a key role to play is an understatement. Finance is connected to most, if not all, of the key business processes within the organization. It should be steering the stewardship and accountability systems that ensure that the organization is conducting its business in an appropriate, ethical manner. It is critical that these foundations are firmly laid. So often they are the litmus test by which public confidence in the institution is either won or lost.
Finance should also be providing the information, analysis and advice to enable the organizations’ service managers to operate effectively. This goes beyond the traditional preoccupation with budgets—how much have we spent so far, how much do we have left to spend? It is about helping the organization to better understand its own performance. That means making the connections and understanding the relationships between given inputs—the resources brought to bear—and the outputs and outcomes that they achieve. It is also about understanding and actively managing risks within the organization and its activities.

FIFO vs. LIFO accounting
When a merchant buys goods from inventory, the value of the inventory account is reduced by the cost of goods sold (COGS). This is simple where the cost has not varied across those held in stock; but where it has, then an agreed method must be derived to evaluate it. For commodity items that one cannot track individually, accountants must choose a method that fits the nature of the sale. Two popular methods in use are: FIFO (first in, first out) and LIFO (last in, first out).
FIFO treats the first unit that arrived in inventory as the first one sold. LIFO considers the last unit arriving in inventory as the first one sold. Which method an accountant selects can have a significant effect on net income and book value and, in turn, on taxation. Using LIFO accounting for inventory, a company generally reports lower net income and lower book value, due to the effects of inflation. This generally results in lower taxation. Due to LIFO's potential to skew inventory value, UK GAAP and IAS have effectively banned LIFO inventory accounting. LIFO accounting is permitted in the United States subject to section 472 of the Internal Revenue Code.

Standard cost accounting
Standard cost accounting uses ratios called efficiencies that compare the labour and materials actually used to produce a good with those that the same goods would have required under "standard" conditions. As long as actual and standard conditions are similar, few problems arise. Unfortunately, standard cost accounting methods developed about 100 years ago, when labor comprised the most important cost in manufactured goods. Standard methods continue to emphasize labor efficiency even though that resource now constitutes a (very) small part of cost in most cases.
Standard cost accounting can hurt managers, workers, and firms in several ways. For example, a policy decision to increase inventory can harm a manufacturing manager's performance evaluation. Increasing inventory requires increased production, which means that processes must operate at higher rates. When (not if) something goes wrong, the process takes longer and uses more than the standard labor time. The manager appears responsible for the excess, even though s/he has no control over the production requirement or the problem.
In adverse economic times, firms use the same efficiencies to downsize, rightsize, or otherwise reduce their labor force. Workers laid off under those circumstances have even less control over excess inventory and cost efficiencies than their managers.
Many financial and cost accountants have agreed for many years on the desirability of replacing standard cost accounting. They have not, however, found a successor.

Theory of constraints cost accounting
Eliyahu M. Goldratt developed the Theory of Constraints in part to address the cost-accounting problems in what he calls the "cost world." He offers a substitute, called throughput accounting, that uses throughput (money for goods sold to customers) in place of output (goods produced that may sell or may boost inventory) and considers labor as a fixed rather than as a variable cost. He defines inventory simply as everything the organization owns that it plans to sell, including buildings, machinery, and many other things in addition to the categories listed here. Throughput accounting recognizes only one class of variable costs: the truly variable costs, like materials and components, which vary directly with the quantity produced
Finished goods inventories remain balance-sheet assets, but labor-efficiency ratios no longer evaluate managers and workers. Instead of an incentive to reduce labor cost, throughput accounting focuses attention on the relationships between throughput (revenue or income) on one hand and controllable operating expenses and changes in inventory on the other.

National accounts
Inventories also play an important role in national accounts and the analysis of the business cycle. Some short-term macroeconomic fluctuations are attributed to the inventory cycle.

Distressed inventory
Also known as distressed or expired stock, distressed inventory is inventory whose potential to be sold at a normal cost has passed or will soon pass. In certain industries it could also mean that the stock is or will soon be impossible to sell. Examples of distressed inventory include products which have reached their expiry date, or have reached a date in advance of expiry at which the planned market will no longer purchase them (e.g. 3 months left to expiry), clothing which is out of fashion, music which is no longer popular and old newspapers or magazines. It also includes computer or consumer-electronic equipment which is obsolete or discontinued and whose manufacturer is unable to support it, along with products which use that type of equipment e.g. VHS format equipment and videos.In 2001, Cisco wrote off inventory worth US$2.25 billion due to duplicate orders. This is considered one of the biggest inventory write-offs in business history.

Stock rotation
Stock rotation is the practice of changing the way inventory is displayed on a regular basis. This is most commonly used in hospitality and retail - particularity where food products are sold. For example, in the case of supermarkets that a customer frequents on a regular basis, the customer may know exactly what they want and where it is. This results in many customers going straight to the product they seek and do not look at other items on sale. To discourage this practice, stores will rotate the location of stock to encourage customers to look through the entire store. This is in hopes the customer will pick up items they would not normally see.

Inventory credit
Inventory credit refers to the use of stock, or inventory, as collateral to raise finance. Where banks may be reluctant to accept traditional collateral, for example in developing countries where land title may be lacking, inventory credit is a potentially important way of overcoming financing constraints. This is not a new concept; archaeological evidence suggests that it was practiced in Ancient Rome. Obtaining finance against stocks of a wide range of products held in a bonded warehouse is common in much of the world. It is, for example, used with Parmesan cheese in Italy. Inventory credit on the basis of stored agricultural produce is widely used in Latin American countries and in some Asian countries. A precondition for such credit is that banks must be confident that the stored product will be available if they need to call on the collateral; this implies the existence of a reliable network of certified warehouses. Banks also face problems in valuing the inventory. The possibility of sudden falls in commodity prices means that they are usually reluctant to lend more than about 60% of the value of the inventory at the time of the loan.

Journal
International Journal of Inventory Research
Omega - The International Journal of Management Science

See also
Notes
References
Further reading
Kieso, DE; Warfield, TD; Weygandt, JJ (2007). Intermediate Accounting 8th Canadian Edition. Canada: John Wiley & Sons. ISBN 978-0-470-15313-0.
Cannella S., Ciancimino E. (2010) Up-to-date Supply Chain Management: the Coordinated (S,R). In "Advanced Manufacturing and Sustainable Logistics". Dangelmaier  W. et al. (Eds.) 175–185. Springer-Verlag  Berlin Heidelberg, Germany.

James P. Womack

James P. Womack was the research director of the International Motor Vehicle Program (IMVP) at the Massachusetts Institute of Technology (MIT) in Cambridge, Massachusetts and is the founder and chairman of the Lean Enterprise Institute, a nonprofit institution for the dissemination and exploration of the Lean thinking with the aim of his further development of the Lean Enterprise.

Life and work
Womack first became widely known as an author in 1990 with publication of the book The Machine That Changed the World, which made the term lean production known worldwide. The book has been translated into eleven languages and has been sold more than 600,000 times. A revised edition was published in 2007.
Womack received his bachelor's degree in political science from the University of Chicago in 1970. He earned his master's degree in transportation systems in 1975 at Harvard University. His Ph.D in political science was received from the Massachusetts Institute of Technology (MIT) in 1982 for a dissertation on the comparison of industrial policy in the United States, Germany and Japan.
From 1975-1991 Womack led a number of comparative studies on worldwide production practices. The study of the automotive industry (the International Motor Vehicle Program - IMVP), funded with more than US$5 million, was the most important. Womack left MIT shortly after the publication of his book and founded the Lean Enterprise Institute (LEI) in 1997.
In addition, he also founded the Lean Global Network (LGN) together with Dr. Bodo Wiegand and José Ferro, which combines the interests and objectives of the Lean Management Community. The umbrella organization wants to ensure uniform standards when implementing learning concepts and to encourage the exchange among its members.
He is married and has two daughters.

Works
Lean Solutions: How Companies and Customers Can Create Value and Wealth Together ISBN 978-3-593-38112-1
Lean Thinking: Banish Waste and Create Wealth in Your Corporation ISBN 3-593-37561-3
The Machine That Changed the World: The Story of Lean Production ISBN 0060974176, ISBN 978-0060974176

References
External links
Womack personal page

Job production

Job production, sometimes called jobbing or one-off production, involves producing custom work, such as a one-off product for a specific customer or a small batch of work in quantities usually less than those of mass-market products. Job production consists of an operator or group of operators to work on a single job and complete it before proceeding to the next similar or different job. Together with batch production and mass production (flow production) it is one of the three main production methods.Job production can be classical craft production by small firms (making railings for a specific house, building/repairing a computer for a specific customer, making flower arrangements for a specific wedding etc.), but large firms use job production, too, and the products of job production are often interchangeable, such as machined parts made by a job shop. Examples include:

Designing and implementing an advertising campaign
Auditing the accounts of a large public limited company
Building a new factory
Installing machinery in a factory
Machining a batch of parts per a CAD drawing supplied by a customer
Building the Golden Gate bridgeFabrication shops and machine shops whose work is primarily of the job production type are often called job shops. The associated people or corporations are sometimes called jobbers.
Job production is, in essence, manufacturing on a contract basis, and thus it forms a subset of the larger field of contract manufacturing. But the latter field also includes, in addition to jobbing, a higher level of outsourcing in which a product-line-owning company entrusts its entire production to a contractor, rather than just outsourcing parts of it.

Benefits and disadvantages
Key benefits of job production include:

can provide emergency parts or services, such as quickly making a machine part that would take a long time to acquire otherwise
can provide parts or services for machinery or systems that are otherwise not available, as when the original supplier no longer supports the product or goes out of business (orphaned)
work is generally of a high quality
a high level of customization is possible to meet the customer's exact requirements
significant flexibility is possible, especially when compared to mass production
workers can be easily motivated due to the skilled nature of the work they are performingDisadvantages include:

higher cost of production
re-engineering: sometimes engineering drawings or an engineering assessment, including calculations or specifications, needs to be made before the work can be done
requires the use of specialist labor (compared with the repetitive, low-skilled jobs in mass production)
slow compared to other methods (batch production and mass production)

Essential features
There are a number of features that should be implemented in a job production environment, they include:

Clear definitions of objectives should be set.
Clearly outlined decision making process.
Clear list of specifications should be set.

See also
Instant manufacturing
Just In Time
Lean manufacturing
Manufacturing
Piece work
Outline of industrial organization


== References ==

John Deere

Deere & Company, doing business as John Deere (), is an American corporation that manufactures agricultural machinery, heavy equipment, forestry machinery, diesel engines, drivetrains (axles, transmissions, gearboxes) used in heavy equipment, and lawn care equipment. It also provides financial services and other related activities.
Deere & Company is listed on the New York Stock Exchange under the symbol DE. The company's slogan is "Nothing Runs Like a Deere", and its logo is a leaping deer with the words 'JOHN DEERE'. It has used various logos incorporating a leaping deer for over 155 years. It is headquartered in Moline, Illinois.
It ranked No. 84 in the 2022 Fortune 500 list of the largest United States corporations. Its tractor series include D series, E series, Speciality Tractors, Super Heavy Duty Tractors, and JDLink.

History
19th century
Deere & Company began when John Deere, born in Rutland, Vermont, United States, on February 7, 1804, moved to Grand Detour, Illinois, in 1836, to escape bankruptcy in Vermont. Already an established blacksmith, Deere opened a 1,378-square-foot (128 m2) shop in Grand Detour in 1837, which allowed him to serve as a general repairman in the village, as well as a manufacturer of tools such as pitchforks and shovels.
Tools were just a start; the item that set him apart was the self-scouring steel plow, which was pioneered in 1837 when John Deere fashioned a Scottish steel saw blade into a plow. Prior to Deere's steel plow, most farmers used iron or wooden plows to which the rich Midwestern soil stuck, so they had to be cleaned frequently. Deere created a highly polished steel surface that allowed the soil to slide easily. This tool addressed the difficulty of tilling the Prairie State's soil and greatly aided migration into the American Great Plains in the 19th and early 20th centuries.
The traditional way of doing business was to make the product as and when it was ordered. This style was very slow. As Deere realized that this was not going to be a viable business model, he increased the rate of production by manufacturing plows before putting them up for sale; this allowed customers to not only see what they were buying beforehand but also allowed his customers to purchase his products straight away. Word of his products began to spread quickly.

In early 1843, Deere entered a business partnership with Leonard Andrus and purchased land for the construction of a new, two-story factory along the Rock River in Illinois. It used water power to operate machineries. This factory, named the "L. Andrus Plough Manufacturer", produced about 100 plows in 1842, and around 400 plows during the next year. Deere's partnership with Andrus ended in 1848, and Deere relocated to Moline, Illinois, to have access to the railroad and the Mississippi River. There, Deere formed a partnership with Robert Tate and built a 1,440-square-foot (134 m2) factory the same year. John Gould was later brought in to manage the accounts. Production rose quickly, and by 1849, the Deere, Tate & Gould Company was producing over 200 plows a month. A two-story addition to the plant was built, allowing further production.

Deere bought out Tate and Gould's interests in the company in 1853, and was joined in the business by his son Charles Deere. At that time, the company was manufacturing a variety of farm equipment products in addition to plows, including wagons, corn planters, and cultivators. In 1857, the company's production totals reached almost 1,120 implements per month. In 1858, a nationwide financial recession took a toll on the company. To prevent bankruptcy, the company was reorganized and Deere sold his interests in the business to his son-in-law, Christopher Webber, and his son, Charles Deere, who would take on most of his father's managerial roles. John Deere served as president of the company until his retirement in April 1886, but died one month later in May 1886. The company was reorganized again in 1868 when it was incorporated as Deere & Company. While the company's original stockholders were Charles Deere, Stephen Velie, George Vinton, and John Deere, Charles effectively ran the company. In 1869, Charles began to introduce marketing centers and independent retail dealers to advance the company's sales nationwide. This same year, Deere & Company won "Best and Greatest Display of Plows in Variety" at the 17th Annual Illinois State Fair, for which it won $10 and a silver medal.The core focus remained on the agricultural implements, but John Deere also made a few bicycles in the 1890s.

20th century
Increased competition during the early 1900s from the new International Harvester Company led the company to expand its offerings in the implement business, but the production of gasoline tractors came to define Deere & Company's operations during the 20th century.

In 1912, Deere & Company president William Butterworth (Charles' son-in-law), who had replaced Charles Deere after his death in 1907, began the company's expansion into the tractor business. Deere & Company briefly experimented with its own tractor models, the most successful of which was the Dain All-Wheel-Drive, but in the end decided to continue its foray into the tractor business by purchasing the Waterloo Gasoline Engine Company in 1918, which manufactured the popular Waterloo Boy tractor at its facilities in Waterloo, Iowa. Deere & Company continued to sell tractors under the Waterloo Boy name until 1923, when the John Deere Model D was introduced. The company continues to manufacture a large percentage of its tractors in Waterloo, Iowa, namely the 7R, 8R, and 9R series.
The company produced its first combine harvester, the John Deere No. 2, in 1927. This featured improvements and modifications to Model D such as higher power level due to increased cylinder bore. A year later, this innovation was followed up by the introduction of John Deere No. 1, a smaller machine that was more popular with customers. By 1929, the No. 1 and No. 2 were replaced by newer, lighter-weight harvesters. In the 1930s, John Deere and other farm equipment manufacturers began developing hillside harvesting technology. Harvesters now had the ability to effectively use their combines to harvest grain on hillsides with up to a 50% slope gradient.On an episode of the Travel Channel series Made in America that profiled Deere & Company, host John Ratzenberger stated that the company never repossessed any equipment from American farmers during the Great Depression.During World War II, the great-grandson of John Deere, Charles Deere Wiman, was president of the company, but he accepted a commission as a colonel in the U.S. Army. Burton F. Peek was hired as president during this period. Before Wiman returned to work at the company in late 1944, he directed the farm machinery and equipment division of the War Production Board. In addition to farm machinery, John Deere manufactured military tractors, and transmissions for the M3 tank. They also made aircraft parts, ammunition, and mobile laundry units to support the war effort.In 1947, John Deere introduced its first self-propelled combine, model 55. It was soon followed by the smaller models 40 and 45, the larger model 95, and an even larger model 105 was introduced in the 1960s. In the mid-1950s, Deere introduced attachable corn head, allowing crop producers to cut, shell, and clean corn in one smooth operation.In 1956, Deere & Company bought-out the German tractor manufacturer, Heinrich Lanz AG (see Lanz Bulldog).

In the last months of 1958, John Deere constructed a factory in northern Rosario, Argentina. In Argentina, the make was managed by Agar Cross & Co.
John Deere made the following models of tractors in Argentina: 445, 730; the models of the series 20 like 1420, 2420, 3420, 4420; the models of the serie 30 like 2330, 2530, 2730, 3330, 3530, 4530; the models of the serie 40 like 2140, 3140 / 3140 DT, 3440, 3540 and the last made in Baigorria of the serie 50 like 2850, 3350, 3550 until 1994.
Seventeen years later, (in 2011) the Argentinian plant returns the assembly of tractors with the following models: 5036C, 5045D (45 HP) Serie 5D, 5045E (45 HP) Serie 5E, 5065E (65 HP) Serie 5E, 5075E (75 HP) Serie 5E, 5425N (77 HP) Serie 5000, 5725 (92 HP) Serie 5025, 5725HC (92 HP) Serie 5025, 5090E, 5090EH, 5076EF, 6110J, 6130J, 6145J and 6165J.
Plus, in 2012, added in SKD/CKD format, the assembly of combine harvesters 9570 STS Serie 70, 9470 STS, 9670 STS and 9770 STS.Also, with the green line, the Argentinian facility made some backhoe loaders and motor graders like 570 A/B, 544 A/B, 507, 308, 200 and the 627, 727 model tractors.
On August 30, 1960, John Deere dealers from around the world converged on Dallas, Texas, for an unprecedented product showcase. Deere Day in Dallas, as the event was called, introduced the world to the "New Generation of Power", the company's first modern four-cylinder and six-cylinder tractors, during a day packed with high-tech presentations, live demonstrations, and a parking lot full of brand-new green and yellow machines. The line of tractors introduced that day was five years in the making, and the event itself took months to plan. Deere chose Dallas to host the event partly because it was home to facilities large enough to accommodate the 6,000 guests and the equipment they were all there to see. The Dallas Memorial Auditorium, the Texas State Fairgrounds Coliseum, the Cotton Bowl, and the Cotton Bowl parking lot were each the site of part of the event. During the event, a new John Deere tractor with a diamond-covered nameplate was displayed for all to see inside Neiman-Marcus, a popular Dallas-based department store.
According to information released by the company at the time of the event, John Deere dealers and key employees came to Dallas via the "largest commercial airlift of its type ever attempted". During the 24 hours leading up to the event, 16 airlines brought Deere employees and salespeople from all over the United States and Canada to Love Field in Dallas. Bill Hewitt, then chairman and CEO of Deere & Company, welcomed the dealers and introduced the new tractors. Hewitt told the guests they were about to see "a line of entirely new tractors – completely modern in every respect – with outstanding features not duplicated in any other make of tractor".
Since entering the tractor business in 1918, John Deere had focused on two-cylinder machines. The New Generation of Power introduced at Deere Day in Dallas was very different from anything Deere had built before. The new line of four- and six-cylinder tractors, the models 1010, 2010, 3010, and 4010, were far more powerful than Deere's two-cylinder models, and also easier and more comfortable to operate, with conveniently located controls, better visibility, and improved seat suspension. These new tractors were also easier to service.The 4010 was rated at 80 horsepower in 1960, but tested at 84 horsepower during testing trials, making it one of the most powerful two-wheel-drive farm tractors at that time. The 4010 was the predecessor to the 4020, which is widely regarded as the most popular tractor ever produced by John Deere, and perhaps any tractor manufacturer in the United States. Although the 4020, which was available with Deere's optional Power Shift, enjoyed greater popularity, the 4010 moved John Deere into the modern era of farm tractor technology and design following its successful history as a tractor manufacturer that was by the late 1950s experiencing waning market share due to its outdated technology.
In addition to the advanced engine technology, the "10" series tractors offered many other upgrades from the older two-cylinder models they replaced, including significantly higher horsepower-to-weight ratio, advanced hydraulics, more convenient and comfortable operator stations, and many other improvements. Of the "10" series John Deere tractors introduced in 1960, the 4010 was by far the most popular, with more than 58,000 units sold from 1960 to 1963. The success of the "10" series John Deere tractors, led by the 4010, helped propel John Deere from a 23% market share in 1959 to 34% by 1964 when the 4020 was introduced, making it the top manufacturer of farm equipment in the United States.In 1973, Deere introduced its new 'Soundguard' tractors, the 4030, 4230, 4430, and 4630. While these tractors were mechanically similar to the New Generation tractors they replaced, and the 4230, 4430, and 4630 used a 404-cubic-inch displacement engine like the 4020, they featured redesigned sheet metal and most importantly they were available with an optional completely integrated operator's cab that John Deere called the Sound Gard body. This insulated cab that included a roll-over protective structure had a distinctive rounded windshield and came equipped with heat and air conditioning, as well as speakers for an optional radio. An 8-track tape player was also available as an option. The 5020 was replaced by the very similar 6030 and continued in production with New Generation styling until 1977 when the 30 Series tractors were replaced by Deere's 'Iron Horses' series that included the 90-hp 4040, 110-hp 4240, 130-hp 4440, 150-HP 4640, and 180-hp 4840. The 4240, 4440, 4640, and 4840 featured a new 466-cubic-inch displacement engine, and improvements to the cab including an optional hydraulic seat for a smoother ride. The Sound Gard body and Power Shift transmission were standard equipment on the 4840.
In 1983, Deere introduced the 4050, 4250, 4450, 4650, and 4850. These tractors were essentially the same machines as the Iron Horses they replaced, but with significant upgrades. They offered a new 15-speed PowerShift transmission and were available with an optional mechanical front-wheel drive featuring caster action for better traction and a tighter turning radius. They also featured cosmetic upgrades, including a new light brown cab interior, instead of the black interior on previous models. These tractors were followed by the mechanically similar 55 and 60 series tractors before they were replaced by Deere's completely redesigned 7000 and 8000 series tractors in the early 1990s.
In the 1962 Illinois Manufacturers Directory (50th-anniversary edition), John Deere, listed as Deere and Company, claimed a total workforce of 35,000, of which 9,000 were in Illinois. The corporate headquarters were located at 1325 Third Ave. in Moline, Illinois, with six manufacturing plants located around that city and a seventh plant in Hoopeston, Illinois. The six plants in Moline were listed as:

John Deere Harvester Works at 1100 - 13th Ave., East Moline, where 3,000 employees made agricultural implements
John Deere Industrial Equipment Works at 301 Third Ave., Moline, where 500 employees made earth-moving equipment
John Deere Malleable Works at 1335-13th Street, East Moline, where 600 employees made malleable and nodular iron castings
John Deere Planter Works at 501 Third Ave., Moline, where 1,000 employees made agricultural implements
John Deere Plow Works at 1225 Third Ave., Moline, where 1,100 employees made agricultural implements
John Deere Spreader Works at 1209-13th Ave., Moline where 800 employees made agricultural implementsThe John Deere Vermilion Works was located at North Sixth Ave., Hoopeston, Illinois, where 140 employees were listed as making iron work and implement parts. Moline, with 42,705 residents in 1962, had the local 7,000 employees of John Deere represent 16% of the city's entire population.In 1969, John Deere followed its New Generation tractors of the 1960s with a New Generation of combines. These included the 3300, 4400, 6600, and 7700. These models were also the first to come with Quik-Tatch header mounting capabilities as standard equipment. In the 1980s, these combines were followed by the 4420, 6620, 7720, and 8820 that were essentially updated and improved versions of the previous models with larger capacity, a better cab, and easier maintenance and service. The 4420 was discontinued in 1984 and replaced by the 4425 combines imported from Germany, and the 6620, 7720, and 8820 received the Titan II updates.
In 1989, Deere replaced the 6620, 7720, and 8820 with a new line of completely redesigned 'Maximizer' combines that included the 9400, 9500, and 9600 walker combines. These combines featured a center-mounted cab, rear-mounted engine, and more comforts in the cab. Also in 1989, Deere was inducted into the National Inventors Hall of Fame. In 1997, Deere celebrated 50 years of self-propelled combine production, and the 1997 models featured a 50th-anniversary decal. In 1998, the 9410, 9510, and 9610 were introduced. These were essentially the same machines, but with minor upgrades. Deere dealers offered '10 series' upgrades to owners of older 9000 series Maximizer combines. In 1999, Deere introduced the 50 series Maximizer combines. These machines featured significant cosmetic upgrades including a more streamlined appearance, improved ergonomics in the cab, PTO shaft-style header hook-up, and the larger models were available as rotary machines which were a complete departure from the combines that Deere had built in the past.In the late 1970s, International Harvester had pioneered rotary combines with their Axial flow machines and were soon followed by other manufacturers, but Deere continued to build only conventional walker combines through the 1980s and 1990s. In 1999, John Deere introduced the Single-Tine Separation (STS) system on its 9550, 9650, and 9750 combines, representing a step forward in rotary combine technology. The STS system uses less horsepower and improves material handling.

21st century
As of 2018, Deere & Company employed about 67,000 people worldwide, of which half are in the United States and Canada, and is the largest agriculture machinery company in the world. In August 2014, the company announced it was indefinitely laying off 600 of its workers at plants in Illinois, Iowa, and Kansas due to less demand for its products. Inside the United States, the company's primary locations are its administrative center in Moline, Illinois, and manufacturing factories in central and southeastern United States. As of 2016, the company experiments with an electric farm tractor.The logo of the leaping deer has been used by this company for over 155 years. Over the years, the logo has had minor changes and pieces removed. Some of the older style logos have the deer leaping over a log. The company uses different logo colors for agricultural vs. construction products. The company's agricultural products are identifiable by a distinctive shade of green paint, with the inside border being yellow. By contrast, the construction products are identifiable by a shade of black with the deer being yellow, and the inside border also being yellow.
In September 2017, Deere & Company signed a definitive agreement to acquire Blue River Technology, which is based in Sunnyvale, California, and is applying machine learning to agriculture. Blue River has designed and integrated computer vision and machine learning technology that will enable growers to reduce the use of herbicides by spraying only where weeds are present.On August 29, 2019, it was announced that Samuel R. Allen will step down as CEO and president of John Deere. John May, president of the Worldwide Agriculture and Turf and Integrated Solutions divisions will replace him in November 2019.
In October 2021, about 10,000 employees, unionized with the United Auto Workers, went on strike following an impasse in contract negotiations.In January 2022, the company introduced a self-driving tractor at the annual Consumer Electronics Show, designed for large-scale farming as opposed to existing comparable tractors designed for small-scale agriculture. It was part of a larger effort to develop so-called smart machines to make farming faster and more efficient than it would be relying on human labor, including through software, which would mean higher margins. The company said it wanted to "connect 1.5 million machines in service and a half billion acres in use to its cloud-based John Deere Operations Center."In June 2022, Deere announced it would cease production of its model 3710 moldboard plow but would continue to offer its model 995 reversible plow.

Non-serviceability by owners or third parties
John Deere farm equipment has been criticised for being impossible to be serviced or repaired by owners or third parties; only John Deere has access to computer code required for this and to accept non-John-Deere replacement parts. Remote locking by the manufacturer may also be possible. This effectively makes the equipment unusable without the continued involvement of John Deere. It was reported that during the 2022 Russian invasion of Ukraine Russian troops stole Ukrainian farm equipment, but that the dealers who owned the equipment locked it remotely.John Deere's license covering the internal software on tractor control computers does not allow users or independent mechanics to modify the software. This prevents repairs by farmers and creates a monopoly for John Deere dealerships. John Deere claims user repair is forbidden by the Digital Millennium Copyright Act, through bypassing of digital rights management. Groups including the Electronic Frontier Foundation have criticised this activity, being contrary to the right to repair. Some farmers use Ukrainian versions of John Deere software to circumvent restrictions on repair. In February 2022, the US Senate introduced a bill to allow farmers to perform their own repairs. As of April 2022, right-to-repair bills had been introduced in 26 states.On January 9, 2023, John Deere agreed to allow its US customers to fix their own equipment.

Products
John Deere manufactures a wide range of products, with several models of each in many cases.

Agricultural equipment
Agricultural products include, among others, tractors, combine harvesters, cotton harvesters, balers, planters/seeders, silage machines, and sprayers.

Construction equipment
Construction equipment includes:

Forestry equipment
John Deere manufactures a range of forestry machinery, among others, harvesters, forwarders, skidders, feller bunchers and log loaders. Timberjack was a subsidiary of John Deere from 2000 to 2006.

Other products
Other products the company manufactures include consumer and commercial equipment such as lawn mowers, compact utility tractors, snow throwers, snowmobiles, all-terrain vehicles, and StarFire (a wide-area differential GPS). It is also a supplier of diesel engines and powertrains (axles, transmissions, etc.) used especially in heavy equipment.John Deere leasing has expanded to non-equipment loans. As of 2017, this is the leading division of John Deere. With a loan portfolio of $2 billion, it accounts for a third of John Deere's income.

Factories
Major North American factories include:

Harvester Works (large combine harvesters), East Moline, Illinois
Cylinder Internal Platform (hydraulic cylinders), Moline, Illinois
Seeding Group (planting equipment), Moline, Illinois and Valley City, North Dakota
Davenport Works (wheel loaders, motor graders, articulated dump trucks, wheeled forestry equipment), Davenport, Iowa
Dubuque Works (backhoes, crawlers, skid-steer loaders, tracked forestry equipment), Dubuque, Iowa
Des Moines Works (tillage equipment, cotton harvesters, sprayers), Ankeny, Iowa
Ottumwa Works (hay and forage equipment), Ottumwa, Iowa
Thibodaux Works (cane-harvesting equipment, scrapers), Thibodaux, Louisiana
Horicon Works (lawn and garden and turf care), Horicon, Wisconsin
Augusta Works (small commercial and agricultural tractors), Grovetown, Georgia
Turf Care (specialty golf equipment and commercial mowing), Fuquay-Varina, North Carolina
Industrias John Deere (agricultural tractors; construction equipment), (Monterrey, Mexico)
Motores John Deere (power systems; 6- and 4-cylinder engines, heavy-duty axles), Torreon, Mexico
Coffeyville Works (transmissions, pump drives, planetaries), Coffeyville, Kansas
Waterloo Works (tractor, cab, and assembly operations, drivetrain operations, foundry operations, service parts operations), Waterloo, Iowa
Power Systems and Engine Works (power systems and engines), Waterloo, Iowa
Greeneville Works (entry-level lawn care equipment), Greeneville, TennesseeOther important factories:

John Deere Usine Saran (power systems), Fleury-les-Aubrais, France
John Deere Argentina (engines, tractors, and combine harvesters), Granadero Baigorria, Santa Fe, Argentina
John Deere Equipment Pvt Ltd (5000-series tractors), Pune, India
John Deere Equipment Pvt Ltd (5000-series tractors), Dewas, India
John Deere Electronic Solutions, Fargo, ND and Pune, India
John Deere Harvester Works, Sirhind-Fategarh, India
John Deere Werke Mannheim (6000-series tractors), Mannheim, Germany
John Deere Brasil: Montenegro, Rio Grande do Sul (tractors), Horizontina - RS (harvesters and planters), Catalão - GO (sugarcane harvesters)
John Deere Brasil Construction & Forestry (tractors and excavators), Indaiatuba, Brazil
John Deere Werke Zweibrücken (harvesting equipment), Zweibrücken, Germany
John Deere Fabriek Horst (pulled and self-propelled agricultural sprayers), Horst, The Netherlands
John Deere Forestry Oy (forwarders, wheeled harvesters), Joensuu, Finland
John Deere Reman,

Equipment divisions
Subsidiaries and affiliates
Current
AGRIS Corporation (John Deere Agri Services)
Bear Flag Robotics – Autonomous agricultural technology & equipment company
John Deere Ag Management Solutions (intelligent mobile equipment technologies), Urbandale, Iowa
John Deere Capital Corporation
John Deere Financial (John Deere Credit and Finance), Johnston, Iowa
Kemper (row tolerant headers for forage harvesters and combines), Stadtlohn, Germany
Waratah Forestry Attachments (forestry harvesting heads), Tokoroa, New Zealand
Agreentech
NavCom Technology, Inc. (precision positioning systems, see also StarFire), Torrance, California
John Deere Electronic Solutions (Ruggedized electronics), Fargo, North Dakota
Ningbo Benye Tractor & Automobile Manufacture Co. Ltd. (low HP tractors), Ningbo, China
Machinefinder (used equipment division and marketplace)
John Deere Technology Innovation Center, Research Park, University of Illinois at Urbana-Champaign
QCFS and Consolidating (attachment distribution center), Davenport, Iowa
Hagie Sprayers (Upfront Sprayers)
¨KingAgro (Sprayers) Argentina
PLA (sprayers) Argentina
Wirtgen Group
Blue River Technology – Pioneer in the use of computer vision and robotics for agriculture bringing crop protection into the digital era with see and spray machines that precisely observe and treat each plant in the field.

Former
John Deere Renewables, LLC, a wind energy plant manufacturing arm which represented John Deere's extension into the renewable energy industry – under which it had successfully completed 36 projects in eight US states – was sold to Exelon Energy in August 2010.

Finances
Carbon footprint
John Deere reported total CO2e emissions (direct and indirect) for the twelve months ending 30 September 2020 at 766 Kt (-155 /-16.8% y-o-y) and plans to reduce emissions 15% by 2022 from a 2017 base year.

Sponsorships
The John Deere Classic is an American professional golf tournament sponsored by the company.
John Deere sponsored the #23 and #97 cars for NASCAR driver Chad Little in the late 1990s.
John Deere sponsored the #17 car for NASCAR driver Ricky Stenhouse Jr. in the Monster Energy NASCAR Cup Series in the late 2010s.
John Deere previously sponsored the Carolina Hurricanes' ice resurfacers from early 2000s to mid 2010s.

Green Magazine
Green Magazine is a publication devoted to John Deere enthusiasts. It was begun in November 1984 by Richard and Carol Hain of Bee, Nebraska.
The first issue was mailed in early November 1984 to 135 paid subscribers and had 10 black-and-white pages with features on tractors, letters from readers, and advertisements. At the time, the magazine was published bimonthly. It was written in Lincoln, Nebraska, and it was mailed from the Bee post office.
The magazine grew rapidly, and in 1990, bowing to public demand, it became a monthly. Circulation continued to increase, and currently hovers around 30,000. The magazine now generally contains 88 full-color pages and is perfect bound. It is now printed in Michigan and mailed from several post offices throughout the country.
Current content usually includes a "Tip of the Month" article covering New Generation restoration written by Dan Brotzman, a "Youngtimer" article written by Tyler Buchheit, "Shop Talk" by Ron and JoAnn O'Neill, "Saw It On eBay" by Adam Smith and Benjamin Hain, "Scale Models" by Bill Proft, "What's New and Old" by Greg Stephen, "Feature Model" by Benjamin Hain, "Do You Have One of These" by Richard Hain, and "Mr. Thinker", which is said to be written by "a variety of experts".

See also
John Deere World Headquarters
List of John Deere tractors
John Deere Buck

References
Sources
Broehl, Wayne G. Jr. (1984). John Deere's Company: A History of Deere & Company and Its Times. New York City: Doubleday. ISBN 9780385196642. OCLC 10606276.
Dahlstrom, Jeremy; Dahlstrom, Neil (2005). The John Deere Story: A Biography of Plowmakers John & Charles Deere. Dekalb, Illinois: Northern Illinois University Press. ISBN 9780875803364. OCLC 56753352.
Kendall, Edward C. (1959). John Deere's Steel Plow. Washington, D.C.: Smithsonian Institution. OCLC 3302873.

External links

Official website
Green Magazine
John Deere at RitchieWiki, the Equipment Wiki
John Deere's official youtube
Business data for John Deere:

John Krafcik

John F. Krafcik (born September 18, 1961) was the CEO of Waymo from 2015 to 2021. Krafcik was the former president of TrueCar and president and CEO of Hyundai Motor America. He was named CEO of Google's self-driving car project in September 2015. Krafcik remained CEO after Google separated its self-driving car project and transitioned it into a new company called Waymo, housed under Google's parent company Alphabet Inc.

Early life and education
Krafcik grew up in Southington, Connecticut. He studied mechanical engineering at Stanford University, where he graduated in 1983. He received his master's degree in management from Massachusetts Institute of Technology in 1988, where he studied under James P. Womack.

Career
Krafcik worked in traditional automotive manufacturing for several decades before moving to Google's self-driving car project in 2015. His first job was at New United Motor Manufacturing, Inc., a joint venture between General Motors and Toyota, as a quality and manufacturing engineer from 1984 to 1986. He worked in the International Motor Vehicle Program at MIT as a lean production researcher and consultant from 1986 to 1990. During this time, Krafcik traveled and studied 90 manufacturing plants in 20 countries, comparing their productivity and quality. His studies formed the data behind Womack's book, The Machine That Changed the World. The book was a study on "lean production", a term Krafcik coined. In 1990, Krafcik moved to Ford Motor Company where he held several positions, including chief engineer for the Ford Expedition and Lincoln Navigator in the late 1990s and early 2000s and the chief engineer for truck chassis engineering.Krafcik started at Hyundai Motor America as vice president for product development and strategic planning in 2004. Within a few years he was promoted to become the president and CEO of Hyundai Motor America until the end of 2013. During Krafcik's tenure, Hyundai reported record sales and increased U.S. market share. Following the financial crisis of 2007–2008, Krafcik oversaw a group at Hyundai to create an "Assurance Program". The program allowed Americans to return their new cars if they lost their jobs within a year.Krafcik moved to become president of True Car, Inc. in 2014 and served as a director of the company's board.Google hired Krafcik to head its self-driving cars unit in September 2015, as the company struggled to build relationships in the Motor City.In 2018, Krafcik was awarded Smithsonian Magazine's American Ingenuity Award for Technology alongside Dmitri Dolgov.

Waymo
In December 2016, more than a year after Krafcik joined Google, the company's self-driving car project was spun off as its own company, Waymo, a subsidiary of Google parent Alphabet, and Krafcik became its CEO. In 2017 and 2018, under Krafcik, Waymo struck partnerships with Lyft, Fiat Chrysler, and Jaguar Land Rover. Krafcik initiated talks with Fiat Chrysler CEO Sergio Marchionne. Two months later, the two announced a deal to integrate Waymo hardware into Fiat Chrysler minivans. Krafcik also made a deal with Avis Budget Group to maintain its growing fleet.As CEO, Krafcik leads Waymo's efforts to commercialize the company's autonomous technology. These include ride sharing, trucking, urban last-mile solutions and passenger cars. Krafcik also pursued licensing Waymo's technology to vehicle manufacturers and has deployed the largest fleet of self-driving cars. Krafcik has told auto makers that Waymo's goal is to make better drivers, not cars.In April 2021, Krafcik resigned as Waymo CEO to "kick-off new adventures".

References
External links
John Krafcik on Twitter

Just in case

Just-in-case manufacturing (JIC) is a term sometimes applied to traditional manufacturing systems used before the influence of modern technologies and newer transportation infrastructures.  It is the contrary in many ways to the recently evolved Just In Time manufacturing system.

Operation
In JIC, manufacturers need to maintain large inventories of supplies, parts, warehousing resources, and extra workers to meet production contingencies. These contingencies, more common in less industrialized countries, can be poor transportation infrastructure, poor quality control, vulnerability to other suppliers' production problems, and natural disasters. These supply-chain instabilities could lead to costly production inefficiencies therefore a manufacturer may maintain and pay for excess inventory and backups of "fragile" production stages which could get out of sync, cause production shutdowns, or create supply-chain delays for other manufacturers. In JIC, manufacturers reorder stock before it reaches the buffer level or minimum level to allow themselves to have inventories to be sold while the suppliers are supplying the goods. This time range from the time the firm reorders the stock to the time the supplier provides the new stock is known as lead time. Thus a JIC inventory system tries to keep a minimum level of inventories just in case of emergencies, hence the name "Just In Case".
One major reason for practicing a more costly JIC system are the potential losses paid (i.e. permanent loss of major customers, loss of suppliers, supply-chain collapse) if supply-chain shocks occur on several occasions. If the JIT response contingencies are too slow or fail to keep production flowing additional costs may be incurred. Under these circumstances the additional costs due to maintaining extra storage, resources, and system resiliency may potentially be more cost effective than using a more efficient JIT system. A JIC examples of buyers would be the military or hospitals who need to maintain large inventories because waiting for JIT producers to ramp up production for needed supplies may result in losses (i.e. wars, lives).


== References ==

Kaizen

Kaizen (Japanese: 改善, "improvement") is a concept referring to business activities that continuously improve all functions and involve all employees from the CEO to the assembly line workers. Kaizen also applies to processes, such as purchasing and logistics, that cross organizational boundaries into the supply chain. It has been applied in healthcare, psychotherapy, life coaching, government, manufacturing, and banking.
By improving standardized programs and processes, kaizen aims to eliminate waste and redundancies (lean manufacturing). Kaizen was first practiced in Japanese businesses after World War II, influenced in part by American business and quality-management teachers, and most notably as part of The Toyota Way. It has since spread throughout the world and has been applied to environments outside of business and productivity.

Overview
The Japanese word kaizen means 'change for better' (from 改 kai - change, revision; and 善 zen - virtue, goodness) with the inherent meaning of either 'continuous' or 'philosophy' in Japanese dictionaries and in everyday use. The word refers to any improvement, one-time or continuous, large or small, in the same sense as the English word improvement. However, given the common practice in Japan of labeling industrial or business improvement techniques with the word kaizen, particularly the practices spearheaded by Toyota, the word kaizen in English is typically applied to measures for implementing continuous improvement, especially those with a "Japanese philosophy". The discussion below focuses on such interpretations of the word, as frequently used in the context of modern management discussions. Two kaizen approaches have been distinguished:

Point kaizen
Point kaizen is one of the most commonly implemented types of kaizen. It happens very quickly and usually without much planning. As soon as something is found broken or incorrect, quick and immediate measures are taken to correct the issues. These measures are generally small, isolated and easy to implement.; however, they can have a huge impact.
In some cases, it is also possible that the positive effects of point kaizen in one area can reduce or eliminate benefits of point kaizen in some other area.
Examples of point kaizen include a shop inspection by a supervisor who finds broken materials or other small issues, and then asks the owner of the shop to perform a quick kaizen (5S) to rectify those issues, or a line worker who notices a potential improvement in efficiency by placing the materials needed in another order or closer to the production line in order to minimize downtime.

System kaizen
System kaizen is accomplished in an organized manner and is devised to address system-level problems in an organization.
It is an upper-level strategic planning method for a short period of time.

Line kaizen
Line kaizen refers to communication of improvements between the upstream and downstream of a process. This can be extended in several ways.

Plane kaizen
This is the next upper level of line kaizen, in that several lines are connected together. In modern terminologies, this can also be described as a value stream, where instead of traditional departments, the organization is structured into product lines or families and value streams. It can be visualized as changes or improvements made to one line being implemented to multiple other lines or processes.

Cube kaizen
Cube kaizen describes the situation where all the points of the planes are connected to each other and no point is disjointed from any other. This would resemble a situation where Lean has spread across the entire organization. Improvements are made up and down through the plane, or upstream or downstream, including the complete organization, suppliers and customers. This might require some changes in the standard business processes as well.

Benefits and tradeoffs
Kaizen is a daily process, the purpose of which goes beyond simple productivity improvement. It is also a process that, when done correctly, humanizes the workplace, eliminates overly hard work (muri), and teaches people how to perform experiments on their work using the scientific method and how to learn to spot and eliminate waste in business processes. In all, the process suggests a humanized approach to workers and to increasing productivity: "The idea is to nurture the company's people as much as it is to praise and encourage participation in kaizen activities." Successful implementation requires "the participation of workers in the improvement."
People at all levels of an organization participate in kaizen, from the CEO down to janitorial staff, as well as external stakeholders when applicable. Kaizen is most commonly associated with manufacturing operations, as at Toyota, but has also been used in non-manufacturing environments. The format for kaizen can be individual, suggestion system, small group, or large group. At Toyota, it is usually a local improvement within a workstation or local area and involves a small group in improving their own work environment and productivity. This group is often guided through the kaizen process by a line supervisor; sometimes this is the line supervisor's key role. Kaizen on a broad, cross-departmental scale in companies, generates total quality management, and frees human efforts through improving productivity using machines and computing power.While kaizen (at Toyota) usually delivers small improvements, the culture of continual aligned small improvements and standardization yields large results in terms of overall improvement in productivity. This philosophy differs from the "command and control" improvement programs (e.g., Business Process Improvement) of the mid-20th century. Kaizen methodology includes making changes and monitoring results, then adjusting. Large-scale pre-planning and extensive project scheduling are replaced by smaller experiments, which can be rapidly adapted as new improvements are suggested.In modern usage, it is designed to address a particular issue over the course of a week and is referred to as a "kaizen blitz" or "kaizen event". These are limited in scope, and issues that arise from them are typically used in later blitzes. A person who makes a large contribution in the successful implementation of kaizen during kaizen events is awarded the title of "Zenkai". In the 21st century, business consultants in various countries have engaged in widespread adoption and sharing of the kaizen framework as a way to help their clients restructure and refocus their business processes.

History
The small-step work improvement approach was developed in the USA under Training Within Industry program (TWI Job Methods). Instead of encouraging large, radical changes to achieve desired goals, these methods recommended that organizations introduce small improvements, preferably ones that could be implemented on the same day. The major reason was that during WWII there was neither time nor resources for large and innovative changes in the production of war equipment. The essence of the approach came down to improving the use of the existing workforce and technologies.
As part of the aid to allied nations after the war, not directly including the Marshall Plan after World War II, American occupation forces brought in experts to help with the rebuilding of Japanese industry while the Civil Communications Section (CCS) developed a management training program that taught statistical control methods as part of the overall material. Homer Sarasohn and Charles Protzman developed and taught this course in 1949–1950. Sarasohn recommended W. Edwards Deming for further training in statistical methods.
The Economic and Scientific Section (ESS) group was also tasked with improving Japanese management skills and Edgar McVoy was instrumental in bringing Lowell Mellen to Japan to properly install the Training Within Industry (TWI) programs in 1951. The ESS group had a training film to introduce TWI's three "J" programs: Job Instruction, Job Methods and Job Relations. Titled "Improvement in Four Steps" (Kaizen eno Yon Dankai), it thus introduced kaizen to Japan.
For the pioneering, introduction, and implementation of kaizen in Japan, the Emperor of Japan awarded the Order of the Sacred Treasure to Dr. Deming in 1960. Subsequently, the Union of Japanese Scientists and Engineers (JUSE) instituted the annual Deming Prizes for achievement in quality and dependability of products. On October 18, 1989, JUSE awarded the Deming Prize to Florida Power & Light Co. (FPL), based in the US, for its exceptional accomplishments in process and quality-control management, making it the first company outside Japan to win the Deming Prize.Kaoru Ishikawa took up this concept to define how continuous improvement or kaizen can be applied to processes, as long as all the variables of the process are known.

Implementation
The Toyota Production System is known for kaizen, where all line personnel are expected to stop their moving production line in case of any abnormality, and, along with their supervisor, suggest an improvement to resolve the abnormality which may initiate a kaizen.  This feature is called Jidoka or "autonomation".

The cycle of kaizen activity can be defined as: Plan → Do → Check → Act. This is also known as the Shewhart cycle, Deming cycle, or PDCA.
Another technique used in conjunction with PDCA is the five whys, which is a form of root cause analysis in which the user asks a series of five "why" questions about a failure that has occurred, basing each subsequent question on the answer to the previous. There are normally a series of causes stemming from one root cause, and they can be visualized using fishbone diagrams or tables. The five whys can be used as a foundational tool in personal improvement, or as a means to create wealth.Masaaki Imai made the term famous in his book Kaizen: The Key to Japan's Competitive Success.In the Toyota Way Fieldbook, Liker and Meier discuss the kaizen blitz and kaizen burst (or kaizen event) approaches to continuous improvement. A kaizen blitz, or rapid improvement, is a focused activity on a particular process or activity. The basic concept is to identify and quickly remove waste. Another approach is that of the kaizen burst, a specific kaizen activity on a particular process in the value stream.In the 1990s, Professor Iwao Kobayashi published his book 20 Keys to Workplace Improvement and created a practical, step-by-step improvement framework called "the 20 Keys". He identified 20 operations focus areas which should be improved to attain holistic and sustainable change. He went further and identified the five levels of implementation for each of these 20 focus areas. Four of the focus areas are called Foundation Keys. According to the 20 Keys, these foundation keys should be launched ahead of the others in order to form a strong constitution in the company. The four foundation keys are:

Key 1 – Cleaning and Organizing to Make Work Easy, which is based on the 5S methodology.
Key 2 – Goal Alignment/Rationalizing the System
Key 3 – Small Group Activities
Key 4 – Leading and Site Technology

Related topics
References
Further reading
Dinero, Donald (2005). Training Within Industry: The Foundation of. Productivity Press. ISBN 1-56327-307-1.
Bodek, Norman (2010). How to do Kaizen: A new path to innovation - Empowering everyone to be a problem solver. Vancouver, WA, US: PCS Press. ISBN 978-0-9712436-7-5.
Graban, Mark; Joe, Swartz (2012). Healthcare Kaizen: Engaging Front-Line Staff in Sustainable Continuous Improvements (1 ed.). Productivity Press. ISBN 978-1439872963.
Maurer, Robert (2012). The Spirit of Kaizen: Creating Lasting Excellence One Small Step at a Time (1 ed.). McGraw-Hill. ISBN 978-0071796170.
Emiliani, Bob; Stec, David; Grasso, Lawrence; Stodder, James (2007). Better Thinking, Better Results: Case Study and Analysis of an Enterprise-Wide Lean Transformation (2e. ed.). Kensington, CT, US: The CLBM, LLC. ISBN 978-0-9722591-2-5.
Hanebuth, D. (2002). Rethinking Kaizen: An empirical approach to the employee perspective. In J. Felfe (Ed.), Organizational Development and Leadership (Vol. 11, pp. 59-85). Frankfurt a. M.: Peter Lang. ISBN 978-3-631-38624-8.
Imai, Masaaki (1986). Kaizen: The Key to Japan's Competitive Success. McGraw-Hill/Irwin. ISBN 0-07-554332-X.
Imai, Masaaki (1997-03-01). Gemba Kaizen: A Commonsense, Low-Cost Approach to Management (1e. ed.). McGraw-Hill. ISBN 0-07-031446-2.
Scotchmer, Andrew (2008). 5S Kaizen in 90 Minutes. Management Books 2000 Ltd. ISBN 978-1-85252-547-7.
Kobayashi, Iwao (1995). 20 Keys to Workplace Improvement. Portland, OR, USA: Productivity, Inc. ISBN 1-56327-109-5.

External links

Toyota stumbles but its "kaizen" cult endures, Reuters
Warping Forward with Kaizen, Karn G. Bulsuk
Kaizen Glossary, Joe Marshall
Guide to Kaizen startup Best Practice Guide, Ben Geck
Definition of Kaizen, Masaaki Imai
Management by Stress, Jane Slaughter

Kanban

Kanban (Japanese: カンバン Japanese pronunciation: [kambaɴ] and Chinese: 看板, meaning signboard or billboard) is a scheduling system for lean manufacturing (also called just-in-time manufacturing, abbreviated JIT). Taiichi Ohno, an industrial engineer at Toyota, developed kanban to improve manufacturing efficiency. The system takes its name from the cards that track production within a factory. Kanban is also known as the Toyota nameplate system in the automotive industry.
In kanban, problem areas are highlighted by measuring lead time and cycle time of the full process and process steps. One of the main benefits of kanban is to establish an upper limit to work in process (commonly referred as "WIP") inventory to avoid overcapacity. Other systems with similar effect exist, for example CONWIP. A systematic study of various configurations of kanban systems, such as generalized kanban or production authorization card (PAC) and extended kanban, of which CONWIP is an important special case, can be found in Tayur (1993), and more recently Liberopoulos and Dallery (2000), among other papers.A goal of the kanban system is to limit the buildup of excess inventory at any point in production. Limits on the number of items waiting at supply points are established and then reduced as inefficiencies are identified and removed. Whenever a limit is exceeded, this points to an inefficiency that should be addressed.

Origins
The system originates from the simplest visual stock replenishment signaling system, an empty box. This was first developed in the UK factories producing Spitfires during the Second World War, and was known as the "two bin system." In the late 1940s, Toyota started studying supermarkets with the idea of applying shelf-stocking techniques to the factory floor. In a supermarket, customers generally retrieve what they need at the required time—no more, no less. Furthermore, the supermarket stocks only what it expects to sell in a given time, and customers take only what they need, because future supply is assured. This observation led Toyota to view a process as being a customer of one or more preceding processes and to view the preceding processes as a kind of store.
Kanban aligns inventory levels with actual consumption. A signal tells a supplier to produce and deliver a new shipment when a material is consumed. This signal is tracked through the replenishment cycle, bringing visibility to the supplier, consumer, and buyer.
Kanban uses the rate of demand to control the rate of production, passing demand from the end customer up through the chain of customer-store processes. In 1953, Toyota applied this logic in their main plant machine shop.

Operation
A key indicator of the success of production scheduling based on demand, pushing, is the ability of the demand-forecast to create such a push. Kanban, by contrast, is part of an approach where the pull comes from demand and products are made to order. Re-supply or production is determined according to customer orders.
In contexts where supply time is lengthy and demand is difficult to forecast, often the best one can do is to respond quickly to observed demand. This situation is exactly what a kanban system accomplishes, in that it is used as a demand signal that immediately travels through the supply chain. This ensures that intermediate stock held in the supply chain are better managed, and are usually smaller. Where the supply response is not quick enough to meet actual demand fluctuations, thereby causing potential lost sales, a stock building may be deemed more appropriate and is achieved by placing more kanban in the system.
Taiichi Ohno stated that to be effective, kanban must follow strict rules of use. Toyota, for example, has six simple rules, and close monitoring of these rules is a never-ending task, thereby ensuring that the kanban does what is required.

Toyota's six rules
Toyota has formulated six rules for the application of kanban:
Each process issues requests (kanban) to its suppliers when it consumes its supplies.
Each process produces according to the quantity and sequence of incoming requests.
No items are made or transported without a request.
The request associated with an item is always attached to it.
Processes must not send out defective items, to ensure that the finished products will be defect-free.
Limiting the number of pending requests makes the process more sensitive and reveals inefficiencies.

Kanban (cards)
Kanban cards are a key component of kanban and they signal the need to move materials within a production facility or to move materials from an outside supplier into the production facility. The kanban card is, in effect, a message that signals a depletion of product, parts, or inventory. When received, the kanban triggers replenishment of that product, part, or inventory.  Consumption, therefore, drives demand for more production, and the kanban card signals demand for more product—so kanban cards help create a demand-driven system.
It is widely held by proponents of lean production and manufacturing that demand-driven systems lead to faster turnarounds in production and lower inventory levels, helping companies implementing such systems be more competitive.
In the last few years, systems sending kanban signals electronically have become more widespread. While this trend is leading to a reduction in the use of kanban cards in aggregate, it is still common in modern lean production facilities to find the use of kanban cards.  In various software systems, kanban is used for signalling demand to suppliers through email notifications.  When stock of a particular component is depleted by the quantity assigned on kanban card, a "kanban trigger" is created (which may be manual or automatic), a purchase order is released with predefined quantity for the supplier defined on the card, and the supplier is expected to dispatch material within a specified lead-time.Kanban cards, in keeping with the principles of kanban, simply convey the need for more materials. A red card lying in an empty parts cart conveys that more parts are needed.

Three-bin system
An example of a simple kanban system implementation is a "three-bin system" for the supplied parts, where there is no in-house manufacturing. One bin is on the factory floor (the initial demand point), one bin is in the factory store (the inventory control point), and one bin is at the supplier. The bins usually have a removable card containing the product details and other relevant information, the classic kanban card.
When the bin on the factory floor is empty (because the parts in it were used up in a manufacturing process), the empty bin and its kanban card are returned to the factory store (the inventory control point). The factory store replaces the empty bin on the factory floor with the full bin from the factory store, which also contains a kanban card. The factory store sends the empty bin with its kanban card to the supplier. The supplier's full product bin, with its kanban card, is delivered to the factory store; the supplier keeps the empty bin.  This is the final step in the process.  Thus, the process never runs out of product—and could be described as a closed loop, in that it provides the exact amount required, with only one spare bin so there is never oversupply. This 'spare' bin allows for uncertainties in supply, use, and transport in the inventory system. A good kanban system calculates just enough kanban cards for each product. Most factories that use kanban use the colored board system (heijunka box).

Electronic kanban
Many manufacturers have implemented electronic kanban (sometimes referred to as e-kanban) systems. These help to eliminate common problems such as manual entry errors and lost cards. E-kanban systems can be integrated into enterprise resource planning (ERP) systems, enabling real-time demand signaling across the supply chain and improved visibility. Data pulled from e-kanban systems can be used to optimize inventory levels by better tracking supplier lead and replenishment times.E-kanban is a signaling system that uses a mix of technology to trigger the movement of materials within a manufacturing or production facility.  Electronic kanban differs from traditional kanban in using technology to replace traditional elements like kanban cards with barcodes and electronic messages like email or electronic data interchange.
A typical electronic kanban system marks inventory with barcodes, which workers scan at various stages of the manufacturing process to signal usage. The scans relay messages to internal/external stores to ensure the restocking of products. Electronic kanban often uses the Internet as a method of routing messages to external suppliers and as a means to allow a real-time view of inventory, via a portal, throughout the supply chain.
Organizations like the Ford Motor Company and Bombardier Aerospace have used electronic kanban systems to improve processes. Systems are now widespread from single solutions or bolt on modules to ERP systems.

Types of kanban systems
In a kanban system, adjacent upstream and downstream workstations communicate with each other through their cards, where each container has a kanban associated with it. Economic order quantity is important. The two most important types of kanbans are:

Production (P) Kanban: A P-kanban, when received, authorizes the workstation to produce a fixed amount of products. The P-kanban is carried on the containers that are associated with it.
Transportation (T) Kanban: A T-kanban authorizes the transportation of the full container to the downstream workstation. The T-kanban is also carried on the containers that are associated with the transportation to move through the loop again.The Kanban philosophy and task boards are also used in agile project management to coordinate tasks in project teams. An online demonstration can be seen in an agile simulator.Implementation of kanban can be described in the following manner:
The workflow consists of logical steps.
There are two steps to a workflow viz. queue and work in progress/process.
The team in charge decides on the maximum amount of work each step of the workflow can hold.
Work is pushed into the queue step and pulled into the process step.
If need be, work is halted in two successive stages to clear bottleneck.

Kanbrain
A third type involves corporate training. Following the just-in-time principle, computer-based training permits those who need to learn a skill to do so when the need arises, rather than take courses and lose the effectiveness of what they've learned from lack of practice.

See also
References
Further reading
Louis, Raymond (2006). Custom Kanban: Designing the System to Meet the Needs of Your Environment. University Park, IL: Productivity Press. ISBN 978-1-56327-345-2.
Waldner, Jean-Baptiste (1992). Principles of Computer-Integrated Manufacturing. John Wiley. ISBN 0-471-93450-X.

External links

Lean Lab: How to Calculate the number of Kanban
Toyota: Kanban System

Kiichiro Toyoda

Kiichiro Toyoda (Japanese: 豊田 喜一郎(とよだ きいちろう), Hepburn: Toyoda Kiichirō, June 11, 1894 – March 27, 1952) was a Japanese businessman and the son of Toyoda Loom Works founder Sakichi Toyoda. His decision to change Toyoda's focus from automatic loom manufacture into automobile manufacturing created what would become Toyota Motor Corporation.

Toyoda Loom Works and Toyota Motor Corporation
Kiichiro Toyoda persuaded his father, who was responsible as head of the family business, to invest in the expansion of Toyoda Loom Works into a concept automobiles division, which was considered a risk to the family business at the time. Shortly before Sakichi Toyoda died, he encouraged his son to follow his dream and pursue automobile manufacturing — Kiichiro would solidify the mechanical prowess the family had experienced inventing steam, oil, and electric looms, and would develop and institute what eventually became the global powerhouse of modern fame today, Toyota Motor Corporation. He would also institute the spelling of the automobile company away from the family name to famously garner good luck.
Toyoda would never know the success that would come to him as he resigned from the company he developed in 1950 in reaction to flagging sales and profitability. He died two years later; his contemporaries would call him "Japan's Thomas Edison". In 1957, his cousin and confidant Eiji Toyoda, would follow him as head of Toyota Motor Corporation, and build the late Toyoda's successful expansion into a world-class conglomeration of engineering and the launch of Japan's most prominent luxury brand, Lexus.

Early life
Childhood
Toyoda was born on June 11, 1894, in Yamaguchi in the village of Yoshitsu in Shizuoka Prefecture, Japan (currently Yamaguchi, Kosai, Shizuoka), the eldest son of Sakichi Toyoda and Tami Sahara.Before Kiichiro was born, Sakichi stayed in Toyohashi. At the time, Sakichi came to Yoshitsumachi to give a name to Kiichiro. However, after Sakichi named Kiichiro, he soon returned to Toyohashi. Also, less than two months after his mother, Tami, gave birth to him, she left him and her husband. She did so because she was sick of her husband, who, in her eyes, was too preoccupied with industrial inventions to pay any attention to their family life. Therefore, Kiichiro was raised in Yoshitsu village by his grandparents. At the age of three, Kiichiro moved to what is now Higashi-ku, Nagoya, Aichi Prefecture, where Sakichi lived. In terms of education, Kiichiro entered Kyodo Kanji Ordinary Elementary School and then changed to Takadake Ordinary Elementary School (currently Nagoya Municipal Higashisakura Elementary School). After that, he entered Aichi Normal School Elementary School (currently Aichi University of Education Nagoya Elementary School), Meirin Junior High School (currently Aichi Prefectural Meiwa High School), and Second High School. In 1920, he graduated from the Department of Mechanical Engineering, Faculty of Engineering, at Tokyo Imperial University. After graduation, he remained in Tokyo Imperial University at the Faculty of Law for about seven months until March 1921. Kiichiro excelled at his studies.

After graduating university
After graduating, Toyoda returned to his hometown, Nagoya, and joined Toyota Boshoku, which was founded by his father, Sakichi, in 1918 (Sakichi served as company president since then). From July 1921 to February 1922, Kiichirō visited San Francisco, London, Oldham (a large town in Greater Manchester, England), etc. to learn about the spinning and weaving industry and then returned from Marseille via Shanghai. After he returned to Japan, in December 1922, he married Hatako Iida, the daughter of the Takashimaya department store chain co-founder, Shinshichi Iida. In 1926, Kiichiro established Toyota Industries Corporation and became its managing director. Also, he became interested in automatic looms, so he set up a pilot plant in Kariya, Aichi to start development for them even though his father, Sakichi, disagreed. He traveled to Europe and America from September 1929 to April 1930, and thought that the automobile industry, which was in its infancy at that time, would greatly develop in the future. Therefore, in 1933, an automobile manufacturing department (later the automobile department) was newly established in Toyota Industries Corporation. In 1936, it was designated as a licensed company under the Automotive Manufacturing Act. In 1937, it became independent as Toyota Motor Corporation. Kiichirō became the vice president the same year (the president was Rizaburo Toyoda). In 1941, Kiichiro took office as president. At the height of the Pacific theater of World War II, the Toyoda family would be affected on both family business and home fronts. His children's education would be delayed by civil ramifications, and his business would be compelled to manufacture trucks for the Imperial Japanese Army. The family firms would be spared destruction in the days before the Japanese government's surrender.

Expanding to automotive industry
He is a key figure in paving the way for the Japanese automobile industry, and without him, today's Japanese automobile industry might have been less developed. The automobile industry plays a very important role in supporting the Japanese economy. The number of automobiles produced in Japan dramatically increased from 70,000 to 11.4 million between 1955 and 1980, and in 1980 exceeded the number of automobiles produced by the United States. In addition, the ratio of overseas exports of Japanese automobiles was 55% in 1985.Kiichiro is said to have opened the path for the Japanese automobile industry, and he is credited for creating from scratch domestic cars that were superior to foreign cars. The differences between Japan and the United States in the automobile industry during World War II were quite large. In the early 1930s, Kiichiro proceeded with the development toward the domestic production of automobiles. In 1933, Toyota Industries Corporation set up an automobile department and began full-scale development of automobiles. However, the development of the car did not proceed smoothly. For instance, no one had experience in automobile manufacturing, so he gathered those who had experience in automobile manufacturing from across Japan. Also, it took six months to manufacture the engine. Then, in May 1935, the first A1 passenger car was finally completed. After that, it produced AA passenger cars that improved the A1 type and GA trucks that improved the G1 type. Moreover, Toyota Industries Corporation was designated with the Nissan Motor Company in September as a licensed company under the Automotive Manufacturing Act. However, Kiichiro was worried that being selected as the licensed company would lead to the loss of the competitiveness of the automobile industry and it would cause the destruction of the Japanese automobile industry. In 1937, Toyota Motor Corporation was established and Kiichiro was elected as vice president. Kiichiro's management was very good for mainly two reasons. First, He controlled and made the operation simpler to produce more cars on a shoestring. Specifically, to clarify the internal organization, the company was divided into seven divisions, and the purpose and jurisdiction of each division were clearly decided. Second, he reduced the risk of mass-produced cars by being involved not only in mass-produced cars but also in the manufacture of specialty cars. In November 1938, the Koromo Factory was established, and it worked hard to manufacture automobiles. However, the problems of automobile quality and cost arose, and the management was put into a critical situation. To overcome this situation, Kiichiro solved those problems by taking prompt action and in-house manufacturing of automobile parts. In 1941, Kiichiro became president of Toyota Motor Industry.

Toyota Motor Corporation and war
Wartime
During Sino-Japanese War
Toyota Motor Corporation had restricted its activities due to the war. In 1937, the second Sino-Japanese War broke out and it caused great challenges to the Japanese automobile industry including Toyota Motor Corporation. Following the end of the war, the Japanese government restricted automobile production in favor of products for the military, making the production and purchase of passenger cars difficult. In 1938, the National General Mobilization Law was enacted to maximize the use of human and physical resources. This law required Toyota Motor Corporation to provide trucks to the military and munitions industries with priority. In addition, the production of passenger cars was restricted and small passenger car production ceased to increase the production of military trucks.
Starting in 1939, automobile industries had to obtain permission from the Minister of Commerce and Industry to sell passenger cars. Even after that, control was further strengthened. In 1940, Toyota Motor Corporation and Nissan were reinforced control for selling large trucks and buses by the Japanese government, so the control made customers buy these trucks and buses personally. Those who wished to purchase were approved by a long procedure. To be specific, they applied for purchase at a dealer with permission of local police officers, next, obtained a survey from the dealer, and then, applying for a vehicle dispatch to the manufacturer was required. However, the possibility of accepting general orders has become extremely low.

During World War II
This situation became worse after the entry into World War II. Toyota Motor Corporation was designated as a munitions company in 1944, and strict control continued. Kiichiro must have had very unsatisfactory days during this period about not being permitted to make passenger cars. However, even during this time, Kiichiro concentrated on thinking about technical problems. He believed that the automobile business would definitely serve post-war Japan and continued to explore the technical issues despite such difficult times.

Postwar
Under occupation and Kiichiro's actions
Japan accepted the Potsdam Declaration on August 14, 1945, which brought the war to an end on the following day, 15th in Japan. Postwar Japan was occupied by GHQ (General Headquarters). In terms of the automobile industry, GHQ did not allow a Japanese automobile company to make passenger cars except for trucks. Like this, Kiichiro still faced difficult problems. However, he did not lose in this hard situation. He took a leading place and took many actions for the restoration of the Japanese automotive industry. First, in November 1945, he established an umbrella organization for the automotive industry (自動車統括団体) initiatively and he became the chairman of the organization. In the following year, he negotiated with GHQ by himself, and made GHQ admit that the new organization was different from the one of the wartime. Kiichiro also invited representatives of distribution companies nationwide to Koromo. He gave speeches about policy changes at Toyota Motor Corporation and proceeded with talks for the recovery of the automobile industry. His forceful speeches impressed the representatives. These actions show how much he was passionate about the car and the automobile industry and justified him acting on the behalf of the automobile industry. In addition, after the war, many dealers of each company were released from control and became independent. Some of these dealers' companies restarted as Toyota dealerships and were able to back to the prewar company. This shows how much he was trusted as the president of Toyota Motor Corporation by the owners of the dealerships. In fact, Kiichiro valued relationship with dealerships, so he usually communicated with people directly and dealt with problems as soon as possible if they suggested.

Hard situation
However, producing and selling cars was still difficult. Raw materials and parts were not immediately available in the poor period after the war, and it was harder to obtain cheap and high-quality genuine parts than before. The production of trucks did not reach the monthly production goal of 500 cars. In 1945, the annual production volume was 3275 units, and in the following year, 5821 units. Although the production of passenger cars was prohibited, research for passenger cars was allowed by the GHQ. Also, as a part of the occupation policy, the automobile company was contracted to repair United States military vehicles in Japan, which was a good opportunity for Toyota employees, including Kiichiro, to learn more about the structure of American cars. They analized and absorbed the advanced parts of American cars, and then used them as a reference for the development of their own passenger car. Kiichiro kept working hard to develop the Japanese automobile industry, even under the adverse circumstances of the postwar period.

Passenger car production resumes
Kiichiro continued to put a great deal of effort into passenger cars in terms of research, manufacturing and selling. In June 1947, GHQ approved the production of up to 300 passenger cars under 1500 cc per year, so he started to work on passenger car production from this day officially. In October 1947, the first Japanese passenger car after the war, the SA model with an S engine, was released and was nicknamed "Toyopet." However, there were mainly two problems during this period. First, these passenger cars did not sell at all. This car was highly evaluated in the automobile industry at the time, but during that time, there were not many customs for the common people to buy a car for a drive. Therefore, the passenger car was sold only 197 units during the five years since it released, even though 12,796 trucks were sold over the same period. Second problem was that the production facilities were not good. There were many machines that had been used for many years and were not well maintained, so Kiichiro was worried about whether the cars of Toyota Motor Corporation could beat foreign cars. Moreover, if Toyota Motor Company couldn't complete a car that would be better and cheaper than a foreign car in a few years and that Japanese people would be willing to buy, Kiichiro was thinking about working with a foreign car manufacturer. From the description, it is clear that Kiichiro aimed to make passenger cars for Japanese with cheap and high quality, and make Japanese happy.

Dodge Line recession
Causes and influences
The automobile industry was hit hard by the recession caused by the 1949 Dodge Line. Toyota Motor Corporation slowed the collection of sales proceeds due to the effects of inflation control and the setting of a single exchange rate. The reason why the inflation broke out in Japan was that the Japanese government spent a great deal of money to support soldiers returning to Japan and withdrawals from overseas and then, increased currency. From the background, GHQ decided to set a single exchange rate of 360 yen per dollar to stabilize the Japanese economy. Due to the influence of recession, demand for automobiles declined furthermore. In addition, the price of materials for cars was risen, and cash management was deteriorated considerably. This was how the management of the company was deteriorated significantly. In response to this, Kiichiro went out to sell together with the executives and went to collect accounts receivable. Furthermore, he made his best effort to save money for materials, but there was a limit, and in the end, there was a deficit of 22 million yen every month. Due to Dodge Line recession, over 8000 companies went bankrupt during the year of 1949.

Avoiding dismissal of employees
In August 1949, the company finally proposed to cut 10% of wages and cut retirement pay in half. As a result, the company promised not to dismiss employees, instead of accepting a 10% wage reduction. Under this circumstance, the other car companies laid down their workforce. For example, at Nihon Denso (currently Denso), which was established in December 1949, a labor dispute arose over personnel rearrangement. On March 31, 1950, four months after its establishment, Nihon Denso announced a company restructuring plan that included personnel reduction of 473 people. Under such situation, the reason why Kiichiro did not perform personnel reduction was that he experienced the employment problem at Toyota Industries Corporation during the Showa Depression in 1930, so decided that such a situation would never occur again.  Moreover, the advance into the automobile business was also a measure to prevent the recurrence of employment problems due to business diversification. Therefore, He absolutely tried to avoid personnel reduction. Kiichiro visited the banks in the city every day to get finance account. After all, no financial institution provided the funds for company. However, Shotaro Kamiya, who was a managing director of sales, persistently requested the financing provision from Sogo Takanashi who was a branch manager of the Bank of Japan, Nagoya Branch. After that, finally, the syndicate consisting of 24 banks was established through the placement of the Bank of Japan. Toyota Motor Company could get 188.2 million yen in loans, subject to Toyota's reconstruction plan formulation. In this way, Kiichiro overcame the bankruptcy crisis of 1949.

Labor dispute
Despite the strong promotion of management rationalization measures, the company's business performance never recovered. The reason is, on October 25, 1949, GHQ issued a "Memorandum on the total removal of restrictions on the production and sale of automobiles". As a result, the production and sale of automobiles became free in principle, but about the supply of production materials, the allocation and distribution system by the Ministry of International Trade and Industry remained, and the prices of materials and automobiles remained regulated. Moreover, while the controlled prices of materials were gradually raised thereafter, while the controlled prices of automobiles remained unchanged until April 1950, the profitability of the automobile business remained extremely difficult. Not only Toyota Motor Corporation but also Nissan Motor Corporation and Isuzu Motors Corporation deteriorated in business performance. For the four and a half months from November 16, 1949, to March 31, 1950, the loss was 76.52 million yen, so Toyota Labor Union judged that the personnel cut was inevitable, and a quasi-fighting system was established in March of the same year. Since then, labor-management negotiations have intensified into long-standing disputes. Under such tension, Kiichiro, who was originally hypertensive, became ill, so negotiations with the labor union were handled by the management army instead of Kiichiro. However, On April 22, 1950, the company announced that it would carry out 1,600 voluntary retirements to the labor union. On the other hand, since the company had promised not to lay off its personnel, the union naturally became furious and continued with extreme strikes. The strikes continued daily for about two months after the declaration, which caused production in April and May to drop 70% from its previous average. Since the company would be destroyed as it is, on June 5, 1950, Kiichiro announced that he would resign as the president to take on this series of responsibilities. By his retirement, the strike ended finally. Everyone was shocked by Kiichiro's resignation, and the union also had a respect for Kiichiro.

Retirement and death
After retiring from the role of president, he created a laboratory at his home in Okamoto, Setagaya, Tokyo, and worked every day to design a small helicopter. On March 27, 1952, Toyoda died after suffering a fall resulting from a cerebral hemorrhage caused by chronic disease. He was 57 years old.

Family tree
References
External links
 Quotations related to Kiichiro Toyoda at Wikiquote

Lean CFP driven

Lean CFP (Complex Flow Production) Driven is a new approach which takes into account not only the widely implemented Lean manufacturing, but combines the principles of Lean with the Operating Curve, an approach based on the theoretical approach of  queuing theory developed in academia in the 1970s. The goal of Lean CFP Driven is to eliminate waste in order to achieve higher quality, increase productivity and at the same time understand the relationship between utilization, lead time and variability in order to maximize performance within the semiconductor industry.

Lean CFP Driven – Lean Complex Flow Production Driven
Background Semiconductor industry
The semiconductor industry is one of the most productive and dynamic industries in the world. It faces a continuous and rapid advancement in technology which puts the companies under constant pressure to come up with superior and cheaper goods than those that were state-of-the-art only a few months ago. The market and development of the market is based on Moore's Law or More than Moore.Customer demand in the semiconductor market evolves and changes at a swift pace which leads to the fact that a high level of flexibility is necessary to serve and meet the requirements of the customers. The semiconductor industry is furthermore very capital intensive based on the fact that the production equipment is highly complex, specialized and thus incredibly expensive. Challenges that the industry is facing are to continuously improve yield performance, achieve the highest possible return on the expensive equipment, speed and zero defects.

Lean CFP Driven and Traditional Lean
Lean CFP Driven moves in a new direction from the traditional Lean because of the additional focus on utilization, cycle time and variability. The different characteristics of the semiconductor industry, e.g. production structure and production related costs compared to other industries, forms the need to approach the Lean philosophy in a new way in order to meet these specific characteristics.
There are five key characteristics for the semiconductor industry:

Long cycle time.
No parallel process possible, high complexity
Short product life cycle.
Capital intensive production
Drastic cost decrease over timeThe complex production flow of a semiconductor fab is due to what is called a reentrance flow. A reentrant flow is a well-known attribute within a wafer fab and refers to the wafer visiting each tool not only once, but maybe 20 times during the course through the fab. To duplicate the expensive equipment and create a linear flow would make it even more challenging to get the highest possible return on the equipment and reach an optimized utilization of each tool, even though it results in a very complex production.The reentrant flow does require a certain level of flexibility, which in terms of Lean, could be seen as muda (Waste). The necessary flexibility, also in order to meet fluctuations in customer demand, requires the companies to apply other tools to measure and forecast performance and this is what Lean CFP Driven provides to the Semiconductor Industry. Lean CFP Driven adds the Operating Curve to evaluate the factors utilization, cycle time and variability which cannot be done through implementation of Traditional Lean.
Typical tools within the Traditional Lean which are also included in the new approach of Lean CFP Driven are as follows:

Poka-Yoke
Visual Management
Value Stream Mapping
Kanban
JIT
5S
5 WhysWhat distinguishes Lean CFP Driven from the traditional approach of Lean in terms of tools is that the new approach applies the tool Operating Curve in addition to the tools listed above. An example of how the Operating Curve could look like is shown in the figure below. The optimal operating point is indicated for different variabilities describing the non-uniformity of the production, 
  
    
      
        α
      
    
    {\displaystyle \alpha }
  . The great advantage of adding the Operating Curve tool is to maximize performance by optimizing both utilization and speed at the same time for the complex industry of Semiconductors by reducing the variability via the 4-partner method.

The Operating Curve is a tool initially developed in academia in the 1970s, based on the queuing theory, which uses the indicators Cycle Time and Utilization to benchmark and forecast a manufacturing line’s performance. The Operating Curve can be applied for different reasons, for example:

Understanding the relationship between variability, cycle time and utilization
Quantify trade-off between cycle time and utilization 
Documenting a single factory’s performance over time 
Calculate and Measure line performance The Operating curve can be described by the following formula:

  
    
      
        F
        F
        =
        α
        
          
            
              U
              
                U
                
                  m
                
              
            
            
              1
              −
              U
              
                U
                
                  m
                
              
            
          
        
        +
        1
      
    
    {\displaystyle FF=\alpha {\frac {UU_{m}}{1-UU_{m}}}+1}
  
where :

The flow factor can also be described as:

  
    
      
        F
        F
        =
        
          
            
              C
              T
            
            
              R
              P
              T
            
          
        
      
    
    {\displaystyle FF={\frac {CT}{RPT}}}
  
Where:


== References ==

Lean IT

Lean IT is the extension of lean manufacturing and lean services principles to the development and management of information technology (IT) products and services. Its central concern, applied in the context of IT, is the elimination of waste, where waste is work that adds no value to a product or service.Although lean principles are generally well established and have broad applicability, their extension from manufacturing to IT is only just emerging. Lean IT poses significant challenges for practitioners while raising the promise of no less significant benefits. And whereas Lean IT initiatives can be limited in scope and deliver results quickly, implementing Lean IT is a continuing and long-term process that may take years before lean principles become intrinsic to an organization's culture.

Extension to IT
As lean manufacturing has become more widely implemented, the extension of lean principles is beginning to spread to IT (and other service industries). Industry analysts have identified many similarities or analogues between IT and manufacturing. For example, whereas the manufacturing function manufactures goods of value to customers, the IT function “manufactures” business services of value to the parent organization and its customers. Similar to manufacturing, the development of business services entails resource management, demand management, quality control, security issues, and so on.Moreover, the migration by businesses across virtually every industry sector towards greater use of online or e-business services suggests a likely intensified interest in Lean IT as the IT function becomes intrinsic to businesses’ primary activities of delivering value to their customers. Already, even today, IT's role in business is substantial, often providing services that enable customers to discover, order, pay, and receive support. IT also provides enhanced employee productivity through software and communications technologies and allows suppliers to collaborate, deliver, and receive payment.
Consultants and evangelists for Lean IT identify an abundance of waste across the business service “production line”, including legacy infrastructure and fractured processes. By reducing waste through application of lean Enterprise IT Management (EITM) strategies, CIOs and CTOs in companies such as Tesco, Fujitsu Services, and TransUnion are driving IT from the confines of a back-office support function to a central role in delivering customer value.

Types of waste
Lean IT promises to identify and eradicate waste that otherwise contributes to poor customer service, lost business, higher than necessary business costs, and lost employee productivity. To these ends, Lean IT targets eight elements within IT operations that add no value to the finished product or service or to the parent organization (see Table 1).

Whereas each element in the table can be a significant source of waste in itself, linkages between elements sometimes create a cascade of waste (the so-called domino effect). For example, a faulty load balancer (waste element: Defects) that increases web server response time may cause a lengthy wait for users of a web application (waste element: Waiting), resulting in excessive demand on the customer support call center (waste element: Excess Motion) and, potentially, subsequent visits by account representatives to key customers’ sites to quell concerns about the service availability (waste element: Transportation). In the meantime, the company's most likely responses to this problem — for example, introducing additional server capacity and/or redundant load balancing software), and hiring extra customer support agents — may contribute yet more waste elements (Overprovisioning and Excess Inventory).

Principles
Value streams
In IT, value streams are the services provided by the IT function to the parent organization for use by customers, suppliers, employees, investors, regulators, the media, and any other stakeholders. These services may be further differentiated into:

Business services (primary value streams). Examples: point-of-sale transaction processing, ecommerce, and supply chain optimization
IT services (secondary value streams). Examples: application performance management, data backup, and service catalogThe distinction between primary and secondary value streams is meaningful. Given Lean IT's objective of reducing waste, where waste is work that adds no value to a product or service, IT services are secondary (i.e. subordinate or supportive) to business services. In this way, IT services are tributaries that feed and nourish the primary business service value streams. If an IT service is not contributing value to a business service, it is a source of waste. Such waste is typically exposed by value-stream mapping.

Value-stream mapping
Lean IT, like its lean manufacturing counterpart, involves a methodology of value-stream mapping — diagramming and analyzing services (value streams) into their component process steps and eliminating any steps (or even entire value streams) that do not deliver value.

Flow
Flow relates to one of the fundamental concepts of Lean as formulated within the Toyota Production System — namely, mura. A Japanese word that translates as “unevenness,” mura is eliminated through just-in-time systems that are tightly integrated. For example, a server provisioning process may carry little or no inventory (a waste element in Table 1 above) with labor and materials flowing smoothly into and through the value stream.
A focus on mura reduction and flow may bring benefits that would be otherwise missed by focus on muda (the Japanese word for waste) alone. The former necessitates a system-wide approach whereas the latter may produce suboptimal results and unintended consequences. For example, a software development team may produce code in a language familiar to its members and which is optimal for the team (zero muda). But if that language lacks an API standard by which business partners may access the code, a focus on mura will expose this otherwise hidden source of waste.

Pull/demand system
Pull (also known as demand) systems are themselves closely related to the aforementioned flow concept. They contrast with push or supply systems. In a pull system, a pull is a service request. The initial request is from the customer or consumer of the product or service. For example, a customer initiates an online purchase. That initial request in turn triggers a subsequent request (for example, a query to a database to confirm product availability), which in turn triggers additional requests (input of the customer's credit card information, credit verification, processing of the order by the accounts department, issuance of a shipping request, replenishment through the supply-chain management system, and so on).
Push systems differ markedly. Unlike the “bottom-up,” demand-driven, pull systems, they are “top-down,” supply-driven systems whereby the supplier plans or estimates demand. Push systems typically accumulate large inventory stockpiles in anticipation of customer need. In IT, push systems often introduce waste through an over-abundance of “just-in-case” inventory, incorrect product or service configuration, version control problems, and incipient quality issues.

Implementation
Implementation begins with identification and description of one or more IT value streams. For example, aided by use of interviews and questionnaires, the value stream for a primary value stream such as a point-of-sale business service may be described as shown in Table 2.

Table 2 suggests that the Executive Vice President (EVP) of Store Operations is ultimately responsible for the point-of-sale business service, and he/she assesses the value of this service using metrics such as CAPEX, OPEX, and check-out speed. The demand pulls or purposes for which the EVP may seek these metrics might be to conduct a budget review or undertake a store redesign. Formal service-level agreements (SLAs) for provision of the business service may monitor transaction speed, service continuity, and implementation speed. The table further illustrates how other users of the point-of-sale service — notably, cashiers and shoppers — may be concerned with other value metrics, demand pulls, and SLAs.
Having identified and described a value stream, implementation usually proceeds with construction of a value stream map — a pictorial representation of the flow of information, beginning with an initial demand request or pull and progressing up the value stream. Although value streams are not as readily visualizable as their counterparts in lean manufacturing, where the flow of materials is more tangible, systems engineers and IT consultants are practiced in the construction of schematics to represent information flow through an IT service. To this end, they may use productivity software such as Microsoft Visio and computer-aided design (CAD) tools. However, alternatives to these off-the-shelf applications may be more efficient (and less wasteful) in the mapping process.
One alternative is use of a configuration management database (CMDB), which describes the authorized configuration of the significant components of an IT environment. Workload automation software, which helps IT organizations optimize real-time performance of complex business workloads across diverse IT infrastructures, and other application dependency mapping tools can be an additional help in value stream mapping.After mapping one or more value streams, engineers and consultants analyze the stream(s) for sources of waste. The analysis may adapt and apply traditional efficiency techniques such as time-and-motion studies as well as more recent lean techniques developed for the Toyota Production System and its derivatives. Among likely outcomes are methods such as process redesign, the establishment of “load-balanced” workgroups (for example, cross-training of software developers to work on diverse projects according to changing business needs), and the development of performance management “dashboards” to track project and business performance and highlight trouble spots.

Trends
Recessionary pressure to reduce costs
The onset of economic recession in December 2007 was marked by a decrease in individuals’ willingness to pay for goods and services — especially in face of uncertainty about their own economic futures. Meanwhile, tighter business and consumer credit, a steep decline in the housing market, higher taxes, massive lay-offs, and diminished returns in the money and bond markets have further limited demand for goods and services.
When an economy is strong, most business leaders focus on revenue growth. During periods of weakness, when demand for good and services is curbed, the focus shifts to cost-cutting. In-keeping with this tendency, recessions initially provoke aggressive (and sometimes panic-ridden) actions such as deep discounting, fire sales of excess inventory, wage freezes, short-time working, and abandonment of former supplier relationships in favor of less costly supplies. Although such actions may be necessary and prudent, their impact may be short-lived. Lean IT can expect to garner support during economic downturns as business leaders seek initiatives that deliver more enduring value than is achievable through reactive and generalized cost-cutting.

Proliferation of online transactions
IT has traditionally been a mere support function of business, in common with other support functions such as shipping and accounting. More recently, however, companies have moved many mission-critical business functions to the Web. This migration is likely to accelerate still further as companies seek to leverage investments in service-oriented architectures, decrease costs, improve efficiency, and increase access to customers, partners, and employees.The prevalence of web-based transactions is driving a convergence of IT and business. In other words, IT services are increasingly central to the mission of providing value to customers. Lean IT initiatives are accordingly becoming less of a peripheral interest and more of an interest that is intrinsic to the core business.

Green IT
Though not born of the same motivations, lean IT initiatives are congruent with a broad movement towards conservation and waste reduction, often characterized as green policies and practices. Green IT is one part of this broad movement.Waste reduction directly correlates with reduced energy consumption and carbon generation. Indeed, IBM asserts that IT and energy costs can account for up to 60% of an organization's capital expenditures and 75% of operational expenditures. In this way, identification and streamlining of IT value streams supports the measurement and improvement of carbon footprints and other green metrics. For instance, implementation of Lean IT initiatives is likely to save energy through adoption of virtualization technology and data center consolidation.

Challenges
Value-stream visualization
Unlike lean manufacturing, from which the principles and methods of Lean IT derive, Lean IT depends upon value streams that are digital and intangible rather than physical and tangible. This renders difficult the visualization of IT value streams and hence the application of Lean IT. Whereas practitioners of lean manufacturing can apply visual management systems such as the kanban cards used in the Toyota Production System, practitioners of Lean IT must use enterprise IT management tools to help visualize and analyze the more abstract context of IT value streams.

Reference implementations
As an emerging area in IT management (see Deployment and Commercial Support), lean IT has relatively few reference implementations. Moreover, whereas much of the supporting theory and methodology is grounded in the more established field of lean manufacturing, adaptation of such theory and methodology to the digital service-oriented process of IT is likewise only just beginning. This lack makes implementation challenging, as evidenced by the problems experienced with the March 2008 opening of Heathrow Terminal 5. British airports authority BAA and airline British Airways, which has exclusive use of the new terminal, used process methodologies adapted from the motor industry to speed development and achieve cost savings in developing and integrating systems at the new terminal. However, the opening was marred by baggage handling backlogs, staff parking problems, and cancelled flights.

Resistance to change
The conclusions or recommendations of Lean IT initiatives are likely to demand organizational, operational, and/or behavioral changes that may meet with resistance from workers, managers, and even senior executives. Whether driven by a fear of job losses, a belief that existing work practices are superior, or some other concern, such changes may encounter resistance.
For example, a lean IT recommendation to introduce flexible staffing whereby application development and maintenance managers share personnel is often met with resistance by individual managers who may have relied on certain people for many years. Also, existing incentives and metrics may not align with the proposed staff sharing.

Fragmented IT departments
Even though business services and the ensuing flow of information may span multiple departments, IT organizations are commonly structured in a series of operational or technology-centric silos, each with its own management tools and methods to address perhaps just one particular aspect of waste. Unfortunately, fragmented efforts at Lean IT contribute little benefit because they lack the integration necessary to manage cumulative waste across the value chain.

Integration of lean production and lean consumption
Related to the aforementioned issue of fragmented IT departments is the lack of integration across the entire supply chain, including not only all business partners but also consumers. To this end, lean IT consultants have recently proposed so-called lean consumption of products and services as a complement to lean production. In this regard, the processes of provision and consumption are tightly integrated and streamlined to minimize total cost and waste and to create new sources of value.

Deployment and commercial support
Deployment of lean IT has been predominantly limited to application development and maintenance (ADM). This focus reflects the cost of ADM. Despite a trend towards increased ADM outsourcing to lower-wage economies, the cost of developing and maintaining applications can still consume more than half of the total IT budget. In this light, the potential of Lean IT to increase productivity by as much as 40% while improving the quality and speed of execution makes ADM a primary target within the IT department.
Opportunity to apply Lean IT exists in multiple other areas of IT besides ADM. For example, service catalog management is a Lean IT approach to provisioning IT services. When, say, a new employee joins a company, the employee's manager can log into a web-based catalog and select the services needed. This particular employee may need a CAD workstation as well as standard office productivity software and limited access to the company's extranet. On submitting this request, provisioning of all hardware and software requirements would then be automatic through a lean value stream. In another example, a Lean IT approach to application performance monitoring would automatically detect performance issues at the customer experience level as well as triage, notify support personnel, and collect data to assist in root-cause analysis. Research suggests that IT departments may achieve sizable returns from investing in these and other areas of the IT function.Among notable corporate examples of Lean IT adopters is UK-based grocer Tesco, which has entered into strategic partnerships with many of its suppliers, including Procter & Gamble, Unilever, and Coca-Cola, eventually succeeding in replacing weekly shipments with continuous deliveries throughout the day. By moving to eliminate stock from either the back of the store or in high-bay storage, Tesco has gotten markedly closer to a just-in-time pull system (see Pull/demand system).
Lean IT is also attracting public-sector interest, in-keeping with the waste-reduction aims of the lean government movement. One example is the City of Cape Coral, Florida, where several departments have deployed lean IT. The city's police records department, for instance, reviewed its processing of some 20,000 traffic tickets written by police officers each year, halving the time for an officer to write a ticket and saving $2 million. Comparable benefits have been achieved in other departments such as public works, finance, fire, and parks and recreation.

Complementary methodologies
Although Lean IT typically entails particular principles and methods such as value streams and value-stream mapping, Lean IT is, on a higher level, a philosophy rather than a prescribed metric or process methodology. In this way, Lean IT is pragmatic and agnostic. It seeks incremental waste reduction and value enhancement, but it does not require a grand overhaul of an existing process, and is complementary rather than alternative to other methodologies.

Agile, Scrum and lean software development
Agile software development is a set of software development methods that originated as a response for the indiscriminated use of CMMI, RUP and PMBOK creating fat and slow software development processes that normally increased the lead time, the work in progress and non-value added/value added activities ratio on projects. Agile software development methods include XP, Scrum, FDD, AUP, DSDM, Crystal, and others.
Scrum is one of the more well known agile methods for project management, and has as one of its origins concepts from Lean Thinking. Scrum also organizes work in a cross-functional, multidisciplinary work cell. It uses some form of kanban system to visualize and limit work in progress, and follows the PDCA cycle, and continuous improvements, that is the base of Lean.

Six Sigma
Whereas Lean IT focuses on customer satisfaction and reducing waste, Six Sigma focuses on removing the causes of defects (errors) and the variation (inconsistency) in manufacturing and business processes using quality management and, especially, statistical methods. Six Sigma also differs from Lean methods by introducing a special infrastructure of personnel (e.g. so-called “Green Belts” and “ Black Belts”) in the organization. Six Sigma is more oriented around two particular methods (DMAIC and DMADV), whereas Lean IT employs a portfolio of tools and methods. These differences notwithstanding, Lean IT may be readily combined with Six Sigma such that the latter brings statistical rigor to measurement of the former's outcomes.

Capability Maturity Model Integration (CMMI)
The Capability Maturity Model Integration (CMMI) from the Software Engineering Institute of Carnegie Mellon University (Pittsburgh, Pennsylvania) is a process improvement approach applicable to a single project, a division, or an entire organization. It helps integrate traditionally separate organizational functions, set process improvement goals and priorities, provide guidance for quality processes, and provide a benchmark or point of reference for assessing current processes. However, unlike Lean IT, CMMI (and other process models) doesn't directly address sources of waste such as a lack of alignment between business units and the IT function or unnecessary architectural complexity within a software application.

ITIL
ITIL contains concepts, policies, and recommended practices on a broad range of IT management topics. These are again entirely compatible with the objectives and methods of Lean IT. Indeed, as another best-practice framework, ITIL may be considered alongside the CMMI for process improvement and COBIT for IT governance.

Universal Service Management Body of Knowledge (USMBOK)
The Universal Service Management Body of Knowledge (USMBOK) — is a single book published by Service Management 101 and endorsed by numerous professional trade associations as the definitive reference for service management. The USMBOK contains a detailed specification of a service system and organization and leverages the rich history of service management as defined within product management and marketing professions.  The service organization specification describes seven key knowledge domains, equivalent to roles, and forty knowledge areas, representing areas of practice and skills.  Amongst these, within the Service Value Management knowledge domain, are a number of Lean relevant skills, including Lean Thinking and Value Mapping.  The USMBOK also provides detailed information on how problem management and lean thinking are combined with outside-in (customer centric) thinking, in the design of a continuous improvement program.

COBIT
Control Objectives for Information and Related Technology – better known as COBIT – is a framework or set of best practices for IT management created by the Information Systems Audit and Control Association (ISACA), and the IT Governance Institute (ITGI). It provides managers, auditors, and IT users a set of metrics, processes, and best practices to assist in maximizing the benefits derived through the use of IT, achieving compliance with regulations such as Sarbanes-Oxley, and aligning IT investments with business objectives. COBIT also aims to unify global IT standards, including ITIL, CMMI, and ISO 17799.

Notes
References
Bell, Steve (2012), Run Grow Transform, Integrating Business and Lean IT, Productivity Press, ISBN 978-1-4665-0449-3.
Bell, Steve and Orzen, Mike (2010) Lean IT, Enabling and Sustaining Your Lean Transformation, Productivity Press, ISBN 978-1-4398-1757-5. Shingo Prize Research Award 2011
Bell, Steve (2006) Lean Enterprise Systems, Using IT for Continuous Improvement, John R. Wiley, ISBN 978-0-471-67784-0.
Yasuhiro Monden (1998), Toyota Production System, An Integrated Approach to Just-In-Time, Third edition, Norcross, GA: Engineering & Management Press, ISBN 0-412-83930-X.
Womack, James P., and Roos, Daniel T. (2007), The Machine That Changed the World, Free Press, ISBN 978-0-7432-9979-4.
Womack, James P. and Jones, Daniel T. (2005) “Lean Consumption.” Harvard Business Review.

Lean Six Sigma

Lean Six Sigma is a process improvement approach that uses a collaborative team effort to improve performance by systematically removing operational waste and reducing process variation. It combines Lean Management and Six Sigma to increase the velocity of value creation in business processes.

History
1980s–2000s
Lean Six Sigma's predecessor, Six Sigma, originated from the Motorola company in the United States in 1986. Six Sigma was developed within Motorola to compete with the kaizen (or lean manufacturing) business model in Japan.In the 1990s, Allied Signal hired Larry Bossidy and introduced Six Sigma in heavy manufacturing. A few years later, General Electric's Jack Welch consulted Bossidy and implemented Six Sigma at the conglomerate. 
During the 2000s, Lean Six Sigma forked from Six Sigma and became its own unique process. While Lean Six Sigma developed as a specific process of Six Sigma, it also incorporates ideas from lean manufacturing, which was developed as a part of the Toyota Production System in the 1950s.

2000s–2010s
The first concept of Lean Six Sigma was created in Chuck Mills, Barbara Wheat, and Mike Carnell's 2001 book, Leaning into Six Sigma: The Path to Integration of Lean Enterprise and Six Sigma. It was developed as a guide for managers of manufacturing plants on how to combine lean manufacturing and Six Sigma to improve quality and cycle time in the plant.In the early 2000s Six Sigma principles expanded into other sectors of the economy, such as healthcare, finance, and supply chains.

Description
Lean Six Sigma is a synergized managerial concept of Lean and Six Sigma. Lean traditionally focuses on eliminating the eight kinds of waste ("muda"), and Six Sigma focuses on improving process output quality by identifying and removing the causes of defects (errors) and minimizing variability in (manufacturing and business) processes.
Lean Six Sigma uses the Define, Measure, Analyze, Improve and Control (DMAIC) phases similar to that of Six Sigma. The five phases used in Lean Six Sigma aim to identify the root cause of inefficiencies and work with any process, product, or service that has a large amount of data or measurable characteristics available.The different levels of certifications are divided into belt colors. The highest level of certification is a black belt, signifying a deep knowledge of Lean Six Sigma principles. Below the black belt are the green and yellow belts. For each of these belts, level skill sets are available that describe which of the overall Lean Six Sigma tools are expected to be part of a certain belt level. The skill sets reflect elements from Six Sigma, Lean and other process improvement methods like the theory of constraints and total productive maintenance. In order to achieve any of the certification levels, a proctored exam must be passed that asks questions about Lean Six Sigma and its applications.

Waste
Waste (muda) is defined by Fujio Cho as "anything other than the minimum amount of equipment, materials, parts, space, and workers time, which are absolutely essential to add value to the product".Different types of waste have been defined in the form of a mnemonic of "downtime":

Defects: A defect is a product that is declared unfit for use, which requires the product to be either scrapped or reworked, costing the company time and money. Examples include a product that is scratched during the production process and incorrect assembly of a product due to unclear instructions.
Over-production: Over-production refers to products made in excess or before it is needed. Examples include creating unnecessary reports and overproduction of a product before a customer has requested it.
Waiting: Waiting involves delays in process steps and is split into two different categories: waiting for material and equipment and idle equipment. Examples include waiting for authorization from a superior, waiting for an email response, waiting for material delivery, and slow or faulty equipment.
Non-Used Talent: Non-Used Talent refers to the waste of human potential and skill. The main cause is when management is segregated from employees; when this occurs, employees are not given the opportunity to provide feedback and recommendations to managers in order to improve the process flow and production suffers. Examples include poorly trained employees, lack of incentives for employees, and placing employees in jobs or positions that do not use all of their knowledge or skill.
Transportation: Transportation is the unnecessary or excessive movement of materials, products, people, equipment, and tools. Transportation adds no value to the product and can lead to product damage and defects. Examples include moving products between different functional areas and sending overstocked inventory back to an outlet warehouse.
Inventory: Inventory refers to an excess in products and materials that are unprocessed. It is a problem because the product may become obsolete before the customer requires it, storing the inventory costs the company time and money, and the possibility of damage and defects increases over time. Examples include excess finished goods, finished goods that cannot be sold, and broken machines on the manufacturing floor.
Motion: Motion is unnecessary movement by people. Excessive motion wastes time and increases the chance of injury. Examples include walking to get tools, reaching for materials, and walking to different parts of the manufacturing floor to complete different tasks.
Extra-processing: Extra-processing is doing more work than required or necessary to complete a task. Examples include double-entering data, unnecessary steps in production, unnecessary product customization, and using higher precision equipment than necessary.

See also
Business process
Design for Six Sigma
DMAIC
Industrial Engineering
Lean IT
Lean manufacturing
Six Sigma
Total productive maintenance
Total quality management

References
Citations
Bibliography
External links
Lean Six Sigma for Real Business Results, IBM Redguide

Lean dynamics

Lean dynamics is a business management practice that emphasizes the same primary outcome as lean manufacturing or lean production of eliminating wasteful expenditure of resources.  However, it is distinguished by its different focus of creating a structure for accommodating the dynamic business conditions that cause these wastes to accumulate in the first place.
Like lean manufacturing, lean dynamics is a variation on the theme of creating efficiencies and greater value by optimizing flow rather than by maximizing economies of scale.  As such, it represents an important chapter in the broader discussion of Taylorism, Fordisim, Alfred Sloan's standard volume methodology, Peter Drucker's philosophy on the "theory of the business" and Genichi Taguchi's analysis of loss.  Its general philosophy has grown in popularity over recent years, in large part because of the increasingly challenging circumstances faced by the global business world (particularly evident during the 2008–2009 worldwide economic downturn.)This need to create greater efficiencies while competing in an environment that demands constant change and innovation seems to be responsible for the emergence of lean dynamics as a recognized business improvement approach.

Overview
The term "lean" was first coined by a researcher at MIT and later popularized by the best-selling book, The Machine that Changed the World (1990).  Those implementing lean principles generally focus on applying lean tools which have been described in a number of references over the past two decades with the focus of seeking out and directly targeting "waste" (its seven forms described by Taiichi Ohno in his book Toyota Production System are well known.) This emphasis can result in greater efficiencies that do not necessarily respond well when business conditions shift.
Lean dynamics takes a different approach.  Introduced by the book, Going Lean, it does not directly target the desired outcome of waste elimination; instead, it focuses on identifying and addressing sources of "lag", or imbedded disconnects in flowing value through operations, decision-making, information, and innovation that lead to workarounds and amplify disruption when business conditions change.  It promotes a different way of structuring the business that creates an inherent "dynamic stability" or greater responsiveness for accommodating shifting business conditions.  Companies that are structured in this way show dramatically greater customer value as measured by their quality, innovation, and customer satisfaction; they also sustain greater corporate value as measured by profitability, market capitalization, and growth.
Lean dynamics uses the value curve as a data-driven way of directly comparing companies to distinguish lean firms from other industries.
Implementation of lean dynamics focuses on driving down the impact that variation has on loss (based on the loss function from the Taguchi methods often described by the famous business statistician W. Edwards Deming), a concept describing the dramatic reduction of value-creating capabilities that traditional management systems display when subjected to sudden shifts in product demand, energy prices, or other conditions that affect operational stability or predictability.
Lean dynamics is particularly versatile in that it can be applied to a wide range of manufacturing and service industries.  Its methods have been studied for Aerospace and Defense (particularly as applied to procuring hard to get spare parts), and medical, and distinguishes the few that stand out during crisis such as airlines, and retail.

Background
Lean dynamics has its roots in a global study of lean manufacturing in the aerospace industry aimed at understanding how to break its cycle of cost escalation that was making new products unaffordable.  These results were described in the Shingo Prize winning book, Breaking the Cost Barrier.  This study showed that directly targeting “waste” reduction as the focal point for lean programs does not lead to significant cost savings.  Instead, it provided strong evidence that emphasis should be placed on applying lean principles to mitigate the amplification of variation that causes workarounds.  Addressing this variation was critical to overcoming the disruption that often causes waste to accumulate in the first place. Other sources cited this, identifying occurrences across other industries.Subsequent research of the aerospace industry showed that firms had accepted these findings, describing this approach of Variation Management as "...one of the most prominent approaches to transforming and improving military enterprise performance."This phenomenon was validated by correlating the disruption caused by variation in flow (measured as cycle time variation) with "loss" as described by the Taguchi Loss Function (see Taguchi Methods).  Sudden changes in business conditions, such as spikes or decreases in production demand, cause increases in variation from forecasted conditions, causing disruption, and causing waste to accumulate.  A lean dynamics approach restructures the way operations, organizations, information, and innovation are structured to overcome this.One method for companies to deal internally with externally driven variation (such as sudden spikes in demand), a core tenet of lean dynamics, was explored by the Defense Department under the Supplier Utilization through Responsive Grouped Enterprises (SURGE) Program.  The SURGE program was partially sponsored by the Joint Strike Fighter Program (JSF)(F-35 Lightning II) This program aimed to reduce factory disruption due to demand variation by grouping parts together that shared similar manufacturing processes. It succeeded in reducing lead time on a number of critical aerospace items by as much as 60%.  The SURGE program gained notoriety when it was mentioned on the popular TV Show JAG.

See also
Taguchi loss function
Six Sigma
Statistical process control

Terminology
Production leveling
Muda, Mura, Muri
Workcell
Cycle time variation


== References ==

Lean higher education

Lean Higher Education (LHE) refers to the adaptation of lean thinking to higher education, typically with the goal of improving the efficiency and effectiveness of operations. Lean, originally developed at the Toyota Motor Corporation, is a management philosophy that emphasizes "respect for people" and "continuous improvement" as core tenets. Lean encourages employees at all organizational levels to re-imagine services from a customer's point of view, removing process steps that do not add value and emphasizing steps that add the most value. While the concept of "customers" and "products" is controversial in higher education settings, there are certainly diverse stakeholders who are interested in the success of colleges and universities, the most common of which are students, faculty, administrators, potential employers and various levels of government.
Lean in higher education has been applied both to administrative and academic services. Balzer (2010) described such initiatives within university settings, including the critical factors for success and ways to measure progress. He noted that LHE can be effective to  respond to higher education's heightened expectations, reducing expenses in an era of rising costs, meeting demands of public accountability, and leveraging institutional resources to fulfill the educational, scholarship, and outreach missions of higher education. A comprehensive literature review examining Lean's impact on higher education has been published. The authors reported that Lean has a significant and measurable impact when used to improve academic and administrative operations. Such improvements are effective at the department/unit level or throughout an entire institution. However, the authors noted that implementing Lean is a serious undertaking that is most impactful if it involves long-term, strategic planning.
Though the application of Lean management in higher education is more prevalent in administrative processes (e.g., admissions, registration, HR, and procurement) it also has been applied to academic processes (e.g., course design and teaching, improving degree programs, student feedback, and handling of assignments) in an increasing number of cases.
Pioneering academic institutions who have implemented Lean include: Cardiff University (Wales), Edinburgh Napier University (Scotland), Michigan Technological University (USA), Rensselaer Polytechnic Institute (USA), University of Aberdeen (Scotland), University of Central Oklahoma (USA), University of St. Andrews (Scotland), Winona State University (USA) and others. A group of universities in the U.K. formed the LeanHEHub in 2012/2013. In 2016 the network was restructured due to growth, and is now known as Lean HE - the Lean in Higher Education Network. The Lean HE network has three continental divisions (Lean HE Americas, Lean HE Europe and Lean HE AsiaPacific). In Scotland, the Scottish Higher Education Improvement Network (SHEIN)  is a collaborative network of HE professionals working within the area of continuous improvement. SHEIN exists to encourage the sharing of resources and best practice, online and face-to-face. In 2020 SHEIN became Lean HE Scotland, a sub-group of Lean HE Europe.

Lean Principles
Of great importance in the application of Lean management in any organization is the recognition and daily practice of the Lean principles: "Continuous Improvement" and "Respect for People." The "Respect for People" principle is challenging for management to implement, because most managers have risen to their level of responsibility based on their superior "fire-fighting" skills. With Lean, managers are coaches who guide their employees through a problem solving process. The employee learns how to ask themselves the questions that enable them to solve problems on their own, with the same or better quality that the manager would have achieved. Problems occur when managers cannot relinquish control, resulting in zero-sum (win-lose) outcomes for people and inferior results. In other words, one party gains at another party's expense, and the losers are much less willing to participate in continuous improvement. This outcome impedes teamwork and information flows, and discourages daily efforts by administration, faculty, and staff to improve processes. In order to function properly, Lean management must be understood and practiced in a plus-sum (win-win) manner. The "Respect for People" principle is required in order to sustain continuous improvement [7].

Lean Practices
The origins of Lean practices date from late 19th- and early 20th-century industrial engineering. Lean practices have evolved over the decades since then to become much easier for non-specialists to understand and use. It is now common for people with backgrounds and interests far from industrial engineering to become highly competent Lean management practitioners. Therefore, the Lean management system has the benefit that everyone in an organization can apply the practices without the need for specialists.
Seminal work in the application of Lean to academic processes was done by Prof. M.L. "Bob" Emiliani when he was at Rensselaer Polytechnic University in the early 2000s and is described in two papers: M.L. "Bob" Emiliani (2004) "Improving Business School Courses by Applying Lean Principles and Practices," and M.L. "Bob" Emiliani (2005) "Using Kaizen to Improve Graduate Business School Degree Programs,". The former paper describes what individual faculty can do to improve their courses and delivery using Lean principles and practices. The latter paper describes what teams of faculty, staff, administrators, students, alumni, and employers can do to improve their courses using kaizen (literal translation: "change for the better"). Prof. Emiliani also produced a Kaizen Team Leader's Manual for improving academic courses and programs based on his work.The use of Lean practices in academic processes are described in two papers written by Prof. Emiliani cited above (Refs. 2,3), and in the book Lean Higher Education: Increasing the Value and Performance of University Processes.

Differences Between Lean in Higher Education and Lean in Other Sectors
Lean in HE follows the same principles and practices of Lean management as applied in service, manufacturing, or government sectors. Lean management readily takes into account the unique governance structures of higher education institutions. Lean management is responsive to the needs of multiple stakeholders in a non-zero-sum fashion and is therefore well-suited for the governance and ongoing improvement of HEIs.
The business of teaching in, or the back office administration of, Higher Education Institutions (HEI's) is similar to Lean management practiced in other service sectors because teaching and administration consist of repeatable transactional processes, in whole or part. Guidance for Lean implementation in HE administration, and, to a lesser extent in teaching, is presented in the book Lean Higher Education, Increasing the Value and Performance of University Processes.

Impact of Lean in Higher Education
The impact of Lean in HE (namely in academic activities), have been studied and found to be potentially beneficial. The benefits include lead-time reduction, increase in throughput, lower cost, increased student satisfaction scores, etc. Reports analyzing Lean in higher education indicate that Lean principles are being successfully applied.
 Various HE stakeholders will likely perceive their organization to be substantially different or possess unique characteristics compared to other service organizations or businesses using Lean management. These reports, as well as a wide range of empirical results, show such perceptions to be erroneous.

Criticisms of Lean Management
The principal criticisms of Lean management are well known, relatively few in number, and have been constant over time. Workers may view Lean management as undesirable if it is incorrectly implemented, because it could make them work harder, they might have less time to spend with customers, and, ultimately, they could lose their jobs. These criticisms, which will surely be voiced by faculty and staff in HE, are predictable  and the result of zero-sum (win-lose) application of Lean management by senior managers.
Supporters of Lean might assert that Lean management can be conducted in a non-zero-sum (win-win) manner—the criticism is simply a result of misapplication of the central concepts.

References
Balzer, W.K., (2010) Lean Higher Education: Increasing the Value and Performance of University Processes, CRC Press (Taylor and Francis Group). ISBN 978-1439814659 [8]
Balzer, W.K., Francis, D., Krehbiel, T. & Shea, N. (2016). A review and perspective on Lean in higher education. Quality Assurance in Education. 24(4). http://hdl.handle.net/2374.MIA/5995
Emiliani, B., (2005) "Lean in Higher Education", LeanCEO [9]
Emiliani, B., (2015) Lean University: A Guide to Renewal and Prosperity, [10]
Emiliani, B., (2015) Lean Teaching: A Guide to Becoming a Better Teacher, [11]
Moore, M., Nash, M., and Henderson, K. (2007) "Becoming a Lean University", University of Central Oklahoma [12]
Yorkstone, S., (2013) "Lean Goes Back to School", Lean Management Journal (subscription required), [13]
Yorkstone, S., (2016). Lean universities. In Netland, T. & Powell, D. J. (Eds.). The Routledge Companion to Lean Management. Taylor & Francis (Routledge). ISBN 978-1138920590
Yorkstone, S.(ed), (2019) Global Lean for Higher Education, A Themed Anthology of Case Studies, Approaches, and Tools. CRC Press (Taylor and Francis Group). ISBN 978-0367024284 [14]

External links
Prof. Bob Emiliani's Lean Professor Blog
Blog review of William Balzer's book, "Lean Higher Education"
Lean HE: Practitioner Led Community of Practice for Lean in Higher Education and the University Sector
Lean in the University of St Andrews
Lean in Cardiff University
Business Improvement at University of Aberdeen
Lean University at the University of Central Oklahoma

Lean product development

Lean product development (LPD) is a lean approach to counter the challenges of product development, notably:

Lack of innovative solutions
Long development cycle times
Many redevelopment cycles
High development costs
Long production cycle times
High production costs

See also
Design for lean manufacturing
Muntzing
Toyota Production System

History of lean product development
Toyota started its journey with lean product development at Toyota Loom Works (see History of Toyota). Their early approach is notably different from Lean manufacturing that became famous through the book "The Machine that changed the world".
When Toyota started manufacturing cars, there was a difference in manufacturing conditions between Japan and the USA. Toyota had few educated engineers and little prior experience. Car companies in US employed a well-educated work force in the cities and benefited from the research and student skill-sets of established engineering schools. To tackle this shortfall in knowledge and experience, Toyota conducted an incremental approach to development that built on their existing knowledge and became the basis of the lean systems Toyota uses today.Allen Ward studied Toyota’s lean product development system and found parallels with the US airplane industry. For instance, the Wright brothers’ method of constructing their airplanes became one of the legacies they passed on to the aviation industry. This approach enabled the USA to create one of World War II's most successful fighter planes from scratch in the short span of six months. After the war, Toyota incorporated many of the airline industry's findings into its own product development methodology.

Differences between lean product development and lean production
While some basic principles and guidelines are applicable across Lean product development and  Lean production (such as waste reduction), many applications of lean process for development have focused more on the production approach.The purpose of production is to manufacture products reliably within margins of control. The flow of value is physically evident, and the link between cause and effect is easy to see. For example, feedback on adjusting the speed of production is immediately realized in an increase or decrease in rejected items. Any decisions made must be based on best practice.
On the other hand, the purpose of product development is to design new products that improve the lives of customers. This is a complex space where the flow of value can only be discerned at an abstract level and where cause and effect might be separated by time and space. For example, feedback on the decision to design a certain feature will not be received until the product has been built and is in the hands of the customer. This means that decisions are made on short-cycle experimentation, prototyping, set-based design, and emergent practice. A premium is placed on creating reusable knowledge and reducing risk at handover points.
An essential point about these differences is summarized in the advice Jim Womack gives Harley Davidson: "Don't try to bring lean manufacturing upstream to product development. The application of Lean in product development and manufacturing are different. Some aspects may look similar, but they are not! Be leery of an expert with experience in lean manufacturing that claims to know product development" The most common high level concepts associated with lean product development are:

Creation of re-usable knowledge. Knowledge is created and maintained so that it can be leveraged for successive products or iterations.
Set-based concurrent engineering. Different stages of product development run simultaneously rather than consecutively to decrease development time, improve productivity, and reduce costs.
Teams of responsible experts. Lean product development organizations develop cross-functional teams and reward competence building in teams and individuals.
Cadence and pull. Managers of lean product development organizations develop autonomous teams, where engineers plan their own work and work their own plans.
Visual management. Visualization is a main enabler of lean product development.
Entrepreneurial system designer. The lean product development organization makes one person responsible for the engineering and aesthetic design, and market and business success, of the product.
Flow management.

Results of lean product development
Lean product development has been claimed to produce the following results:

Increase innovation ten-fold
Increase introduction of new products 400%-500% Companies such as Toyota can attribute their success to lean product development. In 2000, Toyota launched 14 new products, a larger product line than GM's entire product offering. At that point, Toyota had just 70,000 employees while GM had more than five times as many.

Applicability of lean product development
Researchers divide product development projects accordingly to their need drivers:

Wished: there is no such product on the market, only a wish for such a product. These projects can be on the edge of what is possible to do.
Wanted: there are only a few basic similar products on the market that usually require improvement.
Needed: there are enough products in existence so knowledge about this is abundant on the market.For example, the mobile phone was a Wanted product in the 1990s because it was on the leading edge of technology. Today it is regarded as a Needed product. It is common in the market. There is enough knowledge in the public domain so that even small companies can make a good mobile phone.
Product development methods can be classified according to whether they are focused on handling stable or non-stable conditions. Lean product development is a dynamic method of product development that handles unstable conditions.The influence of need drivers and stability (or lack of stability) on product development are illustrated in the table below.

Related concepts
Lean startup
Lean project management

Notes and references

Exchange ref 12 with:
Ottosson, S. (2016): Developing Sustainable Product Innovations, ISBN 978-91-639-1980-0 page 112

Lean services

Lean services is the application of lean manufacturing production methods in the service industry (and related method adaptations). Lean services have among others been applied to US health care providers and the UK HMRC.

History
Definition of "Service": see Service, Business Service and/or Service Economics. Lean Services history, see Lean manufacturing. 
Lean manufacturing and Services, contrasted by Levitt; "Manufacturing looks for solutions inside the very tasks to be done... Service looks for solutions in the performer of the task." (T.Levitt, Production-Line Approach to Service, Harvard Business Review, September 1972).

Method
Underlying method; Lean manufacturing.
Bicheno & Holweg provides an adapted view on waste for the method ("waste", see Lean manufacturing, waste and The Toyota Way, principle 2):
Delay on the part of customers waiting for service, for delivery, in queues, for response, not arriving as promised.
Duplication. Having to re-enter data, repeat details on forms, copy information across, answer queries from several sources within the same organisation.
Unnecessary Movement. Queuing several times, lack of one-stop, poor ergonomics in the service encounter.
 Unclear communication, and the wastes of seeking clarification, confusion over product or service use, wasting time finding a location that may result in misuse or duplication.
Incorrect inventory. Being out-of-stock, unable to get exactly what was required, substitute products or services.
An opportunity lost to retain or win customers, a failure to establish rapport, ignoring customers, unfriendliness, and rudeness.
Errors in the service transaction, product defects in the product-service bundle, lost or damaged goods.
Service quality errors, lack of quality in service processes.Shillingburg and Seddon separately provides an additional type of waste for the method:
Value Demand, services demanded by the customer. Failure Demand, production of services as a result of defects in the upstream system.

Criticism
John Seddon outlines challenges with Lean Services in his paper "Rethinking Lean Service" (Seddon 2009) using examples from the UK tax-authorities HMRC.

See also
Lean construction
Lean government
Lean Higher Education
Lean IT


== References ==

Lean software development

Lean software development is a translation of lean manufacturing principles and practices to the software development domain. Adapted from the Toyota Production System, it is emerging with the support of a pro-lean subculture within the agile community. Lean offers a solid conceptual framework, values and principles, as well as good practices, derived from experience, that support agile organizations.

Origin
The expression "lean software development" originated in a book by the same name, written by Mary Poppendieck and Tom Poppendieck in 2003. The book restates traditional lean principles, as well as a set of 22 tools and compares the tools to corresponding agile practices. The Poppendiecks' involvement in the agile software development community, including talks at several Agile conferences  has resulted in such concepts being more widely accepted within the agile community.

Lean principles
Lean development can be summarized by seven principles, very close in concept to lean manufacturing principles:
Eliminate waste
Amplify learning
Decide as late as possible
Deliver as fast as possible
Empower the team
Build integrity in
Optimize the whole

Eliminate waste
Lean philosophy regards everything not adding value to the customer as waste (muda). Such waste may include:
Partially done work
Extra features
Relearning
Task switching
Waiting
Handoffs
Defects
Management activitiesIn order to eliminate waste, one should be able to recognize it. If some activity could be bypassed or the result could be achieved without it, it is waste. Partially done coding eventually abandoned during the development process is waste. Extra features like paperwork and features not often used by customers are waste. Switching people between tasks is waste (because of time spent, and often lost, by people involved in context-switching). Waiting for other activities, teams, processes is waste. Relearning requirements to complete work is waste. Defects and lower quality are waste. Managerial overhead not producing real value is waste.
A value stream mapping technique is used to identify waste. The second step is to point out sources of waste and to eliminate them. Waste-removal should take place iteratively until even seemingly essential processes and procedures are liquidated.

Amplify learning
Software development is a continuous learning process based on iterations when writing code. Software design is a problem-solving process involving the developers writing the code and what they have learned. Software value is measured in fitness for use and not in conformance to requirements.
Instead of adding more documentation or detailed planning, different ideas could be tried by writing code and building. The process of user requirements gathering could be simplified by presenting screens to the end-users and getting their input. The accumulation of defects should be prevented by running tests as soon as the code is written.
The learning process is sped up by usage of short iteration cycles – each one coupled with refactoring and integration testing. Increasing feedback via short feedback sessions with customers helps when determining the current phase of development and adjusting efforts for future improvements. During those short sessions, both customer representatives and the development team learn more about the domain problem and figure out possible solutions for further development. Thus the customers better understand their needs, based on the existing result of development efforts, and the developers learn how to better satisfy those needs. Another idea in the communication and learning process with a customer is set-based development – this concentrates on communicating the constraints of the future solution and not the possible solutions, thus promoting the birth of the solution via dialogue with the customer.

Decide as late as possible
As software development is always associated with some uncertainty, better results should be achieved with a set-based or options-based approach, delaying decisions as much as possible until they can be made based on facts and not on uncertain assumptions and predictions. The more complex a system is, the more capacity for change should be built into it, thus enabling the delay of important and crucial commitments. The iterative approach promotes this principle – the ability to adapt to changes and correct mistakes, which might be very costly if discovered after the release of the system.

With set-based development: If a new brake system is needed for a car, for example, three teams may design solutions to the same problem. Each team learns about the problem space and designs a potential solution. As a solution is deemed unreasonable, it is cut. At the end of a period, the surviving designs are compared and one is chosen, perhaps with some modifications based on learning from the others - a great example of deferring commitment until the last possible moment. Software decisions could also benefit from this practice to minimize the risk brought on by big up-front design. Additionally, there would then be multiple implementations that work correctly, yet are different (implementation-wise, internally). These could be used to implement fault-tolerant systems which check all inputs and outputs for correctness, across the multiple implementations, simultaneously.An agile software development approach can move the building of options earlier for customers, thus delaying certain crucial decisions until customers have realized their needs better. This also allows later adaptation to changes and the prevention of costly earlier technology-bounded decisions. This does not mean that no planning should be involved – on the contrary, planning activities should be concentrated on the different options and adapting to the current situation, as well as clarifying confusing situations by establishing patterns for rapid action. Evaluating different options is effective as soon as it is realized that they are not free, but provide the needed flexibility for late decision making.

Deliver as fast as possible
In the era of rapid technology evolution, it is not the biggest that survives, but the fastest. The sooner the end product is delivered without major defects, the sooner feedback can be received, and incorporated into the next iteration. The shorter the iterations, the better the learning and communication within the team. With speed, decisions can be delayed. Speed assures the fulfilling of the customer's present needs and not what they required yesterday. This gives them the opportunity to delay making up their minds about what they really require until they gain better knowledge. Customers value rapid delivery of a quality product.
The just-in-time production ideology could be applied to software development, recognizing its specific requirements and environment. This is achieved by presenting the needed result and letting the team organize itself and divide the tasks for accomplishing the needed result for a specific iteration. At the beginning, the customer provides the needed input. This could be simply presented in small cards or stories – the developers estimate the time needed for the implementation of each card. Thus the work organization changes into self-pulling system – each morning during a stand-up meeting, each member of the team reviews what has been done yesterday, what is to be done today and tomorrow, and prompts for any inputs needed from colleagues or the customer. This requires transparency of the process, which is also beneficial for team communication.
The myth underlying this principle is haste makes waste. However, lean implementation has shown that it is a good practice to deliver fast in order to see and analyze the output as early as possible.

Empower the team
There has been a traditional belief in most businesses about the decision-making in the organization – the managers tell the workers how to do their own job. In a work-out technique, the roles are turned – the managers are taught how to listen to the developers, so they can explain better what actions might be taken, as well as provide suggestions for improvements. The lean approach follows the agile principle "build projects around motivated individuals [...] and trust them to get the job done", encouraging progress, catching errors, and removing impediments, but not micro-managing.
Another mistaken belief has been the consideration of people as resources. People might be resources from the point of view of a statistical data sheet, but in software development, as well as any organizational business, people do need something more than just the list of tasks and the assurance that they will not be disturbed during the completion of the tasks. People need motivation and a higher purpose to work for – purpose within the reachable reality, with the assurance that the team might choose its own commitments. The developers should be given access to the customer; the team leader should provide support and help in difficult situations, as well as ensure that skepticism does not ruin the team’s spirit. Respecting people and acknowledging their work is one way to empower the team.

Build integrity in
The customer needs to have an overall experience of the system. This is the so-called perceived integrity: how it is being advertised, delivered, deployed, accessed, how intuitive its use is, its price and how well it solves problems.
Conceptual integrity means that the system’s separate components work well together as a whole with balance between flexibility, maintainability, efficiency, and responsiveness. This could be achieved by understanding the problem domain and solving it at the same time, not sequentially. The needed information is received in small batch pieces – not in one vast chunk - preferably by face-to-face communication and not any written documentation. The information flow should be constant in both directions – from customer to developers and back, thus avoiding the large stressful amount of information after long development in isolation.
One of the healthy ways towards integral architecture is refactoring. As more features are added to the original code base, the harder it becomes to add further improvements. Refactoring is about keeping simplicity, clarity, minimum number of features in the code. Repetitions in the code are signs of bad code designs and should be avoided (i.e. by applying the DRY rule). The complete and automated building process should be accompanied by a complete and automated suite of developer and customer tests, having the same versioning, synchronization and semantics as the current state of the system. At the end the integrity should be verified with thorough testing, thus ensuring the System does what the customer expects it to. Automated tests are also considered part of the production process, and therefore if they do not add value they should be considered waste. Automated testing should not be a goal, but rather a means to an end, specifically the reduction of defects.

Optimize the whole
Modern software systems are not simply the sum of their parts, but also the product of their interactions. Defects in software tend to accumulate during the development process – by decomposing the big tasks into smaller tasks, and by standardizing different stages of development, the root causes of defects should be found and eliminated. The larger the system, the more organizations that are involved in its development and the more parts are developed by different teams, the greater the importance of having well defined relationships between different vendors, in order to produce a system with smoothly interacting components. During a longer period of development, a stronger subcontractor network is far more beneficial than short-term profit optimizing, which does not enable win-win relationships.
Lean thinking has to be understood well by all members of a project, before implementing in a concrete, real-life situation. "Think big, act small, fail fast; learn rapidly" – these slogans summarize the importance of understanding the field and the suitability of implementing lean principles along the whole software development process. Only when all of the lean principles are implemented together, combined with strong "common sense" with respect to the working environment, is there a basis for success in software development.

Lean software practices
Lean software development practices, or what the Poppendiecks call "tools" are restated slightly from the original equivalents in agile software development. Examples of such practices include:

Seeing waste
Value stream mapping
Set-based development
Pull systems
Queueing theory
Motivation
Measurements
Test-driven developmentSince agile software development is an umbrella term for a set of methods and practices based on the values and principles expressed in the Agile Manifesto, lean software development is considered an agile software development method.

See also
Extreme programming
DevOps
Kanban
Kanban board
Lean integration
Lean services
Scrum (development)

References
Further reading
Ladas, Corey (January 2008). Scrumban: Essays on Kanban Systems for Lean Software Development. Modus Cooperandi Press. ISBN 978-0-578-00214-9.
Ries, Eric (September 2011). The Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses. Crown Business. ISBN 978-0307887894.

Lean thinking

Lean thinking is a management framework made up of a philosophy, practices and principles which aim to help practitioners improve efficiency and the quality of work. Lean thinking encourages whole organisation participation. The goal is to organise human activities to deliver more benefits to society and value to individuals while eliminating waste.

History
The term "lean thinking" was coined by mechanical engineer and MIT graduate student John Krafcik in 1988, who subsequently went on to run Google LLC's autonomous driving unit for many years.

Principles
Lean thinking is a way of thinking about an activity and seeing the waste inadvertently generated by the way the process is organized. It uses five key principles:

Value
Value streams
Flow
Pull
Perfection The aim of lean thinking is to create a lean culture, one that sustains growth by aligning customer satisfaction with employee satisfaction, and that offers innovative products or services profitably while minimizing unnecessary over-costs to customers, suppliers and the environment. The basic insight of lean thinking is that if you train every person to identify wasted time and effort in their own job and to better work together to improve processes by eliminating such waste, the resulting culture (basic thinking, mindset and assumptions) will deliver more value at less expense while developing every employee's confidence, competence and ability to work with others.

Overview
Lean thinking was born out of studying the rise of Toyota Motor Company from a bankrupt Japanese automaker in the early 1950s to today's dominant global player. At every stage of its expansion, Toyota remained a puzzle by capturing new markets with products deemed relatively unattractive and with systematically lower costs while not following any of the usual management dictates. In studying the company firsthand it appeared that it had a unique group of elders (sensei) and coordinators (trainers from Japan) dedicated to help managers think differently. Contrarily to every other large company, Toyota's training in its formative years was focused on developing people's reasoning abilities rather than pushing them to execute specialist-derived systems.
These sensei, or masters in lean thinking, would challenge line managers to look differently at their own jobs by focusing on:

The workplace: Going and seeing firsthand work conditions in practice, right now, and finding out the facts for oneself rather than relying on reports and boardroom meeting. The workplace is also where real people make real value, and going to see is a mark of respect and the opportunity to support employees to add value through their ideas and initiative more than merely make value through prescribed work. The management revolution brought by lean thinking can be summed up by describing jobs in terms of Job = Work + Kaizen
Value through built-in quality: Understanding that customer satisfaction is paramount and is built-in at every step of the enterprise's process, from building in satisfying features (such as peace of mind) to correctly building in quality at every production step. Built-in quality means to stop at every doubtful part and to train yourself and others not to pass on defective work, not to do defective work and not to accept defective work by stopping the process and reacting immediately whenever things go wrong.
Value streams through understanding "takt" time: By calculating the ratio of open production time to averaged customer demand one can have a clear idea of the capacity needed to offer a steady flow of products. This “takt” rhythm, be it a minute for cars, two months for software projects or two years for a new book leads to creating stable value streams where stable teams work on a stable set of products with stable equipment rather than optimize the use of specific machines or processes. Takt time thinking leads to completely different capacity reasoning than traditional costing and is the key to far more frugal processes.
Flow through reducing batch sizes: Every traditional business, whether in production or services, is addicted to batch. The idea is that once work is set up one way, we'd better get on and quickly make as many pieces of work as we can to keep the unit cost down. Lean thinking looks at this differently in trying to optimize the flow of work in order to satisfy real demand now, not imaginary demand next month. By working strenuously on reducing change-over time and difficulty, it is possible to approach the lean thinking ideal of single piece flow. In doing so, one reduces dramatically the general cost of the business by eliminating the need for warehouses, transports, systems, subcontractor use and so on.
Pull to visualize takt time through the flow: pulling work from upstream at takt time through visual devices such as kanban cards is the essential piece that enables lean thinkers to visualize the gaps between the ideal and the actual at the workplace at any time. Pull is what creates a creative tension in the workplace by both edging closer to single-piece-work and by highlighting problems one at a time as they occur so complex situations can be resolved piecemeal. Pull is the basic technique used to "lean" a company and, by and large, without pull there is no lean thinking.
Seeking perfection through kaizen: The old time sensei used to teach that the aim of lean thinking was not to apply lean tools to every process, but to develop the kaizen spirit in every employee. Perfection is not sought through better, more clever systems or go-it-alone heroes but through a commitment to improve things together step-by-small-step. Kaizen literally means change for the better and Kaizen spirit is about seeking a hundred 1% improvements from everyone every day everywhere rather than one 100% leap forward. The practice of kaizen is what anchors deep lean thinking in people's minds and which, ultimately, leads to complete transformation. Practising kaizen together builds self-confidence and the collective confidence that we can face our larger challenges and solve our problems together.

Evolution
The idea of lean thinking gained popularity in the business world and has evolved in three different directions:

Lean thinking converts who keep seeking to understand how to seek dynamic gains rather than static efficiencies. For this group of thinkers, lean thinking continuously evolves as they seek to better understand the possibilities of the way opened up by Toyota and have grasped the fact that the aim of continuous improvement is continuous improvement. Lean thinking as such is a movement of practitioners and writers who experiment and learn in different industries and conditions, to lean think any new activity.
Lean production adepts who have interpreted the term "lean" as a form of operational excellence and have turned to company programs aimed at taking costs out of processes. Lean activities are used to improve processes without ever challenging the underlying thinking, with powerful low-hanging fruit results but little hope of transforming the enterprise as a whole. This "corporate lean" approach is fundamentally opposed to the ideals of lean thinking, but has been taken up by a great number of large businesses seeking to cut their costs without challenging their fundamental management assumptions.
Lean services, as an extent area of application to all the learnings gathered from the industry and adapted to a whole new set of scenarios, including human resources, accounting, retail, health, education, product development, startup/ entrepreneurship and digitalization. Lean basic principles can be applied basically to all scopes of action, regardless of geography and culture.

Lean thinking practices
Experience shows that adopting lean thinking requires abandoning deeply engrained mainstream management thought routines, and this is never easy. The three main ways to adopt lean thinking are, unsurprisingly:

"Aha!" moments by seeing someone behave in a striking way, or hitting upon a new idea by reading a book, visiting a workplace, or being beaten over the head by an old time sensei. Aha! moments are powerful, but unfortunately rare, and need the right conditions to occur.
Everyday practice by the daily use of "lean" practices. These practices mainly originate from Toyota and are essentially "think with your hand" exercises. Their purpose is not to implement new processes (as they are too often interpreted) but practical activities to lead one to see the situation differently and have new ideas about it – to adopt a leaner way of thinking.
Joining lean self-study groups by practising kaizen with others and identifying which role models one would like to follow. The lean community is now a generation strong and has many great examples to offer to any lean learner, whether beginner or experienced. Workplace visits with experienced lean thinkers remain one of the most effective ways to grasp their meaning.In the lean thinking tradition, the teacher should not explain but demonstrate – learning is the full responsibility of the learner. However, to create the proper conditions for learning the lean tradition has adopted a number of practices from Toyota's own learning curve. The aim of these practices is not to improve processes per se but to create an environment for teachable and learnable moments.

Kaizen activities: Whether cross-functional workshops, team quality circles, individual suggestions, and many other exercises, kaizen activities are about scheduled moments to improve the work within the normal working day. The point of kaizen is that improvement is a normal part of the job, not something to be done "when there is time left after having done everything else". Kaizen is scheduled, planned, and controlled by a teacher who makes sure Deming's  plan–do–check–act is followed rigorously.
Kanban: Kanban is the foundational practice of lean thinking (the Toyota Production System used to be first known as the Kanban system). Any process will have different output. For instance, nowadays, a writer will produce books, keynote speeches, blog posts, tweets and answer e-mails. The question is, at the present time right now, how can the person using the process know whether they are doing what is needed for customers right now or whether they are working ahead on something not that important and lagging behind on something critical. In project management, this creates segments ahead and segments late, and end of project panic. In production, this creates entire warehouses of inventories to compensate for the inability to produce right now what is needed. Kanban is a simple technique using cards or post-it notes to visualize "leveled" (i.e. averaged to avoid peaks and troughs) activity at the process. The writer will start a new book when she's delivered one. She will worry about the new conference when it's time to. She will write a new blog post at a steady rhythm rather than publish five in a rush and then one and so on. In production, kanban cards make sure employees are working on what is needed right now and not overproducing parts which will then linger in inventory whilst others will be unavailable. Kanban is the main practice to reveal all misfits between today's activities and how the market behaves. Kanban teaches one lean thinking by constantly challenging assumptions about market behaviour and our own flexibility.
Autonomation: In any contemporary setting, everyone uses either machines or software to do any work. Yet, this automated work still requires specific human judgments to be done right. As a result, many machines can't be left alone to work because they're likely to go wrong if someone doesn't watch them all the time. Autonomation is the practice of progressively imparting human judgement to a system so that it self-monitors and stops and calls a human when it feels it went wrong, just as a desktop computer will flag a virus alert if it feels under attack. Autonomation is essential to separate people from machines and not have humans doing machine work and vice versa. Automation teaches lean thinking by revealing new ways of designing lighter, smarter machines with less capital expenditure.
Andon: Calling out when something feels out of kilt and to visualize that call on central board so that help can come quickly. Lean thinking is thinking together and no employee should be left alone with a problem. Andon is a critical system to be able to train employees in the details of their jobs within their own operations. Andon teaches lean thinking in highlighting the immediate barriers to the lean goal of zero defect at every step of the process at all time. Through andon it is possible to think better about training people and improving their work conditions to take all difficulties away.
SMED: Originally known as single-minute exchange of die (changing tools under 10 minutes), SMED is a key lean thinking practice to focus directly on flexibility. Flexibility is central to flow and always a problem, even for an engineer's mind – how flexible is the group to move from one topic to the next? Flexibility doesn't mean changing everything all the time, but the ability to switch quickly from one known activity to the next. SMED teaches lean thinking in always seeking to improve flexibility until one reaches true single-piece-flow in the right sequence to respond to instant customer demand.
Standardized work: Lean thinking is about seeking the smoothest flow in any work, in order to see problems one by one and resolve them one by one, thus improving both the flow of work and the autonomy of the person. Standardized work is the graphic description of this smooth flow of work at takt time with zero or one piece of work-in-process and clear location for everything and steps. Tricky quality points are also identified clearly, to make sure the person visualizes first, what is important for the customer, how to distinguish OK from not OK at every step and have to move confidently from one step to the next. Standardized work teaches lean thinking by visualizing every obstacle to smooth work each person encounters and highlighting topics for kaizen.
Visualization: Most lean thinking techniques are about visualization in some form or other so that people can see together, know together and thus learn together. Visual control is the essential trigger to creative problem solving as all can see the gap between what was planned and what actually happened and can seek both immediate countermeasures and root causes. Visualization teaches lean thinking by getting people to work together on their own problems and develop their responsibility to reaching objectives without overburden.

Controversies
There are two controversies surrounding the word “lean,” one concerning the image of lean with the general public and the other within the lean movement itself.
Lean has repeatedly been accused of being a form of turbo-charged Taylorism, the harbinger of productivity pressure, detrimental to employee's health and autonomy at work. Unfortunately, some company programs calling themselves “lean” have indeed had a severely negative effect on the business and work relations. This problem arises when senior leaders do not seek to adopt lean thinking but instead delegate to outside consultants or internal specialist team the job of “leaning” processes. Lean thinking very clearly states that it seeks cost reductions – finding the policy origins of unnecessary costs and eliminating at the cause – and not cost cutting – forcing people to work within reduced budgets and degraded conditions in order to achieve line by line cost advantage. There is no doubt about this, but to many managers, the latter option is far more expedient than the former and it's easy to call “lean” a cost-cutting program. Nonetheless, this is not that, and any approach that doesn't have the explicit aim to develop lean thinking in every employee should not be considered to be "lean".

Putting people first
These controversies largely emerge around the radical organizational innovation proposed by lean thinking: putting people first rather than systems. In this, lean thinking departs markedly from mainstream management:

Individual customers rather than market segments: Without denying the need to think in terms of segments, lean thinking is about taking seriously every single customer complaint and opinion of the product or service, as a fact. The ability to service every customer specifically is only limited by the flexibility of the company's process and lean thinking is about seeking a way to reach the ideal of serving each individual's preferences.
Teaching employees how to learn rather than telling them what to do: Lean thinking's aim is to develop each person's autonomy in problem solving by supporting them in their continuous improvement activities. This is a radical break from Taylorism where a group of specialists will devise the “one-best-way” and line management will be tasked to enforce it. By contrast, lean thinking is taught to managers so that they help their own direct reports to think lean and reduce overburden, unneeded variation and activity waste by working more closely with their teams and across functional boundaries.Lean thinking at senior level creates leaner enterprises because sales increase through customer satisfaction with higher quality products or services, because cash improve as flexibility reduces the need for inventories or backlogs, because costs reduce through identifying costly policies that create waste at value-adding level, and because capital expenditure is less needed as people themselves invent smarter, leaner processes to flow work continuously at takt time without waste.

Lean and green
Lean thinking goes beyond improving business profitability. In their book Natural Capitalism, authors Paul Hawken, Amory Lovins and L. Hunter Lovins reference lean thinking as a way to sustain growth with less collateral damage to the environment. Lean thinking's approach, seeking to eliminate waste in the form of muri (overburden), mura (unlevelness) and muda (unnecessary resource use), is a proven practical way to attack complex problems piece by piece through concrete action. Toyota industrial sites are well known for their sustainability efforts and well ahead of the "zero landfill" goal – all waste recycled within the site. Practising lean thinking offers a radically new way to look at traditional goods and service production to learn how to sustain the same benefits at a lower cost, financially and environmentally.

See also
Lean enterprise
Lean manufacturing
Push–pull strategy

References
External links
Lean Enterprise Research Centre at Cardiff University, which works to "research, apply and communicate lean thinking".

Machine industry

The machine industry or machinery industry is a subsector of the industry, that produces and maintains machines for consumers, the industry, and most other companies in the economy.
This machine industry traditionally belongs to the heavy industry. Nowadays, many smaller companies in this branch are considered part of the light industry. Most manufacturers in the machinery industry are called machine factories.

Overview
The machine industry is a subsector of the industry that produces a range of products from power tools, different types of machines, and domestic technology to factory equipment etc. On the one hand the machine industry provides:

The means of production for businesses in the agriculture, mining, industry and construction.
The means of production for public utility, such as equipment for the production and distribution of gas, electricity and water.
A range of supporting equipment for all sectors of the economy, such as equipment for heating, ventilation, and air conditioning of buildings.These means of production are called capital goods, because a certain amount of capital is invested. Much of those production machines require regular maintenance, which becomes supplied specialized companies in the machine industry.
On the other end the machinery industry supplies consumer goods, including kitchen appliances, refrigerators, washers, dryers and a like. Production of radio and television, however, is generally considered belonging to the electrical equipment industry. The machinery industry itself is a major customer of the steel industry.
The production of the machinery industry varies widely from single-unit production and series production to mass production. Single-unit production is about constructing unique products, which are specified in specific customer requirements. Due to modular design such devices and machines can often be manufactured in small series, which significantly reduces the costs. From a certain stage in the production, the specific customer requirements are built-in, and the unique product is created.

History
The machinery industry came into existence during the Industrial Revolution. Companies in this emerging field grew out of iron foundries, shipyards, forges and repair shops. Often companies were a combination of machine factory and shipyard. Early in the 20th century several motorcycle and automobile manufacturers began their own machine factories.
Prior to the industrial revolution a variety of machines existed such as clocks, weapons and running gear for mills (watermill, windmill, horse mill etc.) Production of these machines were on much smaller scale in artisan workshops mostly for the local or regional market. With the advent of the industrial revolution manufacturing began of composite tools with more complex construction, such as steam engines and steam generators for the evolving industry and transport. In addition, the emerging machine factories started making machines for production machines as textile machinery, compressors, agricultural machinery, and engines for ships.

18th century
During the first decades of the industrial revolution in England, from 1750, there was a concentration of labor usually in not yet mechanized factories. Many new machines were invented, which were initially made by the inventors themselves. Early in the 18th century, the first steam engines, the Newcomen engine, came into use throughout Britain and Europe, principally to pump water out of mines.
In the 1770s James Watt significantly improved this design. He introduced a steam engine easy employable to supply a large amounts of energy, which set the mechanization of factories underway. In England certain cities concentrated on making specific products, such as specific types of textiles or pottery. Around these cities specialized machinery industry arose in order to enable the mechanization of the plants. Hereby late in the 18th century arose the first machinery industry in the UK and also in Germany and Belgium.

19th century
The Industrial Revolution received a further boost with the upcoming railways. These arose at the beginning of the 19th century in England as innovation in the mining industry. The work in coal mines was hard and dangerous, and so there was a great need for tools to ease this work. In 1804, Richard Trevithick placed the first steam engine on rails, and was in 1825 the Stockton and Darlington Railway was opened, intended to transport coals from the mine to the port. In 1835 the first train drove in continental Europe between Mechelen and Brussels, and in the Netherlands in 1839 the first train drove between Amsterdam and Haarlem. For the machinery industry this brought all sorts of new work with new machinery for metallurgy, machine tool for metalworking, production of steam engines for trains with all its necessities etc.
In time the market for the machine industry became wider, specialized products were manufactured for a greater national and often international market. For example, it was not uncommon in the second half of the 19th century that American steelmakers ordered their production in England, where new steelmaking techniques were more advanced. In the far east Japan would import these product until the early 1930s, the creation of an own machinery industry got underway.

20th century to now
The term "machinery industry" came into existence later in the 19th century. One of the first times this branch of industry was recognized as such, and was investigated, was in a production statistics of 1907 created by the British Ministry of Trade and Industry. In this statistic the output of the engineering industry, was divided into forty different categories, including for example, agricultural machinery, machinery for the textile industry and equipment, and parts for train and tram.The inventions of new propulsion techniques based on electric motors, internal combustion engines and gas turbines brought a new generation of machines in the 20th century from cars to household appliances. Not only the product range of the machinery industry increased considerably, but especially smaller machines could also deliver products in much greater numbers fabricated in mass production.  With the rise of mass production in other parts of the industry, there was also a high demand for manufacturing and production systems, to increase the entire production.
Shortage of labor in agriculture and industry at the beginning of the second half of the 20th century, raised the need for further mechanization of production, which required for more specific machines. The rise of the computer made further automation of production possible, which in turn set new demands on the machinery industry.

Classification
The machinery industry produces different kind of products, for example, engines, pumps, logistics equipment; for different kind of markets from the agriculture industry, food & beverage industry, manufacturing industry, health industry, and amusement industry till different branches of the consumer market. As such companies in the machine industry can be classified by product of market.
In the world of today, all kinds of Industry classifications exists. Some classifications recognize the machine industry as a specific class, and offer a subdivision for this field. For example, the Dutch Standard Industrial Classification of 1993, developed by the  Statistics Netherlands, give the following breakdown of the machinery industry:

This composition of the machinery industry has been significantly altered with the latest revision of the Dutch Standard Industrial Classification of 1993. The Standard Industrial Classification of 1974 broke down the machinery industry into nine sectors:

Agricultural machine industry
Metalworking Machine - Industry and machine tool factories
Manufacturers of machinery and equipment for the food, chemical and allied industries
Manufacturers of machinery and equipment for the rubber and plastics
Manufacturers of gears, gearing and driving elements
Manufacturers of machinery and equipment and wood furniture etc.
Manufacturers of steam boiler, and power tools industry
Office machinery industry
Other machinery and equipment industryIt may be clear that classification is by markets, and the more recent classification is by product.

Products of the machine industry
The machine industry makes a very diverse range of products. A selection:

Machine industry in different countries
Germany
In Germany, in 2011 about 900,000 people were employed  in the machine industry and an estimated of 300,000 abroad. The combined turnover of the sector was €238 billion, of which 60% came from export. There were about 6,600 active companies, and 95% of those companies employed less than 500 people. Each employee generated an average of 148,000 Euro. Some of the largest companies in Germany are DMG Mori Seiki AG, GEA Group, Siemens AG, and ThyssenKrupp.

France
In the French machinery industry in 2009 about 650,000 people were employed, and the sector generated a turnover of 44 billion euros. Because of the crisis, the turnover of the sector had fallen by 15 percent. Due to stronger consumer spending and continuing demand from the energy sector and transport sector, the damage of the crisis was still limited.
Alternatively, some companies decided to focus their request on used industrial equipment. This guarantee attractive prices and better time delivery.

The Netherlands
In the Netherlands in 1996, a total of some 93,000 workers were employed in the machinery industry, with approximately 2,500 companies present. In 1000 of these companies there were working 20 or more employees. In the Netherlands, according to the Chamber of Commerce in this subsector of the industry in 2011 some 15,000 companies were active. Some of the largest companies in the Netherlands are Lely, Philips and Stork B.V.

United States
U.S. machinery industries had total domestic and foreign sales of $413.7 billion in 2011.  The United States is the world’s largest market for machinery, as well as the third-largest supplier. American manufacturers held a 58.5 percent share of the U.S. domestic market.

See also
Outline of industrial machinery
Automotive industry
Machine factory
Machine shop
Machine tool
Machine tool builder

References
Further reading
Brewster, John M. "The machine process in agriculture and industry." Journal of Farm Economics 32.1 (1950): 69-81.
Florian Geiger. Mergers & Acquisitions in the Machinery Industry. 2010.
Rees, John, Ronald Briggs, and Raymond Oakey. "The adoption of new technology in the American machinery industry." Regional Studies 18.6 (1984): 489-504.
Rosenberg, Nathan. "Technological change in the machine tool industry, 1840–1910." The Journal of Economic History 23.04 (1963): 414-443.

External links
 Media related to Machinery industry at Wikimedia Commons

Machine shop in Abu Dhabi

Management accounting

In management accounting or managerial accounting, managers use accounting information in decision-making and to assist in the management and performance of their control functions.

Definition
One simple definition of management accounting is the provision of financial and non-financial decision-making information to managers. In other words, management accounting helps the directors inside an organization to make decisions. This can also be known as Cost Accounting. This is the way toward distinguishing, examining, deciphering and imparting data to supervisors to help accomplish business goals. The information gathered includes all fields of accounting that educates the administration regarding business tasks identifying with the financial expenses and decisions made by the organization. Accountants use plans to measure the overall strategy of operations within the organization.According to the Institute of Management Accountants (IMA), "Management accounting is a profession that involves partnering in management decision making, devising planning and performance management systems, and providing expertise in financial reporting and control to assist management in the formulation and implementation of an organization's strategy".Management accountants (also called managerial accountants) look at the events that happen in and around a business while considering the needs of the business. From this, data and estimates emerge. Cost accounting is the process of translating these estimates and data into knowledge that will ultimately be used to guide decision-making.The Chartered Institute of Management Accountants (CIMA) being the largest management accounting institute with over 100,000 members describes Management accounting as analysing information to advise business strategy and drive sustainable business success.The Institute of Certified Management Accountants (ICMA) has over 15,000 qualified professionals worldwide, with members in 50-countries. Its CMA postgraduate education program now is firmly established in 19 overseas markets, namely Bangladesh, Cambodia, China, Cyprus, Dubai, Hong Kong, India, Indonesia, Iran, Japan, Lebanon, Malaysia, Nepal, New Zealand, Papua New Guinea, Philippines; Singapore, Sri Lanka, Thailand and Vietnam. 
To facilitate its educational objectives, the Institute has accredited a number of universities which have master's degree subjects that are equivalent to the CMA program. Some of these universities also provide in-house training and examinations of the CMA program. Accounting graduates can do CMA accredited units at these universities to qualify for CMA status. The ICMA also has a number of Recognised Provider Institutions (RPIs) that run the CMA program in Australia and overseas. The CMA program is also available online in regions where the face-to-face delivery of the program is not possible.

Scope, practice, and application
The Association of International Certified Professional Accountants (AICPA) states management accounting as a practice that extends to the following three areas:

Strategic management — advancing the role of the management accountant as a strategic partner in the organization
Performance management — developing the practice of business decision-making and managing the performance of the organization
Risk management — contributing to frameworks and practices for identifying, measuring, managing and reporting risks to the achievement of the objectives of the organizationThe Institute of Certified Management Accountants (CMA) states, "A management accountant applies his or her professional knowledge and skill in the preparation and presentation of financial and other decision oriented information in such a way as to assist management in the formulation of policies and in the planning and control of the operation undertaking".
Management accountants are seen as the "value-creators" amongst the accountants. They are more concerned with forward-looking and taking decisions that will affect the future of the organization; than in the historical recording and compliance (score keeping) aspects of the profession. Management accounting knowledge and experience can be obtained from varied fields and functions within an organization, such as information management, treasury, efficiency auditing, marketing, valuation, pricing, and logistics. In 2014 CIMA created the Global Management Accounting Principles (GMAPs). The result of research from across 20 countries in five continents, the principles aim to guide best practice in the discipline.

Financial versus Management accounting
Management accounting information differs from financial accountancy information in several ways:

while shareholders, creditors, and public regulators use publicly reported financial accountancy, information, only managers within the organization use the normally confidential management accounting information
while financial accountancy information is historical, management accounting information is primarily forward-looking;
while financial accountancy information is case-based, management accounting information is model-based with a degree of abstraction in order to support generic decision making;
while financial accountancy information is computed by reference to general financial accounting standards, management accounting information is computed by reference to the needs of managers, often using management information systems.Focus:  

Financial accounting focuses on the company as a whole.
Management accounting provides detailed and disaggregated information about products, individual activities, divisions, plants, operations and tasks.

Traditional versus innovative practices
The distinction between traditional and innovative accounting practices is illustrated with the visual timeline (see sidebar) of managerial costing approaches presented at the Institute of Management Accountants 2011 Annual Conference.
Traditional standard costing (TSC), used in cost accounting, dates back to the 1920s and is a central method in management accounting practiced today because it is used for financial statement reporting for the valuation of income statement and balance sheet line items such as cost of goods sold (COGS) and inventory valuation.  Traditional standard costing must comply with generally accepted accounting principles (GAAP US) and actually aligns itself more with answering financial accounting requirements rather than providing solutions for management accountants.  Traditional approaches limit themselves by defining cost behavior only in terms of production or sales volume.
In the late 1980s, accounting practitioners and educators were heavily criticized on the grounds that management accounting practices (and, even more so, the curriculum taught to accounting students) had changed little over the preceding 60 years, despite radical changes in the business environment. In 1993, the Accounting Education Change Commission Statement Number 4 calls for faculty members to expand their knowledge about the actual practice of accounting in the workplace. Professional accounting institutes, perhaps fearing that management accountants would increasingly be seen as superfluous in business organizations, subsequently devoted considerable resources to the development of a more innovative skills set for management accountants.
Variance analysis is a systematic approach to the comparison of the actual and budgeted costs of the raw materials and labour used during a production period.  While some form of variance analysis is still used by most manufacturing firms, it nowadays tends to be used in conjunction with innovative techniques such as life cycle cost analysis and activity-based costing, which are designed with specific aspects of the modern business environment in mind. Life-cycle costing recognizes that managers' ability to influence the cost of manufacturing a product is at its greatest when the product is still at the design stage of its product life-cycle (i.e., before the design has been finalized and production commenced), since small changes to the product design may lead to significant savings in the cost of manufacturing the products.
Activity-based costing (ABC) recognizes that, in modern factories, most manufacturing costs are determined by the amount of 'activities' (e.g., the number of production runs per month, and the amount of production equipment idle time) and that the key to effective cost control is therefore optimizing the efficiency of these activities.  Both lifecycle costing and activity-based costing recognize that, in the typical modern factory, the avoidance of disruptive events (such as machine breakdowns and quality control failures) is of far greater importance than (for example) reducing the costs of raw materials. Activity-based costing also de-emphasizes direct labor as a cost driver and concentrates instead on activities that drive costs, as the provision of a service or the production of a product component.
Other approach is the German Grenzplankostenrechnung (GPK) costing methodology. Although it has been in practiced in Europe for more than 50 years, neither GPK nor the proper treatment of 'unused capacity' is widely practiced in the U.S.Another accounting practice available today is resource consumption accounting (RCA).  RCA has been recognized by the International Federation of Accountants (IFAC) as a "sophisticated approach at the upper levels of the continuum of costing techniques" The approach provides the ability to derive costs directly from operational resource data or to isolate and measure unused capacity costs.  RCA was derived by taking costing characteristics of GPK, and combining the use of activity-based drivers when needed, such as those used in activity-based costing.A modern approach to close accounting is continuous accounting, which focuses on achieving a point-in-time close, where accounting processes typically performed at period-end are distributed evenly throughout the period.

Role within a corporation
Consistent with other roles in modern corporations, management accountants have a dual reporting relationship. As a strategic partner and provider of decision based financial and operational information, management accountants are responsible for managing the business team and at the same time having to report relationships and responsibilities to the corporation's finance organization and finance of an organization.
The activities management accountants provide inclusive of forecasting and planning, performing variance analysis, reviewing and monitoring costs inherent in the business are ones that have dual accountability to both finance and the business team. Examples of tasks where accountability may be more meaningful to the business management team vs. the corporate finance department are the development of new product costing, operations research, business driver metrics, sales management scorecarding, and client profitability analysis. (See financial planning.) Conversely, the preparation of certain financial reports, reconciliations of the financial data to source systems, risk and regulatory reporting will be more useful to the corporate finance team as they are charged with aggregating certain financial information from all segments of the corporation.
In corporations that derive much of their profits from the information economy, such as banks, publishing houses, telecommunications companies and defence contractors, IT costs are a significant source of uncontrollable spending, which in size is often the greatest corporate cost after total compensation costs and property related costs. A function of management accounting in such organizations is to work closely with the IT department to provide IT cost transparency.Given the above, one view of the progression of the accounting and finance career path is that financial accounting is a stepping stone to management accounting. Consistent with the notion of value creation, management accountants help drive the success of the business while strict financial accounting is more of a compliance and historical endeavor.

Specific methodologies
Activity-based costing (ABC)
Activity-based costing was first clearly defined in 1987 by Robert S. Kaplan and W. Bruns as a chapter in their book Accounting and Management: A Field Study Perspective. They initially focused on the manufacturing industry, where increasing technology and productivity improvements have reduced the relative proportion of the direct costs of labor and materials, but have increased relative proportion of indirect costs. For example, increased automation has reduced labor, which is a direct cost, but has increased depreciation, which is an indirect cost.

Grenzplankostenrechnung
Grenzplankostenrechnung (GPK) is a German costing methodology, developed in the late 1940s and 1960s, designed to provide a consistent and accurate application of how managerial costs are calculated and assigned to a product or service. The term Grenzplankostenrechnung, often referred to as GPK, has best been translated as either marginal planned cost accounting or flexible analytic cost planning and accounting.The origins of GPK are credited to Hans Georg Plaut, an automotive engineer, and Wolfgang Kilger, an academic, working towards the mutual goal of identifying and delivering a sustained methodology designed to correct and enhance cost accounting information. GPK is published in cost accounting textbooks, notably Flexible Plankostenrechnung und Deckungsbeitragsrechnung and taught at German-speaking universities.

Lean accounting (accounting for lean enterprise)
In the mid- to late-1990s several books were written about accounting in the lean enterprise (companies implementing elements of the Toyota Production System). The term lean accounting was coined during that period. These books contest that traditional accounting methods are better suited for mass production and do not support or measure good business practices in just-in-time manufacturing and services. The movement reached a tipping point during the 2005 Lean Accounting Summit in Dearborn, Michigan, United States. 320 individuals attended and discussed the advantages of a new approach to accounting in the lean enterprise. 520 individuals attended the 2nd annual conference in 2006 and it has varied between 250 and 600 attendees since that time.

Resource consumption accounting (RCA)
Resource consumption accounting (RCA) is formally defined as a dynamic, fully integrated, principle-based, and comprehensive management accounting approach that provides managers with decision support information for enterprise optimization. RCA emerged as a management accounting approach around 2000 and was subsequently developed at CAM-I, the Consortium for Advanced Manufacturing–International, in a Cost Management Section RCA interest group in December 2001.

Throughput accounting
The most significant recent direction in managerial accounting is throughput accounting; which recognizes the interdependencies of modern production processes. For any given product, customer or supplier, it is a tool to measure the contribution per unit of constrained resource.

Transfer pricing
Management accounting is an applied discipline used in various industries. The specific functions and principles followed can vary based on the industry. Management accounting principles in banking are specialized but do have some common fundamental concepts used whether the industry is manufacturing-based or service-oriented. For example, transfer pricing is a concept used in manufacturing but is also applied in banking. It is a fundamental principle used in assigning value and revenue attribution to the various business units. Essentially, transfer pricing in banking is the method of assigning the interest rate risk of the bank to the various funding sources and uses of the enterprise. Thus, the bank's corporate treasury department will assign funding charges to the business units for their use of the bank's resources when they make loans to clients. The treasury department will also assign funding credit to business units who bring in deposits (resources) to the bank. Although the funds transfer pricing process is primarily applicable to the loans and deposits of the various banking units, this proactive is applied to all assets and liabilities of the business segment. Once transfer pricing is applied and any other management accounting entries or adjustments are posted to the ledger (which are usually memo accounts and are not included in the legal entity results), the business units are able to produce segment financial results which are used by both internal and external users to evaluate performance.

Resources and continuous learning
There are a variety of ways to keep current and continue to build one's knowledge base in the field of management accounting. Certified Management Accountants (CMAs) are required to achieve continuing education hours every year, similar to a Certified Public Accountant. A company may also have research and training materials available for use in a corporate owned library. This is more common in Fortune 500 companies who have the resources to fund this type of training medium.
There are also journals, online articles and blogs available.  The journal Cost Management (ISSN 1092-8057) and the Institute of Management Accounting (IMA) site are sources which include Management Accounting Quarterly and Strategic Finance publications.

Tasks and services provided
Listed below are the primary tasks/services performed by management accountants. The degree of complexity relative to these activities are dependent on the experience level and abilities of any one individual.

Rate and volume analysis
Business metrics development
Price modeling
Product profitability
Geographic vs. industry or client segment reporting
Sales management scorecards
Cost analysis
Cost–benefit analysis
Cost-volume-profit analysis
Life cycle cost analysis
Client profitability analysis
IT cost transparency
Capital budgeting
Buy vs. lease analysis
Strategic planning
Strategic management advice
Internal financial presentation and communication
Sales forecasting
Financial forecasting
Annual budgeting
Cost allocation

Related qualifications
There are several related professional qualifications and certifications in the field of accountancy including:

Management Accountancy Qualifications
CIMA
ICMA
ICAI-CMA
ICMAP
CMA
Other Professional Accountancy Qualifications
Chartered Institute of Public Finance and Accountancy, CIPFA
Chartered Certified Accountant (ACCA)
Cost & Management Accountant (CMA)
Chartered Accountant (CA)
Chartered Professional Accountant (CPA - Canada)
Certified Public Accountant (CPA - US)
American Institute of Certified Public Accountants
Certified Practicing Accountant (CPA Australia)
Chartered Global Management Accountant

Methods
Activity-based costing
Grenzplankostenrechnung (GPK)
Lean accounting
Resource consumption accounting
Standard cost accounting
Throughput accounting
Transfer pricing

See also
Managerial risk accounting
Profit model

References
["https://aimsoftech.com/">best accounting software]

Further reading
Kurt Heisinger and Joe Hoyle, Managerial Accounting, ISBN 978-1-4533452-9-0.
James R. Martin, Ph.D., CMA, Management And Accounting Web.

External links
CAM-I Consortium for Advanced Manufacturing–International
AICPA Financial Management Center – resource for CPAs working in business, industry and government
Institute of Management Accountants – resource for management accountants (CMAs) working in industry
Chartered Institute of Management Accountants
International Federation of Accountants
The Accounting Adventurista Management Accounting
The Institute of Cost Accountants of India

Managerial finance

Managerial finance is the branch of finance that concerns itself with the financial aspects of managerial decisions. 
Finance addresses the ways in which organizations (and individuals) raise and allocate monetary resources over time, taking into account the risks entailed in their projects;
Managerial finance, then, emphasizes the managerial application of these finance techniques and theories. 
The techniques assessed (and developed) are drawn in the main from managerial accounting and corporate finance; 
the former allow management to better understand, and hence act on, financial information relating to profitability and performance; 
the latter are about optimizing the overall financial-structure;
see Financial management § Role.
In both cases, the discipline addresses these from the Managerial perspectives of Planning, Directing, and Controlling;
here in the more specific context of strategic planning, organizing, directing, and controlling of the organization's financial undertakings.
Academics working in this area are typically based in business school finance departments, in accounting, or in management science.

Managerial accounting techniques
Management accounting techniques are applied in the preparation and presentation of financial and other decision oriented information "in such a way as to assist management in the formulation of policies and in the planning and control of the operation undertaking". 
The analytics here are thus concerned with forward-looking decisions, as opposed to the historical and compliance perspective of financial accounting.
Undertaking these tasks, financial managers use various management accounting and financial analysis techniques  to accurately assess the results and performance of the business lines and units, and to monitor resource allocation within the organization; 
this includes
profitability analysis and cost analytics  
– employing techniques such as activity based costing, whole-life cost analysis, cost–volume–profit analysis, and variance analysis – 
as well budget analytics more generally.
(See also cash flow forecast and financial forecast.)

Corporate finance techniques
Managerial finance is, as above, also focused on the overall financial-structure of the business, including its realized impact on cash flow and profitability. It is thus interested in long-term revenue / business optimization, while also minimizing the potential impact of any financial shocks on short term performance.  To accomplish these goals, managerial finance addresses techniques utilized in Corporate finance, usually organized re the following:

Working capital management - addressing short term current assets and current liabilities and optimizing cash flow
Capital budgeting, i.e. selection / valuation and funding of "projects" – addressing long term investments
Capital structure and dividend policy – addressing long-term financial capital and attempting to optimize the balance sheet.The discipline also considers the various applications of risk management here.

See also
Capital management
FP&A
List of accounting topics
List of management topics
Managerial economics

References
Further reading
Jonathan Lewellen (2003). Financial Management, MIT OpenCourseWare 15.414
Weston, Fred and Brigham, Eugene (1972), Managerial Finance, Dryden Press, Hinsdale Illinois, 1972
Chen, Henry editor, (1967), Frontiers of Managerial Finance, Gulf Publishing, Houston Texas, 1967
Brigham, Eugene and Johnson, Ramon (1980), Issues in Managerial Finance, Holt Rinehart and Winston Publishers, Hindale Illinois,
Lawrence Gitman and Chad J. Zutter  (2019). Principles of Managerial Finance, 14th edition, Addison-Wesley Publishing, ISBN 978-0133507690.
Clive Marsh (2009). Mastering Financial Management, Financial Times Prentice Hall ISBN 978-0-273-72454-4
James Van Horne and John Wachowicz (2009). Fundamentals of Financial Management, 13th ed., Pearson Education Limited. ISBN 9780273713630

Manufacturing

Manufacturing is the creation or production of goods with the help of equipment, labor, machines, tools, and chemical or biological processing or formulation. It is the essence of the secondary sector of the economy. The term may refer to a range of human activity, from handicraft to high-tech, but it is most commonly applied to industrial design, in which raw materials from the primary sector are transformed into finished goods on a large scale. Such goods may be sold to other manufacturers for the production of other more complex products (such as aircraft, household appliances, furniture, sports equipment or automobiles), or distributed via the tertiary industry to end users and consumers (usually through wholesalers, who in turn sell to retailers, who then sell them to individual customers).
Manufacturing engineering is the field of engineering that designs and optimizes the manufacturing process, or the steps through which raw materials are transformed into a final product. The manufacturing process begins with the product design, and materials specification. These materials are then modified through manufacturing to become the desired product.
Modern manufacturing includes all intermediate processes involved in the production and integration of a product's components. Some industries, such as semiconductor and steel manufacturers, use the term fabrication instead.
The manufacturing sector is closely connected with the engineering and industrial design industries.

Etymology
The Modern English word manufacture is likely derived from the Middle French manufacture ("process of making") which itself originates from the Classical Latin manū ("hand") and Middle French facture ("making"). Alternatively, the English word may have been independently formed from the earlier English manufacture ("made by human hands") and fracture. Its earliest usage in the English language was recorded in the mid-16th century to refer to the making of products by hand.

History and development
Prehistory and ancient history
Human ancestors manufactured objects using stone and other tools long before the emergence of Homo sapiens about 200,000 years ago. The earliest methods of stone tool making, known as the Oldowan "industry", date back to at least 2.3 million years ago, with the earliest direct evidence of tool usage found in Ethiopia within the Great Rift Valley, dating back to 2.5 million years ago. To manufacture a stone tool, a "core" of hard stone with specific flaking properties (such as flint) was struck with a hammerstone. This flaking produced sharp edges that could be used as tools, primarily in the form of choppers or scrapers. These tools greatly aided the early humans in their hunter-gatherer lifestyle to form other tools out of softer materials such as bone and wood. The Middle Paleolithic, approximately 300,000 years ago, saw the introduction of the prepared-core technique, where multiple blades could be rapidly formed from a single core stone. Pressure flaking, in which a wood, bone, or antler punch could be used to shape a stone very finely was developed during the Upper Paleolithic, beginning approximately 40,000 years ago. During the Neolithic period, polished stone tools were manufactured from a variety of hard rocks such as flint, jade, jadeite, and greenstone. The polished axes were used alongside other stone tools including projectiles, knives, and scrapers, as well as tools manufactured from organic materials such as wood, bone, and antler.Copper smelting is believed to have originated when the technology of pottery kiln allowed sufficiently high temperatures. The concentration of various elements such as arsenic increase with depth in copper ore deposits and smelting of these ores yields arsenical bronze, which can be sufficiently work-hardened to be suitable for manufacturing tools. Bronze is an alloy of copper with tin; the latter being found in relatively few deposits globally caused a long time to elapse before true tin bronze became widespread. During the Bronze Age, bronze was a major improvement over stone as a material for making tools, both because of its mechanical properties like strength and ductility and because it could be cast in molds to make intricately shaped objects. Bronze significantly advanced shipbuilding technology with better tools and bronze nails, which replaced the old method of attaching boards of the hull with cord woven through drilled holes. The Iron Age is conventionally defined by the widespread manufacturing of weapons and tools using iron and steel rather than bronze. Iron smelting is more difficult than tin and copper smelting because smelted iron requires hot-working and can be melted only in specially designed furnaces. The place and time for the discovery of iron smelting is not known, partly because of the difficulty of distinguishing metal extracted from nickel-containing ores from hot-worked meteoritic iron.During the growth of the ancient civilizations, many ancient technologies resulted from advances in manufacturing. Several of the six classic simple machines were invented in Mesopotamia. Mesopotamians have been credited with the invention of the wheel. The wheel and axle mechanism first appeared with the potter's wheel, invented in Mesopotamia (modern Iraq) during the 5th millennium BC. Egyptian paper made from papyrus, as well as pottery, were mass-produced and exported throughout the Mediterranean basin. Early construction techniques used by the Ancient Egyptians made use of bricks composed mainly of clay, sand, silt, and other minerals.

Medieval and early modern
The Middle Ages witnessed new inventions, innovations in the ways of managing traditional means of production, and economic growth. Papermaking, a 2nd-century Chinese technology, was carried to the Middle East when a group of Chinese papermakers were captured in the 8th century. Papermaking technology was spread to Europe by the Umayyad conquest of Hispania. A paper mill was established in Sicily in the 12th century. In Europe the fiber to make pulp for making paper was obtained from linen and cotton rags. Lynn Townsend White Jr. credited the spinning wheel with increasing the supply of rags, which led to cheap paper, which was a factor in the development of printing. Due to the casting of cannon, the blast furnace came into widespread use in France in the mid 15th century. The blast furnace had been used in China since the 4th century BC. The stocking frame, which was invented in 1598, increased a knitter's number of knots per minute from 100 to 1000.

First and Second Industrial Revolutions
The Industrial Revolution was the transition to new manufacturing processes in Europe and the United States from 1760 to the 1830s. This transition included going from hand production methods to machines, new chemical manufacturing and iron production processes, the increasing use of steam power and water power, the development of machine tools and the rise of the mechanized factory system. The Industrial Revolution also led to an unprecedented rise in the rate of population growth. Textiles were the dominant industry of the Industrial Revolution in terms of employment, value of output and capital invested. The textile industry was also the first to use modern production methods.: 40  Rapid industrialization first began in Britain, starting with mechanized spinning in the 1780s, with high rates of growth in steam power and iron production occurring after 1800. Mechanized textile production spread from Great Britain to continental Europe and the United States in the early 19th century, with important centres of textiles, iron and coal emerging in Belgium and the United States and later textiles in France.An economic recession occurred from the late 1830s to the early 1840s when the adoption of the Industrial Revolution's early innovations, such as mechanized spinning and weaving, slowed down and their markets matured. Innovations developed late in the period, such as the increasing adoption of locomotives, steamboats and steamships, hot blast iron smelting and new technologies, such as the electrical telegraph, were widely introduced in the 1840s and 1850s, were not powerful enough to drive high rates of growth. Rapid economic growth began to occur after 1870, springing from a new group of innovations in what has been called the Second Industrial Revolution. These innovations included new steel making processes, mass-production, assembly lines, electrical grid systems, the large-scale manufacture of machine tools and the use of increasingly advanced machinery in steam-powered factories.Building on improvements in vacuum pumps and materials research, incandescent light bulbs became practical for general use in the late 1870s. This invention had a profound effect on the workplace because factories could now have second and third shift workers. Shoe production was mechanized during the mid 19th century. Mass production of sewing machines and agricultural machinery such as reapers occurred in the mid to late 19th century. The mass production of bicycles started in the 1880s. Steam-powered factories became widespread, although the conversion from water power to steam occurred in England earlier than in the U.S.

Modern manufacturing
Electrification of factories, which had begun gradually in the 1890s after the introduction of the practical DC motor and the AC motor, was fastest between 1900 and 1930. This was aided by the establishment of electric utilities with central stations and the lowering of electricity prices from 1914 to 1917. Electric motors allowed more flexibility in manufacturing and required less maintenance than line shafts and belts. Many factories witnessed a 30% increase in output owing to the increasing shift to electric motors. Electrification enabled modern mass production, and the biggest impact of early mass production was in the manufacturing of everyday items, such as at the Ball Brothers Glass Manufacturing Company, which electrified its mason jar plant in Muncie, Indiana, U.S. around 1900. The new automated process used glass blowing machines to replace 210 craftsman glass blowers and helpers. A small electric truck was now used to handle 150 dozen bottles at a time whereas previously used hand trucks could only carry 6 dozen bottles at a time. Electric mixers replaced men with shovels handling sand and other ingredients that were fed into the glass furnace. An electric overhead crane replaced 36 day laborers for moving heavy loads across the factory.Mass production was popularized in the late 1910s and 1920s by Henry Ford's Ford Motor Company, which introduced electric motors to the then-well-known technique of chain or sequential production. Ford also bought or designed and built special purpose machine tools and fixtures such as multiple spindle drill presses that could drill every hole on one side of an engine block in one operation and a multiple head milling machine that could simultaneously machine 15 engine blocks held on a single fixture. All of these machine tools were arranged systematically in the production flow and some had special carriages for rolling heavy items into machining positions. Production of the Ford Model T used 32,000 machine tools.Lean manufacturing, also known as just-in-time manufacturing, was developed in Japan in the 1930s. It is a production method aimed primarily at reducing times within the production system as well as response times from suppliers and to customers. It was introduced in Australia in the 1950s by the British Motor Corporation (Australia) at its Victoria Park plant in Sydney, from where the idea later migrated to Toyota. News spread to western countries from Japan in 1977 in two English-language articles: one referred to the methodology as the "Ohno system", after Taiichi Ohno, who was instrumental in its development within Toyota. The other article, by Toyota authors in an international journal, provided additional details. Finally, those and other publicity were translated into implementations, beginning in 1980 and then quickly multiplying throughout the industry in the United States and other countries.

Manufacturing strategy
According to a "traditional" view of manufacturing strategy, there are five key dimensions along which the performance of manufacturing can be assessed: cost, quality, dependability, flexibility and innovation. In regard to manufacturing performance, Wickham Skinner, who has been called "the father of manufacturing strategy", adopted the concept of "focus", with an implication that a business cannot perform at the highest level along all five dimensions and must therefore select one or two competitive priorities. This view led to the theory of "trade offs" in manufacturing strategy. Similarly, Elizabeth Haas wrote in 1987 about the delivery of value in manufacturing for customers in terms of "lower prices, greater service responsiveness or higher quality". The theory of "trade offs" has subsequently being debated and questioned, but Skinner wrote in 1992 that at that time "enthusiasm for the concepts of 'manufacturing strategy' [had] been higher", noting that in academic papers, executive courses and case studies, levels of interest were "bursting out all over".Manufacturing writer Terry Hill has commented that manufacturing is often seen as a less "strategic" business activity than functions such as marketing and finance, and that manufacturing managers have "come late" to business strategy-making discussions, where as a result they make only a reactive contribution.

Industrial policy
Economics of manufacturing
Emerging technologies have offered new growth methods in advanced manufacturing employment opportunities, for example in the Manufacturing Belt in the United States. Manufacturing provides important material support for national infrastructure and also for national defense.
On the other hand, most manufacturing processes may involve significant social and environmental costs. The clean-up costs of hazardous waste, for example, may outweigh the benefits of a product that creates it. Hazardous materials may expose workers to health risks. These costs are now well known and there is effort to address them by improving efficiency, reducing waste, using industrial symbiosis, and eliminating harmful chemicals.
The negative costs of manufacturing can also be addressed legally. Developed countries regulate manufacturing activity with labor laws and environmental laws. Across the globe, manufacturers can be subject to regulations and pollution taxes to offset the environmental costs of manufacturing activities. Labor unions and craft guilds have played a historic role in the negotiation of worker rights and wages. Environment laws and labor protections that are available in developed nations may not be available in the third world. Tort law and product liability impose additional costs on manufacturing. These are significant dynamics in the ongoing process, occurring over the last few decades, of manufacture-based industries relocating operations to "developing-world" economies where the costs of production are significantly lower than in "developed-world" economies.

Finance
From a financial perspective, the goal of the manufacturing industry is mainly to achieve cost benefits per unit produced, which in turn leads to cost reductions in product prices for the market towards end customers. This relative cost reduction towards the market, is how manufacturing firms secure their profit margins.

Safety
Manufacturing has unique health and safety challenges and has been recognized by the National Institute for Occupational Safety and Health (NIOSH) as a priority industry sector in the National Occupational Research Agenda (NORA) to identify and provide intervention strategies regarding occupational health and safety issues.

Manufacturing and investment
Surveys and analyses of trends and issues in manufacturing and investment around the world focus on such things as:

The nature and sources of the considerable variations that occur cross-nationally in levels of manufacturing and wider industrial-economic growth;
Competitiveness; and
Attractiveness to foreign direct investors.In addition to general overviews, researchers have examined the features and factors affecting particular key aspects of manufacturing development. They have compared production and investment in a range of Western and non-Western countries and presented case studies of growth and performance in important individual industries and market-economic sectors.On June 26, 2009, Jeff Immelt, the CEO of General Electric, called for the United States to increase its manufacturing base employment to 20% of the workforce, commenting that the U.S. has outsourced too much in some areas and can no longer rely on the financial sector and consumer spending to drive demand. Further, while U.S. manufacturing performs well compared to the rest of the U.S. economy, research shows that it performs poorly compared to manufacturing in other high-wage countries. A total of 3.2 million – one in six U.S. manufacturing jobs – have disappeared between 2000 and 2007. In the UK, EEF the manufacturers organisation has led calls for the UK economy to be rebalanced to rely less on financial services and has actively promoted the manufacturing agenda.

Major manufacturing nations
According to the United Nations Industrial Development Organization (UNIDO), China is the top manufacturer worldwide by 2019 output, producing 28.7% of the total global manufacturing output, followed by the United States, Japan, Germany, and India.UNIDO also publishes a Competitive Industrial Performance (CIP) Index, which measures the competitive manufacturing ability of different nations. The CIP Index combines a nation's gross manufacturing output with other factors like high-tech capability and the nation's impact on the world economy. Germany topped the 2020 CIP Index, followed by China, South Korea, the United States, and Japan.

List of countries by manufacturing output
These are the top 50 countries by total value of manufacturing output in U.S. dollars for its noted year according to World Bank.

See also
Discrete manufacturing
Outline of manufacturing
Process manufacturing
3D printing

References
Sources
Kalpakjian, Serope; Steven Schmid (2005). Manufacturing, Engineering & Technology. Prentice Hall. pp. 22–36, 951–988. ISBN 978-0-13-148965-3.

External links

"Manufactures" . New International Encyclopedia. 1905.
EEF, the manufacturers' organisation – industry group representing uk manufacturers 
Enabling the Digital Thread for Smart Manufacturing
Evidences of Metal Manufacturing History
Grant Thornton IBR 2008 Manufacturing industry focus
How Everyday Things Are Made: video presentations
Manufacturing Sector of the National Occupational Research Agenda, US, 2018.

Manufacturing resource planning

Manufacturing resource planning  (MRP II) is a method for the effective planning of all resources of a manufacturing company. Ideally, it addresses operational planning in units, financial planning, and has a simulation capability to answer "what-if" questions and is an extension of closed-loop MRP (Material Requirements Planning).This is not exclusively a software function, but the management of people skills, requiring a dedication to database accuracy, and sufficient computer resources.  It is a total company management concept for using human and company resources more productively.

Key functions and features
MRP II is not a single specific proprietary software system and can thus take many forms. It is almost impossible to visualize an MRP II system that does not use a computer. An MRP II system can be based on either purchased–licensed or in-house software.
Almost every MRP II system is modular in construction. Characteristic basic modules in an MRP II system are:

Master production schedule (MPS)
Item master data (technical data)
Bill of materials (BOM) (technical data)
Production resources data (manufacturing technical data)
Inventories and orders (inventory control)
Purchasing management
Material requirements planning (MRP)
Shop floor control (SFC)
Capacity planning or capacity requirements planning (CRP)
Standard costing (cost control) and frequently also Actual or FIFO costing, and Weighted Average costing.
Cost reporting / management (cost control)together with auxiliary systems such as:

and related systems such as:

The MRP II system integrates these modules together so that they use common data and freely exchange information, in a model of how a manufacturing enterprise should and can operate. The MRP II approach is therefore very different from the "point solution" approach, where individual systems are deployed to help a company plan, control or manage a specific activity. MRP II is by definition fully integrated or at least fully interfaced.

MRP and MRPII
History and evolution
Material requirements planning (MRP) and manufacturing resource planning (MRPII) are predecessors of enterprise resource planning (ERP), a business information integration system. The development of these manufacturing coordination and integration methods and tools made today's ERP systems possible. Both MRP and MRPII are still widely used, independently and as modules of more comprehensive ERP systems, but the original vision of integrated information systems as we know them today began with the development of MRP and MRPII in manufacturing.
MRP (and MRPII) evolved from the earliest commercial database management package developed by Gene Thomas at IBM in the 1960s. The original structure was called BOMP (bill-of-materials processor), which evolved in the next generation into a more generalized tool called DBOMP (Database Organization and Maintenance Program).  These were run on mainframes, such as IBM/360.The vision for MRP and MRPII was to centralize and integrate business information in a way that would facilitate decision making for production line managers and increase the efficiency of the production line overall. In the 1980s, manufacturers developed systems for calculating the resource requirements of a production run based on sales forecasts. In order to calculate the raw materials needed to produce products and to schedule the purchase of those materials along with the machine and labor time needed, production managers recognized that they would need to use computer and software technology to manage the information. Originally, manufacturing operations built custom software programs that ran on mainframes.
Material requirements planning (MRP) was an early iteration of the integrated information systems vision. MRP information systems helped managers determine the quantity and timing of raw materials purchases. Information systems that would assist managers with other parts of the manufacturing process, MRPII, followed. While MRP was primarily concerned with materials, MRPII was concerned with the integration of all aspects of the manufacturing process, including materials, finance and human resources.
Like today's ERP systems, MRPII was designed to tell us about a lot of information by way of a centralized database. However, the hardware, software, and relational database technology of the 1980s was not advanced enough to provide the speed and capacity to run these systems in real-time, and the cost of these systems was prohibitive for most businesses. Nonetheless, the vision had been established, and shifts in the underlying business processes along with rapid advances in technology led to the more affordable enterprise and application integration systems that big businesses and many medium and smaller businesses use today.

General concepts
Material requirements planning (MRP) and manufacturing resource planning (MRPII) are both incremental information integration business process strategies that are implemented using hardware and modular software applications linked to a central database that stores and delivers business data and information.
MRP is concerned primarily with manufacturing materials while MRPII is concerned with the coordination of the entire manufacturing production, including materials, finance, and human resources. The goal of MRPII is to provide consistent data to all members in the manufacturing process as the product moves through the production line.
Paper-based information systems and non-integrated computer systems that provide paper or disk outputs result in many information errors, including missing data, redundant data, numerical errors that result from being incorrectly keyed into the system, incorrect calculations based on numerical errors, and bad decisions based on incorrect or old data. In addition, some data is unreliable in non-integrated systems because the same data is categorized differently in the individual databases used by different functional areas. 
MRPII systems begin with MRP, material requirements planning. MRP allows for the input of sales forecasts from sales and marketing, or of actual sales demand in the form of customers orders. These demands determine the raw materials demand. MRP and MRPII systems draw on a master production schedule, the breakdown of specific plans for each product on a line. While MRP allows for the coordination of raw materials purchasing, MRPII facilitates the development of a detailed production schedule that accounts for machine and labor capacity, scheduling the production runs according to the arrival of materials. An MRPII output is a final labor and machine schedule. Data about the cost of production, including machine time, labor time and materials used, as well as final production numbers, is provided from the MRPII system to accounting and finance.For the companies that want to integrate their other departments with their manufacturing management, ERP software are necessary.

Benefits
MRP II systems can provide:

Better control of inventories
Improved scheduling
Productive relationships with suppliersFor design / engineering:

Improved design control
Better quality and quality controlFor financial and costing:

Reduced working capital for inventory
Improved cash flow through quicker deliveries
Accurate inventory records

Criticism
Authors like Pochet and Wolsey  argue that MRP and MRP II, as well as the planning modules in current APS and ERP systems, are actually sets of heuristics. Better production plans could be obtained by optimization over more powerful mathematical programming models, usually integer programming models. While they acknowledge that the use of heuristics, like those prescribed by MRP and MRP II, were necessary in the past due to lack of computational power to solve complex optimization models, this is mitigated to some extent by recent improvements in computers.

See also


== References ==

Marketing

Marketing is the process of identifying customers and "creating, communicating, delivering, and exchanging" goods and services for the satisfaction and retention of those customers. It is one of the primary components of business management and commerce.Marketing is typically conducted by the seller, typically a retailer or manufacturer. Products can be marketed to other businesses (B2B) or directly to consumers (B2C). Sometimes tasks are contracted to a dedicated marketing firms, like a media, market research, or advertising agencies. More rarely, a trade association or government agency (such as the Agricultural Marketing Service) advertises on behalf of an entire industry or locality, often a specific type of food (e.g. Got Milk?), food from a specific area, or a city or region as a tourism destination.
Market orientations are philosophies concerning the factors that should go into market planning. The marketing mix, which outlines the specifics of the product and how it will be sold, is affected by the environment surrounding the product, the results of marketing research and market research, and the characteristics of the product's target market. Once these factors are determined, marketers must then decide what methods of promoting the product, including use of coupons and other price inducements.The term marketing, what is commonly known as attracting customers, incorporates knowledge gained by studying the management of exchange relationships and is the business process of identifying, anticipating and satisfying customers' needs and wants.
Marketing can include activities like:

Selection of a target audience
Selection of certain attributes or themes to emphasize in advertising
Operation of advertising campaigns
Attendance at trade shows and public events
Design of products and packaging to be more attractive to buyers
Selection of the terms of sale, such as price, discounts, warranty, and return policy
Product placement in media or with people believed to influence the buying habits of others
Contracts with retailers, wholesale distributors, or resellers
Attempts to create awareness of, loyalty to, and positive feelings about a brand

Definition
Marketing is currently defined by the American Marketing Association (AMA) as "the activity, set of institutions, and processes for creating, communicating, delivering, and exchanging offerings that have value for customers, clients, partners, and society at large". However, the definition of marketing has evolved over the years. The AMA reviews this definition and its definition for "marketing research" every three years. The interests of "society at large" were added into the definition in 2008. The development of the definition may be seen by comparing the 2008 definition with the AMA's 1935 version: "Marketing is the performance of business activities that direct the flow of goods, and services from producers to consumers". The newer definition highlights the increased prominence of other stakeholders in the new conception of marketing.
Recent definitions of marketing place more emphasis on the consumer relationship, as opposed to a pure exchange process. For instance, prolific marketing author and educator, Philip Kotler has evolved his definition of marketing. In 1980, he defined marketing as "satisfying needs and wants through an exchange process", and in 2018 defined it as "the process by which companies engage customers, build strong customer relationships, and create customer value in order to capture value from customers in return". A related definition, from the sales process engineering perspective, defines marketing as "a set of processes that are interconnected and interdependent with other functions of a business aimed at achieving customer interest and satisfaction".Some definitions of marketing highlight marketing's ability to produce value to shareholders of the firm as well. In this context, marketing can be defined as "the management process that seeks to maximise returns to shareholders by developing relationships with valued customers and creating a competitive advantage". For instance, the Chartered Institute of Marketing defines marketing from a customer-centric perspective, focusing on "the management process responsible for identifying, anticipating and satisfying customer requirements profitably".In the past, marketing practice tended to be seen as a creative industry, which included advertising, distribution and selling, and even today many parts of the marketing process (e.g. product design, art director, brand management, advertising, inbound marketing, copywriting etc.) involve the use of the creative arts. However, because marketing makes extensive use of social sciences, psychology, sociology, mathematics, economics, anthropology and neuroscience, the profession is now widely recognized as a science. Marketing science has developed a concrete process that can be followed to create a marketing plan.

Concept
The "marketing concept" proposes that to complete its organizational objectives, an organization should anticipate the needs and wants of potential consumers and satisfy them more effectively than its competitors. This concept originated from Adam Smith's book The Wealth of Nations but would not become widely used until nearly 200 years later. Marketing and Marketing Concepts are directly related.
Given the centrality of customer needs, and wants in marketing, a rich understanding of these concepts is essential:
Needs: Something necessary for people to live a healthy, stable and safe life. When needs remain unfulfilled, there is a clear adverse outcome: a dysfunction or death. Needs can be objective and physical, such as the need for food, water, and shelter; or subjective and psychological, such as the need to belong to a family or social group and the need for self-esteem.
Wants: Something that is desired, wished for or aspired to. Wants are not essential for basic survival and are often shaped by culture or peer-groups.
Demands: When needs and wants are backed by the ability to pay, they have the potential to become economic demands.Marketing research, conducted for the purpose of new product development or product improvement, is often concerned with identifying the consumer's unmet needs. Customer needs are central to market segmentation which is concerned with dividing markets into distinct groups of buyers on the basis of "distinct needs, characteristics, or behaviors who might require separate products or marketing mixes." Needs-based segmentation (also known as benefit segmentation) "places the customers' desires at the forefront of how a company designs and markets products or services." Although needs-based segmentation is difficult to do in practice, it has been proved to be one of the most effective ways to segment a market. In addition, a great deal of advertising and promotion is designed to show how a given product's benefits meet the customer's needs, wants or expectations in a unique way.

B2B and B2C marketing
The two major segments of marketing are business-to-business (B2B) marketing and business-to-consumer (B2C) marketing.

B2B marketing
B2B (business-to-business) marketing refers to any marketing strategy or content that is geared towards a business or organization. Any company that sells products or services to other businesses or organizations (vs. consumers) typically uses B2B marketing strategies.
Examples of products sold through B2B marketing include:

Major equipment
Accessory equipment
Raw materials
Component parts
Processed materials
Supplies
Venues
Business servicesThe four major categories of B2B product purchasers are:

Producers- use products sold by B2B marketing to make their own goods (e.g.: Mattel buying plastics to make toys)
Resellers- buy B2B products to sell through retail or wholesale establishments (e.g.: Walmart buying vacuums to sell in stores)
Governments- buy B2B products for use in government projects (e.g.: purchasing weather monitoring equipment for a wastewater treatment plant)
Institutions- use B2B products to continue operation (e.g.: schools buying printers for office use)

B2C marketing
Business-to-consumer marketing, or B2C marketing, refers to the tactics and strategies in which a company promotes its products and services to individual people.
Traditionally, this could refer to individuals shopping for personal products in a broad sense. More recently the term B2C refers to the online selling of consumer products.

C2B marketing
Consumer-to-business marketing or C2B marketing is a business model where the end consumers create products and services which are consumed by businesses and organizations. It is diametrically opposed to the popular concept of B2C or Business- to- Consumer where the companies make goods and services available to the end consumers.  In this type of business model, businesses profit from consumers' willingness to name their own price or contribute data or marketing to the company, while consumers benefit from flexibility, direct payment, or free or reduced-price products and services. One of the major benefit of this type of business model is that it offers a company a competitive advantage in the market.

C2C marketing
Customer to customer marketing or C2C marketing represents a market environment where one customer purchases goods from another customer using a third-party business or platform to facilitate the transaction. C2C companies are a new type of model that has emerged with e-commerce technology and the sharing economy.Most companies think of C2C marketing as the use of social media channels such as Facebook and Twitter. However, in many cases, the messaging tends to be business to consumer. A study performed in Colombia with 686 surveys analyzed factors C2C users perceive as influences in the intention to use and the acceptance of a marketplace: trust,  high web quality, low prices and social influence are key drivers that engage consumers to this marketplaces.

Differences in B2B and B2C marketing
The different goals of B2B and B2C marketing lead to differences in the B2B and B2C markets. The main differences in these markets are demand, purchasing volume, number of customers, customer concentration, distribution, buying nature, buying influences, negotiations, reciprocity, leasing and promotional methods.
Demand: B2B demand is derived because businesses buy products based on how much demand there is for the final consumer product. Businesses buy products based on customer's wants and needs. B2C demand is primarily because customers buy products based on their own wants and needs.
Purchasing volume: Businesses buy products in large volumes to distribute to consumers. Consumers buy products in smaller volumes suitable for personal use.
Number of customers: There are relatively fewer businesses to market to than direct consumers.
Customer concentration: Businesses that specialize in a particular market tend to be geographically concentrated while customers that buy products from these businesses are not concentrated.
Distribution: B2B products pass directly from the producer of the product to the business while B2C products must additionally go through a wholesaler or retailer.
Buying nature: B2B purchasing is a formal process done by professional buyers and sellers, while B2C purchasing is informal.
Buying influences: B2B purchasing is influenced by multiple people in various departments such as quality control, accounting, and logistics while B2C marketing is only influenced by the person making the purchase and possibly a few others.
Negotiations: In B2B marketing, negotiating for lower prices or added benefits is commonly accepted while in B2C marketing (particularly in Western cultures) prices are fixed.
Reciprocity: Businesses tend to buy from businesses they sell to. For example, a business that sells printer ink is more likely to buy office chairs from a supplier that buys the business's printer ink. In B2C marketing, this does not occur because consumers are not also selling products.
Leasing: Businesses tend to lease expensive items while consumers tend to save up to buy expensive items.
Promotional methods: In B2B marketing, the most common promotional method is personal selling. B2C marketing mostly uses sales promotion, public relations, advertising, and social media.

Marketing management orientations
A marketing orientation has been defined as a "philosophy of business management." or "a corporate state of mind" or as an "organizational culture" Although scholars continue to debate the precise nature of specific concepts that inform marketing practice, the most commonly cited orientations are as follows:
Product concept: mainly concerned with the quality of its product. It has largely been supplanted by the marketing orientation, except for haute couture and arts marketing.
Production concept: specializes in producing as much as possible of a given product or service in order to achieve economies of scale or economies of scope. It dominated marketing practice from the 1860s to the 1930s, yet can still be found in some companies or industries. Specifically, Kotler and Armstrong note that the production philosophy is "one of the oldest philosophies that guides sellers... [and] is still useful in some situations."
Selling concept: focuses on the selling/promotion of the firm's existing products, rather than developing new products to satisfy unmet needs or wants primarily through promotion and direct sales techniques, largely for "unsought goods" in industrial companies. A 2011 meta analyses found that the factors with the greatest impact on sales performance are a salesperson's sales related knowledge (market segments, presentation skills, conflict resolution, and products), degree of adaptiveness, role clarity, cognitive aptitude, motivation and interest in a sales role).Marketing concept: This is the most common concept used in contemporary marketing, and is a customer-centric approach based on products that suit new consumer tastes. These firm engage in extensive market research, use R&D (Research & Development), and then use promotion techniques. The marketing orientation includes:
Customer orientation: A firm in the market economy can survive by producing goods that people are willing and able to buy. Consequently, ascertaining consumer demand is vital for a firm's future viability and even existence as a going concern.
Organizational orientation: The marketing department is of prime importance within the functional level of an organization. Information from the marketing department is used to guide the actions of a company's other departments. A marketing department could ascertain (via marketing research) that consumers desired a new type of product, or a new usage for an existing product. With this in mind, the marketing department would inform the R&D department to create a prototype of a product/service based on consumers' new desires. The production department would then start to manufacture the product. The finance department may oppose required capital expenditures since it could undermine a healthy cash flow for the organization.Societal marketing concept: Social responsibility that goes beyond satisfying customers and providing superior value embraces societal stakeholders such as employees, customers, and local communities. Companies that adopt this perspective typically practice triple bottom line reporting and publish financial, social and environmental impact reports. Sustainable marketing or green marketing is an extension of societal marketing.

The marketing mix
A marketing mix is a foundational tool used to guide decision making in marketing. The marketing mix represents the basic tools that marketers can use to bring their products or services to the market. They are the foundation of managerial marketing and the marketing plan typically devotes a section to the marketing mix.

The 4Ps
The traditional marketing mix refers to four broad levels of marketing decision, namely: product, price, promotion, and place. The origins of the 4 Ps can be traced to the late 1940s. The first known mention of a mix has been attributed to a Professor of Marketing at Harvard University, James Culliton.The 4 Ps, in its modern form, was first proposed in 1960 by E. Jerome McCarthy; who presented them within a managerial approach that covered analysis, consumer behavior, market research, market segmentation, and planning. Phillip Kotler, popularised this approach and helped spread the 4 Ps model. McCarthy's 4 Ps have been widely adopted by both marketing academics and practitioners.

Outline
Product
The product aspects of marketing deal with the specifications of the actual goods or services, and how it relates to the end-user's needs and wants. The product element consists of product design, new product innovation, branding, packaging, labeling. The scope of a product generally includes supporting elements such as warranties, guarantees, and support. Branding, a key aspect of the product management, refers to the various methods of communicating a brand identity for the product, brand, or company.
Pricing
This refers to the process of setting a price for a product, including discounts. The price need not be monetary; it can simply be what is exchanged for the product or services, e.g. time, energy, or attention or any sacrifices consumers make in order to acquire a product or service. The price is the cost that a consumer pays for a product—monetary or not. Methods of setting prices are in the domain of pricing science.
Place (or distribution)
This refers to how the product gets to the customer; the distribution channels and intermediaries such as wholesalers and retailers who enable customers to access products or services in a convenient manner. This third P has also sometimes been called Place or Placement, referring to the channel by which a product or service is sold (e.g. online vs. retail), which geographic region or industry, to which segment (young adults, families, business people), etc. also referring to how the environment in which the product is sold in can affect sales.
Promotion
This includes all aspects of marketing communications: advertising, sales promotion, including promotional education, public relations, personal selling, product placement, branded entertainment, event marketing, trade shows, and exhibitions. This fourth P is focused on providing a message to get a response from consumers.  The message is designed to persuade or tell a story to create awareness.

Criticisms
One of the limitations of the 4Ps approach is its emphasis on an inside-out view. An inside-out approach is the traditional planning approach where the organization identifies its desired goals and objectives, which are often based around what has always been done. Marketing's task then becomes one of "selling" the organization's products and messages to the "outside" or external stakeholders. In contrast, an outside-in approach first seeks to understand the needs and wants of the consumer.From a model-building perspective, the 4 Ps has attracted a number of criticisms. Well-designed models should exhibit clearly defined categories that are mutually exclusive, with no overlap. Yet, the 4 Ps model has extensive overlapping problems. Several authors stress the hybrid nature of the fourth P, mentioning the presence of two important dimensions, "communication" (general and informative communications such as public relations and corporate communications) and "promotion" (persuasive communications such as advertising and direct selling). Certain marketing activities, such as personal selling, may be classified as either promotion or as part of the place (i.e., distribution) element. Some pricing tactics, such as promotional pricing, can be classified as price variables or promotional variables and, therefore, also exhibit some overlap.
Other important criticisms include that the marketing mix lacks a strategic framework and is, therefore, unfit to be a planning instrument, particularly when uncontrollable, external elements are an important aspect of the marketing environment.

Modifications and extensions
To overcome the deficiencies of the 4P model, some authors have suggested extensions or modifications to the original model. Extensions of the four P's are often included in cases such as services marketing where unique characteristics (i.e. intangibility, perishability, heterogeneity and the inseparability of production and consumption) warrant additional consideration factors. Other extensions have been found necessary for retail marketing, industrial marketing, and internet marketing
include "people", "process", and "physical evidence" and are often applied in the case of services marketing Other extensions have been found necessary in retail marketing, industrial marketing and internet marketing.

The 4Cs
In response to environmental and technological changes in marketing, as well as criticisms towards the 4Ps approach, the 4Cs has emerged as a modern marketing mix model. Robert F. Lauterborn proposed a 4 Cs classification in 1990. His classification is a more consumer-orientated version of the 4 Ps that attempts to better fit the movement from mass marketing to niche marketing

Outline
Consumer (or client)
The consumer refers to the person or group that will acquire the product. This aspect of the model focuses on fulfilling the wants or needs of the consumer.Cost
Cost refers to what is exchanged in return for the product. Cost mainly consists of the monetary value of the product. Cost also refers to anything else the consumer must sacrifice to attain the product, such as time or money spent on transportation to acquire the product.Convenience
Like "Place" in the 4Ps model, convenience refers to where the product will be sold. This, however, not only refers to physical stores but also whether the product is available in person or online. The convenience aspect emphasizes making it as easy as possible for the consumer to attain the product, thus making them more likely to do so.Communication
Like "Promotion" in the 4Ps model, communication refers to how consumers find out about a product. Unlike promotion, communication not only refers to the one-way communication of advertising, but also the two-way communication available through social media.

Environment
The term "marketing environment" relates to all of the factors (whether internal, external, direct or indirect) that affect a firm's marketing decision-making/planning. A firm's marketing environment consists of three main areas, which are:

The macro-environment (Macromarketing), over which a firm holds little control, consists of a variety of external factors that manifest on a large (or macro) scale. These include: economic, social, political and technological factors. A common method of assessing a firm's macro-environment is via a PESTLE (Political, Economic, Social, Technological, Legal, Ecological) analysis. Within a PESTLE analysis, a firm would analyze national political issues, culture and climate, key macroeconomic conditions, health and indicators (such as economic growth, inflation, unemployment, etc.), social trends/attitudes, and the nature of technology's impact on its society and the business processes within the society.
The micro-environment, over which a firm holds a greater amount (though not necessarily total) control, typically includes: Customers/consumers, Employees, Suppliers and the Media. In contrast to the macro-environment, an organization holds a greater (though not complete) degree of control over these factors.
The internal environment, which includes the factors inside of the company itself A firm's internal environment consistsof: Labor, Inventory, Company Policy, Logistics, Budget, and Capital Assets.

Research
Marketing research is a systematic process of analyzing data that involves conducting research to support marketing activities and the statistical interpretation of data into information. This information is then used by managers to plan marketing activities, gauge the nature of a firm's marketing environment and to attain information from suppliers. A distinction should be made between marketing research and market research. Market research involves gathering information about a particular target market. As an example, a firm may conduct research in a target market, after selecting a suitable market segment. In contrast, marketing research relates to all research conducted within marketing. Market research is a subset of marketing research. (Avoiding the word consumer, which shows up in both, market research is about distribution, while marketing research encompasses distribution, advertising effectiveness, and salesforce effectiveness).The stages of research include:

Define the problem
Plan research
Research
Interpret data
Implement findings

Segmentation
Market segmentation consists of taking the total heterogeneous market for a product and dividing it into several sub-markets or segments, each of which tends to be homogeneous in all significant aspects. The process is conducted for two main purposes: better allocation of a firm's finite resources and to better serve the more diversified tastes of contemporary consumers. A firm only possesses a certain amount of resources. Thus, it must make choices (and appreciate the related costs) in servicing specific groups of consumers. Moreover, with more diversity in the tastes of modern consumers, firms are noting the benefit of servicing a multiplicity of new markets.
Market segmentation can be defined in terms of the STP acronym, meaning Segmentation, Targeting, and Positioning.
Segmentation involves the initial splitting up of consumers into persons of like needs/wants/tastes. Commonly used criteria include:

Geographic (such as a country, region, city, town)
Psychographic (e.g. personality traits or lifestyle traits which influence consumer behaviour)
Demographic (e.g. age, gender, socio-economic class, education)
Gender
Income
Life-Cycle (e.g. Baby Boomer, Generation X, Millennial, Generation Z)
Lifestyle (e.g. tech savvy, active)
Behavioral (e.g. brand loyalty, usage rate)Once a segment has been identified to target, a firm must ascertain whether the segment is beneficial for them to service. The DAMP acronym is used as criteria to gauge the viability of a target market. The elements of DAMP are:

Discernable – how a segment can be differentiated from other segments.
Accessible – how a segment can be accessed via Marketing Communications produced by a firm
Measurable – can the segment be quantified and its size determined?
Profitable – can a sufficient return on investment be attained from a segment's servicing?The next step in the targeting process is the level of differentiation involved in a segment serving. Three modes of differentiation exist, which are commonly applied by firms. These are:

Undifferentiated – where a company produces a like product for all of a market segment
Differentiated – in which a firm produced slight modifications of a product within a segment
Niche – in which an organization forges a product to satisfy a specialized target marketPositioning concerns how to position a product in the minds of consumers and inform what attributes differentiate it from the competitor's products. A firm often performs this by producing a perceptual map, which denotes similar products produced in the same industry according to how consumers perceive their price and quality. From a product's placing on the map, a firm would tailor its marketing communications to meld with the product's perception among consumers and its position among competitors' offering.

Promotional mix
The promotional mix outlines how a company will market its product. It consists of five tools: personal selling, sales promotion, public relations, advertising and social media

Personal selling involves a presentation given by a salesperson to an individual or a group of potential customers. It enables two-way communication and relationship building, and is most commonly seen in business-to-business marketing but can also be found in business-to-consumer marketing (e.g.: selling cars at a dealership).Sales promotion involves short-term incentives to encourage the buying of products. Examples of these incentives include free samples, contests, premiums, trade shows, giveaways, coupons, sweepstakes and games. Depending on the incentive, one or more of the other elements of the promotional mix may be used in conjunction with sales promotion to inform customers of the incentives.
Public relations is the use of media tools to promote and monitor for a positive view of a company or product in the public's eye. The goal is to either sustain a positive opinion or lessen or change a negative opinion. It can include interviews, speeches/presentations, corporate literature, social media, news releases and special events.
Advertising occurs when a firm directly pays a media channel, directly via an in-house agency or via an advertising agency or media buying service, to publicize its product, service or message. Common examples of advertising media include:
Social media is used to facilitate two-way communication between companies and their customers. Outlets such as Facebook, Instagram, Twitter, Tumblr, Pinterest, Snapchat , Tik Tok and YouTube allow brands to start a conversation with regular and prospective customers. Viral marketing can be greatly facilitated by social media and if successful, allows key marketing messages and content in reaching a large number of target audiences within a short time frame. These platforms can also house advertising and public relations content.

The marketing plan
The area of marketing planning involves forging a plan for a firm's marketing activities. A marketing plan can also pertain to a specific product, as well as to an organization's overall marketing strategy. An organization's marketing planning process is derived from its overall business strategy. Thus, when top management is devising the firm's strategic direction/mission, the intended marketing activities are incorporated into this plan.

Outline of the marketing plan
Within the overall strategic marketing plan, the stages of the process are listed as thus:

Executive Summary
Current marketing situation
Threats and opportunities analysis
Objectives and issues
Marketing Strategy
Action programs
Budgets
Control

Levels of marketing objectives within an organization
As stated previously, the senior management of a firm would formulate a general business strategy for a firm. However, this general business strategy would be interpreted and implemented in different contexts throughout the firm.
At the corporate level, marketing objectives are typically broad-based in nature, and pertain to the general vision of the firm in the short, medium or long-term. As an example, if one pictures a group of companies (or a conglomerate), top management may state that sales for the group should increase by 25% over a ten-year period.
A strategic business unit (SBU) is a subsidiary within a firm, which participates within a given market/industry. The SBU would embrace the corporate strategy, and attune it to its own particular industry. For instance, an SBU may partake in the sports goods industry. It thus would ascertain how it would attain additional sales of sports goods, in order to satisfy the overall business strategy.
The functional level relates to departments within the SBUs, such as marketing, finance, HR, production, etc. The functional level would adopt the SBU's strategy and determine how to accomplish the SBU's own objectives in its market. To use the example of the sports goods industry again, the marketing department would draw up marketing plans, strategies and communications to help the SBU achieve its marketing aims.

Product life cycle
The product life cycle (PLC) is a tool used by marketing managers to gauge the progress of a product, especially relating to sales or revenue accrued over time. The PLC is based on a few key assumptions, including:

A given product would possess introduction, growth, maturity, and decline stage
No product lasts perpetually on the market
A firm must employ differing strategies, according to where a product is on the PLCIn the introduction stage, a product is launched onto the market. To stimulate the growth of sales/revenue, use of advertising may be high, in order to heighten awareness of the product in question.
During the growth stage, the product's sales/revenue is increasing, which may stimulate more marketing communications to sustain sales. More entrants enter into the market, to reap the apparent high profits that the industry is producing.
When the product hits maturity, its starts to level off, and an increasing number of entrants to a market produce price falls for the product. Firms may use sales promotions to raise sales.
During decline, demand for a good begins to taper off, and the firm may opt to discontinue the manufacture of the product. This is so, if revenue for the product comes from efficiency savings in production, over actual sales of a good/service. However, if a product services a niche market, or is complementary to another product, it may continue the manufacture of the product, despite a low level of sales/revenue being accrued.

See also
Types of marketing
Marketing orientations or philosophies
References
Bibliography
Bartels, Robert, The History of Marketing Thought, Columbus, Ohio, Grid, (1976) 1988 online
Christensen, Clayton M. (1997). The innovator's dilemma: when new technologies cause great firms to fail. Boston, Massachusetts, US: Harvard Business School Press. ISBN 978-0-87584-585-2.
Church, Roy and Godley, Andrew (eds), The Emergence of Modern Marketing, London, Frank Cass, 2003 online edition Archived 18 October 2009 at the Wayback Machine
Hollander, Stanley C., Rassuli, Kathleen M.; Jones, D.G. Brian; Dix and Farlow, L., "Periodization in Marketing History", Journal of Macromarketing, Vol 25, no.1, 2005, pp. 32–41. online
Tedlow, Richard S., and Jones, Geoffrey G. (eds), The Rise and Fall of Mass Marketing, Routledge, 2014
Weitz, Barton A. and Robin Wensley (eds). Handbook of Marketing, 2002

External links

 The dictionary definition of marketing at Wiktionary
 Quotations related to marketing at Wikiquote
 Marketing at Wikibooks

Mass production

Mass production, also known as flow production or continuous production, is the production of substantial amounts of standardized products in a constant flow, including and especially on assembly lines. Together with job production and batch production, it is one of the three main production methods.The term mass production was popularized by a 1926 article in the Encyclopædia Britannica supplement that was written based on correspondence with Ford Motor Company. The New York Times used the term in the title of an article that appeared before publication of the Britannica article.The concepts of mass production are applied to various kinds of products: from fluids and particulates handled in bulk (food, fuel, chemicals and mined minerals), to parts and assemblies of parts (household appliances and automobiles).
Some mass production techniques, such as standardized sizes and production lines, predate the Industrial Revolution by many centuries; however, it was not until the introduction of machine tools and techniques to produce interchangeable parts were developed in the mid-19th century that modern mass production was possible.

Overview
Mass production involves making many copies of products, very quickly, using assembly line techniques to send partially complete products to workers who each work on an individual step, rather than having a worker work on a whole product from start to finish. The emergence of mass production allowed supply to outstrip demand in many markets, forcing companies to seek new ways to become more competitive. Mass production makes a lot of things quickly and cheaply, so more people can buy them.
Mass production of fluid matter typically involves pipes with centrifugal pumps or screw conveyors (augers) to transfer raw materials or partially complete products between vessels. Fluid flow processes such as oil refining and bulk materials such as wood chips and pulp are automated using a system of process control which uses various instruments to measure variables such as temperature, pressure, volumetric and level, providing feedback.
Bulk materials such as coal, ores, grains and wood chips are handled by belt, chain, slat, pneumatic or screw conveyors, bucket elevators and mobile equipment such as front-end loaders. Materials on pallets are handled with forklifts. Also used for handling heavy items like reels of paper, steel or machinery are electric overhead cranes, sometimes called bridge cranes because they span large factory bays.
Mass production is capital-intensive and energy-intensive, for it uses a high proportion of machinery and energy in relation to workers. It is also usually automated while total expenditure per unit of product is decreased. However, the machinery that is needed to set up a mass production line (such as robots and machine presses) is so expensive that in order to attain profits there must be some assurance that the product is to be successful to.
One of the descriptions of mass production is that "the skill is built into the tool", which means that the worker using the tool may not need the skill. For example, in the 19th or early 20th century, this could be expressed as "the craftsmanship is in the workbench itself" (not the training of the worker). Rather than having a skilled worker measure every dimension of each part of the product against the plans or the other parts as it is being formed, there were jigs ready at hand to ensure that the part was made to fit this set-up. It had already been checked that the finished part would be to specifications to fit all the other finished parts—and it would be made more quickly, with no time spent on finishing the parts to fit one another. Later, once computerized control came about (for example, CNC), jigs were obviated, but it remained true that the skill (or knowledge) was built into the tool (or process, or documentation) rather than residing in the worker's head. This is the specialized capital required for mass production; each workbench and set of tools (or each CNC cell, or each fractionating column) is different (fine-tuned to its task).

History
Pre-industrial
Standardized parts and sizes and factory production techniques were developed in pre-industrial times; before the invention of machine tools the manufacture of precision parts, especially metal ones, was highly labour-intensive.
Crossbows made with bronze parts were produced in China during the Warring States period. The Qin Emperor unified China at least in part by equipping large armies with these weapons, which were equipped with a sophisticated trigger mechanism made of interchangeable parts. The Terracotta Army guarding the Emperor's necropolis is also believed to have been created through the use of standardized molds on an assembly line.In ancient Carthage, ships of war were mass-produced on a large scale at a moderate cost, allowing them to efficiently maintain their control of the Mediterranean. Many centuries later, the Republic of Venice would follow Carthage in producing ships with prefabricated parts on an assembly line: the Venetian Arsenal produced nearly one ship every day in what was effectively the world's first factory, which at its height employed 16,000 people.The invention of movable type has allowed for documents such as books to be mass produced. The first movable type system was invented in China by Bi Sheng, during the reign of the Song Dynasty, where it was used to, among other things, issue paper money. The oldest extant book produced using metal type is the Jikji, printed in Korea in the year 1377. Johannes Gutenberg, through his invention of the printing press and production of the Gutenberg Bible, introduced movable type to Europe. Through this introduction, mass production in the European publishing industry was made commonplace, leading to a democratization of knowledge, increased literacy and education, and the beginnings of modern science.Jean-Baptiste de Gribeauval, a French artillery engineer, introduced the standardization of cannon design in the mid-18th century. He developed a 6-inch (150 mm) field howitzer whose gun barrel, carriage assembly and ammunition specifications were made uniform for all French cannons. The standardized interchangeable parts of these cannons down to the nuts, bolts and screws made their mass production and repair easier than before.

Industrial
In the Industrial Revolution, simple mass production techniques were used at the Portsmouth Block Mills in England to make ships' pulley blocks for the Royal Navy in the Napoleonic Wars. It was achieved in 1803 by Marc Isambard Brunel in cooperation with Henry Maudslay under the management of Sir Samuel Bentham. The first unmistakable examples of manufacturing operations carefully designed to reduce production costs by specialized labour and the use of machines appeared in the 18th century in England.
The Navy was in a state of expansion that required 100,000 pulley blocks to be manufactured a year. Bentham had already achieved remarkable efficiency at the docks by introducing power-driven machinery and reorganising the dockyard system. Brunel, a pioneering engineer, and Maudslay, a pioneer of machine tool technology who had developed the first industrially practical screw-cutting lathe in 1800 which standardized screw thread sizes for the first time which in turn allowed the application of interchangeable parts, collaborated on plans to manufacture block-making machinery. By 1805, the dockyard had been fully updated with the revolutionary, purpose-built machinery at a time when products were still built individually with different components. A total of 45 machines were required to perform 22 processes on the blocks, which could be made into one of three possible sizes. The machines were almost entirely made of metal thus improving their accuracy and durability. The machines would make markings and indentations on the blocks to ensure alignment throughout the process. One of the many advantages of this new method was the increase in labour productivity due to the less labour-intensive requirements of managing the machinery. Richard Beamish, assistant to Brunel's son and engineer, Isambard Kingdom Brunel, wrote:

So that ten men, by the aid of this machinery, can accomplish with uniformity, celerity and ease, what formerly required the uncertain labour of one hundred and ten.
By 1808, annual production from the 45 machines had reached 130,000 blocks and some of the equipment was still in operation as late as the mid-twentieth century. Mass production techniques were also used to rather limited extent to make clocks and watches, and to make small arms, though parts were usually non-interchangeable. Though produced on a very small scale, Crimean War gunboat engines designed and assembled by John Penn of Greenwich are recorded as the first instance of the application of mass production techniques (though not necessarily the assembly-line method) to marine engineering. In filling an Admiralty order for 90 sets to his high-pressure and high-revolution horizontal trunk engine design, Penn produced them all in 90 days. He also used Whitworth Standard threads throughout. Prerequisites for the wide use of mass production were interchangeable parts, machine tools and power, especially in the form of electricity.
Some of the organizational management concepts needed to create 20th-century mass production, such as scientific management, had been pioneered by other engineers (most of whom are not famous, but Frederick Winslow Taylor is one of the well-known ones), whose work would later be synthesized into fields such as industrial engineering, manufacturing engineering, operations research, and management consultancy. Although after leaving the Henry Ford Company which was rebranded as Cadillac and later was awarded the Dewar Trophy in 1908 for creating interchangeable mass-produced precision engine parts, Henry Ford downplayed the role of Taylorism in the development of mass production at his company. However, Ford management performed time studies and experiments to mechanize their factory processes, focusing on minimizing worker movements. The difference is that while Taylor focused mostly on efficiency of the worker, Ford also substituted for labor by using machines, thoughtfully arranged, wherever possible.
In 1807, Eli Terry was hired to produce 4,000 wooden movement clocks in the Porter Contract. At this time, the annual yield for wooden clocks did not exceed a few dozen on average. Terry developed a milling machine in 1795, in which he perfected Interchangeable parts. In 1807, Terry developed a spindle cutting machine, which could produce multiple parts at the same time. Terry hired Silas Hoadley and Seth Thomas to work the Assembly line at the facilities. The Porter Contract was the first contract which called for mass production of clock movements in history. In 1815, Terry began mass-producing the first shelf clock. Chauncey Jerome, an apprentice of Eli Terry mass-produced up to 20,000 brass clocks annually in 1840 when he invented the cheap 30-hour OG clock.The United States Department of War sponsored the development of interchangeable parts for guns produced at the arsenals at Springfield, Massachusetts and Harpers Ferry, Virginia (now West Virginia) in the early decades of the 19th century, finally achieving reliable interchangeability by about 1850. This period coincided with the development of machine tools, with the armories designing and building many of their own. Some of the methods employed were a system of gauges for checking dimensions of the various parts and jigs and fixtures for guiding the machine tools and properly holding and aligning the work pieces. This system came to be known as armory practice or the American system of manufacturing, which spread throughout New England aided by skilled mechanics from the armories who were instrumental in transferring the technology to the sewing machines manufacturers and other industries such as machine tools, harvesting machines and bicycles. Singer Manufacturing Co., at one time the largest sewing machine manufacturer, did not achieve interchangeable parts until the late 1880s, around the same time Cyrus McCormick adopted modern manufacturing practices in making harvesting machines.
During World War II, The United States mass-produced many vehicles and weapons, such as ships (i.e. Liberty Ships, Higgins boats ), aircraft (i.e. North American P-51 Mustang, Consolidated B-24 Liberator, Boeing B-29 Superfortress), jeeps (i.e. Willys MB), trucks, tanks (i.e. M4 Sherman) and M2 Browning and M1919 Browning machine guns. Many vehicles, transported by ships have been shipped in parts and later assembled on-site.For the ongoing energy transition, many wind turbine components and solar panels are being mass-produced. Wind turbines and solar panels are being used in respectively wind farms and solar farms.
In addition, in the ongoing climate change mitigation, large-scale carbon sequestration (through reforestation, blue carbon restoration, etc) has been proposed. Some projects (such as the Trillion Tree Campaign) involve planting a very large amount of trees. In order to speed up such efforts, fast propagation of trees may be useful. Some automated machines have been produced to allow for fast (vegetative) plant propagation.Also, for some plants that help to sequester carbon (such as seagrass), techniques have been developed to help speed up the process .Mass production benefited from the development of materials such as inexpensive steel, high strength steel and plastics. Machining of metals was greatly enhanced with high-speed steel and later very hard materials such as tungsten carbide for cutting edges. Fabrication using steel components was aided by the development of electric welding and stamped steel parts, both which appeared in industry in about 1890. Plastics such as polyethylene, polystyrene and polyvinyl chloride (PVC) can be easily formed into shapes by extrusion, blow molding or injection molding, resulting in very low cost manufacture of consumer products, plastic piping, containers and parts.
An influential article that helped to frame and popularize the 20th century's definition of mass production appeared in a 1926 Encyclopædia Britannica supplement. The article was written based on correspondence with Ford Motor Company and is sometimes credited as the first use of the term.

Factory electrification
Electrification of factories began very gradually in the 1890s after the introduction of a practical DC motor by Frank J. Sprague and accelerated after the AC motor was developed by Galileo Ferraris, Nikola Tesla and Westinghouse, Mikhail Dolivo-Dobrovolsky and others. Electrification of factories was fastest between 1900 and 1930, aided by the establishment of electric utilities with central stations and the lowering of electricity prices from 1914 to 1917.Electric motors were several times more efficient than small steam engines because central station generation were more efficient than small steam engines and because line shafts and belts had high friction losses. Electric motors also allowed more flexibility in manufacturing and required less maintenance than line shafts and belts. Many factories saw a 30% increase in output simply from changing over to electric motors.
Electrification enabled modern mass production, as with Thomas Edison's iron ore processing plant (about 1893) that could process 20,000 tons of ore per day with two shifts, each of five men. At that time it was still common to handle bulk materials with shovels, wheelbarrows and small narrow-gauge rail cars, and for comparison, a canal digger in previous decades typically handled five tons per 12-hour day.
The biggest impact of early mass production was in manufacturing everyday items, such as at the Ball Brothers Glass Manufacturing Company, which electrified its mason jar plant in Muncie, Indiana, U.S., around 1900. The new automated process used glass-blowing machines to replace 210 craftsman glass blowers and helpers. A small electric truck was used to handle 150 dozen bottles at a time where previously a hand truck would carry six dozen. Electric mixers replaced men with shovels handling sand and other ingredients that were fed into the glass furnace. An electric overhead crane replaced 36 day laborers for moving heavy loads across the factory.According to Henry Ford:
The provision of a whole new system of electric generation emancipated industry from the leather belt and line shaft, for it eventually became possible to provide each tool with its own electric motor. This may seem only a detail of minor importance. In fact, modern industry could not be carried out with the belt and line shaft for a number of reasons. The motor enabled machinery to be arranged in the order of the work, and that alone has probably doubled the efficiency of industry, for it has cut out a tremendous amount of useless handling and hauling. The belt and line shaft were also tremendously wasteful – so wasteful indeed that no factory could be really large, for even the longest line shaft was small according to modern requirements. Also high speed tools were impossible under the old conditions – neither the pulleys nor the belts could stand modern speeds. Without high speed tools and the finer steels which they brought about, there could be nothing of what we call modern industry.
Mass production was popularized in the late 1910s and 1920s by Henry Ford's Ford Motor Company, which introduced electric motors to the then-well-known technique of chain or sequential production. Ford also bought or designed and built special purpose machine tools and fixtures such as multiple spindle drill presses that could drill every hole on one side of an engine block in one operation and a multiple head milling machine that could simultaneously machine 15 engine blocks held on a single fixture. All of these machine tools were arranged systematically in the production flow and some had special carriages for rolling heavy items into machining position. Production of the Ford Model T used 32,000 machine tools.

Buildings
The process of prefabrication, wherein parts are created separately from the finished product, is at the core of all mass-produced construction. Early examples include movable structures reportedly utilized by Akbar the Great, and the chattel houses built by emancipated slaves on Barbados. The Nissen hut, first used by the British during World War I, married prefabrication and mass production in a way that suited the needs of the military. The simple structures, which cost little and could be erected in just a couple of hours, were highly successful: over 100,000 Nissen huts were produced during World War I alone, and they would go on to serve in other conflicts and inspire a number of similar designs.Following World War II, in the United States, William Levitt pioneered the building of standardized tract houses in 56 different locations around the country. These communities were dubbed Levittowns, and they were able to be constructed quickly and cheaply through the leveraging of economies of scale, as well as the specialization of construction tasks in a process akin to an assembly line. This era also saw the invention of the mobile home, a small prefabricated house that can be transported cheaply on a truck bed.
In the modern industrialization of construction, mass production is often used for prefabrication of house components.
Fabrics and Materials
Mass production has significantly impacted the fashion industry, particularly in the realm of fibers and materials. The advent of synthetic fibers, such as polyester and nylon, revolutionized textile manufacturing by providing cost-effective alternatives to natural fibers. This shift enabled the rapid production of inexpensive clothing, contributing to the rise of fast fashion. This reliance on mass production has raised concerns about environmental sustainability and labor conditions, spurring the need for greater ethical and sustainable practices within the fashion industry.

The use of assembly lines
Mass production systems for items made of numerous parts are usually organized into assembly lines. The assemblies pass by on a conveyor, or if they are heavy, hung from an overhead crane or monorail.
In a factory for a complex product, rather than one assembly line, there may be many auxiliary assembly lines feeding sub-assemblies (i.e. car engines or seats) to a backbone "main" assembly line. A diagram of a typical mass-production factory looks more like the skeleton of a fish than a single line.

Vertical integration
Vertical integration is a business practice that involves gaining complete control over a product's production, from raw materials to final assembly.
In the age of mass production, this caused shipping and trade problems in that shipping systems were unable to transport huge volumes of finished automobiles (in Henry Ford's case) without causing damage, and also government policies imposed trade barriers on finished units.Ford built the Ford River Rouge Complex with the idea of making the company's own iron and steel in the same large factory site where parts and car assembly took place. River Rouge also generated its own electricity.
Upstream vertical integration, such as to raw materials, is away from leading technology toward mature, low-return industries. Most companies chose to focus on their core business rather than vertical integration. This included buying parts from outside suppliers, who could often produce them as cheaply or cheaper.
Standard Oil, the major oil company in the 19th century, was vertically integrated partly because there was no demand for unrefined crude oil, but kerosene and some other products were in great demand. The other reason was that Standard Oil monopolized the oil industry. The major oil companies were, and many still are, vertically integrated, from production to refining and with their own retail stations, although some sold off their retail operations. Some oil companies also have chemical divisions.
Lumber and paper companies at one time owned most of their timber lands and sold some finished products such as corrugated boxes. The tendency has been to divest of timber lands to raise cash and to avoid property taxes.

Advantages and disadvantages
The economies of mass production come from several sources. The primary cause is a reduction of non-productive effort of all types. In craft production, the craftsman must bustle about a shop, getting parts and assembling them. He must locate and use many tools many times for varying tasks. In mass production, each worker repeats one or a few related tasks that use the same tool to perform identical or near-identical operations on a stream of products. The exact tool and parts are always at hand, having been moved down the assembly line consecutively. The worker spends little or no time retrieving and/or preparing materials and tools, and so the time taken to manufacture a product using mass production is shorter than when using traditional methods.
The probability of human error and variation is also reduced, as tasks are predominantly carried out by machinery; error in operating such machinery has more far-reaching consequences. A reduction in labour costs, as well as an increased rate of production, enables a company to produce a larger quantity of one product at a lower cost than using traditional, non-linear methods.
However, mass production is inflexible because it is difficult to alter a design or production process after a production line is implemented. Also, all products produced on one production line will be identical or very similar, and introducing variety to satisfy individual tastes is not easy. However, some variety can be achieved by applying different finishes and decorations at the end of the production line if necessary. The starter cost for the machinery can be expensive so the producer must be sure it sells or the producers will lose a lot of money.
The Ford Model T produced tremendous affordable output but was not very good at responding to demand for variety, customization, or design changes. As a consequence Ford eventually lost market share to General Motors, who introduced annual model changes, more accessories and a choice of colors.With each passing decade, engineers have found ways to increase the flexibility of mass production systems, driving down the lead times on new product development and allowing greater customization and variety of products.
Compared with other production methods, mass production can create new occupational hazards for workers. This is partly due to the need for workers to operate heavy machinery while also working close together with many other workers. Preventative safety measures, such as fire drills, as well as special training is therefore necessary to minimise the occurrence of industrial accidents.

Socioeconomic impacts
In the 1830s, French political thinker and historian Alexis de Tocqueville identified one of the key characteristics of America that would later make it so amenable to the development of mass production: the homogeneous consumer base. De Tocqueville wrote in his Democracy in America (1835) that "The absence in the United States of those vast accumulations of wealth which favor the expenditures of large sums on articles of mere luxury... impact to the productions of American industry a character distinct from that of other countries' industries. [Production is geared toward] articles suited to the wants of the whole people".
Mass production improved productivity, which was a contributing factor to economic growth and the decline in work week hours, alongside other factors such as transportation infrastructures (canals, railroads and highways) and agricultural mechanization. These factors caused the typical work week to decline from 70 hours in the early 19th century to 60 hours late in the century, then to 50 hours in the early 20th century and finally to 40 hours in the mid-1930s.
Mass production permitted great increases in total production. Using a European crafts system into the late 19th century it was difficult to meet demand for products such as sewing machines and animal powered mechanical harvesters. By the late 1920s many previously scarce goods were in good supply. One economist has argued that this constituted "overproduction" and contributed to high unemployment during the Great Depression. Say's law denies the possibility of general overproduction and for this reason classical economists deny that it had any role in the Great Depression.
Mass production allowed the evolution of consumerism by lowering the unit cost of many goods used.
Mass production has been linked to the Fast Fashion Industry, often leaving the consumer with lower quality garments for a lower cost. Most fast-fashion clothing is mass-produced, which means it is typically made of cheap fabrics, such as polyester, and constructed poorly in order to keep short turnaround times to meet the demands of consumers and shifting trends.

See also
References
Further reading
Beaudreau, Bernard C. (1996). Mass Production, the Stock Market Crash and the Great Depression. New York / Lincoln / Shanghai: Authors Choice Press.
Borth, Christy. Masters of Mass Production, Bobbs-Merrill Company, Indianapolis, IN, 1945.
Herman, Arthur. Freedom's Forge: How American Business Produced Victory in World War II, Random House, New York, NY, 2012. ISBN 978-1-4000-6964-4.

External links
 Quotations related to Mass production at Wikiquote
 Media related to Mass production at Wikimedia Commons

Mergers and acquisitions

Mergers and acquisitions (M&A) are business transactions in which the ownership of companies, business organizations, or their operating units are transferred to or consolidated with another company or business organization. As an aspect of strategic management, M&A can allow enterprises to grow or downsize, and change the nature of their business or competitive position.
Technically, a merger is the legal consolidation of two business entities into one, whereas an acquisition occurs when one entity takes ownership of another entity's share capital, equity interests or assets. A deal may be euphemistically called a "merger of equals" if both CEOs agree that joining together is in the best interest of both of their companies. From a legal and financial point of view, both mergers and acquisitions generally result in the consolidation of assets and liabilities under one entity, and the distinction between the two is not always clear.
In most countries, mergers and acquisitions must comply with antitrust or competition law. In the United States, for example, the Clayton Act outlaws any merger or acquisition that may "substantially lessen competition" or "tend to create a monopoly", and the Hart–Scott–Rodino Act requires companies to get "pre-clearance" from either the Federal Trade Commission or the U.S. Department of Justice's Antitrust Division for all mergers or acquisitions over a certain size.

Acquisition
An acquisition/takeover is the purchase of one business or company by another company or other business entity. Specific acquisition targets can be identified through myriad avenues, including market research, trade expos, sent up from internal business units, or supply chain analysis. Such purchase may be of 100%, or nearly 100%, of the assets or ownership equity of the acquired entity. 
A consolidation/amalgamation occurs when two companies combine to form a new enterprise altogether, and neither of the previous companies remains independently owned. Acquisitions are divided into "private" and "public" acquisitions, depending on whether the acquiree or merging company (also termed a target) is or is not listed on a public stock market. Some public companies rely on acquisitions as an important value creation strategy.  An additional dimension or categorization consists of whether an acquisition is friendly or hostile.Achieving acquisition success has proven to be very difficult, while various studies have shown that 50% of acquisitions were unsuccessful. "Serial acquirers" appear to be more successful with M&A than companies who make acquisitions only occasionally (see Douma & Schreuder, 2013, chapter 13). The new forms of buy out created since the crisis are based on serial type acquisitions known as an ECO Buyout which is a co-community ownership buy out and the new generation buy outs of the MIBO (Management Involved or Management & Institution Buy Out) and MEIBO (Management & Employee Involved Buy Out).
Whether a purchase is perceived as being "friendly" or "hostile" depends significantly on how the proposed acquisition is communicated to and perceived by the target company's board of directors, employees, and shareholders. It is normal for M&A deal communications to take place in a so-called "confidentiality bubble," wherein the flow of information is restricted pursuant to confidentiality agreements. In the case of a friendly transaction, the companies cooperate in negotiations; in the case of a hostile deal, the board and/or management of the target is unwilling to be bought or the target's board has no prior knowledge of the offer. Hostile acquisitions can, and often do, ultimately become "friendly" as the acquirer secures endorsement of the transaction from the board of the acquiree company. This usually requires an improvement in the terms of the offer and/or through negotiation.
"Acquisition" usually refers to a purchase of a smaller firm by a larger one. Sometimes, however, a smaller firm will acquire management control of a larger and/or longer-established company and retain the name of the latter for the post-acquisition combined entity. This is known as a reverse takeover. Another type of acquisition is the reverse merger, a form of transaction that enables a private company to be publicly listed in a relatively short time frame. A reverse merger is a type of merger where a privately held company, typically one with promising prospects and a need for financing, acquires a publicly listed shell company that has few assets and no significant business operations.
The combined evidence suggests that the shareholders of acquired firms realize significant positive "abnormal returns," while shareholders of the acquiring company are most likely to experience a negative wealth effect. Most studies indicate that M&A transactions have a positive net effect, with investors in both the buyer and target companies seeing positive returns. This suggests that M&A creates economic value, likely by transferring assets to more efficient management teams who can better utilize them. (See Douma & Schreuder, 2013, chapter 13).
There are also a variety of structures used in securing control over the assets of a company, which have different tax and regulatory implications:

The buyer buys the shares, and therefore control, of the target company being purchased. Ownership control of the company in turn conveys effective control over the assets of the company, but since the company is acquired intact as a going concern, this form of transaction carries with it all of the liabilities accrued by that business over its past and all of the risks that company faces in its commercial environment and corporate environmentThe buyer buys the assets of the target company. The cash the target receives from the sell-off is paid back to its shareholders by dividend or through liquidation. This type of transaction leaves the target company as an empty shell, if the buyer buys out the entire assets. A buyer often structures the transaction as an asset purchase to "cherry-pick" the assets that it wants and leave out the assets and liabilities that it does not. This can be particularly important where foreseeable liabilities may include future, unquantified damage awards such as those that could arise from litigation over defective products, employee benefits or terminations, or environmental damage. A disadvantage of this structure is the tax that many jurisdictions, particularly outside the United States, impose on transfers of the individual assets, whereas stock transactions can frequently be structured as like-kind exchanges or other arrangements that are tax-free or tax-neutral, both to the buyer and to the seller's shareholders.The terms "demerger", "spin-off" and "spin-out" are sometimes used to indicate a situation where one company splits into two, generating a second company which may or may not become separately listed on a stock exchange.
As per knowledge-based views, firms can generate greater values through the retention of knowledge-based resources which they generate and integrate. Extracting technological benefits during and after acquisition is an ever-challenging issue because of organizational differences. Based on the content analysis of seven interviews, the authors concluded the following components for their grounded model of acquisition:

Improper documentation and changing implicit knowledge makes it difficult to share information during acquisition.
For acquired firm symbolic and cultural independence which is the base of technology and capabilities are more important than administrative independence.
Detailed knowledge exchange and integrations are difficult when the acquired firm is large and high performing.
Management of executives from acquired firm is critical in terms of promotions and pay incentives to utilize their talent and value their expertise.
Transfer of technologies and capabilities are most difficult task to manage because of complications of acquisition implementation. The risk of losing implicit knowledge is always associated with the fast pace acquisition.An increase in acquisitions in the global business environment requires enterprises to evaluate the key stake holders of acquisitions very carefully before implementation. It is imperative for the acquirer to understand this relationship and apply it to its advantage. Employee retention is possible only when resources are exchanged and managed without affecting their independence.

Legal structures
A corporate acquisition can be structured legally as either an "asset purchase" in which the seller sells business assets and liabilities to the buyer, an "equity purchase" in which the buyer purchases equity interests in a target company from one or more selling shareholders or a "merger" in which one legal entity is combined into another entity by operation of the corporate law statute(s) of the jurisdiction of the merging entities. In a transaction structured as a merger or an equity purchase, the buyer acquires all of the assets and liabilities of the acquired entity. In a transaction structured as an asset purchase, the buyer and seller agree on which assets and liabilities the buyer will acquire from the seller.
Asset purchases are common in technology transactions where the buyer is most interested in particular intellectual property rights but does not want to acquire liabilities or other contractual relationships. An asset purchase structure may also be used when the buyer wishes to buy a particular division or unit of a company which is not a separate legal entity. Divestitures present a variety of unique challenges, such as identifying the assets and liabilities that pertain solely to the unit being sold, determaining whether the unit relies on services from other parts of the seller's organization, transferring employees, moving permits and licenses, and safeguarding against potential competition from the seller in the same business sector after the transaction is completed.

Types of mergers
From an economic point of view, business combinations can also be classified as horizontal, vertical and conglomerate mergers (or acquisitions). A horizontal merger is between two competitors in the same industry. A
vertical merger occurs when two firms combine across the value chain, such as when a firm buys a former supplier (backward integration) or a former customer (forward integration). When there is no strategic relatedness between an acquiring firm and its target, this is called a conglomerate merger (Douma & Schreuder, 2013)The form of merger most often employed is a triangular merger, where the target company merges with a shell company wholly owned by the buyer, thus becoming a subsidiary of the buyer.  In a "forward triangular merge,", the target company merges into the subsidiary, with the subsidiary as the surviving company of the merger; a "reverse triangular merger" is similar except that the subsidiary merges into the target company, with the target company surviving the merger.Mergers, asset purchases and equity purchases are each taxed differently, and the most beneficial structure for tax purposes is highly situation-dependent. Under the U.S. Internal Revenue Code, a forward triangular merger is taxed as if the target company sold its assets to the shell company and then liquidated, them whereas a reverse triangular merger is taxed as if the target company's shareholders sold their stock in the target company to the buyer.

Documentation
The documentation of an M&A transaction often begins with a letter of intent. The letter of intent generally does not bind the parties to commit to a transaction, but may bind the parties to confidentiality and exclusivity obligations so that the transaction can be considered through a due diligence process involving lawyers, accountants, tax advisors, and other professionals, as well as business people from both sides.After due diligence is complete, the parties may proceed to draw up a definitive agreement, known as a "merger agreement", "share purchase agreement," or "asset purchase agreement" depending on the structure of the transaction. Such contracts are typically 80 to 100 pages long and focus on five key types of terms:
Conditions, which must be satisfied before there is an obligation to complete the transaction. Conditions typically include matters such as regulatory approvals and the lack of any material adverse change in the target's business.
Representations and warranties by the seller with regard to the company, which are claimed to be true at both the time of signing and the time of closing. Sellers often attempt to craft their representations and warranties with knowledge qualifiers, dictating the level of knowledge applicable and which seller parties' knowledge is relevant. Some agreements provide that if the representations and warranties by the seller prove to be false, the buyer may claim a refund of part of the purchase price, as is common in transactions involving privately held companies (although in most acquisition agreements involving public company targets, the representations and warranties of the seller do not survive the closing). Representations regarding a target company's net working capital are a common source of post-closing disputes.
Covenants, which govern the conduct of the parties, both before the closing (such as covenants that restrict the operations of the business between signing and closing) and after the closing (such as covenants regarding future income tax filings and tax liability or post-closing restrictions agreed to by the buyer and seller parties).
Termination rights, which may be triggered by a breach of contract, a failure to satisfy certain conditions or the passage of a certain period of time without consummating the transaction, and fees and damages payable in case of a termination for certain events (also known as breakup fees).
Provisions relating to obtaining required shareholder approvals under state law and related SEC filings required under federal law, if applicable, and terms related to the mechanics of the legal transactions to be consummated at closing (such as the determination and allocation of the purchase price and post-closing adjustments (such as adjustments after the final determination of working capital at closing or earnout payments payable to the sellers), repayment of outstanding debt, and the treatment of outstanding shares, options and other equity interests).
An indemnification provision, which provides that an indemnitor will indemnify, defend, and hold harmless the indemnitee(s) for losses incurred by the indemnitees as a result of the indemnitor's breach of its contractual obligations in the purchase agreementFollowing the closing of a deal, adjustments may be made to some of the provisions outlined in the purchase agreement, such as the purchase price. These adjustments are subject to enforceability issues in certain situations. Alternatively, certain transactions use the 'locked box' approach, where the purchase price is fixed at signing and based on the seller's equity value at a pre-signing date and an interest charge.

Business valuation
The assets of a business are pledged to two categories of stakeholders: equity owners and owners of the business' outstanding debt. The core value of a business, which accrues to both categories of stakeholders, is called the Enterprise Value (EV), whereas the value which accrues just to shareholders is the Equity Value (also called market capitalization for publicly listed companies). Enterprise Value reflects a capital structure neutral valuation and is frequently a preferred way to compare value as it is not affected by a company's, or management's, strategic decision to fund the business either through debt, equity, or a portion of both. Five common ways to "triangulate" the enterprise value of a business are:

asset valuation: the price paid is the value of the "easily salable parts"; the main approaches to valuing these are book value and liquidation value
historical earnings valuation: the price is such that the payment for the business (or return targeted by the investor), would have been supported by the business's own earnings or cash-flow averaged over the previous 3-5 years; see also Earnout
future maintainable earnings valuation: similarly, but forward looking; see generally, Cash flow forecasting and Financial forecast, and re "maintainability", Sustainable growth rate § From a financial perspective and Owner earnings.
relative valuation: the price paid per dollar of earnings or revenue is based on the same multiple for comparable companies and / or recent comparable transactions
discounted cash flow valuation (DCF): the price equates to the value of "all"  future cash-flows -  with synergies and tax given special attention  - as discounted to today; see § Determine cash flow for each forecast period under Valuation using discounted cash flows, which compares M&A DCF models to other cases.Professionals who value businesses generally do not use just one method, but a combination. Valuations implied using these methodologies can prove different to a company's current trading valuation. For public companies, the market based enterprise value and equity value can be calculated by referring to the company's share price and components on its balance sheet. The valuation methods described above represent ways to determine value of a company independently from how the market currently, or historically, has determined value based on the price of its outstanding securities.
Most often value is expressed in a Letter of Opinion of Value  (LOV) when the business is being valued informally. Formal valuation reports generally get more detailed and expensive as the size of a company increases, but this is not always the case as the nature of the business and the industry it is operating in can influence the complexity of the valuation task.
Objectively evaluating the historical and prospective performance of a business is a challenge faced by many. Generally, parties rely on independent third parties to conduct due diligence studies or business assessments. To yield the most value from a business assessment, objectives should be clearly defined and the right resources should be chosen to conduct the assessment in the available timeframe.
As synergy plays a large role in the valuation of acquisitions, it is paramount to get the value of synergies right; as briefly alluded to re DCF valuations. Synergies are different from the "sales price" valuation of the firm, as they will accrue to the buyer. Hence, the analysis should be done from the acquiring firm's point of view. Synergy-creating investments are started by the choice of the acquirer, and therefore they are not obligatory, making them essentially real options. To include this real options aspect into analysis of acquisition targets is one interesting issue that has been studied lately. See also contingent value rights.

Financing
Mergers are generally differentiated from acquisitions partly by the way in which they are financed and partly by the relative size of the companies. Various methods of financing an M&A deal exist:

Cash
Payment by cash. Such transactions are usually termed acquisitions rather than mergers because the shareholders of the target company are removed from the picture and the target comes under the (indirect) control of the bidder's shareholders.

Stock
Payment in the form of the acquiring company's stock, issued to the shareholders of the acquired company at a given ratio proportional to the valuation of the latter. They receive stock in the company that is purchasing the smaller subsidiary. See Stock swap, Swap ratio.

Financing options
There are some elements to think about when choosing the form of payment. When submitting an offer, the acquiring firm should consider other potential bidders and think strategically. The form of payment might be decisive for the seller. With pure cash deals, there is no doubt on the real value of the bid (without considering an eventual earnout). The contingency of the share payment is indeed removed. Thus, a cash offer preempts competitors better than securities. Taxes are a second element to consider and should be evaluated with the counsel of competent tax and accounting advisers. Third, with a share deal the buyer's capital structure might be affected and the control of the buyer modified. If the issuance of shares is necessary, shareholders of the acquiring company might prevent such capital increase at the general meeting of shareholders. The risk is removed with a cash transaction. Then, the balance sheet of the buyer will be modified and the decision maker should take into account the effects on the reported financial results. For example, in a pure cash deal (financed from the company's current account), liquidity ratios might decrease. On the other hand, in a pure stock for stock transaction (financed from the issuance of new shares), the company might show lower profitability ratios (e.g. ROA). However, economic dilution must prevail towards accounting dilution when making the choice. The form of payment and financing options are tightly linked. If the buyer pays cash, there are three main financing options:

Cash on hand: it consumes financial slack (excess cash or unused debt capacity) and may decrease debt rating. There are no major transaction costs.
Issue of debt: It consumes financial slack, may decrease debt rating and increase cost of debt.

Specialist advisory firms
M&A advice is provided by full-service investment banks- who often advise and handle the biggest deals in the world (called bulge bracket) - and specialist M&A firms, who provide M&A only advisory, generally to mid-market, select industries and SBEs.
Highly focused and specialized M&A advice investment banks are called boutique investment banks.

Motivation
Improving financial performance or reducing risk
The dominant rationale used to explain M&A activity is that acquiring firms seek improved financial performance or reduce risk. The following motives are considered to improve financial performance or reduce risk:

Economy of scale: This refers to the fact that the combined company can often reduce its fixed costs by removing duplicate departments or operations, lowering the costs of the company relative to the same revenue stream, thus increasing profit margins.
Economy of scope: This refers to the efficiencies primarily associated with demand-side changes, such as increasing or decreasing the scope of marketing and distribution, of different types of products.
Increased revenue or market share: This assumes that the buyer will be absorbing a major competitor and thus increase its market power (by capturing increased market share) to set prices.
Cross-selling: For example, a bank buying a stock broker could then sell its banking products to the stock broker's customers, while the broker can sign up the bank's customers for brokerage accounts. Or, a manufacturer can acquire and sell complementary products.
Synergy: For example, managerial economies such as the increased opportunity of managerial specialization. Another example is purchasing economies due to increased order size and associated bulk-buying discounts.
Taxation: A profitable company can buy a loss maker to use the target's loss as their advantage by reducing their tax liability. In the United States and many other countries, rules are in place to limit the ability of profitable companies to "shop" for loss making companies, limiting the tax motive of an acquiring company.
Geographical or other diversification: This is designed to smooth the earnings results of a company, which over the long term smoothens the stock price of a company, giving conservative investors more confidence in investing in the company. However, this does not always deliver value to shareholders (see below).
Resource transfer: resources are unevenly distributed across firms (Barney, 1991) and the interaction of target and acquiring firm resources can create value through either overcoming information asymmetry or by combining scarce resources.
Vertical integration: Vertical integration occurs when an upstream and downstream firm merge (or one acquires the other). There are several reasons for this to occur. One reason is to internalise an externality problem. A common example of such an externality is double marginalization. Double marginalization occurs when both the upstream and downstream firms have monopoly power and each firm reduces output from the competitive level to the monopoly level, creating two deadweight losses. After a merger, the vertically integrated firm can collect one deadweight loss by setting the downstream firm's output to the competitive level. This increases profits and consumer surplus. A merger that creates a vertically integrated firm can be profitable.
Hiring: some companies use acquisitions as an alternative to the normal hiring process. This is especially common when the target is a small private company or is in the startup phase. In this case, the acquiring company simply hires ("acquhires") the staff of the target private company, thereby acquiring its talent (if that is its main asset and appeal). The target private company simply dissolves and few legal issues are involved.
Absorption of similar businesses under single management: similar portfolio invested by two different mutual funds namely united money market fund and united growth and income fund, caused the management to absorb united money market fund into united growth and income fund.
Access to hidden or nonperforming assets (land, real estate).
Acquire innovative intellectual property. Nowadays, intellectual property has become one of the core competences for companies. Studies have shown that successful knowledge transfer and integration after a merger or acquisition has a positive impact to the firm's innovative capability and performance.
Killer Acquisitions: Incumbent firms may acquire innovative targets solely to discontinue the target's innovation projects and preempt future competition.
Exit Strategy: Some start-ups in technological and pharmaceutical industries explicitly cite a potential future acquisition as an "exit strategy" when seeking early VC funding. The potential for an acquisition therefore leads to higher levels of funding for risky or innovative projects.Megadeals—deals of at least one $1 billion in size—tend to fall into four discrete categories: consolidation, capabilities extension, technology-driven market transformation, and going private.

Other types
On average and across the most commonly studied variables, acquiring firms' financial performance does not positively change as a function of their acquisition activity. Therefore, additional motives for merger and acquisition that may not add shareholder value include:

Diversification: While this may hedge a company against a downturn in an individual industry it fails to deliver value, since it is possible for individual shareholders to achieve the same hedge by diversifying their portfolios at a much lower cost than those associated with a merger. (In his book One Up on Wall Street, Peter Lynch termed this "diworseification".)
Manager's hubris: manager's overconfidence about expected synergies from M&A which results in overpayment for the target company. The effect of manager's overconfidence on M&A has been shown to hold both for CEOs and board directors.
Empire-building: Managers have larger companies to manage and hence more power.
Manager's compensation: In the past, certain executive management teams had their payout based on the total amount of profit of the company, instead of the profit per share, which would give the team a perverse incentive to buy companies to increase the total profit while decreasing the profit per share (which hurts the owners of the company, the shareholders).

Different types
By functional roles in market
The M&A process itself is a multifaceted which depends upon the type of merging companies.

A horizontal merger is usually between two companies in the same business sector. An example of horizontal merger would be if a video game publisher purchases another video game publisher, for instance, Square Enix acquiring Eidos Interactive. This means that synergy can be obtained through many forms such as; increased market share, cost savings and exploring new market opportunities.
A vertical merger represents the buying of supplier of a business. In a similar example, if a video game publisher purchases a video game development company in order to retain the development studio's intellectual properties, for instance, Kadokawa Corporation acquiring FromSoftware. The vertical buying is aimed at reducing overhead cost of operations and economy of scale.
Conglomerate M&A is the third form of M&A process which deals the merger between two irrelevant companies. The relevant example of conglomerate M&A would be if a video game publisher purchases an animation studio, for instance, when Sega Sammy Holdings subsidized TMS Entertainment. The objective is often diversification of goods and services and capital investment.

By business outcome
The M&A process results in the restructuring of a business's purpose, corporate governance and brand identity.

A statutory merger is a merger in which the acquiring company survives and the target company dissolves. The purpose of this merger is to transfer the assets and capital of the target company into the acquiring company without having to maintain the target company as a subsidiary.
A consolidated merger is a merger in which an entirely new legal company is formed through combining the acquiring and target company. The purpose of this merger is to create a new legal entity with the capital and assets of the merged acquirer and target company. Both the acquiring  and target company are dissolved in the process.

Arm's length mergers
An arm's length merger is a merger: 

approved by disinterested directors and
approved by disinterested stockholders:″The two elements are complementary and not substitutes. The first element is important because the directors have the capability to act as effective and active bargaining agents, which disaggregated stockholders do not. But, because bargaining agents are not always effective or faithful, the second element is critical, because it gives the minority stockholders the opportunity to reject their agents' work. Therefore, when a merger with a controlling stockholder was: 1) negotiated and approved by a special committee of independent directors; and 2) conditioned on an affirmative vote of a majority of the minority stockholders, the business judgment standard of review should presumptively apply, and any plaintiff ought to have to plead particularized facts that, if true, support an inference that, despite the facially fair process, the merger was tainted because of fiduciary wrongdoing.″

Strategic mergers
A Strategic merger usually refers to long-term strategic holding of target (Acquired) firm. This type of M&A process aims at creating synergies in the long run by increased market share, broad customer base, and corporate strength of business. A strategic acquirer may also be willing to pay a premium offer to target firm in the outlook of the synergy value created after M&A process.

Acqui-hire
The term "acqui-hire" is used to refer to acquisitions where the acquiring company seeks to obtain the target company's talent, rather than their products (which are often discontinued as part of the acquisition so the team can focus on projects for their new employer). In recent years, these types of acquisitions have become common in the technology industry, where major web companies such as Facebook, Twitter, and Yahoo! have frequently used talent acquisitions to add expertise in particular areas to their workforces.

Merger of equals
Merger of equals is often a combination of companies of a similar size. Since 1990, there have been more than 625 M&A transactions announced as mergers of equals with a total value of US$2,164.4 bil. Some of the largest mergers of equals took place during the dot-com bubble of the late 1990s and in the year 2000: AOL and Time Warner (US$164 bil.), SmithKline Beecham and Glaxo Wellcome (US$75 bil.), Citicorp and Travelers Group (US$72 bil.). More recent examples this type of combinations are DuPont and Dow Chemical (US$62 bil.) and Praxair and Linde (US$35 bil.).

Research and statistics for acquired organizations
An analysis of 1,600 companies across industries revealed the rewards for M&A activity were greater for consumer products companies than the average company. For the period 2000–2010, consumer products companies turned in an average annual TSR of 7.4%, while the average for all companies was 4.8%.
Given that the cost of replacing an executive can run over 100% of his or her annual salary, any investment of time and energy in re-recruitment will likely pay for itself many times over if it helps a business retain just a handful of key players that would have otherwise left.
Organizations should move rapidly to re-recruit key managers. It's much easier to succeed with a team of quality players that one selects deliberately rather than try to win a game with those who randomly show up to play.

Brand considerations
Mergers and acquisitions often create brand problems, beginning with what to call the company after the transaction and going down into detail about what to do about overlapping and competing product brands. Decisions about what brand equity to write off are not inconsequential. And, given the ability for the right brand choices to drive preference and earn a price premium, the future success of a merger or acquisition depends on making wise brand choices. Brand decision-makers essentially can choose from four different approaches to dealing with naming issues, each with specific pros and cons:
Keep one name and discontinue the other. The strongest legacy brand with the best prospects for the future lives on. In the merger of United Airlines and Continental Airlines, the United brand will continue forward, while Continental is retired.
Keep one name and demote the other. The strongest name becomes the company name and the weaker one is demoted to a divisional brand or product brand. An example is Caterpillar Inc. keeping the Bucyrus International name.
Keep both names and use them together. Some companies try to please everyone and keep the value of both brands by using them together. This can create an unwieldy name, as in the case of PricewaterhouseCoopers, which has since changed its brand name to "PwC".
Discard both legacy names and adopt a totally new one. The classic example is the merger of Bell Atlantic with GTE, which became Verizon Communications. Not every merger with a new name is successful. By consolidating into YRC Worldwide, the company lost the considerable value of both Yellow Freight and Roadway Corp.The factors influencing brand decisions in a merger or acquisition transaction can range from political to tactical. Ego can drive choice just as well as rational factors such as brand value and costs involved with changing brands.Beyond the bigger issue of what to call the company after the transaction comes the ongoing detailed choices about what divisional, product and service brands to keep. The detailed decisions about the brand portfolio are covered under the topic brand architecture.

History
Most histories of M&A begin in the late 19th century United States. However, mergers coincide historically with the existence of companies. In 1708, for example, the East India Company merged with an erstwhile competitor to restore its monopoly over the Indian trade. In 1784, the Italian Monte dei Paschi and Monte Pio banks were united as the Monti Reuniti. In 1821, the Hudson's Bay Company merged with the rival North West Company.

The Great Merger Movement: 1895–1905
The Great Merger Movement was a predominantly U.S. business phenomenon that happened from 1895 to 1905. During this time, small firms with little market share consolidated with similar firms to form large, powerful institutions that dominated their markets, such as the Standard Oil Company, which at its height controlled nearly 90% of the global oil refinery industry. It is estimated that more than 1,800 of these firms disappeared into consolidations, many of which acquired substantial shares of the markets in which they operated. The vehicle used were so-called trusts. In 1900 the value of firms acquired in mergers was 20% of GDP. In 1990 the value was only 3% and from 1998 to 2000 it was around 10–11% of GDP. Companies such as DuPont, U.S. Steel, and General Electric that merged during the Great Merger Movement were able to keep their dominance in their respective sectors through 1929, and in some cases today, due to growing technological advances of their products, patents, and brand recognition by their customers. There were also other companies that held the greatest market share in 1905 but at the same time did not have the competitive advantages of the companies like DuPont and General Electric. These companies such as International Paper and American Chicle saw their market share decrease significantly by 1929 as smaller competitors joined forces with each other and provided much more competition. The companies that merged were mass producers of homogeneous goods that could exploit the efficiencies of large volume production. In addition, many of these mergers were capital-intensive. Due to high fixed costs, when demand fell, these newly merged companies had an incentive to maintain output and reduce prices. However more often than not mergers were "quick mergers". These "quick mergers" involved mergers of companies with unrelated technology and different management. As a result, the efficiency gains associated with mergers were not present. The new and bigger company would actually face higher costs than competitors because of these technological and managerial differences. Thus, the mergers were not done to see large efficiency gains, they were in fact done because that was the trend at the time. Companies which had specific fine products, like fine writing paper, earned their profits on high margin rather than volume and took no part in the Great Merger Movement.

Short-run factors
One of the major short run factors that sparked the Great Merger Movement was the desire to keep prices high. However, high prices attracted the entry of new firms into the industry.
A major catalyst behind the Great Merger Movement was the Panic of 1893, which led to a major decline in demand for many homogeneous goods. For producers of homogeneous goods, when demand falls, these producers have more of an incentive to maintain output and cut prices, in order to spread out the high fixed costs these producers faced (i.e. lowering cost per unit) and the desire to exploit efficiencies of maximum volume production. However, during the Panic of 1893, the fall in demand led to a steep fall in prices.
Another economic model proposed by Naomi R. Lamoreaux for explaining the steep price falls is to view the involved firms acting as monopolies in their respective markets. As quasi-monopolists, firms set quantity where marginal cost equals marginal revenue and price where this quantity intersects demand. When the Panic of 1893 hit, demand fell and along with demand, the firm's marginal revenue fell as well. Given high fixed costs, the new price was below average total cost, resulting in a loss. However, also being in a high fixed costs industry, these costs can be spread out through greater production (i.e. higher quantity produced). To return to the quasi-monopoly model, in order for a firm to earn profit, firms would steal part of another firm's market share by dropping their price slightly and producing to the point where higher quantity and lower price exceeded their average total cost. As other firms joined this practice, prices began falling everywhere and a price war ensued.One strategy to keep prices high and to maintain profitability was for producers of the same good to collude with each other and form associations, also known as cartels. These cartels were thus able to raise prices right away, sometimes more than doubling prices. However, these prices set by cartels provided only a short-term solution because cartel members would cheat on each other by setting a lower price than the price set by the cartel. Also, the high price set by the cartel would encourage new firms to enter the industry and offer competitive pricing, causing prices to fall once again. As a result, these cartels did not succeed in maintaining high prices for a period of more than a few years. The most viable solution to this problem was for firms to merge, through horizontal integration, with other top firms in the market in order to control a large market share and thus successfully set a higher price.

Long-run factors
In the long run, due to desire to keep costs low, it was advantageous for firms to merge and reduce their transportation costs thus producing and transporting from one location rather than various sites of different companies as in the past. Low transport costs, coupled with economies of scale also increased firm size by two- to fourfold during the second half of the nineteenth century. In addition, technological changes prior to the merger movement within companies increased the efficient size of plants with capital intensive assembly lines allowing for economies of scale. Thus improved technology and transportation were forerunners to the Great Merger Movement. In part due to competitors as mentioned above, and in part due to the government, however, many of these initially successful mergers were eventually dismantled. The U.S. government passed the Sherman Act in 1890, setting rules against price fixing and monopolies. Starting in the 1890s with such cases as Addyston Pipe and Steel Company v. United States, the courts attacked large companies for strategizing with others or within their own companies to maximize profits. Price fixing with competitors created a greater incentive for companies to unite and merge under one name so that they were not competitors anymore and technically not price fixing.
The economic history has been divided into Merger Waves based on the merger activities in the business world as:

Objectives in more recent merger waves
During the third merger wave (1965–1989), corporate marriages involved more diverse companies. Acquirers more frequently bought into different industries. Sometimes this was done to smooth out cyclical bumps, to diversify, the hope being that it would hedge an investment portfolio.
Starting in the fifth merger wave (1992–1998) and continuing today, companies are more likely to acquire in the same business, or close to it, firms that complement and strengthen an acquirer's capacity to serve customers.
In recent decades however, cross-sector convergence has become more common. For example, retail companies are buying tech or e-commerce firms to acquire new markets and revenue streams. It has been reported that convergence will remain a key trend in M&A activity through 2015 and onward.
Buyers are not necessarily hungry for the target companies' hard assets. Some are more interested in acquiring thoughts, methodologies, people and relationships. Paul Graham recognized this in his 2005 essay "Hiring is Obsolete", in which he theorizes that the free market is better at identifying talent, and that traditional hiring practices do not follow the principles of free market because they depend a lot upon credentials and university degrees. Graham was probably the first to identify the trend in which large companies such as Google, Yahoo! or Microsoft were choosing to acquire startups instead of hiring new recruits, a process known as acqui-hiring.
Many companies are being bought for their patents, licenses, market share, name brand, research staff, methods, customer base, or culture. Soft capital, like this, is very perishable, fragile, and fluid. Integrating it usually takes more finesse and expertise than integrating machinery, real estate, inventory and other tangibles.
Douma and Schreuder (2013) discuss the factors driving the sixth merger wave as follows: Globalization: Many companies found that they were increasingly confronted with global competition. They aspired to obtain global leadership in their business areas. This was also fostered by the lowering of trade barriers as a result of the cooperation in the World Trade Organisation (WTO). Therefore, industry consolidation took place not only at the national or regional levels but also on an international scale. The number of cross-border and inter-continental deals has been steadily rising. Some countries contributed to this trend by encouraging their national champions to become
‘global champions'.
Strong cash flows: After the slump in the years 2001 – 2003, the global economy showed a good performance, generating strong cash flows and healthy balance sheets for many companies. Confidence was rising that this positive development would continue for some time. Strong demand from emerging economies, such as China and India, boosted consumption of many products. Commodity prices were high and companies tried to secure their resource base. Even rather mature industries, such as steel, benefited from this trend. As a result, the steel industry witnessed its own international merger wave with Mittal (India) buying Arcelor (Europe) and Tata (India) buying Corus (Europe)
Private equity: Financing was relatively cheap (in fact too cheap, we would find out later)and private equity funds, among others, used such sources to pursue ever more and larger deals. In 2007, Blackstone bought Equity Office Properties for $38.9 billion, whereas the ownership of Energy Future Holdings passed to KKR, TPG and Goldman Sachs for $44.4 billion. In many industries, about 25 – 30 per cent of all deals in late 2006 and early 2007 were done by private equity players. Favourable debt markets enabled these acquirers to finance their deals with high leverage.
Hedge funds and ‘shareholder activism': In 2007, after acquiring 1 per cent of the shares of major Dutch bank ABN AMRO, the British hedge fund TCI led an attack demanding the bank split up or sell to the highest bidder to produce shareholder value. ABN AMRO was ultimately split and sold to Royal Bank of Scotland (RBS), Fortis, and Banco Santander for nearly $100 billion. The takeover was ill-timed and unsuccessful and was a major contributing factor in the downfall of both RBS and Fortis when the credit crisis of 2008 struck. Hedge funds such as TCI specialized in such ‘event-driven strategies' that allowed them to make handsome profits on their acquired stakes.
The sixth merger wave came to an end when the financial-economic crisis started in 2007, triggered by the bursting bubble of American housing market (financed by very questionable ‘sub- prime'  loans) and accelerated by the collapse of the American investment bank Lehman Brothers in 2008. The ripple effects of these events spread throughout the global banking system and financial markets, leading to the worst financial crisis and economic recession since the Great Depression of the 1930s.
In the seventh merger wave with its peak in 2014/5 many of the factors driving the previous merger wave (such as globalization) remained operative, while specific factors driving the seventh merger wave included (1) deals by American firms aimed at lowering their tax base, for example by moving to Ireland after an acquisition of an Irish company, (2) company spin-offs, like at HP which finally split itself in two companies or at eBay which spun off PayPal. In addition,  the monetary policy pursued by the central banks, which has been called ‘quantitative easing' played a major role. This monetary policy increased money supply and lowered interest rates substantially. Thus, the amount of cash available for M&A activities remained substantial, whereas the cost of borrowing was low. As a result, both companies and private equity (10–15 per cent of deals) remained able to do transactions.

Largest deals in history
The top ten largest deals in M&A history cumulate to a total value of 1,118,963 mil. USD. (1.118 tril. USD).

Cross-border
Introduction
In a study conducted in 2000 by Lehman Brothers, it was found that, on average, large M&A deals cause the domestic currency of the target corporation to appreciate by 1% relative to the acquirer's local currency. Until 2018, around 280,472 cross-border deals have been conducted, which cumulates to a total value of almost 24,069 bil. USD.The rise of globalization has exponentially increased the necessity for agencies such as the Mergers and Acquisitions International Clearing (MAIC), trust accounts and securities clearing services for Like-Kind Exchanges for cross-border M&A. On a global basis, the value of cross-border mergers and acquisitions rose seven-fold during the 1990s. In 1997 alone, there were over 2,333 cross-border transactions, worth a total of approximately $298 billion.
The vast literature on empirical studies over value creation in cross-border M&A is not conclusive, but points to higher returns in cross-border M&As compared to domestic ones when the acquirer firm has the capability to exploit resources and knowledge of the target's firm and of handling challenges.
In China, for example, securing regulatory approval can be complex due to an extensive group of various stakeholders at each level of government. In the United Kingdom, acquirers may face pension regulators with significant powers, in addition to an overall M&A environment that is generally more seller-friendly than the U.S. Nonetheless, the current surge in global cross-border M&A has been called the "New Era of Global Economic Discovery".In little more than a decade, M&A deals in China increased by a factor of 20, from 69 in 2000 to more than 1,300 in 2013.
In 2014, Europe registered its highest levels of M&A deal activity since the financial crisis. Driven by U.S. and Asian acquirers, inbound M&A, at $320.6 billion, reached record highs by both deal value and deal count since 2001.
Approximately 23 percent of the 416 M&A deals announced in the U.S. M&A market in 2014 involved non-U.S. acquirers.
For 2016, market uncertainties, including Brexit and the potential reform from a U.S. presidential election, contributed to cross-border M&A activity lagging roughly 20% behind 2015 activity.
In 2017, the controverse trend which started in 2015, decreasing total value but rising total number of cross border deals, kept going. Compared on a year on year basis (2016-2017), the total number of cross border deals decreased by -4.2%, while cumulated value increased by 0.6%.Even mergers of companies with headquarters in the same country can often be considered international in scale and require MAIC custodial services. For example, when Boeing acquired McDonnell Douglas, the two American companies had to integrate operations in dozens of countries around the world (1997). This is just as true for other apparently "single-country" mergers, such as the 29 billion-dollar merger of Swiss drug makers Sandoz and Ciba-Geigy (now Novartis).

In emerging countries
M&A practice in emerging countries differs from more mature economies, although transaction management and valuation tools (e.g. DCF, comparables) share a common basic methodology. In China, India or Brazil for example, differences affect the formation of asset price and on the structuring of deals. Profitability expectations (e.g. shorter time horizon, no terminal value due to low visibility) and risk represented by a discount rate must both be properly adjusted. In a M&A perspective, differences between emerging and more mature economies include: i) a less developed system of property rights, ii) less reliable financial information, iii) cultural differences in negotiations, and iv) a higher degree of competition for the best targets.

Property rights: the capacity to transfer property rights and legally enforce the protection of such rights after payment may be questionable. Property transfer through the Stock Purchase Agreement can be imperfect (e.g. no real warranties) and even reversible (e.g. one of the multiple administrative authorizations needed not granted after closing) leading to situations where costly remedial actions may be necessary. When the rule of law is not established, corruption can be a rampant problem.
Information: documentation delivered to a buyer may be scarce with a limited level of reliability. As an example, double sets of accounting are common practice and blur the capacity to form a correct judgment. Running valuation on such basis bears the risk to lead to erroneous conclusions. Therefore, building a reliable knowledge base on observable facts and on the result of focused due diligences, such as recurring profitability measured by EBITDA, is a good starting point.
Negotiation: "Yes" may not be synonym that the parties have reached an agreement. Getting immediately to the point may not be considered appropriate in some cultures and even considered rude. The negotiations may continue to the last minute, sometimes even after the deal has been officially closed, if the seller keeps some leverage, like a minority stake, in the divested entity. Therefore, establishing a strong local business network before starting acquisitions is usually a prerequisite to get to know trustable parties to deal with and have allies.
Competition: the race to acquire the best companies in an emerging economy can generate a high degree of competition and inflate transaction prices, as a consequence of limited available targets. This may push for poor management decisions; before investment, time is always needed to build a reliable set of information on the competitive landscape.If not properly dealt with, these factors will likely have adverse consequences on return-on-investment (ROI) and create difficulties in day-to-day business operations. It is advisable that M&A tools designed for mature economies are not directly used in emerging markets without some adjustment. M&A teams need time to adapt and understand the key operating differences between their home environment and their new market.

Failure
Despite the goal of performance improvement, results from mergers and acquisitions (M&A) are often disappointing compared with results predicted or expected. Numerous empirical studies show high failure rates of M&A deals. Studies are mostly focused on individual determinants. A book by Thomas Straub (2007) "Reasons for frequent failure in Mergers and Acquisitions" develops a comprehensive research framework that bridges different perspectives and promotes an understanding of factors underlying M&A performance in business research and scholarship. The study should help managers in the decision making process. The first important step towards this objective is the development of a common frame of reference that spans conflicting theoretical assumptions from different perspectives. On this basis, a comprehensive framework is proposed with which to understand the origins of M&A performance better and address the problem of fragmentation by integrating the most important competing perspectives in respect of studies on M&A. Furthermore, according to the existing literature, relevant determinants of firm performance are derived from each dimension of the model. For the dimension strategic management, the six strategic variables: market similarity, market complementarities, production operation similarity, production operation complementarities, market power, and purchasing power were identified as having an important effect on M&A performance. For the dimension organizational behavior, the variables acquisition experience, relative size, and cultural differences were found to be important. Finally, relevant determinants of M&A performance from the financial field were acquisition premium, bidding process, and due diligence. Three different ways in order to best measure post M&A performance are recognized: synergy realization, absolute performance, and finally relative performance.
Employee turnover contributes to M&A failures. The turnover in target companies is double the turnover experienced in non-merged firms for the ten years after the merger.M&As involving small businesses are particularly problematic and have been found to take longer and cost more than expected with organisation cultural and effective communication with employees being key determinants of success and failure Many M&A fail due to lack of planning or execution of the plan. An empirical research study conducted between 1988 and 2002 found that "Successful acquisitions, as defined by return on investment and time to market, are more likely to involve complex products but minimal uncertainty about whether the product is functional and whether there is an appetite in the market." But failed mergers and acquisitions are caused by "hasty purchases where information platforms between companies were incompatible and the product was not yet tested for release." A recommendation to resolve these failed mergers is to wait for the product to become established in the market and research has been completed.
Deloitte determines most companies do not do their due diligence in determining whether a M&A is the correct move due to these four reasons:

Timing
Cost
Existing knowledge of the industry
Do not see the value in due diligenceTransactions that undergo a due diligence process are more likely to be successful. A considerable body of research suggests that many mergers fail due to human factors such as issues with trust between employees of the two organizations or trust between employees and their leaders.Any M&A transaction, no matter the size or structure, can have a significant impact on the acquiring company. Developing and implementing a robust due diligence process can lead to a much better assessment of the risks and potential benefits of a transaction, enable the renegotiation of pricing and other key terms, and smooth the way towards a more effective integration.M&A can hinder innovation by mismanagement or cultural differences between companies. They can also create bottlenecks when they disrupt the flow of innovation with too many company policies and procedures. Market dominant companies can also be their own demise when presented with an M&A opportunity. Complacency and lack of due diligence may cause the market dominant company to miss the value of a innovative product or service.

See also
References
Further reading
Denison, Daniel, Hooijberg, Robert, Lane, Nancy, Lief, Colleen, (2012). Leading Culture Change in Global Organizations. "Creating One Culture Out of Many", chapter 4. San Francisco: Jossey-Bass. ISBN 9780470908846
Aharon, David Y.; Gavious, Ilanit; Yosef, Rami (2010). "Stock market bubble effects on mergers and acquisitions". The Quarterly Review of Economics and Finance. 50 (4): 456–470. doi:10.1016/j.qref.2010.05.002.
Beech, G. and Thayser, D. (2015). Valuations, Mergers and Acquisitions. Oxford: Oxford University Press. ISBN 978-0-585-13223-5.{{cite book}}:  CS1 maint: multiple names: authors list (link)
Cartwright, Susan; Schoenberg, Richard (2006). "Thirty Years of Mergers and Acquisitions Research: Recent Advances and Future Opportunities". British Journal of Management. 17 (S1): S1–S5. doi:10.1111/j.1467-8551.2006.00475.x. hdl:1826/3570. S2CID 154230290.
Bartram, Söhnke M.; Burns, Natasha; Helwege, Jean (June 2013). "Foreign Currency Exposure and Hedging: Evidence from Foreign Acquisitions". Quarterly Journal of Finance. 3 (2): 1–20. CiteSeerX 10.1.1.580.8086. doi:10.1142/S2010139213500109. SSRN 1116409.
Close, John Weir (2013-10-15). A Giant Cow-tipping by Savages: The Boom, Bust, and Boom Culture of M&A. New York: Palgrave Macmilla. ISBN 9780230341814. OCLC 828246072.
Coispeau, Olivier; Luo, Stephane (2015). Mergers & Acquisitions and Partnerships in China. Singapore: World Scientific. p. 311. ISBN 9789814641029. OCLC 898052215.
DePamphilis, Donald (2008). Mergers, Acquisitions, and Other Restructuring Activities. New York: Elsevier, Academic Press. p. 740. ISBN 978-0-12-374012-0.
Douma, Sytse & Hein Schreuder (2013). "Economic Approaches to Organizations", chapter 13. 5th edition. London: Pearson. ISBN 0273735292 ISBN 9780273735298 See also: https://www.academia.edu/93596473/Economic_approaches_to_mergers_and_acquisitions_PDF_
Fleuriet, Michel (2008). Investment Banking explained: An insider's guide to the industry. New York, NY: McGraw Hill. ISBN 978-0-07-149733-6.
Harwood, I. A. (2006). "Confidentiality constraints within mergers and acquisitions: gaining insights through a 'bubble' metaphor". British Journal of Management. 17 (4): 347–359. doi:10.1111/j.1467-8551.2005.00440.x. S2CID 154600685.
Locke, Bryan; Singh, Harsh; Chung, Joanna; Ferguson, John J. "Selling Acquisitions to Institutional Investors, Proxy Handlers, Regulators, and the Financial Media". Transaction Advisors. ISSN 2329-9134.
"The 2017 M&A Fee Guide". Firmex & Divestopedia. 2017-10-11.
Popp, Karl Michael (2013). Mergers and Acquisitions in the Software Industry - foundations of due diligence. Norderstedt: Books on demand. ISBN 978-3-7322-4381-5.
Popp, Karl (2020). Automation of the M&A process: due diligence tasks and automation. Norderstedt: Books on Demand. ISBN 978-3-750-46205-2.
Reddy, Kotapati Srinivasa; Nangia, Vinay Kumar; Agrawal, Rajat (2014). "The 2007–2008 Global Financial Crisis, and Cross-border Mergers and Acquisitions". Global Journal of Emerging Market Economies. 6 (3): 257–281. doi:10.1177/0974910114540720. S2CID 59268938.
Reddy, K.S.; Nangia, V.K.; Agrawal, R. (2013). "Indian economic-policy reforms, bank mergers, and lawful proposals: The ex-ante and ex-post 'lookup'". Journal of Policy Modeling. 35 (4): 601–622. doi:10.1016/j.jpolmod.2012.12.001.
Reddy, K.S.; Agrawal, R.; Nangia, V.K. (2013). "Reengineering, crafting and comparing business valuation models-the advisory exemplar". International Journal of Commerce and Management. 23 (3): 216–241. doi:10.1108/IJCoMA-07-2011-0018.
Reifenberger, Sabine (28 December 2012). M&A Market: The New Normal. CFO Insight
Rosenbaum, Joshua; Joshua Pearl (2009). Investment Banking: Valuation, Leveraged Buyouts, and Mergers & Acquisitions. Hoboken, NJ: John Wiley & Sons. ISBN 978-0-470-44220-3.
Scott, Andy (2008). China Briefing: Mergers and Acquisitions in China (2nd ed.).
Straub, Thomas (2007). Reasons for frequent failure in Mergers and Acquisitions: A comprehensive analysis. Wiesbaden: Deutscher Universitäts-Verlag (DUV), Gabler Edition Wissenschaft. ISBN 978-3-8350-0844-1.
Patel, Kison (2019). Agile M&A:Proven Techniques to Close Deals Faster and Maximize Value. United States: Dealroom Incorporated. ISBN 978-1733474511.

Mesabi Range

The Mesabi Iron Range is a mining district in northeastern Minnesota following an elongate trend containing large deposits of iron ore. It is the largest of four major iron ranges in the region collectively known as the Iron Range of Minnesota. First described in 1866, it is the chief iron ore mining district in the United States. The district is located largely in Itasca and Saint Louis counties. It has been extensively worked since 1892, and has seen a transition from high-grade direct shipping ores through gravity concentrates to the current industry exclusively producing iron ore (taconite) pellets. Production has been dominantly controlled by vertically integrated steelmakers since 1901, and therefore is dictated largely by US ironmaking capacity and demand.

Name
The Mesabi Range was known to the local Ojibwe as Misaabe-wajiw ('Giant mountain'). Throughout the Mesabi Range, Mesaba and Missabe spelling variations are found along with places containing Giant in their names.

Geology
There are three iron ranges in northern Minnesota, the Cuyuna, the Vermilion, and the Mesabi.  Most of the world's iron ore, including that contained in northern Minnesota, was formed during the middle Precambrian. During this period, erosion leveled mountains. This erosion released iron and silica into the waters of a new sea. Marine algae living in this new sea raised the level of atmospheric oxygen. This oxygen catastrophe caused the eroded iron to precipitate into the banded iron formations found in northern Minnesota and other members of the Animikie Group. Over billions of years, geological forces left behind ore deposits of varied quality and concentrations – differences that would determine how the ore was mined from place to place. On the Mesabi Range, stretching 100 miles (160 km) from Grand Rapids to Babbitt, soft ore lay close to the surface, where it could be scooped from open pit mines.The overall structure of the range is that of a monocline dipping 5 to 15 degrees to the southeast.  Key faults include the Calumet, La Rue, Morton, Biwabik, and the Siphon. The Duluth Gabbro complex to the east has caused metamorphic changes in the Biwabik formation. The natural iron ores and the magnetite taconites occur in this Precambrian Biwabik formation, which is a cherty layer 340–750 feet (100–230 m) thick. The natural ores are located in elongated channels or tabular deposits, while the magnetite taconites occur in stratigraphic zones. Natural ores have an iron content of 51 to 57 per cent while the taconites are 30 to 35 percent iron, and are beneficiated to pellets contain 60 to 67 per cent. The natural ores are mainly mixtures of hematite and goethite.: 519–520, 522, 527–528   The most common silicate is Minnesotaite.  Also of note are the presence of algal structures in the Biwabik formation.

Physical extent
The Mesabi Range is 110 miles (180 km) long. Heights vary from 200–500 feet (61–152 m). The highest point, located about 5.6 miles (9.0 km) northeast of Virginia, is Pike Mountain at 1,950 feet (590 m). The range trends from the northeast to the southwest, extending from Babbitt to Grand Rapids.

Embarrass Mountains
The Embarrass Mountains are a small subrange of the Mesabi Range, spanning about 9 miles (14 km) through northern White Township and Hoyt Lakes in St. Louis County. Heights vary from 200–400 feet (61–122 m). The highest point, at 1,940 feet (590 m), is roughly 1.9 miles (3.1 km) west of the unincorporated community of Hinsdale, near the former Erie Mining Company's pits and taconite processing plant.

Mining operations
Iron-bearing rocks were noted by the Minnesota State Geologist Henry H. Eames in 1866. Iron ore was discovered north of Mountain Iron, Minnesota on 16 Nov. 1890 by J. A. Nichols of the Merritt brothers. The range was defined by 1900. Initially underground mines were employed but these gave way to open pits so that by 1902, half the operations were conducted this way. The last underground mine closed in 1960. Natural ores eventually gave way to iron-ore concentrates from magnetite taconite so that by 1965 one third of production came from these pellets.Iron ore is currently mined only from open pits, although some mines operated underground early on.Much of the softer ore was formed close to the surface, allowing mining operations to be conducted via open pit mines. The world's largest open pit iron ore mine is the Hull–Rust–Mahoning Open Pit Iron Mine in Hibbing. In the early years of mining from the late 19th century until the 1950s, mining focus was on high grade ore that could be processed into steel without much change. However, when that supply dried up, focus shifted to lower-grade ore (taconite) which requires extensive processing at large mining-processing facilities before moving to port. The mined ore is then transported, primarily by the Duluth, Missabe and Iron Range Railway, to the ports of Two Harbors and Duluth. At Duluth, trains of up to eighty 100-ton open cars are moved out on massive ore docks to be dumped into "lakers" of up to 60,000 tons weight for movement to steel mills in Indiana and Ohio.
Dormant and exhausted open pit mines are a common feature along the Iron Range. Some of these sites have been redeveloped for other uses. For instance, the Virginia Pilot is a project which focuses on redeveloping the grounds adjacent to the old mines into low- to moderate-income residential space. The Hill-Annex Mine is now a state park and offers tours to visitors who wish to learn about mine operations. Tours are guided by former mine workers.
Currently, there are six mining-processing facilities in operation on the Iron Range. Cleveland-Cliffs Inc. owns and operates Northshore Mining, which has mining operations in Babbitt and crushing, concentrating (grinding) and pelletizing operations in Silver Bay, along with United Taconite which has mining operations in Eveleth and crushing, concentrating and pelletizing operations in Forbes. Arcelor Mittal owns and operates the Minorca Mine and Plant with mining operations near Biwabik and Gilbert and a crushing, concentrating and pelletizing facility near Virginia (47.5428°N 92.5169°W﻿ / 47.5428; -92.5169). United States Steel owns and operates both KeeTac (47.3992°N 93.0759°W﻿ / 47.3992; -93.0759) and Minntac (47.49730°N 92.61401°W﻿ / 47.49730; -92.61401) with mining and processing facilities in Keewatin and Mountain Iron respectively. The last facility is Hibbing Taconite which operates a mine and plant between the cities of Hibbing and Chisholm. Hibbing Taconite is majority owned and managed by ArcelorMittal USA, with minority stakes held by Cleveland-Cliffs and United States Steel.
In addition, Mesabi Metallics (controlled by Essar Steel) is constructing a mine/plant near Nashwauk that has plans mine and process the taconite. Steel Dynamics and Kobe Steel formerly owned and operated Mesabi Nugget (47.5280°N 92.1216°W﻿ / 47.5280; -92.1216) near Hoyt Lakes which did not mine its own material, but produced high-iron content nuggets from purchased iron ore concentrate. Magnetation, Inc. formerly produced iron ore concentrate reclaimed from tailings with company-designed high-power magnetic separators to produce concentrate to sell and ship throughout the world.

Rockefellers' interests
John D. Rockefeller had previously loaned money to his brother, Frank Rockefeller, and Frank's business associate, James Corrigan, to buy into the Franklin Iron Mine Company, which operated in the Mesabi Range. By late 1896 or early 1897, John D. took Corrigan's shares due to failure to repay loans. Frank and Carrigan were forced to sell the company. The market for ore from the Mesabi range was almost non-existent at this time because American steel furnaces were not built to deal with its powdery nature and steelmakers believed it to be poor ore. John D. invested $40 million to build up the Mesabi ore and transportation business. To reach the steelmakers in Pittsburgh, the ore had to travel across the Great Lakes to Cleveland. He invested $2 million into a railroad to transport the ore from the Mesabi range to Duluth on Lake Superior. By 1896, he controlled the Lake Superior Consolidated Iron Mines Company, which was a holding company of the Merritt brothers. He built a fleet of ore ships. In December 1896, he made a deal with Henry W. Oliver and Andrew Carnegie of Pittsburgh whereby they agreed not to go into the ore field or transportation business, and John D. agreed not to go into the steel business. The steelmakers adapted their mills to process the ore from the Mesabi range. Oliver broke with the agreement, and in response, John D. procured a monopoly of ore ship transportation on the Great Lakes. John D. sold his western ore holdings to J. P. Morgan for $90,900,000 soon after Morgan bought Carnegie's steel interests in 1901.

Labor strikes
Several large-scale strikes took place on the Mesabi Iron Range during the early 1900s. The first began on July 20, 1907 after the Western Federation of Miners (WFM) asked Oliver Iron Mining Company for, among other demands, an eight-hour work day and a pay raise. The strike lasted two months and resulted in thousands of workers being blacklisted.On June 25, 1916, a miner left his shift after being paid less than the contracted rate. His action led to the Mesabi range strike of 1916. The Industrial Workers of the World quickly supported the strike for better pay and shorter hours. In September 1916, the workers voted to resume work, assuming a failed strike. However, shortly after returning to work a 10% raise in wages was issued for workers throughout the Range.

See also
Cliffs Shaft Mine Museum
Cuyuna Range
Gogebic Range
Gunflint Range
Hill-Annex Mine State Park
Iron Mountain Central Historic District
Marquette Iron Range
Rouchleau Mine
Soudan Underground Mine State Park
Vermilion Range (Minnesota)

Citations
General and cited sources
Leith, Charles Kenneth (1903). The Mesabi Iron-bearing District of Minnesota. U.S. Geological Survey Monograph 43. Washington, D.C.: United States Government Printing Office.
Stacy, Francis N. (September 1904). "The Iron Mines That Give Us Leadership: The Most Extraordinary Deposits in the World in the Mesabi Range". The World's Work: A History of Our Time. VIII: 5235–5243. Retrieved July 10, 2009. Includes numerous photos of c. 1904 Mesabi iron works.

Further reading
Beck, J. Robert (2005). Well, Here We Are! The Hansons and the Becks. Lincoln, Nebraska: iUniverse. A history of a Swedish-Finnish immigrant family from the Mesabi Iron Range, which details the social (and socialist) conditions of the area during its heyday.
George, Harrison (December 1916). "The Mesaba Iron Range". International Socialist Review. vol. 17, no. 6, pp. 329–332.
George, Harrison (January 1917). "Victory on the Mesaba Range". International Socialist Review. vol. 17, no. 7, pp. 429–431.
Hawke, David Freeman (198). John D.: The Founding Father of the Rockefellers. New York: Harper & Row. ISBN 978-0060118136.

External links
Hill Annex Mine State Park: Minnesota DNR

Methods of production

Production methods fall into three main categories: job (one-off production), batch (multiple items, one step at a time for all items), and flow

Job production
Job production is used when a product is produced with the labor of one or few workers and is rarely used for bulk and large scale production. It is mainly used for one-off products or prototypes (hence also known as Prototype Production), as it is inefficient; however, quality is greatly enhanced with job production compared to other methods. Individual wedding cakes and made-to-measure suits are examples of job production. New small firms often use job production before they get a chance or have the means to expand. Job Production is highly motivating for workers because it gives the workers an opportunity to produce the whole product and take pride in it.

Batch production
Batch production is the method used to produce or process any product of the groups or batches where the products in the batch go through the whole production process together. An example would be when a bakery produces each different type of bread separately and each product (in this case, bread) is not produced continuously. Batch production is used in many different ways and is most suited to when there is a need for a quality/quantity balance. This technique is probably the most commonly used method for organizing manufacture and promotes specialist labor, as very often batch production involves a small number of persons. Batch production occurs when many similar items are produced together. Each batch goes through one stage of the production before moving onto the next stage.

Flow production
Flow production (mass production) is also a very common method of production. Flow production is when the product is built up through many segregated stages; the product is built upon at each stage and then passed directly to the next stage where it is built upon again. The production method is financially the most efficient and effective because there is less of a need for skilled workers.

Lean Production
Contrary to job production, the method Boutique Manufacturing (Lean) is suitable for the production of very small to small batches, i.e. orders of a few units up to several dozens of similar or equal goods. The workflow organization of a Boutique Manufacturing entity can be a mixture of both jobbing and batch production but involves higher standardization than job production. Boutique Manufacturing is often organized with single workplaces or production cells carrying out a number of subsequent production steps until completion of certain components or even the whole product; large assembly lines are generally not used. The flexibility and variety of products able to be produced in the entity therefore are much higher than with the more standardized method of batch production.

See also
Computer Aided Design
Computer Aided Manufacture
Processing mode

External links
Methods of Production

Motorola

Motorola, Inc. () was an American multinational telecommunications company based in Schaumburg, Illinois. It was founded in 1928 as Galvin Manufacturing Corporation by brothers Paul and Joseph Galvin. The company changed its name to Motorola in 1947. After having lost $4.3 billion from 2007 to 2009, the company split into two independent public companies, Motorola Mobility and Motorola Solutions, on January 4, 2011. The reorganization was structured with Motorola Solutions legally succeeding Motorola, Inc., and Motorola Mobility being spun off.Motorola designed and sold wireless network equipment such as cellular transmission base stations and signal amplifiers. Motorola's home and broadcast network products included set-top boxes, digital video recorders, and network equipment used to enable video broadcasting, computer telephony, and high-definition television. Its business and government customers consisted mainly of wireless voice and broadband systems (used to build private networks), and public safety communications systems like Astro and Dimetra. These businesses, except for set-top boxes and cable modems, became part of Motorola Solutions.
Motorola's wireless telephone handset division was a pioneer in cellular telephones. Also known as the Personal Communication Sector (PCS) prior to 2004, it pioneered the "mobile phone" with DynaTAC, "flip phone" with the MicroTAC as well as the "clam phone" with the StarTAC in the mid-1990s. It had staged a resurgence by the mid-2000s with the RAZR, but lost market share in the second half of that decade. Later it focused on smartphones using Google's open-source Android mobile operating system. The first phone to use Android 2.0 "Eclair", the Motorola Droid, was released in 2009 (the GSM version launched a month later, in Europe, as the Motorola Milestone). The handset division, along with the cable set-top box and modem businesses, were later spun off into Motorola Mobility.

History
Motorola was founded in Chicago, Illinois, as Galvin Manufacturing Corporation (at 847 West Harrison Street) in 1928.
Paul Galvin wanted a brand name for Galvin Manufacturing Corporation's new car radio, and created the name "Motorola" by linking "motor" (for motorcar) with "ola" (from Victrola), which was also a popular ending for many companies at the time, e.g. Moviola, Crayola. The company sold its first Motorola branded radio on June 23, 1930, to H.C. Wall of Fort Wayne, Indiana, for $30. The Motorola brand name became so well known that Galvin Manufacturing Corporation later changed its name to Motorola, Inc., in 1947.Galvin Manufacturing Corporation began selling Motorola car-radio receivers to police departments and municipalities in November 1930. The company's first public safety customers (all in the U.S. state of Illinois) included the Village of River Forest, Village of Bellwood Police Department, City of Evanston Police, Illinois State Highway Police, and Cook County (Chicago area) Police.Many of Motorola's products have been radio-related, starting with a battery eliminator for radios, through the first hand-held walkie-talkie in the world in 1940, defense electronics, cellular infrastructure equipment, and mobile phone manufacturing. In the same year, the company built its research and development program with Dan Noble, a pioneer in FM radio and semiconductor technologies, who joined the company as director of research. The company produced the hand-held AM SCR-536 radio during World War II, which was vital to Allied communication. Motorola ranked 94th among United States corporations in the value of World War II military production contracts.Motorola went public in 1943,  and became Motorola, Inc. in 1947. At that time Motorola's main business was producing and selling televisions and radios.

Post World War II
The last plant was listed in Quincy, Illinois at 1400 North 30th Street where 1,200 employees made radio assemblies for both homes and automobiles.In 1969, Neil Armstrong spoke the famous words "one small step for a man, one giant leap for mankind" from the Moon on a Motorola transceiver.In 1973, Motorola demonstrated the first hand-held portable telephone.In 1974, Motorola introduced its first microprocessor, the 8-bit MC6800, used in automotive, computing and video game applications. That same year, Motorola sold its television business to the Japan-based Matsushita – the parent company of Panasonic.
In 1980, Motorola's next generation 32-bit microprocessor, the MC68000, led the wave of technologies that spurred the computing revolution in 1984, powering devices from companies such as Apple, Commodore, Atari, Sun, and Hewlett-Packard.
In September 1983, the U.S. Federal Communications Commission (FCC) approved the DynaTAC 8000X telephone, the world's first commercial cellular device. By 1998, cellphones accounted for two thirds of Motorola's gross revenue.In 1986 Motorola acquired Storno resulting in a whole new range of innovative communication products for the new owner. including the NMT, an automatic cellular phone system, and made Motorola a more central player in the early stages of the GSM standardisation process in 1987. With this addition Motorola strengthened its position in Europe significantly. As Motorola's European development arm, Storno managed to develop a GSM terminal in 1992.On January 29, 1988, Motorola sold its Arcade, New York facility and automotive alternators, electromechanical speedometers and tachometers products to Prestolite Electric.In 1996, Motorola released the Motorola StarMax, which was a Macintosh clone that was licensed by Apple and it came with System 7. However, with the return of Steve Jobs to Apple in 1997, Apple released Mac OS 8. Because the clone makers' licenses were valid only for Apple's System 7 operating system, Apple's release of Mac OS 8 left the clone manufacturers without the ability to ship a current Mac OS version without negotiation with Apple. A heated telephone conversation between Jobs and Motorola CEO Christopher Galvin resulted in the termination of Motorola's clone contract, the discontinuation of the Motorola StarMax, and the long-favored Apple being demoted to "just another customer" mainly for PowerPC CPUs. Apple (and Jobs) did not want Motorola to limit the PowerPC CPU supply so as retaliation, Apple and IBM expelled Motorola from the AIM alliance and forced Motorola to stop producing any PowerPC CPUs, leaving IBM to make all future PowerPC CPUs. However, Motorola was later reinstated into the alliance in 1998.In 1998, Motorola was overtaken by Nokia as the world's biggest seller of mobile phone handsets.In 1999, Motorola separated a portion of its semiconductor business—the Semiconductor Components Group (SCG)-- and formed ON Semiconductor, whose headquarters are located in Phoenix, Arizona.

After 2000
In June 2000, Motorola and Cisco supplied the world's first commercial GPRS cellular network to BT Cellnet in the United Kingdom. The world's first GPRS cell phone was also developed by Motorola.
In August 2000, Motorola acquired Printrak International Inc. for $160 million. In doing so, Motorola not only acquired computer aided dispatch and related software , but also acquired Automated fingerprint identification system software. With recent acquisitions from that year, Motorola reached its peak employment of 150,000 employees worldwide.  Two years later, employment would be at 93,000 due to layoffs and spinoffs.
In June 2005, Motorola overtook the intellectual property of Sendo for $30,000 and paid £362,575 for the plant, machinery and equipment.In June 2006, Motorola acquired the software platform (AJAR) developed by the British company TTP Communications plc. Later in 2006, the firm announced a music subscription service named iRadio. The technology came after a break in a partnership with Apple Computer (which in 2005 had produced an iTunes compatible cell phone ROKR E1, and most recently, mid-2007, its own iPhone). iRadio was to have many similarities with existing satellite radio services (such as Sirius and XM Radio) by offering live streams of commercial-free music content. Unlike satellite services, however, iRadio content would be downloaded via a broadband internet connection. However, iRadio was never commercially released.In October 2008, Motorola agreed to sell its Biometrics business to Safran, a French defense firm. Motorola's biometric business unit was headquartered in Anaheim, California. The deal closed in April 2009. The unit became part of Sagem Morpho, which was renamed MorphoTrak.

Split
On March 26, 2008, Motorola's board of directors approved a split into two different publicly traded companies. This came after talk of selling the company to another corporation. These new companies would comprise the business units of Motorola Mobile Devices and Motorola Broadband & Mobility Solutions. Originally it was expected that this action would be approved by regulatory bodies and complete by mid-2009, but the split was delayed due to company restructuring problems and the 2008–2009 extreme economic downturn.On February 11, 2010, Motorola announced its separation into two independent, publicly traded companies, effective Q1 2011. The official split occurred at around 12:00 pm EST on January 4, 2011. The cell phone and cable television equipment businesses were spun off to form Motorola Mobility, while the remainder of Motorola, Inc., which comprised the government and enterprise equipment businesses, became Motorola Solutions. Motorola Solutions retained Motorola, Inc.'s pre-2011 stock price history, though it retired the old ticker symbol of "MOT" in favor of "MSI". Motorola Mobility was eventually acquired by Google on May 22, 2012. Google sold Motorola Mobility's cable equipment business to Arris Group in December 2012, and Motorola Mobility itself to Lenovo on October 30, 2014.In March 2018, Danish pension fund Sampension divested from Motorola over the latter’s ties to illegal Israeli settlements.

Divisions
At the time of its split, Motorola had three divisions:

Enterprise Mobility Solutions was headquartered in Schaumburg, Illinois. It comprised communications offered to government and public safety sectors and enterprise mobility business. Motorola developed analog and digital two-way radio, voice and data communications products and systems, mobile computing, advanced data capture, wireless infrastructure and RFID solutions to customers worldwide.
Home & Networks Mobility produced end-to-end systems that facilitate uninterrupted access to digital entertainment, information and communications services via wired and wireless mediums. Motorola developed digital video system solutions, interactive set-top devices, voice and data modems for digital subscriber line and cable networks, broadband access systems for cable and satellite television operators, and also wireline carriers and wireless service providers. It was based in Arlington Heights, Illinois.
Mobile Devices' headquarters were located in Chicago, Illinois, and designed wireless handsets, but also licensed much of its intellectual properties. This included cellular and wireless systems and as well as integrated applications and Bluetooth accessories.

Finances
Motorola's handset division recorded a loss of $1.2 billion in the fourth quarter of 2007, while the company as a whole earned $100 million during that quarter. It lost several key executives to rivals, and the website TrustedReviews called the company's products repetitive and un-innovative. Motorola laid off 3,500 workers in January 2008, followed by a further 4,000 job cuts in June and another 20% cut of its research division a few days later. In July 2008, a large number of executives left Motorola to work on Apple Inc.'s iPhone. The company's handset division was also put on offer for sale.  Also that month, analyst Mark McKechnie from American Technology Research said that Motorola "would be lucky to fetch $500 million" for selling its handset business. Analyst Richard Windsor said that Motorola might have to pay someone to take the division off the company's hands , and that Motorola may even exit the handset market altogether.  Its global market share has been on the decline; from 18.4% of the market in 2007 the company had a share of just 6.0% by Q1 2009, but at last, Motorola scored a profit of $26 million in Q2 and showed an increase of 12% in stocks for the first time after losses in many quarters. During the second quarter of 2010, the company reported a profit of $162 million, which compared very favorably to the $26 million earned for the same period the year before. Its Mobile Devices division reported, for the first time in years, earnings of $87 million.

Environmental record
Motorola, Inc., along with the Arizona Water Co. had been identified as the sources of trichloroethylene (TCE) contamination that took place in Scottsdale, Arizona. The malfunction led to a ban on the use of water that lasted three days and affected almost 5000 people in the area. Motorola was found to be the main source of the TCE, an industrial solvent that is thought to cause cancer. The TCE contamination was caused by a faulty blower on an air stripping tower that was used to take TCE from the water, and Motorola has attributed the situation to operator error.Of eighteen leading electronics manufacturers in Greenpeace's Guide to Greener Electronics (October 2010), Motorola shared sixth place with competitors Panasonic and Sony).Motorola scored relatively well on the chemicals criteria and has a goal to eliminate PVC plastic and Brominated flame retardants (BFRs), though only in mobile devices and not in all its products introduced after 2010, despite the fact that Sony Ericsson and Nokia were already there. All of its mobile phones were now PVC-free and it had two PVC and BFR-free mobile phones, the A45 ECO and the GRASP; all chargers were also free from PVC and BFRs.The company was also increasing the proportion of recycled materials that used in its products. For example, the housings for the MOTO W233 Renew and MOTOCUBO A45 Eco mobile phones contained plastic from post-consumer recycled water cooler bottles. According to the company's information, all of Motorola's newly designed chargers met the current Energy Star requirements and exceed the requirements for standby/no-load modes by at least 67%.

See also
List of Motorola products
List of companies of the United States
List of electronics companies

References
Further reading
Gart, Jason H. (2006). Electronics and Aerospace Industry in Cold War Arizona, 1945–1968: Motorola, Hughes Aircraft, Goodyear Aircraft (PhD. diss.). Arizona State University.

External links

Official website (archived December 31, 2010)

Muri (Japanese term)

Muri (無理) is a Japanese word meaning "unreasonableness; impossible; beyond one's power; too difficult; by force; perforce; forcibly; compulsorily; excessiveness; immoderation", and is a key concept in the Toyota Production System (TPS) as one of the three types of waste (muda, mura, muri).A direct example of Muri is asking workers to stay focused for a period exceeding 8 hours a day or expecting a machine to produce more than it can in a given time.
Some of the most common reasons why production systems experience overburdening:
Unmeasured system capacity, despite assigning production expectations in set numbers
A suboptimal technical condition of used machines, or untrained staff
Poor communication across the team and manager

Avoidance of muri in Toyota manufacturing
Muri is one of three types of waste (muda, mura, muri) identified in the Toyota Production System. Waste reduction is an effective way to increase profitability.
Muri can be avoided through standardized work.  To achieve this a standard condition or output must be defined to assure effective judgment of quality.  Then every process and function must be reduced to its simplest elements for examination and later recombination.
The process must then be standardized to achieve the defined condition. This is done by taking simple work elements and combining them, one-by-one into standardized work sequences.  In manufacturing, this includes:

Work flow, or logical directions to be taken,
Repeatable process steps and machine processes, or rational methods to get there, and
Takt time (combined with production/demand leveling to provide for reasonable lengths of time and endurance allowed for a process e.g. if the takt is 10 minutes while there is no realistic way for a process to do its work in less than 10 minutes, Muri is the natural outcome).When everyone knows the standard condition, and the standardized work sequences, the results observed include:

Heightened employee morale (due to close examination of ergonomics and safety)
Higher quality
Improved productivity
Reduced costs


== References ==

Oregon Tool

Oregon Tool, Inc. is an American company that manufactures saw chain and other equipment for the forestry, agriculture, and construction industries. Based in Portland, Oregon, Oregon Tool globally manufactures their products in ten different plants across five countries. Oregon Tool produces and markets saw chain, chain saw bars and sprockets, battery operated lawn and garden equipment, lawn mower blades, string trimmer line, concrete cutting saws and chain, and agricultural cutting equipment for OEMs, dealers, and end-user markets. Oregon Tool employs approximately 3,300 people across the world in 17 global locations.

History
Joseph Buford Cox founded the Oregon Saw Chain Company in 1947. An avid inventor, Cox designed the modern saw chain after witnessing a timber beetle larvae chewing through some timber in a Pacific Northwest forest. The saw chain he ultimately created serves as the foundation for the modern chipper chain design, and influenced other forms of modern saw chain design. Known as Biomimetics, Cox solved a complex problem by taking inspiration from nature. After experimenting with casting techniques, Cox later founded Precision Castparts Corp.
In 1953, John D. Gray acquired the company and changed the name to Omark Industries. In the 1980s, Omark began researching and adopting just-in-time manufacturing processes. By visiting factories in Japan, Omark studied examples of lean manufacturing.  The concepts kept the saw chain products viable in the export market during a period with a strong dollar. In 1985, Omark Industries was purchased by Blount Industries, Inc. and its founder, Winton M. Blount.In 1993, Blount Industries, Inc. was renamed Blount International, Inc., and shifted its focus from construction to manufacturing. In 1997, Blount purchased Frederick Manufacturing Corp. of Kansas City, Missouri and added lawnmower blades and garden products to its portfolio. In 1999, Blount was acquired by Lehman Brothers Merchant Banking.  In 2002, Blount's corporate headquarters moved from Montgomery, Alabama, to Portland, Oregon.  In 2004, it changed its NYSE symbol to BLT, which remained the company's ticker symbol until 2015.
On December 10, 2015, Blount announced that it would be taken private by American Securities and P2 Capital Partners in an all-cash transaction valued at approximately $855 million, or $10 a share in cash.On June 3, 2020, Paul Tonnesen was appointed chief executive officer.
On May 10, 2021, Blount Inc. unveiled a new corporate name that positions it for growth while honoring its heritage: Oregon Tool. The shift from Blount, Inc. to Oregon Tool became effective June 2, 2021.On June 2, 2021, Blount Inc officially became Oregon Tool, Inc.
On July 14, 2021, American Securities and P2 Capital Partners announced the signing of a definitive agreement under which Platinum Equity will acquire Oregon Tool.On October 18, 2021, Platinum Equity announced that the acquisition of Oregon Tool had been completed.

Products and brands
The company manufactures a variety of products for the chainsaw, agriculture, forestry, and construction industries. Oregon Tool manufactures, markets, and distributes products primarily under two hero brands: Oregon and Woods Equipment. Further brands under Oregon Tool include Carlton, Kox, Speeco, Merit, and Pentruder.
In November 2022, Epiroc acquired Wain-Roy from Oregon Tool.

Oregon products
Oregon Tool is the largest manufacturer of saw chain in the world, with the Oregon brand holding the title of #1 saw chain in the world. Saw chain sold under the Oregon brand is sold to OEMs, dealers, and direct-to-consumer. Other products sold under the Oregon brand include: chain saw bar, sprocket, lawn mower blades, and string trimmer string. In addition, the Oregon brand sells several battery-powered and corded electric lawn and garden tools aimed at DIY consumers. These products include lawn mowers, chainsaws, string trimmers, and leaf blowers.
ICS was rebranded Oregon in October 2022.

Woods equipment
Woods is the inventor of the first tractor-mounted rotary cutter. Over the years, innovations and upgrades in the design evolved and led to the creation of The Batwing. The Batwing name is derived from the folding motion of the cutting decks as they are brought into their transportation position. This design allows for a wide cutting path while also allowing for easy transportation.

Oregon Construction
Oregon, formally ICS, is a pioneering force in the concrete-cutting industry. As most concrete cutters are of the circular style, Oregon develops, markets, and distributes an abrasive style concrete cutting system. This allows users to cut irregular shapes, conduct plunge cuts, and attempt other forms of cutting that are not physically possible with circular cutters. In addition, the abrasive cutting system is safer and has been proven to reduce kickback over circular style cutters.ICS was rebranded Oregon in October 2022.

See also
List of companies based in Oregon


== References ==

Operations management

Operations management is concerned with designing and controlling the production of goods or services, ensuring that businesses are efficient in using resources to meet customer requirements.  
It is concerned with managing an entire production or service system that converts inputs (in the forms of raw materials, labor, consumers, and energy) into outputs (in the form of goods and/or services for consumers). Operation management covers sectors like banking systems, hospitals, companies, working with suppliers, customers, and using technology. Operations is one of the major functions in an organization along with supply chains, marketing, finance and human resources. The operations function requires management of both the strategic and day-to-day production of goods and services.In managing manufacturing or service operations several types of decisions are made including operations strategy, product design, process design, quality management, capacity, facilities planning, production planning and inventory control.  Each of these requires an ability to analyze the current situation and find better solutions to improve the effectiveness and efficiency of manufacturing or service operations.

History
The history of production and operation systems begins around 5000 B.C. when Sumerian priests developed the ancient system of recording inventories, loans, taxes, and business transactions.  The next major historical application of operation systems occurred in 4000 B.C.  It was during this time that the Egyptians started using planning, organization, and control in large projects such as the construction of the pyramids. By 1100 B.C., labor was being specialized in China; by about 370 B.C., Xenophon described the advantages of dividing the various operations necessary for the production of shoes among different individuals in ancient Greece:
"...In large cities, on the other hand, inasmuch as many people have demands to make upon each branch of industry, one trade alone, and very often even less than a whole trade, is enough to support a man: one man, for instance, makes shoes for men, and another for women; and there are places even where one man earns a living by only stitching shoes, another by cutting them out, another by sewing the uppers together, while there is another who performs none of these operations but only assembles the parts. It follows, therefore, as a matter of course, that he who devotes himself to a very highly specialized line of work is bound to do it in the best possible manner."
In the Middle Ages, kings and queens ruled over large areas of land.  Loyal noblemen maintained large sections of the monarch's territory.  This hierarchical organization in which people were divided into classes based on social position and wealth became known as the feudal system.  In the feudal system, vassals and serfs produced for themselves and people of higher classes by using the ruler's land and resources. Although a large part of labor was employed in agriculture, artisans contributed to economic output and formed guilds. The guild system, operating mainly between 1100 and 1500, consisted of two types: merchant guilds, who bought and sold goods, and craft guilds, which made goods. Although guilds were regulated as to the quality of work performed, the resulting system was rather rigid, shoemakers, for example, were prohibited from tanning hides.Services were also performed in the Middle Ages by servants. They provided service to the nobility in the form of cooking, cleaning and providing entertainment.  Court jesters were considered service providers. The medieval army could also be considered a service since they defended the nobility.The industrial revolution was facilitated by two elements: interchangeability of parts and division of labor. Division of labor has been a feature from the beginning of civilization, the extent to which the division is carried out varied considerably depending on period and location. Compared to the Middle Ages, the Renaissance and the Age of Discovery were characterized by a greater specialization in labor, which was a characteristic of the growing cities and trade networks of Europe. An important leap in manufacturing efficiency came in the late eighteenth century as Eli Whitney popularized the concept of interchangeability of parts when he manufactured 10,000 muskets. Up to this point in the history of manufacturing, each product (e.g. each musket) was considered a special order, meaning that parts of a given musket were fitted only for that particular musket and could not be used in other muskets.  Interchangeability of parts allowed the mass production of parts independent of the final products in which they would be used. An entire new market to fill the need for the sale and manufacturing of muskets began at this time.In 1883, Frederick Winslow Taylor introduced the stopwatch method for accurately measuring the time to perform each single task of a complicated job. He developed the scientific study of productivity and identifying how to coordinate different tasks to eliminate wasting of time and increase the quality of work.  The next generation of scientific study occurred with the development of work sampling and predetermined motion time systems (PMTS).  Work sampling is used to measure the random variable associated with the time of each task.  PMTS allows the use of standard predetermined tables of the smallest body movements (e.g. turning the left wrist by 90°), and integrating them to predict the time needed to perform a simple task.  PMTS has gained substantial importance due to the fact that it can predict work measurements without observing the actual work.  The foundation of PMTS was laid out by the research and development of Frank B. and Lillian M. Gilbreth around 1912.  The Gilbreths took advantage of taking motion pictures at known time intervals while operators were performing the given task.Service Industries: At the turn of the twentieth century, the services industries were already developed, but largely fragmented.  In 1900 the U.S. service industry consisted of banks, professional services, schools, general stores, railroads and telegraph. Services were largely local in nature (except for railroads and telegraph) and owned by entrepreneurs and families.  The U.S. in 1900 had 31% employment in services, 31% in manufacturing and 38% in agriculture.The idea of the production line has been used multiple times in history prior to Henry Ford: the Venetian Arsenal (1104); Smith's pin manufacturing, in the Wealth of Nations (1776) or Brunel's Portsmouth Block Mills (1802). Ransom Olds was the first to manufacture cars using the assembly line system, but Henry Ford developed the first auto assembly system where a car chassis was moved through the assembly line by a conveyor belt while workers added components to it until the car was completed. During World War II, the growth of computing power led to further development of efficient manufacturing methods and the use of advanced mathematical and statistical tools. This was supported by the development of academic programs in industrial and systems engineering disciplines, as well as fields of operations research and management science (as multi-disciplinary fields of problem solving). While systems engineering concentrated on the broad characteristics of the relationships between inputs and outputs of generic systems, operations researchers concentrated on solving specific and focused problems. The synergy of operations research and systems engineering allowed for the realization of solving large scale and complex problems in the modern era. Recently, the development of faster and smaller computers, intelligent systems, and the World Wide Web  has opened new opportunities for operations, manufacturing, production, and service systems.

Industrial Revolution
Before the First industrial revolution work was mainly done through two systems: domestic system and craft guilds. In the domestic system merchants took materials to homes where artisans performed the necessary work, craft guilds on the other hand were associations of artisans which passed work from one shop to another, for example: leather was tanned by a tanner, passed to curriers, and finally arrived at shoemakers and saddlers.
The beginning of the industrial revolution is usually associated with the eighteenth-century English textile industry, with the invention of the flying shuttle by John Kay in 1733, the spinning jenny by James Hargreaves in 1765, the water frame by Richard Arkwright in 1769 and the steam engine by James Watt in 1765. In 1851 at the Crystal Palace Exhibition the term American system of manufacturing was used to describe the new approach that was evolving in the United States of America which was based on two central features: interchangeable parts and extensive use of mechanization to produce them.

Second Industrial Revolution and post-industrial society
Henry Ford was 39 years old when he founded the Ford Motor Company in 1903, with $28,000 capital from twelve investors. The model T car was introduced in 1908, however it was not until Ford implemented the assembly line concept, that his vision of making a popular car affordable by every middle-class American citizen would be realized. The first factory in which Henry Ford used the concept of the assembly line was Highland Park (1913), he characterized the system as follows:

"The thing is to keep everything in motion and take the work to the man and not the man to the work. That is the real principle of our production, and conveyors are only one of many means to an end" 
This became one of the central ideas that led to mass production, one of the main elements of the Second Industrial Revolution, along with emergence of the electrical industry and petroleum industry.
The post-industrial economy was noted in 1973 by Daniel Bell.  He stated that the future economy would provide more GDP and employment from services than from manufacturing and have a great effect on society. Since all sectors are highly interconnected, this did not reflect less importance for manufacturing, agriculture, and mining but just a shift in the type of economic activity.

Operations management
Although productivity benefited considerably from technological inventions and division of labor, the problem of systematic measurement of performances and the calculation of these by the use of formulas remained somewhat unexplored until Frederick Taylor, whose early work focused on developing what he called a "differential piece-rate system" and a series of experiments, measurements and formulas dealing with cutting metals and manual labor. The differential piece-rate system consisted in offering two different pay rates for doing a job: a higher rate for workers with high productivity (efficiency) and who produced high quality goods (effectiveness) and a lower rate for those who fail to achieve the standard. One of the problems Taylor believed could be solved with this system was the problem of soldiering: faster workers reducing their production rate to that of the slowest worker.
In 1911 Taylor published his "The Principles of Scientific Management", in which he characterized scientific management (also known as Taylorism) as:

The development of a true science;
The scientific selection of the worker;
The scientific education and development of the worker;
Intimate friendly cooperation between the management and the workers.Taylor is also credited for developing stopwatch time study, this combined with Frank and Lillian Gilbreth motion study gave way to time and motion study which is centered on the concepts of standard method and standard time. Frank Gilbreth is also responsible for introducing the flow process chart in 1921. Other contemporaries of Taylor worth remembering are Morris Cooke (rural electrification in the 1920s and implementer of Taylor's principles of scientific management in the Philadelphia's Department of Public Works), Carl Barth (speed-and-feed-calculating slide rules ) and Henry Gantt (Gantt chart). Also in 1910 Hugo Diemer published the first industrial engineering book: Factory Organization and Administration.
In 1913 Ford Whitman Harris published a paper on "How many parts to make at once", in which he presented the idea of the economic order quantity model. He described the problem as follows:

"Interest on capital tied up in wages, material and overhead sets a maximum limit to the quantity of parts which can be profitably manufactured at one time; "setup costs" on the job fix the minimum. Experience has shown one manager a way to determine the economical size of lots."
Harris described his theory as "reasonably correct", although "not rigorously accurate". His paper inspired a large body of mathematical literature focusing on the problem of production planning and inventory control.In 1924 Walter Shewhart introduced the control chart through a technical memorandum while working at Bell Labs, central to his method was the distinction between common cause and special cause of variation. In 1931 Shewhart published his Economic Control of Quality of Manufactured Product, the first systematic treatment of the subject of Statistical Process Control (SPC). He defined control:

"For our present purpose a phenomenon will be said to be controlled when, through the use of past experience, we can predict, at least within limits, how the phenomenon may be expected to vary in the future. Here it is understood that prediction within limits means that we can state, at least approximately, the probability that the observed phenomenon will fall within the given limits."
In the 1940s methods-time measurement (MTM) was developed by H.B. Maynard, J.L. Schwab and G.J. Stegemerten. MTM was the first of a series of predetermined motion time systems, predetermined in the sense that estimates of time are not determined in loco but are derived from an industry standard. This was explained by its originators in a book they published in 1948 called "Method-Time Measurement".

The methods-time measurement may be defined as follows:
Methods-time measurement is a procedure which analyzes any manual operation or method into the basic motions required to perform it and assigns to each motion a predetermined time standard which is determined by the nature of the motion and the conditions under which it is made.

Thus it may be seen that methods-time measurement is basically a tool of method analysis that gives answers in terms of time without the necessity of making stop-watch time studies.
Up to this point in history, optimization techniques were known for a very long time, from the simple methods employed by Harris to the more elaborate techniques of the calculus of variations developed by Euler in 1733 or the multipliers employed by Lagrange in 1811, and computers were slowly being developed, first as analog computers by Sir William Thomson (1872) and James Thomson (1876) moving to the electromechanical computers of Konrad Zuse (1939 and 1941). During World War II however, the development of mathematical optimization went through a major boost with the development of the Colossus computer, the first electronic digital computer that was all programmable, and the possibility to computationally solve large linear programming problems, first by Kantorovich in 1939 working for the Soviet government and later in 1947 with the simplex method of Dantzig. These methods are known today as belonging to the field of operations research.
From this point on, a curious development took place: while in the United States the possibility of applying the computer to business operations led to the development of management software architecture such as MRP and successive modifications, and ever more sophisticated optimization techniques and manufacturing simulation software, in post-war Japan a series of events at Toyota Motor led to the development of the Toyota Production System (TPS) and Lean Manufacturing.
In 1943, in Japan, Taiichi Ohno arrived at Toyota Motor company. Toyota evolved a unique manufacturing system centered on two complementary notions: just in time (produce only what is needed) and autonomation (automation with a human touch). Regarding JIT, Ohno was inspired by American supermarkets: workstations functioned like a supermarket shelf where the customer can get products they need, at the time they need and in the amount needed, the workstation (shelf) is then restocked. Autonomation was developed by Toyoda Sakichi in Toyoda Spinning and Weaving: an automatically activated loom that was also foolproof, that is automatically detected problems. In 1983 J.N Edwards published his "MRP and Kanban-American style" in which he described JIT goals in terms of seven zeros: zero defects, zero (excess) lot size, zero setups, zero breakdowns, zero handling, zero lead time and zero surging. This period also marks the spread of Total Quality Management (TQM) in Japan, ideas initially developed by American authors such as Deming, Juran and Armand V. Feigenbaum. TQM is a strategy for implementing and managing quality improvement on an organizational basis, this includes: participation, work culture, customer focus, supplier quality improvement and integration of the quality system with business goals. Schnonberger identified seven fundamentals principles essential to the Japanese approach:

Process control: SPC and worker responsibility over quality
Easy able -to-see quality: boards, gauges, meters, etc. and poka-yoke
Insistence on compliance: "quality first"
Line stop: stop the line to correct quality problems
Correcting one's own errors: worker fixed a defective part if he produced it
The 100% check: automated inspection techniques and foolproof machines
Continual improvement: ideally zero defects.Meanwhile, in the sixties, a different approach was developed by George W. Plossl and Oliver W. Wight, this approach was continued by Joseph Orlicky as a response to the TOYOTA Manufacturing Program which led to Material Requirements Planning (MRP) at IBM, latter gaining momentum in 1972 when the American Production and Inventory Control Society launched the "MRP Crusade". One of the key insights of this management system was the distinction between dependent demand and independent demand. Independent demand is demand which originates outside of the production system, therefore not directly controllable, and dependent demand is demand for components of final products, therefore subject to being directly controllable by management through the bill of materials, via product design. Orlicky wrote "Materials Requirement Planning" in 1975, the first hard cover book on the subject. MRP II was developed by Gene Thomas at IBM, and expanded the original MRP software to include additional production functions. Enterprise resource planning (ERP) is the modern software architecture, which addresses, besides production operations, distribution, accounting, human resources and procurement.
Dramatic changes were occurring in the service industries, as well.  Beginning in 1955 McDonald's provided one of the first innovations in service operations.  McDonald's is founded on the idea of the production-line approach to service.  This requires a standard and limited menu, an assembly-line type of production process in the back-room, high customer service in the front-room with cleanliness, courtesy and fast service. While modeled after manufacturing in the production of the food in the back-room, the service in the front-room was defined and oriented to the customer. It was the McDonald's operations system of both production and service that made the difference. McDonald's also pioneered the idea of franchising this operation system to rapidly spread the business around the country and later the world.FedEx in 1971 provided the first overnight delivery of packages in the U.S.  This was based on the innovative idea of flying all packages into the single airport in Memphis Tenn by midnight each day, resorting the packages for delivery to destinations and then flying them back out the next morning for delivery to numerous locations.  This concept of a fast package delivery system created a whole new industry, and eventually allowed fast delivery of online orders by Amazon and other retailers.Walmart provided the first example of very low cost retailing through design of their stores and efficient management of their entire supply chain. Starting with a single store in Roger's Arkansas in 1962, Walmart has now become the world's largest company. This was accomplished by adhering to their system of delivering the goods and the service to the customers at the lowest possible cost.  The operations system included careful selection of merchandise, low cost sourcing, ownership of transportation, cross-docking, efficient location of stores and friendly home-town service to the customer.In 1987 the International Organization for Standardization (ISO), recognizing the growing importance of quality, issued the ISO 9000, a family of standards related to quality management systems. There standards apply to both manufacturing and service organizations. There has been some controversy regarding the proper procedures to follow and the amount of paperwork involved, but much of that has improved in current ISO 9000 revisions.
With the coming of the Internet, in 1994 Amazon devised a service system of on-line retailing and distribution.  With this innovative system customers were able to search for products they might like to buy, enter the order for the product, pay online, and track delivery of the product to their location, all in two days.  This required not only very large computer operations, but dispersed warehouses, and an efficient transportation system.  Service to customers including a high merchandise assortment, return services of purchases, and fast delivery is at the forefront of this business. It is the customer being in the system during the production and delivery of the service that distinguishes all services from manufacturing.
Recent trends in the field revolve around concepts such as:

Business Process Re-engineering (launched by Michael Hammer in 1993): a business management strategy focusing on the analysis and design of workflows and business processes within an organization. BPR seeks to help companies radically restructure their organizations by focusing on the ground-up design of their business processes.
Lean systems is a systemic method for the elimination of waste ("Muda") within a manufacturing or service process.  Lean also takes into account waste created through overburden ("Muri") and waste created through unevenness in work loads ("Mura"). The term lean manufacturing was coined in the book The Machine that Changed the World.  Subsequently, lean services has been widely applied.
Six Sigma (an approach to quality developed at Motorola between 1985 and 1987): Six Sigma refers to control limits placed at six standard deviations from the mean of a normal distribution, this became very famous after Jack Welch of General Electric launched a company-wide initiative in 1995 to adopt this set of methods to all manufacturing, service and administrative processes. More recently, Six Sigma has included DMAIC (for improving processes) and DFSS (for designing new products and new processes)
Reconfigurable Manufacturing Systems: a production system designed at the outset for rapid change in its structure, as well as its hardware and software components, in order to quickly adjust its production capacity and functionality within a part family in response to sudden market changes or intrinsic system change.
Project Production Management: the application of the analytical tools and techniques developed for operations management, as described in Factory Physics to the activities within major capital projects such as encountered in oil & gas and civil infrastructure delivery.

Topics
Production systems
A production system comprises both the technological elements (machines and tools) and organizational behavior (division of labor and information flow). An individual production system is usually analyzed in the literature referring to a single business, therefore it's usually improper to include in a given production system the operations necessary to process goods that are obtained by purchasing or the operations carried by the customer on the sold products, the reason being simply that since businesses need to design their own production systems this then becomes the focus of analysis, modeling and decision making (also called "configuring" a production system).
A first possible distinction in production systems (technological classification) is between continuous process production and discrete part production (manufacturing). 

Process production means that the product undergoes physical-chemical transformations and lacks assembly operations, therefore the original raw materials can't easily be obtained from the final product, examples include: paper, cement, nylon and petroleum products.
Part production (ex:cars and ovens) comprises both fabrication systems and assembly systems. In the first category we find job shops, manufacturing cells, flexible manufacturing systems and transfer lines, in the assembly category we have fixed position systems, assembly lines and assembly shops (both manual and/or automated operations).Another possible classification is one based on Lead Time (manufacturing lead time vs delivery lead time): engineer to order (ETO), purchase to order (PTO), make to order (MTO), assemble to order (ATO) and make to stock (MTS). According to this classification different kinds of systems will have different customer order decoupling points (CODP), meaning that work in progress (WIP) cycle stock levels are practically nonexistent regarding operations located after the CODP (except for WIP due to queues). (See Order fulfillment)
The concept of production systems can be expanded to the service sector world keeping in mind that services have some fundamental differences in respect to material goods: intangibility, client always present during transformation processes, no stocks for "finished goods". Services can be classified according to a service process matrix: degree of labor intensity (volume) vs degree of customization (variety). With a high degree of labor intensity there are Mass Services (e.g., commercial banking bill payments and state schools) and Professional Services (e.g., personal physicians and lawyers), while with a low degree of labor intensity there are Service Factories (e.g., airlines and hotels) and Service Shops (e.g., hospitals and auto mechanics).
The systems described above are ideal types: real systems may present themselves as hybrids of those categories. Consider, for example, that the production of jeans involves initially carding, spinning, dyeing and weaving, then cutting the fabric in different shapes and assembling the parts in pants or jackets by combining the fabric with thread, zippers and buttons, finally finishing and distressing the pants/jackets before being shipped to stores. The beginning can be seen as process production, the middle as part production and the end again as process production: it's unlikely that a single company will keep all the stages of production under a single roof, therefore the problem of vertical integration and outsourcing arises. Most products require, from a supply chain perspective, both process production and part production.

Metrics: efficiency and effectiveness
Operations strategy concerns policies and plans of use of the firm productive resources with the aim of supporting long term competitive strategy. Metrics in operations management can be broadly classified into efficiency metrics and effectiveness metrics. Effectiveness metrics involve: 

Price (actually fixed by marketing, but lower bounded by production cost): purchase price, use costs, maintenance costs, upgrade costs, disposal costs
Quality: specification and compliance
Time: productive lead time, information lead time, punctuality
Flexibility: mix (capacity to change the proportions between quantities produced in the system), volume (capacity to increase system output), gamma (capacity to expand the product family in the system)
Stock availability
Ecological Soundness: biological and environmental impacts of the system under study.A more recent approach, introduced by Terry Hill, involves distinguishing competitive variables in order winner and order qualifiers when defining operations strategy. Order winners are variables which permit differentiating the company from competitors, while order qualifiers are prerequisites for engaging in a transaction. This view can be seen as a unifying approach between operations management and marketing (see segmentation and positioning).
Productivity is a standard efficiency metric for evaluation of production systems, broadly speaking a ratio between outputs and inputs, and can assume many specific forms, for example: machine productivity, workforce productivity, raw material productivity, warehouse productivity (=inventory turnover). It is also useful to break up productivity in use U (productive percentage of total time) and yield η (ratio between produced volume and productive time) to better evaluate production systems performances. Cycle times can be modeled through manufacturing engineering if the individual operations are heavily automated, if the manual component is the prevalent one, methods used include: time and motion study, predetermined motion time systems and work sampling.

ABC analysis is a method for analyzing inventory based on Pareto distribution, it posits that since revenue from items on inventory will be power law distributed then it makes sense to manage items differently based on their position on a revenue-inventory level matrix, 3 classes are constructed (A, B and C) from cumulative item revenues, so in a matrix each item will have a letter (A, B or C) assigned for revenue and inventory. This method posits that items away from the diagonal should be managed differently: items in the upper part are subject to risk of obsolescence, items in the lower part are subject to risk of stockout.
Throughput is a variable which quantifies the number of parts produced in the unit of time. Although estimating throughput for a single process maybe fairly simple, doing so for an entire production system involves an additional difficulty due to the presence of queues which can come from: machine breakdowns, processing time variability, scraps, setups, maintenance time, lack of orders, lack of materials, strikes, bad coordination between resources, mix variability, plus all these inefficiencies tend to compound depending on the nature of the production system. One important example of how system throughput is tied to system design are bottlenecks: in job shops bottlenecks are typically dynamic and dependent on scheduling while on transfer lines it makes sense to speak of "the bottleneck" since it can be univocally associated with a specific station on the line. This leads to the problem of how to define capacity measures, that is an estimation of the maximum output of a given production system, and capacity utilization.
Overall equipment effectiveness (OEE) is defined as the product between system availability, cycle time efficiency and quality rate. OEE is typically used as key performance indicator (KPI) in conjunction with the lean manufacturing approach.

Configuration and management
Designing the configuration of production systems involves both technological and organizational variables. Choices in production technology involve: dimensioning capacity, fractioning capacity, capacity location, outsourcing processes, process technology, automation of operations, trade-off between volume and variety (see Hayes-Wheelwright matrix). Choices in the organizational area involve: defining worker skills and responsibilities, team coordination, worker incentives and information flow.
In production planning, there is a basic distinction between the push approach and the pull approach, with the later including the singular approach of just in time. Pull means that the production system authorizes production based on inventory level; push means that production occurs based on demand (forecasted or present, that is purchase orders). An individual production system can be both push and pull; for example activities before the CODP may work under a pull system, while activities after the CODP may work under a push system.

The traditional pull approach to inventory control, a number of techniques have been developed based on the work of Ford W. Harris (1913), which came to be known as the economic order quantity (EOQ) model. This model marks the beginning of inventory theory, which includes the Wagner-Within procedure, the newsvendor model, base stock model and the fixed time period model. These models usually involve the calculation of cycle stocks and buffer stocks, the latter usually modeled as a function of demand variability. The economic production quantity (EPQ) differs from the EOQ model only in that it assumes a constant fill rate for the part being produced, instead of the instantaneous refilling of the EOQ model.

Joseph Orlickly and others at IBM developed a push approach to inventory control and production planning, now known as material requirements planning (MRP), which takes as input both the master production schedule (MPS) and the bill of materials (BOM) and gives as output a schedule for the materials (components) needed in the production process. MRP therefore is a planning tool to manage purchase orders and production orders (also called jobs).
The MPS can be seen as a kind of aggregate planning for production coming in two fundamentally opposing varieties: plans which try to chase demand and level plans which try to keep uniform capacity utilization. Many models have been proposed to solve MPS problems:

Analytical models (e.g. Magee Boodman model)
Exact optimization algorithmic models (e.g. LP and ILP)
Heuristic models (e.g. Aucamp model).MRP can be briefly described as a 3s procedure: sum (different orders), split (in lots), shift (in time according to item lead time). To avoid an "explosion" of data processing in MRP (number of BOMs required in input) planning bills (such as family bills or super bills) can be useful since they allow a rationalization of input data into common codes.
MRP had some notorious problems such as infinite capacity and fixed lead times, which influenced successive modifications of the original software architecture in the form of MRP II, enterprise resource planning (ERP) and advanced planning and scheduling (APS).
In this context problems of scheduling (sequencing of production), loading (tools to use), part type selection (parts to work on) and applications of operations research have a significant role to play.
Lean manufacturing is an approach to production which arose in Toyota between the end of World War II and the seventies. It comes mainly from the ideas of Taiichi Ohno and Toyoda Sakichi which are centered on the complementary notions of just in time and autonomation (jidoka), all aimed at reducing waste (usually applied in PDCA style). Some additional elements are also fundamental: production smoothing (Heijunka), capacity buffers, setup reduction, cross-training and plant layout.

Heijunka: production smoothing presupposes a level strategy for the MPS and a final assembly schedule developed from the MPS by smoothing aggregate production requirements in smaller time buckets and sequencing final assembly to achieve repetitive manufacturing. If these conditions are met, expected throughput can be equaled to the inverse of takt time. Besides volume, heijunka also means attaining mixed-model production, which however may only be feasible through set-up reduction. A standard tool for achieving this is the Heijunka box.
Capacity buffers: ideally a JIT system would work with zero breakdowns, this however is very hard to achieve in practice, nonetheless Toyota favors acquiring extra capacity over extra WIP to deal with starvation.
Set-up reduction: typically necessary to achieve mixed-model production, a key distinction can be made between internal and external setup. Internal setups (e.g. removing a die) refers to tasks when the machine is not working, while external setups can be completed while the machine is running (ex:transporting dies).
Cross training: important as an element of Autonomation, Toyota cross trained their employees through rotation, this served as an element of production flexibility, holistic thinking and reducing boredom.
Layout: U-shaped lines or cells are common in the lean approach since they allow for minimum walking, greater worker efficiency and flexible capacity.A series of tools have been developed mainly with the objective of replicating Toyota success: a very common implementation involves small cards known as kanbans; these also come in some varieties: reorder kanbans, alarm kanbans, triangular kanbans, etc. In the classic kanban procedure with one card:

Parts are kept in containers with their respective kanbans
The downstream station moves the kanban to the upstream station and starts producing the part at the downstream station
The upstream operator takes the most urgent kanban from his list (compare to queue discipline from queue theory) and produces it and attach its respective kanbanThe two-card kanban procedure differs a bit:

The downstream operator takes the production kanban from his list
If required parts are available he removes the move kanban and places them in another box, otherwise he chooses another production card
He produces the part and attach its respective production kanban
Periodically a mover picks up the move kanbans in upstream stations and search for the respective parts, when found he exchanges production kanbans for move kanbans and move the parts to downstream stationsSince the number of kanbans in the production system is set by managers as a constant number, the kanban procedure works as WIP controlling device, which for a given arrival rate, per Little's law, works as a lead time controlling device.

In Toyota the TPS represented more of a philosophy of production than a set of specific lean tools, the latter would include: 

SMED: a method for reducing changeover times
Value stream mapping: a graphical method for analyzing the current state and designing a future state
lot-size reduction
elimination of time batching
Rank Order Clustering: an algorithm which groups machines and product families together, used for designing manufacturing cells
single-point scheduling, the opposite of the traditional push approach
multi-process handling: when one operator is responsible for operating several machines or processes
poka-yoke: any mechanism in lean manufacturing that helps an equipment operator avoid (yokeru) mistakes (poka)
5S: describes how to organize a work space for efficiency and effectiveness by identifying and storing the items used, maintaining the area and items, and sustaining the new order
backflush accounting: a product costing approach in which costing is delayed until goods are finishedSeen more broadly, JIT can include methods such as: product standardization and modularity, group technology, total productive maintenance, job enlargement, job enrichment, flat organization and vendor rating (JIT production is very sensitive to replenishment conditions).
In heavily automated production systems production planning and information gathering may be executed via the control system, attention should be paid however to avoid problems such as deadlocks, as these can lead to productivity losses.
Project Production Management (PPM) applies the concepts of operations management to the execution of delivery of capital projects by viewing the sequence of activities in a project as a production system. Operations managements principles of variability reduction and management are applied by buffering through a combination of capacity, time and inventory.

Service operations
Service industries are a major part of economic activity and employment in all industrialized countries comprising 80 percent of employment and GDP in the U.S.  Operations management of these services, as distinct from manufacturing, has been developing since the 1970s through publication of unique practices and academic research. Please note that this section does not particularly include "Professional Services Firms" and the professional services practiced from this expertise (specialized training and education within).
According to Fitzsimmons, Fitzsimmons and Bordoloi (2014) differences between manufactured goods and services are as follows:
Simultaneous production and consumption. High contact services (e.g. health care) must be produced in the presence of the customer, since they are consumed as produced. As a result, services cannot be produced in one location and transported to another, like goods. Service operations are therefore highly dispersed geographically close to the customers. Furthermore, simultaneous production and consumption allows the possibility of self-service involving the customer at the point of consumption (e.g. gas stations). Only low-contact services produced in the "backroom" (e.g., check clearing) can be provided away from the customer.
Perishable. Since services are perishable, they cannot be stored for later use. In manufacturing companies, inventory can be used to buffer supply and demand. Since buffering is not possible in services, highly variable demand must be met by operations or demand modified to meet supply.
Ownership. In manufacturing, ownership is transferred to the customer. Ownership is not transferred for service. As a result, services cannot be owned or resold.
Tangibility. A service is intangible making it difficult for a customer to evaluate the service in advance. In the case of a manufactured good, customers can see it and evaluate it. Assurance of quality service is often done by licensing, government regulation, and branding to assure customers they will receive a quality service.These four comparisons indicate how management of service operations are quite different from manufacturing regarding such issues as capacity requirements (highly variable), quality assurance (hard to quantify), location of facilities (dispersed), and interaction with the customer during delivery of the service (product and process design).
While there are differences there are also many similarities.  For example, quality management approaches used in manufacturing such as the Baldrige Award, and Six Sigma have been widely applied to services.  Likewise, lean service principles and practices have also been applied in service operations. The important difference being the customer is in the system while the service is being provided and needs to be considered when applying these practices.One important difference is service recovery.  When an error occurs in service delivery, the recovery must be delivered on the spot by the service provider.  If a waiter in a restaurant spills soup on the customer's lap, then the recovery could include a free meal and a promise of free dry cleaning.  Another difference is in planning capacity.  Since the product cannot be stored, the service facility must be managed to peak demand which requires more flexibility than manufacturing. Location of facilities must be near the customers and scale economics can be lacking. Scheduling must consider the customer can be waiting in line. Queuing theory has been devised to assist in design of service facilities waiting lines.  Revenue management is important for service operations, since empty seats on an airplane are lost revenue when the plane departs and cannot be stored for future use.

Mathematical modeling
There are also fields of mathematical theory which have found applications in the field of operations management such as operations research: mainly mathematical optimization problems and queue theory. Queue theory is employed in modelling queue and processing times in production systems while mathematical optimization draws heavily from multivariate calculus and linear algebra.  Queue theory is based on Markov chains and stochastic processes. Computations of safety stocks are usually based on modeling demand as a normal distribution and MRP and some inventory problems can be formulated using optimal control.When analytical models are not enough, managers may resort to using simulation. Simulation has been traditionally done through the discrete event simulation paradigm, where the simulation model possesses a state which can only change when a discrete event happens, which consists of a clock and list of events. The more recent transaction-level modeling paradigm consists of a set of resources and a set of transactions: transactions move through a network of resources (nodes) according to a code, called a process.

Since real production processes are always affected by disturbances in both inputs and outputs, many companies implement some form of quality management or quality control. The Seven Basic Tools of Quality designation provides a summary of commonly used tools:

check sheets
Pareto charts
Ishikawa diagrams (Cause-and-effect diagram)
control charts
histogram
scatter diagram
stratificationThese are used in approaches like total quality management and Six Sigma. Keeping quality under control is relevant to both increasing customer satisfaction and reducing processing waste.
Operations management textbooks usually cover demand forecasting, even though it is not strictly speaking an operations problem, because demand is related to some production systems variables.  For example, a classic approach in dimensioning safety stocks requires calculating the standard deviation of forecast errors. Demand forecasting is also a critical part of push systems, since order releases have to be planned ahead of actual clients’ orders. Also, any serious discussion of capacity planning involves adjusting company outputs with market demands.

Safety, risk and maintenance
Other important management problems involve maintenance policies (see also reliability engineering and maintenance philosophy), safety management systems (see also safety engineering and Risk management), facility management and supply chain integration.

Organizations
The following organizations support and promote operations management:

Association for Operations Management (APICS) which supports the Production and Inventory Management Journal
European Operations Management Association (EurOMA) which supports the International Journal of Operations & Production Management
Production and Operations Management Society (POMS) which supports the journal: Production and Operations Management
Institute for Operations Research and the Management Sciences (INFORMS)
The Manufacturing and Service Operations Management Society (MSOM) of INFORMS which supports the journal: Manufacturing & Service Operations Management
Institute of Operations Management (UK)
Association of Technology, Management, and Applied Engineering (ATMAE)

Journals
The following high-ranked academic journals are concerned with operations management issues:

Management Science
Manufacturing & Service Operations Management
Operations Research
International Journal of Operations & Production Management
Production and Operations Management
Transportation Research - Part E
Journal of Operations Management
European Journal of Operational Research
Annals of Operations Research

See also
References


== Further reading ==

PDCA

PDCA or plan–do–check–act (sometimes called plan–do–check–adjust) is an iterative design and management method used in business for the control and continual improvement of processes and products. It is also known as the Shewhart cycle, or the control circle/cycle. Another version of this PDCA cycle is OPDCA. The added "O" stands for observation or as some versions say: "Observe the current condition." This emphasis on observation and current condition has currency with the literature on lean manufacturing and the Toyota Production System. The PDCA cycle, with Ishikawa's changes, can be traced back to S. Mizuno of the Tokyo Institute of Technology in 1959.The PDCA cycle is also known as PDSA cycle (where S stands for study). It was an early means of representing the task areas of traditional quality management. The cycle is sometimes referred to as the Shewhart / Deming cycle since it originated with physicist Walter Shewhart at the Bell Telephone Laboratories in the 1920s. W. Edwards Deming modified the Shewhart cycle in the 1940s and subsequently applied it to management practices in Japan in the 1950s.Dr. Deming found that the focus on Check is more about the implementation of a change, with success or failure. His focus was on predicting the results of an improvement effort, studying the actual results, and comparing them to possibly revise the theory.

Meaning
Plan
Establish objectives and processes required to deliver the desired results.

Do
Carry out the objectives from the previous step.

Check
During the check phase, the data and results gathered from the do phase are evaluated. Data is compared to the expected outcomes to see any similarities and differences. The testing process is also evaluated to see if there were any changes from the original test created during the planning phase. If the data is placed in a chart it can make it easier to see any trends if the plan–do–check–act cycle is conducted multiple times. This helps to see what changes work better than others and if said changes can be improved as well.
Example: Gap analysis or appraisals

Act
Also called "adjust", this act phase is where a process is improved. Records from the "do" and "check" phases help identify issues with the process. These issues may include problems, non-conformities, opportunities for improvement, inefficiencies, and other issues that result in outcomes that are evidently less-than-optimal. Root causes of such issues are investigated, found, and eliminated by modifying the process. Risk is re-evaluated. At the end of the actions in this phase, the process has better instructions, standards, or goals. Planning for the next cycle can proceed with a better baseline. Work in the next do phase should not create a recurrence of the identified issues; if it does, then the action was not effective.

About
Plan–do–check–act is associated with W. Edwards Deming, who is considered by many to be the father of modern quality control; however, he used PDSA (Plan-Do-Study-Act) and referred to it as the "Shewhart cycle". Later in Deming's career, he modified PDCA to "Plan, Do, Study, Act" (PDSA) because he felt that "check" emphasized inspection over analysis. The PDSA cycle was used to create the model of know-how transfer process, and other models.The concept of PDCA is based on the scientific method, as developed from the work of Francis Bacon (Novum Organum, 1620). The scientific method can be written as "hypothesis–experiment–evaluation" or as "plan–do–check". Walter A. Shewhart described manufacture under "control"—under statistical control—as a three-step process of specification, production, and inspection.: 45  He also specifically related this to the scientific method of hypothesis, experiment, and evaluation. Shewhart says that the statistician "must help to change the demand [for goods] by showing [...] how to close up the tolerance range and to improve the quality of goods.": 48  Clearly, Shewhart intended the analyst to take action based on the conclusions of the evaluation. According to Deming, during his lectures in Japan in the early 1920s, the Japanese participants shortened the steps to the now traditional plan, do, check, act. Deming preferred plan, do, study, act because "study" has connotations in English closer to Shewhart's intent than "check".
A fundamental principle of the scientific method and plan–do–check–act is iteration—once a hypothesis is confirmed (or negated), executing the cycle again will extend the knowledge further. Repeating the PDCA cycle can bring its users closer to the goal, usually a perfect operation and output.Plan–do–check–act (and other forms of scientific problem solving) is also known as a system for developing critical thinking. At Toyota this is also known as "Building people before building cars". Toyota and other lean manufacturing companies propose that an engaged, problem-solving workforce using PDCA in a culture of critical thinking is better able to innovate and stay ahead of the competition through rigorous problem solving and the subsequent innovations.Deming continually emphasized iterating towards an improved system, hence PDCA should be repeatedly implemented in spirals of increasing knowledge of the system that converge on the ultimate goal, each cycle closer than the previous. One can envision an open coil spring, with each loop being one cycle of the scientific method, and each complete cycle indicating an increase in our knowledge of the system under study. This approach is based on the belief that our knowledge and skills are limited, but improving. Especially at the start of a project, key information may not be known; the PDCA—scientific method—provides feedback to justify guesses (hypotheses) and increase knowledge. Rather than enter "analysis paralysis" to get it perfect the first time, it is better to be approximately right than exactly wrong. With improved knowledge, one may choose to refine or alter the goal (ideal state). The aim of the PDCA cycle is to bring its users closer to whatever goal they choose.: 160 When PDCA is used for complex projects or products with a certain controversy, checking with external stakeholders should happen before the Do stage, since changes to projects and products that are already in detailed design can be costly; this is also seen as Plan-Check-Do-Act.The rate of change, that is, the rate of improvement, is a key competitive factor in today's world. PDCA allows for major "jumps" in performance ("breakthroughs" often desired in a Western approach), as well as kaizen (frequent small improvements). In the United States a PDCA approach is usually associated with a sizable project involving numerous people's time, and thus managers want to see large "breakthrough" improvements to justify the effort expended. However, the scientific method and PDCA apply to all sorts of projects and improvement activities.: 76

See also
References
Further reading

Kolesar, Peter J. (2005) [1994]. "What Deming told the Japanese in 1950". In Wood, John C.; Wood, Michael C. (eds.). W. Edwards Deming: critical evaluations in business and management. Vol. 2. New York: Routledge. pp. 87–107. ISBN 9780415323888. OCLC 55738077. Reprint. Originally published: Quality Management Journal 2(1) (1994): 9–24.
Langley, Gerald J.; Moen, Ronald D.; Nolan, Kevin M.; Nolan, Thomas W.; Norman, Clifford L.; Provost, Lloyd P. (2009) [1996]. The improvement guide: a practical approach to enhancing organizational performance (2nd ed.). San Francisco: Jossey-Bass. ISBN 9780470192412. OCLC 236325893.
Shewhart, Walter Andrew (1980) [1931]. Economic control of quality of manufactured product. Milwaukee: American Society for Quality. ISBN 978-0873890762. OCLC 7543940. 50th anniversary commemorative reissue. Originally published: New York: Van Nostrand, 1931.

Panic buying

Panic buying (alternatively hyphenated as panic-buying; also known as panic purchasing) occurs when consumers buy unusually large amounts of a product in anticipation of, or after, a disaster or perceived disaster, or in anticipation of a large price increase, or shortage.
Panic buying during various health crises is influenced by "(1) individuals' perception of the threat of a health crisis and scarcity of products; (2) fear of the unknown, which is caused by emotional pressure and uncertainty; (3) coping behaviour, which views panic buying as a venue to relieve anxiety and regain control over the crisis; and (4) social psychological factors, which account for the influence of the social network of an individual".Panic buying is a type of herd behavior. It is of interest in consumer behavior theory, the broad field of economic study dealing with explanations for "collective action such as fads and fashions, stock market movements, runs on nondurable goods, buying sprees, hoarding, and banking panics".
Panic buying can lead to genuine shortages regardless of whether the risk of a shortage is real or perceived; the latter scenario is an example of self-fulfilling prophecy.

Examples
Panic buying occurred before, during, or following:

The First (1914-1918) and Second World Wars (1939-1945).
The 1918–1919 global influenza pandemic ("Spanish flu") led to the panic buying of quinine and other remedies for influenza and its symptoms from pharmacists and doctors' surgeries. Sales of Vicks VapoRub increased from $900,000 to $2.9 million in a year.
In the First Austrian Republic in 1922, hyperinflation and the rapid depreciation of the Austrian krone led to panic buying and food hoarding, which continued until a rescue backed by the League of Nations prevented an economic collapse.
Bengal famine of 1943.
1962 Cuban Missile Crisis led to panic buying of canned foods in the United States.
The 1973 toilet paper panic in the United States.
The 1979 oil crisis led to panic buying of oil, led by Japan.
The 1985 arrival of New Coke led many consumers to panic buy the original Coke.
Year 2000 problem – food.
2001 – panic buying of metals, gold and oil on international commodity markets following the September 11 attacks.
Between January and February 2003, during the SARS outbreak, several rounds of panic buying of various products (including salt, rice, vinegar, vegetable oil, antibiotics, face masks, and traditional Chinese medicine) took place in the Chinese province of Guangdong and in neighboring areas such as Hainan and Hong Kong.
2000 and 2005 UK fuel protests.
2005 Jilin chemical plant explosions – water, food.
2008–2016 United States ammunition shortage – panic buying by gun owners who feared tougher gun control laws under President Barack Obama was one cause of ammunition shortages.
In September 2013 during the Venezuelan economic crisis, the Venezuelan government temporarily took over the Aragua-based Paper Manufacturing Company toilet paper plant to manage the "production, marketing and distribution" of toilet paper following months of depleted stocks of basic goods—including toilet paper—and foodstuffs, such as rice and cooking oil. Blame for the shortages was placed on "ill-conceived government policies such as price controls on basic goods and tight restrictions on foreign currency" and hoarding.
Dakazo – Amid decreased support before the 2013 Venezuelan municipal elections, Venezuelan president Nicolás Maduro announced the military occupation of stores on 8 November 2013, proclaiming "Leave nothing on the shelves!" The announcement of lowered prices sparked looting in multiple cities across Venezuela. By the end of the Dakazo, many Venezuelan stores were left empty of their goods. A year later in November 2014, some stores still remained empty following the Dakazo.
In September 2021, panic buying of petrol led to empty fuel filling stations across the United Kingdom. A lack of tanker drivers was blamed, with Brexit  being the primary cause according to most Road Haulage Association respondents.
In November 2021, panic buying of groceries took place in the British Columbia Interior and Fraser Valley owing to the impacts of the 2021 Pacific Northwest floods.
On March 3, 2022, panic buying of IKEA kit furniture and home appliances occurred in Russia due to the company's decision to close their 17 Russian stores in light of the 2022 Russian invasion of Ukraine. Extensive queues were reported in IKEA's Moscow and  Saint Petersburg stores, and customers attempted to enter from the exit doors when entrance doors were closed.
In May 2023, the Malaysian states of Penang and Kedah experienced panic buying of bottled water due to an interruption in tap water supply lasting less than 24 hours.
In Aug 2023, after the discharge of radioactive water of the Fukushima Daiichi Nuclear Power Plant, people in China began panic buying salts and radiation detectors because of the public anxiety towards the radioactive water released.

COVID-19 pandemic
Panic buying became a major international phenomenon between February and March 2020 during the early onset of the COVID-19 pandemic, and continued in smaller, more localized waves throughout during sporadic lockdowns across the world. Stores around the world were depleted of items such as face masks, food, bottled water, milk, toilet paper, hand sanitizer, rubbing alcohol, antibacterial wipes and painkillers. As a result, many retailers rationed the sale of these items.Online retailers such as eBay and Amazon began to pull certain items listed for sale by third parties such as toilet paper, face masks, pasta, canned vegetables, hand sanitizer and antibacterial wipes over price gouging concerns. As a result, Amazon restricted the sale of these items and others (such as thermometers and ventilators) to healthcare professionals and government agencies.  Additionally, panic renting of self-storage units took place during the onset of the pandemic.The massive buyouts of toilet paper caused bewilderment and confusion from the public. Images of empty shelves of toilet paper were shared on social media in many countries around the world, e.g. Australia, United States, the United Kingdom, Canada, Singapore, Hong Kong and Japan. In Australia, two women were charged over a physical altercation over toilet paper at a supermarket. The severity of the panic buying drew criticism; particularly from Prime Minister of Australia Scott Morrison, calling for Australians to "stop it".Research on this specific social phenomenon of toilet paper hoarding suggested that social media had played a crucial role in stimulating mass-anxiety and panic. Social media research found that many people posting about toilet paper panic buying were negative, either expressing anger or frustration over the frantic situation. This high amount of negative viral posts could act as an emotional trigger of anxiety and panic, spontaneously spreading fear and fueling psychological reactions in midst of the crisis. It may have triggered a snowball effect in the public, encouraged by the images and videos of empty shelves and people fighting over toilet rolls.

Gallery
See also
Panic selling
Revenge buying
Stock market crash
Economic bubble
Mass hysteria
Hoarding
Panic room


== References ==

Shortages related to the COVID-19 pandemic

Shortages related to the COVID-19 pandemic are pandemic-related disruptions to goods production and distribution, insufficient inventories, and disruptions to workplaces caused by infections and public policy.
The landscape of shortages changed dramatically over the course of the pandemic. Initially, extreme shortages emerged in the equipment needed to protect healthcare workers, diagnostic testing, equipment and staffing to provide care to seriously ill patients, and basic consumer goods disrupted by panic buying. Many commercial and governmental operatons curtailed or suspended operations, leading to shortages across "non-essential" services. E.g., many health care providers stopped providing some surgeries, screenings, and oncology treatments. In some cases, governmental decisionmaking created shortages, such as when CDC prohibited the use of any diagnostic test other than the one it created. One response was to improvise around shortages, producing supplies ranging from cloth masks to diagnostic tests to ventilators in home workshops, university laboratories, and rapidly repurposed factories.As these initial shortages were gradually remedied throughout 2020/2021, a second group of shortages emerged, afflicting industries dependent on global supply chains, affecting everything from automobiles to semiconductors to home appliances, in part due to China's determination to eliminate COVID-19 from its population by enforcing stringent quarantines and shutdowns, in part by disruptions to goods distribution, and in part by forecasting errors.Shortages were concentrated in America, Europe, Latin America, and China, while other jurisdictions were much less affected, for a variety of reasons.

Background
Historically, governments were the primary source of supplies for pandemics. Their willingness to maintain large stocks has tended to vary with the severity of the most recent pandemic. For example, in the early 2000s, President George W. Bush increased US pandemic stockpiles. These were depleted in the 2009 swine flu pandemic. The pandemic was seen by the public as mild, which led to a backlash over preparedness spending. National stockpiles of medical equipment were not systematically renewed, in the US or in jurisdictions such as France, which in 2013 moved responsibility for personal protective equipment (PPE) stockpiles to public and private enterprises. The French strategic stockpile dropped from one billion surgical masks and 600 million FFP2 masks in 2010 to 150 million and zero, respectively, in early 2020.Manufacturing for many types of health-related equipment had moved offshore, seeking lower costs. American mask manufacturer Prestige Ameritech warned for years that the USA mask supply chain was too dependent on China.Public (World Health Organization (WHO), World Bank, Global Preparedness Monitoring Board) and private initiatives emphasized pandemic threats and preparation. In 2015, Bill Gates began warning about a possible pandemic. This had little impact: WHO's pandemic influenza preparedness project had a US$39 million two-year budget, out of WHO's 2020–2021 budget of US$4.8 billion.In 2018 China experienced a shortage of emergency drugs. In 2019, the Global Preparedness Monitoring Board reported the WHO's pandemic emergency fund remained was depleted by the 2018-19 Kivu Ebola pandemics.As COVID-19 spread in January 2020, China began blocking exports of various medical supplies, including N95 masks, booties, and gloves produced by factories on its territory. Organisations close to the Chinese government began foreign markets for  supplies. This limited other countries' access.

Tests
Initial testing shortages were a key factor limiting authorities ability to measure disease spread. Because the virus was new, tests had to be designed, manufactured, distributed, administered, and evaluated from a standing start. The ability to do this varied dramatically across jurisdictions. Germany started producing and stockpiling COVID-19 tests in January 2020.The United States Centers for Disease Control and Prevention (CDC) initially demanded that universities and other researchers abandon their attempts to make diagnostic PCR tests in favor waiting for the CDC to release its own tests. The CDC then distributed 160,000 defective tests, leaving the US with no testing capacity in the early weeks of the pandemic. By February 27, fewer than 4,000 tests had been conducted in the U.S. CDC released a corrected test in March 2020. By then the pandemic had spread across the country. The Associated Press reported that "the system has been marked by inconsistencies, delays, and shortages", forcing many people with symptoms to wait hours or days to get tested and then days longer to receive the results.Many countries did not have the ability to implement large-scale testing, lacking both tests and the health care infrastructure to administer and evaluate them.

Reagents
In Ireland and the United Kingdom (UK), reagent shortages limited the number of tests evaluated through March and April. Reagent shortages became a bottleneck for mass testing in the European Union (EU).On 1 April, the UK government confirmed that 2,000 NHS staff had been tested for coronavirus since the outbreak began, but Cabinet Office Minister Michael Gove said a shortage of reagents meant it was not possible to screen the NHS's 1.2 million workforce. Gove's statement was contradicted by the Chemical Industries Association, which denied any shortage.Some US hospitals manufactured their own reagents from publicly-available recipes.

Swabs
The US Strategic National Stockpile held no swabs, forcing reliance on commercial supplies, which were soon exhausted by the explosive growth in testing. The US had shortages, despite the fact that one domestic manufacturer increased production to 1 million swabs per day in March, and the government funded it to build a new factory in May. Shortages arose in the UK, but were resolved by 2 April.In May 2020, the US FDA licensed a swab-free saliva test and new swab designs, including 3-D printed swabs that labs, hospitals, and other medical facilities could make themselves. The development process took as little as two weeks.

Personal protective equipment
Personal protective equipment (PPE) stocks ran out around the world in the winter of 2020. People from 86 countries engaged in the voluntary production of PPE to supplement disrupted supply chains.  By summer 2021, shortages had turned to glut, as many manufacturers reduced production.

China
Pre-pandemic, most PPE was made in China. The Chinese government took control of stocks from foreign enterprises whose factories produced these goods. At the outset, China imported some 2.46 billion pieces of PPE between 24 January and 29 February, including 2.02 billion masks and 25.38 million items of protective clothing.In February 2020 WHO minimised the need for PPE, recommending telemedicine; physical barriers such as clear windows; isolating patients; using only PPE necessary for each specific task; reusing respirators without removing them while caring for multiple patients with the same diagnosis; monitoring and coordinating supply chains; and discouraging masking for asymptomatic individuals.China later sent supplies to Spain, Turkey, and the Netherlands that were of poor quality. The Dutch health ministry recalled 600,000 face masks on 21 March for poor fit and dysfunctional filters despite them having a quality certificate. The Spanish government discovered that 60,000 out of 340,000 test kits from a Chinese manufacturer produced inaccurate results. The Chinese Ministry of Foreign Affairs responded that the customer should "double-check the instructions to make sure that you ordered, paid for and distributed the right ones. Do not use non-surgical masks for surgical purposes". In mid-May, the European Commission suspended an order of 10 million Chinese masks after two countries reported receiving sub-standard products. After a batch of 1.5 million masks was distributed, Poland said the 600,000 items they received had no European certificates nor did they comply with the necessary standard.

Hand sanitiser
Hand sanitiser went out of stock in many areas, causing high prices. In response, brewers and distillers began to produce hand sanitizer.

Protective gear
Initial shortages were such that some nurses at one New York City hospital resorted to wearing garbage bags as an alternative to unavailable protective clothing. Small businesses throughout the United States retooled to produce makeshift protective devices, often  through open source initiatives. Many manufacturers donated gear. An example is the COVID-19 Intubation Safety Box, which is an acrylic cube placed over an infected patient's torso, with openings that allow ventilator intubation and extubation while minimising risk to healthcare workers. Amazon banned sales of N95 face masks to prevent price gouging.In March 2020, Doctors' Association UK alleged that shortages were covered-up through intimidating emails, threats of disciplinary action and work suspensions. Some doctors were disciplined by managers annoyed by online postings online regarding shortages. On 18 April, communities secretary Robert Jenrick reported that 400,000 protective gowns and other PPE were on their way to the U.K. from Turkey. One day later, these were delayed, leading hospital leaders to criticise the government for the first time. Only 32,000 items arrived (less than one-tenth). Eventually, all were returned to Turkey for failure to meet NHS standards.Most of the world's glove supply comes from Malaysia; Large orders with trusted companies were typically made "years in advance". Malaysia-based Top Glove and its subsidiary TG Medical were accused of violations of workers' rights, leading U.S. Customs and Border Protection (CBP) to ban their products in July 2020. Thai company Paddy the Room repackaged used gloves and sold tens of millions to U.S. buyers in 2021.

Masks
China
As the pandemic accelerated, the mainland market saw a shortage of face masks. Hoarding and price gouging drove up prices, leading the market regulator to crack down. In January 2020, price controls were imposed on face masks on Taobao, Tmall JD.com, Suning.com, and Pinduoduo; third-party vendors were subject to price caps, and violators to sanctions.

United States
In 2006, 156 million masks were added to the U.S. Strategic National Stockpile in anticipation of a flu pandemic. After they were used against the 2009 flu pandemic, neither the Obama administration nor the Trump administration replenished the stocks. By 1 April, the US stockpile was nearly empty.National Nurses United, the largest organization of registered nurses in the United States, filed over 125 complaints with Occupational Safety and Health Administration (OSHA) offices charging hospitals with failing to comply with laws mandating safe workplaces.

France
In 2010, France's stock included 1 billion surgical masks and 600 million FFP2 masks; in early 2020 it had fallen 150 million and zero, respectively. As the pandemic consumed supplies, stocks ran low and caused national outrage. France instructed its remaining mask-producing factories to work 24/7 shifts, and to expand national production to 40 million masks per month.

Competition for supplies
Countries such as the UK, France, Germany, South Korea, Taiwan, China, India, and others initially responded to the outbreak by limiting or banning exports of medical supplies, including rescinding existing orders. Germany blocked exports of 240,000 masks bound for Switzerland and stopped other shipments to Czechia. Turkey blocked a shipment of ventilators to Spain; 116 were later released.Governments began competing with each other to obtain medical supplies, either through paying higher prices or seizing equipment. Slovakian prime minister Peter Pellegrini said the government was preparing to purchase masks from a Chinese supplier. He then said, "However, a dealer from Germany came there first, paid more for the shipment, and bought it." Ukraine lawmaker Andriy Motovylovets stated, "Our consuls who go to factories find their colleagues from other countries (Russia, USA, France, Germany, Italy, etc) who are trying to obtain our orders. We have paid upfront by wire transfer and have signed contracts. But they have more money, in cash. We have to fight for each shipment." San Marino authorities said they arranged a bank transfer to a supplier in Lugano, Switzerland, to buy a half-million masks to be shared with Italian neighbours, but were outbid.Germany snatched 830,000 surgical masks that were arriving from China and destined for Italy before Italian authorities persuaded Germany to release them. 1.5 million face masks that were supposed to be shipped from Spain to Slovenia were seized by German agents. French guards confiscated lorries filled with 130,000 face masks and boxes of sanitisers bound for the UK. Italian customs police hijacked some 800,000 imported masks and disposable gloves on their way to Switzerland.Trade in medical supplies between the United States and China became politically complicated. Exports of face masks and other medical equipment to China from the United States (and many other countries) spiked in February, according to statistics from Trade Data Monitor, prompting criticism from the Washington Post that the United States government failed to anticipate the domestic needs for that equipment. Similarly, The Wall Street Journal raised concerns that US tariffs on imports from China threaten imports of medical supplies.

Reuse
Mask shortages led to attempts to sanitise and reuse them.
FFP2 masks can be sanitised by 70 °C vapour. Sanitisation is not always simple. Alcohol disrupts N95 mask microfibres' static charge. Chlorine fumes may be harmful.A Singaporean study found no contamination on masks after brief care of COVID-19 patients, suggesting masks could be reused for multiple patients.

DIY
Individuals and volunteers produced cloth masks for themselves and others. Various designs were shared online. 3D-printed "NanoHack" masks allowed hand-cut surgical mask to act as fine-particle filters.Novel mask accessories were created by makers around the world using open source designs such as ear savers to make extended mask use more comfortable.Makers improvised Arduino-controlled disinfection boxes, with temperature controls, to safely sanitise masks.

Face shields
Makers learned to produce face-shields, although these turned out to be of marginal value, as SARS-CoV-2 turned out to be airborne and able to evade the protections that face shields provide. They collectively produced a total of at least 25 million face shields with techniques including 3D printing, laser cutting, and Injection molding.

Medical care devices
Critical care or ICU beds, mechanical ventilation and ECMO devices were critical bottlenecks early in the pandemic.

Oxygen masks
Popular snorkelling masks were adapted into oxygen dispensing respiratory masks via the usage of 3D printed adapters. According to Italian law usage by a patient requires a signed declaration of acceptance of an uncertified biomedical device. The project provided 3D files for free, as well as forms to register hospitals in need and 3D makers willing to produce adapters. In France, sportswear and snorkelling mask producer Decathlon redirect its mask output toward the pandemic. An international collaboration included Decathlon, BIC, Stanford.Maker group Plan B in Romania produced more than 2,000 modified snorkeling masks to combat the pandemic.

Intensive care beds
In early March, the UK government supported a strategy to develop herd immunity to COVID-19, drawing criticism from medical personnel and researchers. Spooked by wildly exaggerated forecasts by the Imperial College COVID-19 Response Team that the demand for intensive care beds would exceed the inventory by 7.5, around 16 March, the UK government switched to a mitigation/suppression strategy.In France, around 15 March, the Grand Est region noted the scarcity of CCB. Assistance-publique Hôpitaux de Paris (AP-HP), which manages most hospitals in the Paris area (~10 million inhabitants), reported the need for 3,000–4,000 ICU beds against a capacity of between 350 and 1500.In France, given shortages of ICU hospital beds in Grand Est and Ile-de-France regions, severe but stable patients with ARS and breathing assistance have been moved toward other regional medical centers within France, Germany, Austria, Luxembourg, or Switzerland.

Mechanical ventilation
Mechanical ventilation was initially called "the device that becomes the decider between life and death" because 3.2% of detected cases were thought to need ventilation during treatment. Ventilator shortages are endemic in the developing world. In case of shortage, triage strategies had been discussed. One strategy was to grade the patient on dimensions such as prospects for short-term survival, prospects for long-term survival, stage of life; pregnancy and fairness. The original 15 to 20 day intubation duration was a complicating factor in the shortage.

Official assessments
In the 2000s, the CDC estimated a national shortage of 40–70,000 ventilators in case of pandemic influenza. This assessment led to Project Aura, a public-private initiative to design a $3,000 simple to mass-produce ventilator that could supply the Strategic National Stockpile. Newport Medical Instruments won the contract, designing and prototyping (2011) the ventilators, and expecting to later profit by moving into the private market where competing devices were sold for $10,000. In April 2012, Health and Human Services officials confirmed to the US Congress that the project was on schedule to file for market approval in late 2013, after which the device would go into mass-production. In May 2012, US$12 billion medical conglomerate Covidien acquired Newport for $100 million. Covidien soon asked to cancel the contract. Former Newport executives, officials and executives at rival ventilator companies claimed that Covidien acquired Newport to avoid disturbing its market. Covidien merged in 2015 into Medtronic. Project Aura contractrf with Philips healthcare. In July 2019, the FDA signed for 10,000 units of their Trilogy Evo portable ventilator to be delivered to the SNS by mid-2020.On 25 March 2020, New York Governor Andrew Cuomo forecasting a severe ventilator shortage. Cuomo claimed his state would need about 30,000 ventilators to handle the pandemic, against an inventory of 4,000. On 27 March, President Donald Trump stated "I don't believe you need 40,000 or 30,000 ventilators", but later that day invoked the Defense Production Act to accelerate production.

Industrial suppliers
In Europe, Löwenstein Medical had been producing 1,500 ICU-level and 20,000 home-level ventilators annually for France alone. The company pointed out the production shortage. Their components were of European origin. It recommended focusing on home-level ventilators that could be assembled in 30 minutes. The bottleneck was trained workers. ICU-level ventilators typically lasted 10 to 15 years. Germany and other European country started to take control over the company's output.Chinese manufacturers also increased production.Medtronic made ventilator design specifications publicly available.

Improvised ventilators
The United Kingdom identified a ventilator shortage in 2016 during the NHS's Exercise Cygnus, but government stockpiles remained insufficient. In March, the British government called for industry to make ventilators for the NHS. Dyson and Babcock revealed plans to create 30,000 ventilators. The Ventilator Challenge involved companies such as Airbus, Rolls-Royce and Ford. This was seen as inadequate; the proposed ventilators would not have been useable in hospitals. None of the companies reached the final stages of testing and the majority were unneeded.Another strategy is to modify circuits to ventilate multiple patients simultaneously from one ventilator. Anesthetist Dr. Alan Gauthier from Ontario, Canada, demonstrated turning one single-patient ventilator into a nine-patient device thanks to a 2006 YouTube video by 2 doctors from Detroit. This and similar methods described for ventilator sharing used T-shaped tubes to split airflow and multiply the number of patients provided with respiratory support. Ventilator sharing was limited by differing lung compliance between the patients (leading to different, possibly harmful, differences in tidal volume delivered to each patient), pendelluft between patients in the circuit, as well as the potential to spread pathogens between the patients.In Ireland, volunteers started the Open Source Ventilator Project in collaboration with medical staff.
In the United States, various teams such at MIT or Princeton developed open ventilator technology.
In Italy, a local journalist and journal director Nunzia Vallini of the Giornale di Brescia (Brescia Daily) was informed that nearby Chiani hospital was running out of valves which mix oxygen with air and are therefore a critical part of reanimation devices. The valves supplier was itself out of stock leading to patient deaths. Vallini contacted FabLab founder Massimo Temporelli, who invited Michele Faini, an expert in additive manufacturing and a research and development designer at Lonati SpA to join a 3D printing effort. When the supplier refused to share design specifics, they reverse-engineered the valves and produced a not-for-profit series for local hospitals. Ventilator splitter valves were used as a last-resort.Hackers of the Ventilator Project proposed to re-purpose CPAP machines (sleep-apnea masks) as ventilators, hacking single ventilators to split air-flow and treat multiple patients, and using grounded aircraft as treatment facilities to leverage their oxygen-mask-per-seat infrastructure. Engineers familiar with device design and production, medical professionals familiar with existing respiratory devices and lawyers able to navigate FDA regulations were participants among the 350 volunteers involved. The central avenue of exploration was to abandon advanced features, including electronics and patient monitoring, to focus solely on respiration by pressured airflow. The group used an old Harry Diamond Laboratories "emergency army respirator" model to study.Shared Ventilation 
Shared ventilation was an idea to fit six people one on ventilator, freeing up five ventilators for use by others. Simulations indicated that this would increase resistance to the extent that it didn't help patients. Increasing the settings restored ventilator function.

Facilities
Hospitals
As Wuhan's situation worsened and to assist the overwhelmed Central Hospital of Wuhan and Dabie Mountain Regional Medical Centre, China built two emergency field hospitals within a few days: the Huoshenshan Hospital and Leishenshan Hospital. The hospitals were phased out in March 2020.French President Emmanuel Macron announced a military hospital would be set up in the Grand-Est region, to provide up to 30 ICU beds. The hospital was tested 7 days later.By 8 March, Lombardy had created 482 new ICU beds. Lodi's ICU director reported that every square metre and every aisle of the hospital had been re-purposed for severe COVID-19 patients, increasing ICU beds from 7 to 24. In Monza, 3 new wards of 50 beds each were opened on 17 March. In Bergamo, gastrology, internal medicine, neurology services were repurposed.In the UK, almost the entire private health stock of beds was requisitioned, numbering 8,000 beds. Three Nightingale Hospitals were created by NHS England and the military, to provide an additional 10–11,000 critical care beds, another 1,000-bed hospital in Scotland, and a 3,000-bed hospital at the Principality Stadium in Cardiff. Temporary wards were constructed in hospital car parks, and existing wards re-organised to free up 33,000 beds in England and 3,000 in Scotland. A hangar at Birmingham Airport was converted into a 12,000 body mortuary.

Morgues
Shortages of space in New York City morgues led the city to propose temporary burial in parks.

Health workers
Healthcare workers also were in critically short supply in some jurisdictions. The pandemic filled many hospitals and limited the personnel who were equipped to care for patients. Training requirements further limited resources, while numerous staff themselves became infected. 
Laboratory workers were brought into the limelight as the COVID-19 testing skyrocketed. Laboratory staff was already reduced, as well as funding shortages, so the pandemic created another strain on those already present issues.Mitigations included recruiting military and sports medics, final-year doctors in training, private sector staff, and re-recruiting retired staff and those who have moved from the medical sector. For non-medical roles, staff have been recruited from other sectors.Various health care systems resorted to increasing patient to nurse ratios when patient loads rose, leading nurses to higher mortality rates, burnout, and dissatisfaction.

Isolation and trauma
The American Medical Association created a guide for healthcare organizations to reduce psychosocial trauma and increase the resilience of medical staff.Paola, Valentine, and Rossella reported that healthcare professionals experienced an impact on their mental health, including stress, anxiety, depression, and sleep disorders, which in some cases, exacerbated staff shortages as workers attempted to cope with high case fatality rates early in the pandemic.

Sickness and death
In Italy, at least 293 doctors died from COVID-19 by mid-2022.Acute respiratory distress syndrome (ARDS) is a form of pneumonia in which air sacs become filled with fluid leaking from the capillaries lungs. Associated inflammation and edema decrease lung compliance, requiring more aggressive treatment and often landing patients in the ICU.In mid-March 2020 in Lombardy, medical staff reported high levels of staff infections. In Lodi, doctors from other services have been called to attend Covid patients. In Cremona, patient admissions were three times normal, with only 50% of staff available. On 12 March 8% of Italy's 13,382 cases were health workers. Between 5 and 10% of deaths were medical staff. On 17 March, one of the largest hospital of the Bergamo region ran out of ICU beds, and patients were flown to other regions.About 14% of Spanish cases in March 2020 were medical staff.In the United States, about 62,000 healthcare workers had tested positive by late May 2020; while 291 had died (0.47%).By late May, Mexico had 11,000 medical staff detected as infected, depleting medical ranks.

Industrial products
Commodities
The pandemic increased consumer demand for propane because more people stayed home during winter, increasing the need for domestic heating and cooking. In the United States, shortages of propane were reported in Kentucky, Louisiana, and Wisconsin in January 2021.In the United States the pandemic caused a shortage of lumber and steel in 2021.

Semiconductors
Increased demand for electronics coincided with semiconductor production disruptions, including a drought in Taiwan (impacting companies such as TSMC). US sanctions on Semiconductor Manufacturing International Corporation (SMIC)—China's largest semiconductor manufacturer—were part of an ongoing trade war. which increased orders for competitors.These shortages reduced production in the automobile and consumer electronics industries. The shortage was amplified by forecasting errors in the automotive industry, which expected that "work-from home" would reduce sales. They reduced their orders, leading semiconductor makers to reduce investment. When automotive demand quickly recovered, the industry was unable to respond. By October 2021, multiple automakers had announced plans to cut or halt production.The chip shortage and a major increase in cryptocurrency mining led to a shortage in high-end graphics cards for computers. Microsoft and Sony Interactive Entertainment warned of shortages of their Xbox Series and PlayStation 5 video game consoles due to high demand and supply chain disruption.

Consumer goods
Some daily goods shortages came as a result of supply chain disruptions and demand spikes, leading to empty shelves. Affected products included toilet paper, hand sanitiser, cleaning supplies, canned food, freezers and other household appliances, sewing machines, blood, baking yeast, game consoles, computers, poultry, and swimming pool chlorine.Bicycle shortages emerge as public transport was impacted. The problem was exacerbated by manufacturing declines.Many shortages were attributed to lean manufacturing, in which many manufactures relied on just-in-time deliveries instead of maintaining larger inventories.In spring 2020, some factories that manufacture condoms were forced to shut down or reduce operations, including the world's largest producer.

Paper products
The pandemic initially led to shortages of toilet paper in various countries. The shortage extended to paper towels, tissues, and diapers. Initially this was blamed on panic buying, despite reassurance from industry and government that neither was likely to occur. Some consumers began hoarding toilet paper, leading to reports of empty shelves, which spiraled into widespread disruption. Essential supply locator sites and tools attempted to assist communities in finding local sources as online retailers stocked out.However, by early April 2020, other factors worsened the situation. Stay-at-home orders led people to spend less time elsewhere. Public toilets were used less and home toilets more. Reorganizing distribution and product mix (public vs home) took time. The Wall Street Journal declared the shortage essentially over in April 2021.

Tampons
Shortages and price increases of tampons and other feminine hygiene products were caused by supply chain disruptions, staffing problems, and raw material costs.  As of mid-June 2022, approximately 7 percent of tampon products were out of stock, and many shoppers struggled to find their preferred brand. Tampons were reported to be in short supply for more than six months..

Aluminium cans
The shift of beverage consumption from public places to homes created an aluminium can shortage in the United States.

Others
In France, closed borders prevented seasonal workers from entering the country. The Minister of Agriculture called for jobless volunteers to contact strawberry farms to help collect the harvest.Laboratory mice were culled, and some strains were at risk of shortage due to lockdowns early in the pandemic.In the United States, social distancing reduced blood donations.

See also

List of countries by hospital beds
Economic impact of the COVID-19 pandemic
2021–2022 global supply chain crisis


== References ==

Peer-to-peer lending

Peer-to-peer lending, also abbreviated as P2P lending, is the practice of lending money to individuals or businesses through online services that match lenders with borrowers. Peer-to-peer lending companies often offer their services online, and attempt to operate with lower overhead and provide their services more cheaply than traditional financial institutions. As a result, lenders can earn higher returns compared to savings and investment products offered by banks, while borrowers can borrow money at lower interest rates, even after the P2P lending company has taken a fee for providing the match-making platform and credit checking the borrower. There is the risk of the borrower defaulting on the loans taken out from peer-lending websites.
Peer-to-peer fundraising encourages supporters of a charity or non-profit organisation to individually raise money. It's a bit subcategory of crowdfunding. Instead of having one main crowdfunding page where everybody donates, people can have multiple individual fundraising pages with peer-to-peer fundraising, which the individual people will share with their own networks.
Also known as crowdlending, many peer-to-peer loans are unsecured personal loans, though some of the largest amounts are lent to businesses. Secured loans are sometimes offered by using luxury assets such as jewelry, watches, vintage cars, fine art, buildings, aircraft, and other business assets as collateral. They are made to an individual, company or charity. Other forms of peer-to-peer lending include student loans, commercial and real estate loans, payday loans, as well as secured business loans, leasing, and factoring.The interest rates can be set by lenders who compete for the lowest rate on the reverse auction model or fixed by the intermediary company on the basis of an analysis of the borrower's credit. The lender's investment in the loan is not normally protected by any government guarantee. On some services, lenders mitigate the risk of bad debt by choosing which borrowers to lend to, and mitigate total risk by diversifying their investments among different borrowers.
The lending intermediaries are for-profit businesses; they generate revenue by collecting a one-time fee on funded loans from borrowers and by assessing a loan servicing fee to investors (tax-disadvantaged in the UK vs charging borrowers) or borrowers (either a fixed amount annually or a percentage of the loan amount). Compared to stock markets, peer-to-peer lending tends to have both less volatility and less liquidity.

Characteristics
Peer-to-peer lending does not fit cleanly into any of the three traditional types of financial institutions – deposit takers, investors, insurers – and is sometimes categorized as an alternative financial service.Typical characteristics of peer-to-peer lending are:

it is sometimes conducted for profit;
no necessary common bond or prior relationship between lenders and borrowers;
intermediation by a peer-to-peer lending company;
transactions take place online;
lenders may often choose which borrowers to invest in, if the P2P platform offers that facility;
the loans can be unsecured or secured and are not normally protected by government insurance;
loans are securities that can be transferred to others, either for debt collection or profit, though not all P2P platforms provide transfer facilities or free pricing choices and costs can be very high, tens of percent of the amount sold, or nil.Early peer-to-peer lending was also characterized by disintermediation and reliance on social networks but these features have started to disappear. While it is still true that the emergence of internet and e-commerce makes it possible to do away with traditional financial intermediaries and that people may be less likely to default to the members of their own social communities, the emergence of new intermediaries has proven to be time and cost saving. Extending crowdsourcing to unfamiliar lenders and borrowers opens up new opportunities.
Most peer-to-peer intermediaries provide the following services:

online investment platform to enable borrowers to attract lenders and investors to identify and purchase loans that meet their investment criteria
development of credit models for loan approvals and pricing
verifying borrower identity, bank account, employment and income
performing borrower credit checks and filtering out the unqualified borrowers
processing payments from borrowers and forwarding those payments to the lenders who invested in the loan
servicing loans, providing customer service to borrowers and attempting to collect payments from borrowers who are delinquent or in default
legal compliance and reporting
finding new lenders and borrowers (marketing)

History
United Kingdom
Zopa, founded in February 2005, was the first peer-to-peer lending company in the United Kingdom. Funding Circle, launched in August 2010, became the first significant peer-to-business lender and offering small businesses loans from investors via the platform. Funding Circle has originated over £6.3 billion in loans.In 2011, Quakle, a UK peer-to-peer lender founded in 2010, closed down with a near 100% default rate after attempting to measure a borrower's creditworthiness according to a group score, similar to the feedback scores on eBay; the model failed to encourage repayment.In 2012, the UK government invested £20 million into British businesses via peer to peer lenders. A second investment of £40 million was announced in 2014. The intention was to bypass the high street banks, which were reluctant to lend to smaller companies. This action was criticised for creating unfair competition in the UK, by concentrating financial support in the largest platforms.Investments have qualified for tax advantages through the Innovative Finance Individual Savings Account (IFISA) since April 2016. In 2016, £80bn was invested in ISAs, creating a significant opportunity for P2P platforms. By January 2017, 17 P2P providers were approved to offer the product.At one stage there were over 100 individual platforms applying for FCA authorisation, although many withdrew their applications as of 2015.Since April 2014, the peer-to-peer lending industry has been regulated by the Financial Conduct Authority to increase accountability with standard reporting and facilitate the growth of the sector. Peer-to-peer investments do not qualify for protection from the Financial Services Compensation Scheme (FSCS), which provides security up to £85,000 per bank, for each saver, but regulations mandate the companies to implement arrangements to ensure the servicing of the loans even if the platform goes bust.In 2015, UK peer-to-peer lenders collectively lent over £3bn to consumers and businesses.According to the Cambridge Centre for Alternative Finance (Entrenching Innovation Report), £3.55B was attributed to Peer to Peer alternative finance models, the largest growth area being property showing a rise of 88% from 2015 to 2016.

United States
The peer-to-peer lending industry in the US started in February 2006 with the launch of Prosper Marketplace, followed by LendingClub. Both Prosper and LendingClub are headquartered in San Francisco, California. Early peer-to-peer platforms had few restrictions on borrower eligibility, which resulted in adverse selection problems and high borrower default rates. In addition, some investors viewed the lack of liquidity for these loans, most of which have a minimum three-year term, as undesirable.In 2008, the U.S. Securities and Exchange Commission (SEC) required that peer-to-peer companies register their offerings as securities, pursuant to the Securities Act of 1933. The registration process was an arduous one; Prosper and LendingClub had to temporarily suspend offering new loans, while others, such as the U.K.-based Zopa Ltd., exited the U.S. market entirely. Both LendingClub and Prosper gained approval from the SEC to offer investors notes backed by payments received on the loans. Prosper amended its filing to allow banks to sell previously funded loans on the Prosper platform. Both LendingClub and Prosper formed partnerships with FOLIOfn to create a secondary market for their notes, providing liquidity to investors. LendingClub had a voluntary registration at this time, whereas Prosper had mandatory registration for all members.This addressed the liquidity problem and, in contrast to traditional securitization markets, resulted in making the loan requests of peer-to-peer companies more transparent for the lenders and secondary buyers who can access the detailed information concerning each individual loan (without knowing the actual identities of borrowers) before deciding which loans to fund. The peer-to-peer companies are also required to detail their offerings in a regularly updated prospectus. The SEC makes the reports available to the public via EDGAR (Electronic Data-Gathering, Analysis, and Retrieval).More people turned to peer-to-peer companies for borrowing following the financial crisis of 2007–2008 because banks refused to increase their loan portfolios. The peer-to-peer market also faced increased investor scrutiny because borrowers' defaults became more frequent and investors were unwilling to take on unnecessary risk.In 2013, LendingClub was the largest peer-to-peer lender in US based upon issued loan volume and revenue, followed by Prosper. LendingClub was also the largest peer-to-peer lending platform worldwide.  The interest rates ranged from 5.6–35.8%, depending on the loan term and borrower rating. The default rates varied from about 1.5% to 10% for the more risky borrowers. Executives from traditional financial institutions are joining the peer-to-peer companies as board members, lenders and investors, indicating that the new financing model is establishing itself in the mainstream. LendingClub abandoned the peer-to-peer lending model in the fall of 2020.

China
Many micro loan companies have emerged to serve the 40 million SMEs, many of which receive inadequate financing from state-owned banks, creating an entire industry that runs alongside big banks.
As the Internet and e-commerce grew in the 2000s, many P2P lenders were founded with various target customers and business models.The first P2PL in Hong Kong was WeLab, which has backing from American venture capital firm Sequoia Capital and Li Ka-Shing's TOM Group.Ezubao, a website launched by Yucheng Group in July 2014 purporting to offer P2P services, was shut down in February 2016 by authorities who described it as a Ponzi scheme. Ezubao took in 50 billion renminbi from 900,000 investors.In China, in 2016 there were more than 4,000 P2P lending platforms, but 2,000 of them had already suspended operations. As of August 2016, cash flow on all P2P lending platform have already exceeded 191 billion Chinese Yuan (US$29 billion) in the month. Lender's return rate across all P2P lending platform in China is about 10% per annum on average, with a few of them offering more than 24% return rate.  A colloquial term for P2P lending in Chinese translates as "grey market", but is not to be confused with grey markets for goods or an underground economy.
In June and July 2018, scores of Chinese online P2P lending platforms fell into financial or legal troubles because of tightened regulation and liquidity. According to WDZJ.com, a P2P industry information provider, 23 P2P platforms were reported to be in financial distress or under investigation in the first 10 days of July. That follows 63 such cases in June, a higher number than in any month in the previous year.In late June, Shanghai police detained four senior executives of Tangxiaoseng, an online lending platform controlled by Zibang Financial Service Internet Technology Co. Ltd. and told investors on June 28, 2018 that Zibang Financial was suspected of "illegally raising funds from the public." On July 20, 2018, iqianbang.com, a Beijing-based P2P lending platform announced to close down, citing "deteriorating online lending environment and drying up liquidity."People's Bank of China announced in early July 2018 said that regulators will extend a two-year-old nationwide campaign to clean up fraud and violations in the online financial market, targeting P2P and other online lending and financial activities. More than 5,000 operations have been shut down since the campaign began in 2016.In April 2019, one of China's top peer-to-peer (P2P) lending platforms, tuandai.com, collapsed, resulting in financial losses for scores of Chinese investors.

Australia
In 2012 Australia's first peer to peer lending platform, SocietyOne, was launched. As of June 2016 the Australian Government has been encouraging the development of financial technology and peer to peer lending startups through its regulatory sandbox program.

New Zealand
In New Zealand, peer-to-peer lending became practicable on April 1, 2014, when the relevant provisions of the Financial Markets Conduct Act 2013 came into force. The Act enables peer-to-peer lending services to be licensed.The Financial Markets Authority issued the first peer-to-peer lending service licence on July 8, 2014, to Harmoney. Harmoney officially launched its service on October 10, 2014.

India
In India, peer-to-peer lending is currently regulated by the Reserve Bank of India, India's Central Bank. It has published a consultation paper on regulation of P2P lending and the final guidelines were released in 2017. There were over 30 peer-to-peer-lending platforms in India in 2016. Even with first-mover advantage many sites were not able to capture market share and grow their user base, arguably because of the reserved nature of Indian investors or lack of awareness of this type of debt financing.  However, peer-to-peer lending platforms in India are helping a huge section of borrowers who have previously been rejected or have failed to qualify for a loan from banks.As on August 31, 2019, 19 companies have been granted licenses by the Reserve Bank of India.

Sweden
Peer-to-peer-lending in Sweden is regulated by Finansinspektionen. Launched in 2007, the company Trustbuddy AB was first out on the Swedish market for peer-to-peer-lending, providing a platform for high risk personal loans between 500SEK and 10,000SEK. Trustbuddy filed for bankruptcy by October 2015, a new board cited abuses by outgoing leadership.

Israel
Several peer-to-peer lending services initiated operation and loan origination during 2014, Following the economic uprising of 2011, and public opinion regarding these platforms is positive. The maximum interest rate in Israeli P2P Arenas is limited by the "Extra-Banking Lending Regulations".

Canada
Loans made under peer-to-peer lending are considered securities and as such P2P platforms must register with securities regulators and adapt themselves to existing regulatory models. This means limiting investors to some institutional investors or finding novel approaches in tandem with regulators. Canadian Capital Markets Securities Regulators (members of the Canadian Securities Administrators) are recent entrants to Canadian Peer-to-Peer P2P lending and are only issuing interim approvals "in order to test their products, services and applications throughout the Canadian market on a time limited basis." through "Regulatory Sandbox" programs including the CSA Regulatory Sandbox and the Ontario Securities Commission Sandbox, branded as "OSC Launchpad".

Brazil
Since April 2018, Brazilian p2p lending companies may operate directly without the intermediation of a bank or other financial institution.By means of the Resolution 4656/2018, the Central Bank of Brazil created a new type of institution called SEP (personal lending society) that aims to provide a platform for direct negotiation of loans between individuals and companies. A SEP cannot lend using its own resources but only operate as an intermediary. The borrower must be Brazilian individual or company, but there isn't a restriction regarding lenders nationality.

Latvia
Latvian P2P lending market is developing rapidly. In Q2 2018 Latvian P2P platforms lent Eur 271.8 million and Eur 1.7 Billion cumulatively. Currently, the most active investors in Latvia's peer-to-peer lending platforms are residents of Germany, Great Britain, and Estonia.The two biggest P2P platforms are Mintos and Twino taking over 60% and 20% of market share respectively. Around nine companies that qualify as P2P investment platform currently operate in Latvia. Mintos was founded in 2015. In September 2018 the total amount of loans funded through Mintos have surpassed Eur 1 billion. Most of the loans funded through Mintos are personal loans with car loans coming second. In 2016 Mintos has raised Eur 2 million in funding from Latvian-based Venture Capital Skillion Ventures. Twino investment platform was launched in 2015, although the company has been operating since 2009 as a loan originator. Since the inception in 2009 Twino has lent more than Eur 500 million in loans. More than 90% of all loans that are on Twino platform are short maturity from one to three months.In 2015, the Ministry of Finance of Latvia initiated development of a new regulation on the peer-to-peer lending in Latvia to establish regulatory requirements, such as rules for management compliance, AML requirements and other prudential measures.

Ireland
The Irish P2P lending platform Linked Finance was launched in 2013. In 2016, Linked Finance was also authorised to operate in the UK by the Financial Conduct Authority. In 2015, Initiative Ireland launched the first property-backed secured lending P2P platform in Ireland.

Indonesia
In Indonesia, P2P lending is growing fast in recent years and is regulated under OJK since 2016. As of April 2019, there are 106 P2P platforms registered in OJK. P2P platforms provide loans targeting particularly into unbanked population, which is estimated to be around 100+ million in Indonesia.
Thousands of P2P platforms are illegal. Their applications are believed to be stealing customer's data such as phone contacts and photos. These are then used by the debt collectors to intimidate the customers. The debt collectors contact family members, friends, and even employers of the customers then telling them that the customers have debt that needs to be paid. Some of them commit suicide due to the pressure. Many cases are reported in the Indonesia's complaint handling system. Yet the police have not taken serious actions against these cases.

Bulgaria
There is no specific Peer-to-Peer lending regulation in Bulgaria. Currently, Klear Lending is the only Bulgarian platform. It was launched in 2016 and provides personal loans to prime customers. The Peer-to-Peer lending platform is operated by Klear Lending AD, a financial institution registered in the Register per art. 3a of the Credit Institutions Act maintained by the Bulgarian National Bank.

Korea
In Korea, Money Auction and Pop Funding are the very first peer to peer lending companies founded in 2006 and 2007 respectively. Korean P2P lending industry did not attract much public attention until late 2014 and early 2015, during which period a number of new fintech companies were founded underpinned by the global fintech wave with the emergence of Lending Club as the mainstream P2P lending player in the US. New P2P lending companies launched in Korea during this period include 8 Percent, Terafunding, Lendit, Honest Fund and Funda. At the beginning, 8 Percent, Lendit and Honest Fund focused on personal loan origination and Terafunding was the only P2P platform dedicated to the real estate backed loan origination, founded by ex-real estate broker and investor, Tae Young Yang.
There was a brief period of regulatory uncertainty on the P2P business model as the P2P lending model was not officially legalized under the then regulatory regime. 8 percent was briefly shut down by the regulator in Feb 2015 and was reopened again. Korean P2P industry saw an explosive growth in a year. According to the regulator, cumulative P2P lending platform loan origination increased to KRW 311,800,000,000 as of December in 2016 from KRW 72,400,000,000 in March and there was a debate as to whether the industry was getting overheated, with questions on whether the industry offered appropriate investor protection. To respond to these concerns, as of February 2017, Korean regulator imposed an annual investment limit of KRW 10,000,000 for a retail investor on these lending platforms, and KRW 40,000,000 for certain qualified investors.As of April 2017, there are 148 P2P lending companies in Korea. However, only 40 companies are official members of the Korea P2P Finance Association. These members include Lendit, Roof Funding, Midrate, HF Honest Fund, Villy, 8 Percent, Terafunding, Together Funding and People Funding. According to the Korea P2P Finance Association, cumulative loan lent by its member P2P companies stands at c. KRW 2.3 TRN as of March 2018. By origination category, real estate project financing origination constitutes c. KRW 768,500,000,000, real estate asset backed origination is KRW 611,500,000,000, other asset backed KRW 472,400,000,000 and personal loan origination stands at KRW 443,200,000,000. Average interest yield offered by the member companies is 14.32%.

Germany
In Germany, P2P lending is growing fast in recent years and is regulated under Federal Financial Supervisory Authority. The transaction volume will reach an estimated value of €252 million in 2020.

Legal regulation
In many countries, soliciting investments from the general public is considered illegal. Crowd sourcing arrangements in which people are asked to contribute money in exchange for potential profits based on the work of others are considered to be securities.
Dealing with financial securities is connected to the question of ownership: in the case of person-to-person loans, the problem is who owns the loans (notes) and how that ownership is transferred between the originator of the loan (the person-to-person lending company) and the individual lender(s). This question arises especially when a peer-to-peer lending company does not merely connect lenders and borrowers but also borrows money from users and then lends it out again. Such activity is interpreted as a sale of securities, and a broker-dealer license and the registration of the person-to-person investment contract is required for the process to be legal. The license and registration can be obtained at a securities regulatory agency such as the U.S. Securities and Exchange Commission (SEC) in the U.S., the Ontario Securities Commission in Ontario, Canada, the Autorité des marchés financiers in France and Québec, Canada, or the Financial Services Authority in the UK.
Securities offered by the U.S. peer-to-peer lenders are registered with and regulated by the SEC. A recent report by the U.S. Government Accountability Office explored the potential for additional regulatory oversight by Consumer Financial Protection Bureau or the Federal Deposit Insurance Corporation, though neither organization has proposed direct oversight of peer-to-peer lending at this time.  In 2016, New York state sent "warning letters" threatening to require 28 peer-to-peer lenders to obtain a license to operate unless they "immediately" complied with responses to demands to disclose their lending practices and products available in the state.In the UK, the emergence of multiple competing lending companies and problems with subprime loans has resulted in calls for additional legislative measures that institute minimum capital standards and checks on risk controls to preclude lending to riskier borrowers, using unscrupulous lenders or misleading consumers about lending terms.

Advantages and criticism
Interest rates
One of the main advantages of person-to-person lending for borrowers can sometimes be better rates than traditional bank rates can offer. The advantages for lenders can be higher returns than obtainable from a savings account or other investments, but subject to risk of loss, unlike a savings account. Interest rates and the methodology for calculating those rates varies among peer-to-peer lending platforms.  The interest rates may also have a lower volatility than other investment types.

Socially-conscious investment
For investors interested in socially conscious investing, peer-to-peer lending offers the possibility of supporting the attempts of individuals to break free from high-rate debt, assist persons engaged in occupations or activities that are deemed moral and positive to the community, and avoid investment in persons employed in industries deemed immoral or detrimental to community.

Credit risk
Peer-to-peer lending also attracts borrowers who, because of their credit status or the lack thereof, are unqualified for traditional bank loans. Because past behavior is frequently indicative of future performance and low credit scores correlate with high likelihood of default, peer-to-peer intermediaries have started to decline a large number of applicants and charge higher interest rates to riskier borrowers that are approved.It seemed initially that one of the appealing characteristics of peer-to-peer lending for investors was low default rates, e.g. Prosper's default rate was quoted to be only at about 2.7% in 2007.The actual default rates for the loans originated by Prosper in 2007 were in fact higher than projected. Prosper's aggregate return (across all credit grades and as measured by LendStats.com, based upon actual Prosper marketplace data) for the 2007 vintage was (6.44)%, for the 2008 vintage (2.44)%, and for the 2009 vintage 8.10%. Independent projections for the 2010 vintage are of an aggregate return of 9.87. During the period from 2006 through October 2008 (referred to as 'Prosper 1.0'), Prosper issued 28,936 loans, all of which have since matured. 18,480 of the loans fully paid off and 10,456 loans defaulted, a default rate of 36.1%. $46,671,123 of the $178,560,222 loaned out during this period was written off by investors, a loss rate of 26.1%.Since inception, Lending Club's default rate ranges from 1.4% for top-rated three-year loans to 9.8% for the riskiest loans.The UK peer-to-peer lenders quote the ratio of bad loans at 0.84% for Zopa of the £200m during its first seven years of lending history. As of November 2013, Funding Circle's current bad debt level was 1.5%, with an average 5.8% return after all bad debt and fees. This is comparable to the 3–5% ratio of mainstream banks and the result of modern credit models and efficient risk management technologies used by P2P companies.At the other end of the range are places such as Bondora that do lending to less credit-worthy customers, with default rates varying up to as high as 70+% for loans made to Slovak borrowers on that platform, well above those of its original Estonian market.

Government protection
Because, unlike depositors in banks, peer-to-peer lenders can choose themselves whether to lend their money to safer borrowers with lower interest rates or to riskier borrowers with higher returns, in the US peer-to-peer lending is treated legally as investment and the repayment in case of borrower defaulting is not guaranteed by the federal government (U.S. Federal Deposit Insurance Corporation) the way bank deposits are.A class action lawsuit, Hellum v. Prosper Marketplace, Inc., was held in Superior Court of California on behalf of all investors who purchased a note on the Prosper platform between January 1, 2006, and October 14, 2008. The plaintiffs alleged that Prosper offered and sold unqualified and unregistered securities, in violation of California and federal securities laws during that period. Plaintiffs further allege that Prosper acted as an unlicensed broker/dealer in California. The Plaintiffs were seeking rescission of the loan notes, rescissory damages, damages, and attorneys' fees and expenses. On July 19, 2013, the class action lawsuit was settled. Under the settlement terms Prosper will pay $10 million to the class action members.

Peer-to-peer lending sponsors
Peer-to-peer lending sponsors are organizations that handle loan administration on behalf of others including individual lenders and lending agencies, but do not loan their own money. Notable peer-to-peer lending sponsors include:

Kiva
Lendwithcare
MYC4 (defunct)
Vittana (defunct)
Wokai (defunct)
Zidisha

See also


== References ==

Personal finance

Personal finance is the financial management which an individual or a family unit performs to budget, save, and spend monetary resources over time, taking into account various financial risks and future life events.
When planning personal finances, the individual would consider the suitability to their needs of a range of banking products (checking, savings accounts, credit cards and consumer loans) or investment in private equity, (companies' shares, bonds, mutual funds) and insurance (life insurance, health insurance, disability insurance) products or participation and monitoring of and- or employer-sponsored retirement plans, social security benefits, and income tax management.

History
Before a specialty in personal finance was developed, various disciplines which are closely related to it, such as family economics, and consumer economics, were taught in various colleges as part of home economics for over 100 years.The earliest known research in personal finance was done in 1920 by Hazel Kyrk. Her dissertation at University of Chicago laid the foundation of consumer economics and family economics. Margaret Reid, a professor of Home Economics at the same university, is recognized as one of the pioneers in the study of consumer behavior and Household behavior.In 1947, Herbert A. Simon, a Nobel laureate, suggested that a decision-maker did not always make the best financial decision because of limited educational resources and personal inclinations. In 2009, Dan Ariely suggested the 2008 financial crisis showed that human beings do not always make rational financial decisions, and the market is not necessarily automated and corrective of any imbalances in the economy.Research into personal finance is based on several theories, such as social exchange theory and andragogy (adult learning theory). Professional bodies such as American Association of Family and Consumer Sciences and the American Council on Consumer Interests started to play an important role in developing this field from the 1950s to the 1970s. The establishment of the Association for Financial Counseling and Planning Education (AFCPE) in 1984 at Iowa State University and the Academy of Financial Services (AFS) in 1985 marked an important milestone in personal finance history. Attendances of the two societies mainly come from faculty and graduates from business and home economics colleges. AFCPE started to offered several certifications for professionals in this field, such as Accredited Financial Counselor (AFC) and Certified Housing Counselor (CHC). Meanwhile, AFS cooperates with Certified Financial Planner (CFP Board).Before 1990, the study of personal finance received little attention from mainstream economists and business faculties. However, several American universities such as Brigham Young University, Iowa State University, and San Francisco State University started to offer financial educational programs in both undergraduate and graduate programs since the 1990s. These institutions published several works in journals such as The Journal of Financial Counseling and Planning and the Journal of Personal Finance. 
As the concerns about consumers' financial capability increased during the early 2000s, various education programs emerged, catering to a broad audience or a specific group of people, such as youth and women. The educational programs are frequently known as "financial literacy". However, there was no standardized curriculum for personal finance education until after the 2008 financial crisis. The United States President's Advisory Council on Financial Capability was set up in 2008 to encourage financial literacy among the American people. It also stressed the importance of developing a standard in financial education.

Personal finance principles
Individual situations vary significantly when it comes to income, wealth, and consumption requirements. Moreover, tax and financial regulations vary between countries, and market conditions change both geographically and over time. This means that advice for one person might not be appropriate for another. A financial advisor can offer personalized advice in complicated situations and for high-wealth individuals. Still, University of Chicago professor Harold Pollack and personal finance writer Helaine Olen argue that in the United States, good personal finance advice boils down to a few simple points:
Pay off your credit card balance every month in full
Save 20% of your income
Create an emergency fund that can last you at least 6 months
Maximize contributions to tax-advantaged funds such as a 401(k) retirement funds, individual retirement accounts, and 529 education savings plans
When investing savings:
Avoid trading individual securities
Look for low-cost, diversified mutual funds that balance risk vs. reward appropriately to your target retirement year
If using a financial advisor, require them to commit to a fiduciary duty to act in your best interest

Personal financial planning process
The key component of personal finance is financial planning, a dynamic process requiring regular monitoring and re-evaluation. In general, it involves five steps:
Assessment: A person's financial situation is assessed by compiling simplified versions of financial statements, including balance sheets and income statements. A personal balance sheet lists the values of personal assets (e.g., car, house, clothes, stocks, bank account, cryptocurrencies), along with personal liabilities (e.g., credit card debt, bank loan, mortgage). A personal income statement lists personal income and expenses.
Goal setting: Multiple goals are expected, including short- and long-term goals. For example, a long-term goal would be to "retire at age 65 with a personal net worth of $1,000,000," while a short-term goal would be to "save up for a new computer in the next month." Setting financial goals helps to direct financial planning. Goal setting is done to meet specific financial requirements.
Plan creation: The financial plan details how to accomplish the goals. It could include, for example, reducing unnecessary expenses, increasing employment income, or investing in the stock market.
Execution: Execution of a financial plan often requires discipline and perseverance. Many people obtain assistance from professionals such as accountants, financial planners, investment advisers, and lawyers.
Monitoring and reassessment: The financial plan is monitored for possible adjustments or reassessments as time passes.Typical goals that most adults and young adults have are paying off credit card/student loan/housing/car loan debt, investing for retirement, investing for college costs for children, and paying medical expenses.Need for Personal Finance
There is a great need for people to understand and take control of their finances. These are some of the overarching reasons for it;
1. No formal education for personal finance: Most countries have a formal education across most disciplines or areas of study.

Their pursuit translates to earning tangible outcomes in the form of money.
Even when we realize the above to be a primary objective, there is no formal education at an elementary level in schools or colleges to learn money management or personal finance.
Hence, it is essential to understand this gap or disconnect in the education system where there is no formal way of equipping individuals to manage their own money.This illustrates the need to learn personal finance from an early stage, to differentiate between needs vs. wants and plan accordingly.
2. Shortened employable age: Over the years, with the advent of automation  and changing needs; it has been witnessed across the globe that several jobs that require manual intervention or that are mechanical are increasingly becoming redundant.

Several employment opportunities are shifting from countries with higher labor costs to countries with lower labor costs keeping margins low for companies.
In economies with a considerably large younger population entering the workforce who are more equipped with the latest technologies, several employees in the middle management who have not up-skilled are easily replaceable with new and fresh talent that is cheaper and more valuable to the organizations.
Cyclical nature of several industries like automobile, chemicals, construction; consumption and demand is driven by the health of the countries economy. It has been observed that when economies stagnate, are in recession, and in war - specific industries suffer more than others. This results in companies rationalizing their workforce. An individual can lose their job quickly and remain unemployed for a considerable time. All these reasons bring to the realization that the legal employable age of 60 is slowly and gradually becoming shorter.These are some of the reasons why individuals should start planning for their retirement and systematically build on their retirement corpus, hence the need for personal finance.
3. Increased life expectancy: With the developments in healthcare, people today live till a much older age than their forefathers. The average life expectancy has changed, and people, even in developing economies, live much longer. The average life expectancy has gradually shifted from 60 to 81 and upwards. Increased life expectancy coupled with a shorter employable age reinforces the need for a large enough retirement corpus and the importance of personal finance.
4. Rising medical expenses: Medical expenses including cost of prescription medicine, hospital admission care and charges, nursing care, specialized care, geriatric care have all seen an exponential rise over the years. Many of these medical expenses are not covered through insurance policies that might either be private/individual insurance coverage or through federal or national insurance coverage.

In developed markets like the US, insurance coverage is provided by either the employers, private insurers or through federal government (Medicare, primarily for senior citizens or Medicaid, primarily for individuals of lower income levels). However, with the rising US fiscal deficit and large proportion of the senior population, it needs to be seen whether the extent of the Medicare program is sustainable in the long run, therapy exclusions in the coverage, co-pay, deductibles - several cost elements are to be borne by individuals continually.
In other developed markets like the EU, most medical care is nationally reimbursed. This leads to the national healthcare budgets being very tightly controlled. Many newer, expensive therapies are frequently excluded from national formularies. This means that patients may not have access through the government policy and would have to pay out of pocket to avail of these medicines
In developing countries like India, China, most of the expenses are out of pocket as there is no overarching government social security system covering medical expenses.These reasons illustrate the need to have medical, accidental, critical illness, life coverage insurance for oneself and one's family as well as the need for emergency corpus; translating the immense need for personal finance.

Areas of focus
Critical areas of personal financial planning, as suggested by the Financial Planning Standards Board, are:
Financial position: is concerned with understanding the personal resources available by examining net worth and household cash flow.  Net worth is a person's balance sheet, calculated by adding up all assets under that person's control, minus all household liabilities, at one point.  Household cash flow totals all the expected income sources within a year minus all expected expenses within the same year.  From this analysis, the financial planner can determine to what degree and when the personal goals can be accomplished.
Adequate protection: or insurance, the analysis of how to protect a household from unforeseen risks.  These risks can be divided into liability, property, death, disability, health, and long-term care.  Some wagers may be self-insurable, while most require an insurance contract.  Determining how much insurance to get, at the most cost-effective terms, requires knowledge of the market for personal insurance.  Business owners, professionals, athletes, and entertainers need specialized insurance professionals to protect themselves adequately.  Since insurance also enjoys some tax benefits, utilizing insurance investment products may be critical to the overall investment planning.
Tax planning: typically, the income tax is the single largest expense in a household. Managing taxes is not a question of whether or not taxes will be paid but when and how much.  The government gives many incentives in the form of tax deductions and credits, which can be used to reduce the lifetime tax burden.  Most modern governments use a progressive tax.  As one's income grows, a higher marginal rate of tax must be paid.  Understanding how to take advantage of the myriad tax breaks when planning one's finances can significantly impact.
Investment and accumulation goals: planning how to accumulate enough money for large purchases and life events is what most people consider financial planning. Significant reasons to get assets include purchasing a house or car, starting a business, paying for education expenses, and saving for retirement.
Achieving these goals requires projecting their costs and when to withdraw funds.  A significant risk to the household in achieving their accumulation goal is the rate of price increases over time, or inflation.  Using net present value calculators, the financial planner will suggest a combination of asset earmarking and regular savings to be invested in various investments.  To overcome the rate of inflation, the investment portfolio has to get a higher rate of return, which typically will subject the portfolio to several risks.  Managing these portfolio risks is often accomplished using asset allocation, which seeks to diversify investment risk and opportunity.  This asset allocation will prescribe a percentage allocation for stocks, bonds, cash, and alternative investments.  The budget should also consider every investor's risk profile since risk attitudes vary from person to person.
Depreciating Assets- One thing to consider with personal finance and net worth goals is depreciating assets. A depreciating asset is an asset that loses value over time or with use. A few examples would be the vehicle a person owns, boats, and capitalizes expenses. They add value to a person's life, but unlike other assets, they do not make money and should be a class of their own. In the business world, these are depreciated over time for tax and bookkeeping purposes because their useful life runs out. This is known as accumulated depreciation, and the asset will eventually need to be replaced.
Retirement planning is understanding how much it costs to live at retirement and developing a plan to distribute assets to meet any income shortfall. Methods for retirement plans include taking advantage of government-allowed structures to manage tax liability, including individual (IRA) structures or employer-sponsored retirement plans.
Estate planning involves planning to disposition one's assets after death.  Typically, a tax is due to the state or federal government when one dies.  Avoiding these taxes means more of one's assets will be distributed to their heirs.  One can leave their assets to family, friends, or charitable groups.
Delayed gratification: Delayed gratification, or deferred gratification, is the ability to resist the temptation for an immediate reward and wait for a later reward. This is thought to be an important consideration in the creation of personal wealth. 
Cash Management: It is the soul of your financial planning, whether you are an employee or planning your retirement. It is a must for every financial planner to know how much they spend before their retirement so that they can save a significant amount. This analysis is a wake-up call as many of us know our income, but very few track their expenses.
Revisiting Written Financial Plan Regularly: Make monitoring your financial plan regularly a habit. An annual financial planning review with a professional keeps you well-positioned and informed about the required changes, if any, in your needs or life circumstances. It would be best to be prepared for all the sudden curve balls life throws.
Education Planning: With the growing interest in students' loans, having a proper financial plan in place is crucial. Parents often want to save for their kids but make the wrong decisions, adversely affecting the savings. We often observe that many parents give their kids expensive gifts or unintentionally endanger the opportunity to obtain the much-needed grant. Instead, one should make their kids prepare for the future and support them financially in their education.
Real Estate Planning:  Shelter is a basic human need, and as such, it is imperative that one understands how to obtain a place to live and at the same time maintain their financial security.  Housing can be very complicated, with decisions regarding buying or renting, mortgages, insurance, taxes, utilities, maintenance, etc.  Apartment or house?  That question is crucial for any individual as each option has pros and cons.
Buy or Rent: If you choose to buy a house, you can make a financial investment in your home and improve your credit score and history. You could make your life more stable. But it would be best if you cared about the price of the house, including the down payment, monthly mortgage payment, repair and maintenance costs, HOA fees, property taxes and other costs. Otherwise, you pay fully. If you choose to rent a home, there is no need to worry about maintenance and no real estate taxes. Then your life will have more flexibility since you can move wherever you want. But it would be best to care about the rent fee, including electricity, water, internet, and parking. If you have a pet, you will also have a pet fee.
Mortgages: When purchasing a home/real estate, it is essential to understand your options. Most people either go with a 15 or 30-year plan. The payment rate can be a fixed plan, a constant payment of the same amount over a certain period. The other is an ARM mortgage (Adjustable-Rate Mortgage). This rate can be adjusted and agreed upon to be changed in the given plan depending on mortgage rate flunctuations. Mortgage plans depend on your situation, and it is essential to assess your credit score and financial status when contemplating plans.
Location / Wants and Needs: When choosing a new home, it is essential to consider where you would like to reside, along with the qualities that you both want and need in a home. These variables can cause an increase or decrease in the price of an estate. When deciding where you want to live, some things you should consider include, but are not limited to, whether you’d prefer the city or rural area, what length of a commute you want, the importance of quality public schools, what level of safety you’d like to have, the amount of land you want, included amenities, and if you’d like to live close to family. Examples of variables that would affect the value of an estate include but are not limited to, the quality of school systems in that area, proximity to the community, shopping and entertainment/recreation, safety levels and crime rates of the neighborhood, amenities, and land size and surrounding developments. It is essential to keep all of this in mind when thinking about the future value of a home and, if you are buying, how much it will be worth if you want to sell it later.
Costs: Real estate can be expensive. A planner must consider all sorts of costs, such as a mortgage, maintenance, taxes, utilities, insurance, etc. What mortgage plan are you looking at: fixed vs. ARM? 15 years vs 30 years? Oh no, a tree just fell on your roof; how will you fix it, with your own time and work or with cash to hire a handy person? Homeowner's insurance is important; however, what should be covered can be tricky. Can you get by with a basic plan covering aspects such as dwelling and loss of use, or do you require additional coverage such as flood insurance? Own a house? Prepare for property tax. The heat won't stay on itself, your energy company needs its cut, and the same goes for electricity and water. Keeping all these aspects in mind, it is no question that there are many associated costs with real estate.

Education and tools
According to a survey done by Harris Interactive, 99% of the adults agreed that personal finance should be taught in schools. Financial authorities and the American federal government had offered free educational materials online to the public. However, a Bank of America poll found that 42% of adults were discouraged. In comparison, 28% of adults thought that personal finance is difficult because of the vast amount of online information. As of 2015, 17 out of 50 states in the United States require high school students to study personal finance before graduation. The effectiveness of financial education on general audience is controversial. For example, a study by Bell, Gorin, and Hogarth (2009) stated that financial education graduates were more likely to use a formal spending plan. Financially educated high school students are more likely to have a savings account with regular savings, fewer overdrafts, and more likely to pay off their credit card balances. However, another study done by Cole and Shastry (Harvard Business School, 2009) found that there were no differences in saving behaviors of people in American states with financial literacy mandate enforced and the states without a literacy mandate.Kiplinger publishes magazines on personal finance.

See also
References
Further reading
Graham, Benjamin; Jason Zweig (8 July 2003) [1949]. The Intelligent Investor. Warren E. Buffett (collaborator) (2003 ed.). HarperCollins. front cover. ISBN 0-06-055566-1.
Kwok, H., Milevsky, M., and Robinson, C. (1994) Asset Allocation, Life Expectancy, and Shortfall, Financial Services Review, 1994, vol 3(2), pg. 109–126.
Rich Dad Poor Dad: What the Rich Teach Their Kids About Money That the Poor and Middle Class Do Not!, by Robert Kiyosaki and Sharon Lechter. Warner Business Books, 2000. ISBN 0-446-67745-0
Stanley, Thomas J. and Danko, W.D. (1998). The Millionaire Next Door. Gallery Books. ISBN 978-0-671-01520-6. LCCN 98046515.{{cite book}}:  CS1 maint: multiple names: authors list (link)
Opdyke, J.D. (2010). The Wall Street Journal. Complete Personal Finance Guidebook. The Wall Street Journal Guidebooks. Crown Publishing Group. p. 256. ISBN 978-0-307-49887-8.
Clason, George (2015). The Richest Man in Babylon: Original 1926 Edition. CreateSpace Independent Publishing Platform. ISBN 978-1-508-52435-9.
Bogle, John Bogle (2007). The Little Book of Common Sense Investing: The Only Way to Guarantee Your Fair Share of Stock Market Returns. John Wiley and Sons. pp. 216. ISBN 9780470102107.
Dominguez, J.R. and Robin, Vicki (1993). Your Money Or Your Life: Transforming Your Relationship with Money and Achieving Financial Independence. Penguin Books. ISBN 978-0-140-16715-3. LCCN 92003027.{{cite book}}:  CS1 maint: multiple names: authors list (link)
Bach, D. (2009). The Automatic Millionaire: Canadian Edition: A Powerful One-Step Plan to Live and Finish Rich. Doubleday Canada. ISBN 978-0-307-37209-3.
Ramsey, Dave (2003). The Total Money Makeover: A Proven Plan for Financial Fitness. Thomas Nelson. ISBN 978-1-418-52999-4.

External links
 Media related to Personal finance at Wikimedia Commons

Poka-yoke

Poka-yoke (ポカヨケ, [poka joke]) is a Japanese term that means "mistake-proofing" or "error prevention". A poka-yoke is any mechanism in a process that helps an equipment operator avoid (yokeru) mistakes (poka) and defects by preventing, correcting, or drawing attention to human errors as they occur.  The concept was formalized, and the term adopted, by Shigeo Shingo as part of the Toyota Production System.

Etymology
Poka-yoke was originally baka-yoke, but as this means "fool-proofing" (or "idiot-proofing") the name was changed to the milder poka-yoke.
Poka-yoke is derived from poka o yokeru (ポカを避ける), a term in shogi that means avoiding an unthinkably bad move.

Usage
More broadly, the term can refer to any behavior-shaping constraint designed into a process to prevent incorrect operation by the user.
A simple poka-yoke example is demonstrated when a driver of the car equipped with a manual gearbox must press on the clutch pedal (a process step, therefore a poka-yoke) prior to starting an automobile. The interlock serves to prevent unintended movement of the car. Another example of poka-yoke would be the car equipped with an automatic transmission, which has a switch that requires the car to be in "Park" or "Neutral" before the car can be started (some automatic transmissions require the brake pedal to be depressed as well). These serve as behavior-shaping constraints as the action of "car in Park (or Neutral)" or "foot depressing the clutch/brake pedal" must be performed before the car is allowed to start. The requirement of a depressed brake pedal to shift most of the cars with an automatic transmission from "Park" to any other gear is yet another example of a poka-yoke application. Over time, the driver's behavior is conformed with the requirements by repetition and habit.

History
The term poka-yoke was applied by Shigeo Shingo in the 1960s to industrial processes designed to prevent human errors. Shingo redesigned a process in which factory workers, while assembling a small switch, would often forget to insert the required spring under one of the switch buttons. In the redesigned process, the worker would perform the task in two steps, first preparing the two required springs and placing them in a placeholder, then inserting the springs from the placeholder into the switch. When a spring remained in the placeholder, the workers knew that they had forgotten to insert it and could correct the mistake effortlessly.Shingo distinguished between the concepts of inevitable human mistakes and defects in the production. Defects occur when the mistakes are allowed to reach the customer. The aim of poka-yoke is to design the process so that mistakes can be detected and corrected immediately, eliminating defects at the source.

Implementation in manufacturing
Poka-yoke can be implemented at any step of a manufacturing process where something can go wrong or an error can be made.  For example, a fixture that holds pieces for processing might be modified to only allow pieces to be held in the correct orientation, or a digital counter might track the number of spot welds on each piece to ensure that the worker executes the correct number of welds.Shingo recognized three types of poka-yoke for detecting and preventing errors in a mass production system:
The contact method identifies product defects by testing the product's shape, size, color, or other physical attributes.
The fixed-value (or constant number) method alerts the operator if a certain number of movements are not made.
The motion-step (or sequence) method determines whether the prescribed steps of the process have been followed.Either the operator is alerted when a mistake is about to be made, or the poka-yoke device actually prevents the mistake from being made. In Shingo's lexicon, the former implementation would be called a warning poka-yoke, while the latter would be referred to as a control poka-yoke.Shingo argued that errors are inevitable in any manufacturing process, but that if appropriate poka-yokes are implemented, then mistakes can be caught quickly and prevented from resulting in defects. By eliminating defects at the source, the cost of mistakes within a company is reduced.A methodic approach to build up poka-yoke countermeasures has been proposed by the Applied Problem Solving (APS) methodology, which consists of a three-step analysis of the risks to be managed:

Identification of the need
Identification of possible mistakes
Management of mistakes before satisfying the needThis approach can be used to emphasize the technical aspect of finding effective solutions during brainstorming sessions.

Benefits of poka-yoke implementation
A typical feature of poka-yoke solutions is that they don't let an error in a process happen. Other advantages include:
Less time spent on training workers;
Elimination of many operations related to quality control;
Unburdening of operators from repetitive operations;
Promotion of the work improvement-oriented approach and actions;
A reduced number of rejects;
Immediate action when a problem occurs;
100% built-in quality control;
Preventing bad products from reaching customers;
Detecting mistakes as they occur;
Eliminating defects before they occur.

See also
Defensive design
Fail-safe
Idiot-proof
Interlock
Murphy's law

References
Further reading
Shingo, Shigeo (1986). Zero quality control: source inspection and the poka-yoke system. Portland, Oregon: Productivity Press. ISBN 0-915299-07-0. OCLC 13457086. Retrieved 30 April 2009.
Nikkan Kogyo Shimbun (1988). Poka-yoke: improving product quality by preventing defects. Portland, Oregon: Productivity Press. ISBN 0-915299-31-3. OCLC 300302752.
Hinckley, C. M.; P. Barkan (1995). "The role of variation, mistakes, and complexity in producing nonconformities". Journal of Quality Technology. 27 (3): 242–249. doi:10.1080/00224065.1995.11979596.
Misiurek, Bartosz (2016). Standardized Work with TWI: Eliminating Human Errors in Production and Service Processes. New York: Productivity Press. ISBN 9781498737548.

External links
Mistake-proofing example wiki
Mistake-Proofing - Fool-Proofing - Failsafing

Portland, Oregon

Portland ( PORT-lənd) is a port city in the Pacific Northwest and the most populous city in the U.S. state of Oregon. Situated in the northwestern area of the state at the confluence of the Willamette and Columbia rivers, Portland is the county seat of Multnomah County, the most populous county in Oregon. As of 2020, Portland had a population of 652,503, making it the 26th-most populated city in the United States, the sixth-most populous on the West Coast, and the second-most populous in the Pacific Northwest, after Seattle. Approximately 2.5 million people live in the Portland–Vancouver–Hillsboro, OR–WA metropolitan statistical area, making it the 25th most populous in the United States. About half of Oregon's population resides within the Portland metropolitan area.Named after Portland, Maine, which is itself named after the English Isle of Portland, the Oregon settlement began to be populated in the 1840s, near the end of the Oregon Trail. Its water access provided convenient transportation of goods, and the timber industry was a major force in the city's early economy. At the turn of the 20th century, the city had a reputation as one of the most dangerous port cities in the world, a hub for organized crime and racketeering. After the city's economy experienced an industrial boom during World War II, its hard-edged reputation began to dissipate. Beginning in the 1960s, Portland became noted for its growing liberal and progressive political values, earning it a reputation as a bastion of counterculture.The city operates with a commission-based government, guided by a mayor and four commissioners, as well as Metro, the only directly elected metropolitan planning organization in the United States. Its climate is marked by warm, dry summers and cool, rainy winters. This climate is ideal for growing roses,  and Portland has been called the "City of Roses" for over a century.

History
Pre-history
During the prehistoric period, the land that would become Portland was flooded after the collapse of glacial dams from Lake Missoula, in what would later become Montana. These massive floods occurred during the last ice age and filled the Willamette Valley with 300 to 400 feet (91 to 122 m) of water.Before American settlers began arriving in the 1800s, the land was inhabited for many centuries by two bands of indigenous Chinook people – the Multnomah and the Clackamas. The Chinook people occupying the land were first documented in 1805 by Meriwether Lewis and William Clark. Before its European settlement, the Portland Basin of the lower Columbia River and Willamette River valleys had been one of the most densely populated regions on the Pacific Coast.

Establishment
Large numbers of pioneer settlers began arriving in the Willamette Valley in the 1840s via the Oregon Trail, with many arriving in nearby Oregon City. A new settlement then emerged ten miles from the mouth of the Willamette River, roughly halfway between Oregon City and Hudson's Bay Company's Fort Vancouver. This community was initially referred to as "Stumptown" and "The Clearing" because of the many trees cut down to allow for its growth. In 1843 William Overton saw potential in the new settlement but lacked the funds to file an official land claim. For 25 cents, Overton agreed to share half of the 640-acre (2.6 km2) site with Asa Lovejoy of Boston.In 1845, Overton sold his remaining half of the claim to Francis W. Pettygrove of Portland, Maine. Both Pettygrove and Lovejoy wished to rename "The Clearing" after their respective hometowns (Lovejoy's being Boston, and Pettygrove's, Portland). This controversy was settled with a coin toss that Pettygrove won in a series of two out of three tosses, thereby providing Portland with its namesake. The coin used for this decision, now known as the Portland Penny, is on display in the headquarters of the Oregon Historical Society. At the time of its incorporation on February 8, 1851, Portland had over 800 inhabitants, a steam sawmill, a log cabin hotel, and a newspaper, the Weekly Oregonian. A major fire swept through downtown in August 1873, destroying twenty blocks on the west side of the Willamette along Yamhill and Morrison Streets, and causing $1.3 million in damage, roughly equivalent to $31.8 million today. By 1879, the population had grown to 17,500 and by 1890 it had grown to 46,385. In 1888, the first steel bridge on the West Coast was opened in Portland, the predecessor of the 1912 namesake Steel Bridge that survives today. In 1889, Henry Pittock's wife, Georgiana, established the Portland Rose Society. The movement to make Portland a "Rose City" started as the city was preparing for the 1905 Lewis and Clark Centennial Exposition.Portland's access to the Pacific Ocean via the Willamette and Columbia rivers, as well as its easy access to the agricultural Tualatin Valley via the "Great Plank Road" (the route of current-day U.S. Route 26), provided the pioneer city with an advantage over other nearby ports, and it grew very quickly. Portland remained the major port in the Pacific Northwest for much of the 19th century, until the 1890s, when Seattle's deepwater harbor was connected to the rest of the mainland by rail, affording an inland route without the treacherous navigation of the Columbia River. The city had its own Japantown, for one, and the lumber industry also became a prominent economic presence, due to the area's large population of Douglas fir, western hemlock, red cedar, and big leaf maple trees.
Portland developed a reputation early in its history as a hard-edged and gritty port town. Some historians have described the city's early establishment as being a "scion of New England; an ends-of-the-earth home for the exiled spawn of the eastern established elite." In 1889, The Oregonian called Portland "the most filthy city in the Northern States", due to the unsanitary sewers and gutters, and, at the turn of the 20th century, it was considered one of the most dangerous port cities in the world. The city housed a large number of saloons, bordellos, gambling dens, and boardinghouses which were populated with miners after the California Gold Rush, as well as the multitude of sailors passing through the port. By the early 20th century, the city had lost its reputation as a "sober frontier city" and garnered a reputation for being violent and dangerous.

20th-century development
Between 1900 and 1930, the city's population tripled from nearly 100,000 to 301,815. During World War II, it housed an "assembly center" from which up to 3,676 people of Japanese descent were dispatched to internment camps in the heartland. It was the first American city to have residents report thus, and the Pacific International Livestock Exposition operated from May through September 10, 1942, processing people from the city, northern Oregon, and central Washington. General John DeWitt called the city the first "Jap-free city on the West Coast."At the same time, Portland became a notorious hub for underground criminal activity and organized crime in the 1940s and 1950s. In 1957, Life magazine published an article detailing the city's history of government corruption and crime, specifically its gambling rackets and illegal nightclubs. The article, which focused on crime boss Jim Elkins, became the basis of a fictionalized film titled Portland Exposé (1957). In spite of the city's seedier undercurrent of criminal activity, Portland enjoyed an economic and industrial surge during World War II. Ship builder Henry J. Kaiser had been awarded contracts to build Liberty ships and aircraft carrier escorts, and chose sites in Portland and Vancouver, Washington, for work yards. During this time, Portland's population rose by over 150,000, largely attributed to recruited laborers.During the 1960s, an influx of hippie subculture began to take root in the city in the wake of San Francisco's burgeoning countercultural scene. The city's Crystal Ballroom became a hub for the city's psychedelic culture, while food cooperatives and listener-funded media and radio stations were established. A large social activist presence evolved during this time as well, specifically concerning Native American rights, environmentalist causes, and gay rights. By the 1970s, Portland had well established itself as a progressive city, and experienced an economic boom for the majority of the decade; however, the slowing of the housing market in 1979 caused demand for the city and state timber industries to drop significantly.

Since 1990
In the 1990s, the technology industry began to emerge in Portland, specifically with the establishment of companies such as Intel, which brought more than US$10 billion in investments in 1995 alone. In the late 1990s, the Portland area was rated the fourth-least affordable place in the United States to purchase a new home. After 2000, Portland experienced significant growth, with a population rise of over 90,000 between the years 2000 and 2014. The city's increasing reputation for culture established it as a popular city for young people, and it was second only to Louisville, Kentucky as one of the cities to attract and retain the highest number of college-educated people in the United States. Between 2001 and 2012, Portland's gross domestic product per person grew by fifty percent, more than any other city in the country.The city acquired a diverse range of nicknames throughout its history, though it is most often called "Rose City" or "The City of Roses" (unofficial nickname since 1888,  official since 2003). Another widely used nickname by local residents in everyday speech is "PDX", the airport code for Portland International Airport. Other nicknames include Bridgetown, Stumptown, Rip City, Soccer City, P-Town, Portlandia, and the more antiquated Little Beirut.

2020 George Floyd protests
From May 28, 2020, until spring 2021, there were daily protests about the murder of George Floyd by police, and racial injustice. There were instances of looting, vandalism, and police actions causing injuries. One protestor was killed by an opposing one. Local businesses reported losses totaling millions of dollars as the result of vandalism and looting, according to Oregon Public Broadcasting. Some protests caused injury to protesters and police. In July, federal officers were deployed to safeguard federal property; their presence and tactics were criticized by Oregon officials, who demanded they leave, while lawsuits were filed against local and federal law enforcement alleging wrongful actions by them.On May 25, 2021,  a protest to commemorate the one-year anniversary of Floyd's murder caused property damage, and was followed by a number of arrests.

Geography
Geology
Portland lies on top of a dormant volcanic field known as the Boring Lava Field, named after the nearby bedroom community of Boring. The Boring Lava Field has at least 32 cinder cones such as Mount Tabor, and its center lies in southeast Portland. Mount St. Helens, a highly active volcano 50 miles (80 km) northeast of the city in Washington state, is easily visible on clear days and is close enough to have dusted the city with volcanic ash after its eruption on May 18, 1980. The rocks of the Portland area range in age from late Eocene to more recent eras.Multiple shallow, active fault lines traverse the Portland metropolitan area. Among them are the Portland Hills Fault on the city's west side, and the East Bank Fault on the east side. According to a 2017 survey, several of these faults were characterized as "probably more of a hazard" than the Cascadia subduction zone due to their proximities to population centers, with the potential of producing magnitude 7 earthquakes. Notable earthquakes that have impacted the Portland area in recent history include the 6.8-magnitude Nisqually earthquake in 2001, and a 5.6-magnitude earthquake that struck on March 25, 1993.Per a 2014 report, over 7,000 locations within the Portland area are at high risk for landslides and soil liquefaction in the event of a major earthquake, including much of the city's west side (such as Washington Park) and sections of Clackamas County.

Topography
Portland is 60 miles (97 km) east of the Pacific Ocean at the northern end of Oregon's most populated region, the Willamette Valley. Downtown Portland straddles the banks of the Willamette River, which flows north through the city center and separates the city's east and west neighborhoods. Less than 10 miles (16 km) from downtown, the Willamette River flows into the Columbia River, the fourth-largest river in the United States, which divides Oregon from Washington state. Portland is approximately 100 miles (160 km) upriver from the Pacific Ocean on the Columbia.
Though much of downtown Portland is relatively flat, the foothills of the Tualatin Mountains, more commonly referred to locally as the "West Hills", pierce through the northwest and southwest reaches of the city. Council Crest Park at 1,073 feet (327 m) is often quoted as the highest point in Portland; however, the highest point in Portland is on a section of NW Skyline Blvd just north of Willamette Stone Heritage site. The highest point east of the river is Mt. Tabor, an extinct volcanic cinder cone, which rises to 636 feet (194 m). Nearby Powell Butte and Rocky Butte rise to 614 feet (187 m) and 612 feet (187 m), respectively. To the west of the Tualatin Mountains lies the Oregon Coast Range, and to the east lies the actively volcanic Cascade Range. On clear days, Mt. Hood and Mt. St. Helens dominate the horizon, while Mt. Adams and Mt. Rainier can also be seen in the distance.
According to the United States Census Bureau, the city has an area of 145.09 square miles (375.78 km2), of which 133.43 square miles (345.58 km2) is land and 11.66 square miles (30.20 km2) is water. Although almost all of Portland is within Multnomah County, small portions of the city are within Clackamas and Washington Counties, with populations estimated at 785 and 1,455, respectively.

Climate
Portland has a warm-summer Mediterranean climate (Köppen Csb), falling just short of a hot-summer Mediterranean climate (Köppen Csa) with cool and rainy winters, and warm and dry summers. This climate is characterized by having overcast, wet, and changing weather conditions in fall, winter, and spring, as Portland lies in the direct path of the stormy westerly flow, and warm, dry summers when the North Pacific High reaches its northernmost point in mid-summer. Portland's USDA Plant Hardiness Zone is 8b, with parts of the Downtown area falling into zone 9a.Winters are cool, cloudy, and rainy. The coldest month is December with an average daily high temperature of 46.9 °F (8.3 °C), although overnight lows usually remain above freezing by a few degrees. Evening temperatures fall to or below freezing 32 nights per year on average, but very rarely below 18 °F (−8 °C). There are only 2.1 days per year where the daytime high temperature fails to rise above freezing; the mean for the coldest high is at the exact freezing point of 32 °F (0 °C). The lowest overnight temperature ever recorded was −3 °F (−19 °C), on February 2, 1950, while the coldest daytime high temperature ever recorded was 14 °F (−10 °C) on December 30, 1968. The average window in which freezing temperatures may occur is between November 15 and March 19, allowing a growing season of 240 days.Annual snowfall in Portland is 4.3 inches (10.9 cm), which usually falls between December and March. The city of Portland avoids snow more frequently than its suburbs, due in part to its low elevation and the urban heat island effect. Neighborhoods outside of the downtown core, especially in slightly higher elevations near the West Hills and Mount Tabor, can experience a dusting of snow while downtown receives no accumulation at all. The city has experienced a few major snow and ice storms in its past, with extreme totals having reached 44.5 in (113 cm) at the airport in 1949–50 and 60.9 in (155 cm) at downtown in 1892–93.
Summers in Portland are warm, dry, and sunny, though the sunny warm weather is short-lived, from mid-June to early September. June, July, August and September account for a combined 4.19 inches (106 mm) of total rainfall –  only 11% of the 36.91 in (938 mm) of annual precipitation. The warmest month is August, with an average high temperature of 82.3 °F (27.9 °C). Because of its inland location 62 miles (100 km) from the coast, as well as the protective nature of the Oregon Coast Range to its west, Portland summers are less susceptible to the moderating influence of the nearby Pacific Ocean. Consequently, Portland occasionally experiences heat waves, with temperatures rising above 90 °F (32 °C) for a few days. However, on average, temperatures reach or exceed 80 °F (27 °C) on only 61 days per year, of which 15 days will reach 90 °F (32 °C) and only 1.3 days will reach 100 °F (38 °C). In 2018 more 90-degree days were recorded than ever before.On June 28, 2021, Portland recorded its all-time record high temperature of 116 °F (47 °C) and its warmest daily low temperature of 75 °F (24 °C) during a major regional heat wave. The record had been broken for three consecutive days with daytime highs of 108 °F (42 °C) on June 26 and 112 °F (44 °C) on June 27; the previous record of 107 °F (42 °C) was set in July 1965 and matched twice in August 1981. A temperature of 100 °F (38 °C) has been recorded in all five months from May through September. The warmest night of the year averages 68 °F (20 °C).Spring and fall can bring variable weather including high-pressure ridging that sends temperatures surging above 80 °F (27 °C) and cold fronts that plunge daytime temperatures into the 40s °F (4–9 °C). However, lengthy stretches of overcast days beginning in mid-fall and continuing into mid-spring are most common. Rain often falls as a light drizzle for several consecutive days at a time, contributing to 155 days on average with measurable (≥0.01 in or 0.25 mm) precipitation annually. Temperatures have reached 90 °F (32 °C) as early as April 30 and as late as October 5, while 80 °F (27 °C) has been reached as early as April 1 and as late as October 21. Thunderstorms are uncommon and tornadoes are very rare, although they do occur.

See or edit raw graph data.

Cityscape
Portland's cityscape derives much of its character from the many bridges that span the Willamette River downtown, several of which are historic landmarks, and Portland has been nicknamed "Bridgetown" for many decades as a result. Three of downtown's most heavily used bridges are more than 100 years old and are designated historic landmarks: Hawthorne Bridge (1910), Steel Bridge (1912), and Broadway Bridge (1913). Portland's newest bridge in the downtown area, Tilikum Crossing, opened in 2015 and is the first new bridge to span the Willamette in Portland since the 1973 opening of the double-decker Fremont Bridge.Other bridges that span the Willamette River in the downtown area include the Burnside Bridge, the Ross Island Bridge (both built 1926), and the double-decker Marquam Bridge (built 1966). Other bridges outside the downtown area include the Sellwood Bridge (built 2016) to the south; and the St. Johns Bridge, a Gothic revival suspension bridge built in 1931, to the north. The Glenn L. Jackson Memorial Bridge and the Interstate Bridge provide access from Portland across the Columbia River into Washington state.

Neighborhoods
The Willamette River, which flows north through downtown, serves as the natural boundary between East and West Portland. The denser and earlier-developed west side extends into the lap of the West Hills, while the flatter east side extends for roughly 180 blocks until it meets the suburb of Gresham. In 1891 the cities of Portland, Albina, and East Portland were consolidated, creating inconsistent patterns of street names and addresses. It was not unusual for a street name to be duplicated in disparate areas. The "Great Renumbering" on September 2, 1931, standardized street naming patterns and divided Portland into five "general districts." It also changed house numbers from 20 per block to 100 per block and adopted a single street name on a grid. For example, the 200 block north of Burnside is either NW Davis Street or NE Davis Street throughout the entire city.
The six previous addressing sections of Portland, which were colloquially known as quadrants despite there being six, have developed distinctive identities, with mild cultural differences and friendly rivalries between their residents, especially between those who live east of the Willamette River versus west of the river. Portland's addressing sections are North, Northwest, Northeast, South, Southeast, and Southwest (which includes downtown Portland). The Willamette River divides the city into east and west while Burnside Street, which traverses the entire city lengthwise, divides the north and south. North Portland consists of the peninsula formed by the Willamette and Columbia Rivers, with N Williams Ave serving as its eastern boundary. All addresses and streets within the city are prefixed by N, NW, NE, S, SW or SE with the exception of Burnside Street, which is prefixed with W or E. Starting on May 1, 2020, former Southwest prefix addresses with house numbers on east–west streets leading with zero dropped the zero and the street prefix on all streets (including north–south streets) converted from Southwest to South. For example, the current address of 246 S California St. was changed from 0246 SW California St. and the current address of 4310 S Macadam Ave. was converted from 4310 SW Macadam Ave.

The new South Portland addressing section was approved by the Portland City Council on June 6, 2018 and is bounded by SW Naito Parkway, SW View Point Terrace and the Tryon Creek State Natural Area to the west, SW Clay Street to the north, the Willamette River to the east, and city limits to the south. It includes the Lair Hill, Johns Landing and South Waterfront districts and Lewis & Clark College as well as the Riverdale area of unincorporated Multnomah County south of the Portland city limits. In 2018, the city's Bureau of Transportation finalized a plan to transition this part of Portland into South Portland, beginning on May 1, 2020, to reduce confusion by 9-1-1 dispatchers and delivery services. With the addition of South Portland, all six addressing sectors (N, NE, NW, S, SE and SW) are now officially known as sextants.The Pearl District in Northwest Portland, which was largely occupied by warehouses, light industry and railroad classification yards in the early to mid-20th century, now houses upscale art galleries, restaurants, and retail stores, and is one of the wealthiest neighborhoods in the city. Areas further west of the Pearl District include neighborhoods known as Uptown and Nob Hill, as well as the Alphabet District and NW 23rd Ave., a major shopping street lined with clothing boutiques and other upscale retail, mixed with cafes and restaurants.Northeast Portland is home to the Lloyd District, Alberta Arts District, and the Hollywood District.
North Portland is largely residential and industrial. It contains Kelley Point Park, the northernmost point of the city. It also contains the St. Johns neighborhood, which is historically one of the most ethnically diverse and poorest neighborhoods in the city.Old Town Chinatown is next to the Pearl District in Northwest Portland. In 1890 it was the second largest Chinese community in the United States. In 2017, the crime rate was several times above the city average. This neighborhood has been called Portland's skid row. Southwest Portland is largely residential. Downtown district, made up of commercial businesses, museums, skyscrapers, and public landmarks represents a small area within the southwest address section. Portland's South Waterfront area has been developing into a dense neighborhood of shops, condominiums, and apartments starting in the mid-2000s. Development in this area is ongoing. The area is served by the Portland Streetcar, the MAX Orange Line and four TriMet bus lines. This former industrial area sat as a brownfield prior to development in the mid-2000s.Southeast Portland is largely residential, and consists of several neighborhoods, including Hawthorne District, Belmont, Brooklyn, and Mount Tabor. Reed College, a private liberal arts college that was founded in 1908, is located within the confines of Southeast Portland as is Mount Tabor, a volcanic landform.

Demographics
Ethnicity
The 2020 census reported the city as 73.8% White (449,025 people), 8.2% Asian (52,854), 5.8% Black or African American (38,217), 0.9% Native American (7,335), 0.5% Pacific Islander (3,919), and 5.0% from two or more races (69,898). 10.3% were Hispanic or Latino, of any race (72,336). Whites not of Hispanic origin made up 68.8% of the total population.The 2010 census reported the city as 76.1% White (444,254 people), 7.1% Asian (41,448), 6.3% Black or African American (36,778), 1.0% Native American (5,838), 0.5% Pacific Islander (2,919), 4.7% belonging to two or more racial groups (24,437) and 5.0% from other races (28,987). 9.4% were Hispanic or Latino, of any race (54,840). Whites not of Hispanic origin made up 72.2% of the total population.In 1940, Portland's African-American population was approximately 2,000 and largely consisted of railroad employees and their families. During the war-time Liberty Ship construction boom, the need for workers drew many Black people to the city. The new influx of Black people settled in specific neighborhoods, such as the Albina district and Vanport. The May 1948 flood which destroyed Vanport eliminated the only integrated neighborhood, and an influx of blacks into the northeast quadrant of the city continued. Portland's longshoremen racial mix was described as being "lily-white" in the 1960s when the local International Longshore and Warehouse Union declined to represent grain handlers since some were black.

Over two-thirds of Oregon's African-American residents live in Portland. As of the 2000 census, three of its high schools (Cleveland, Lincoln and Wilson) were over 70% White, reflecting the overall population, while Jefferson High School was 87% non-White. The remaining six schools have a higher number of non-Whites, including Blacks and Asians. Hispanic students average from 3.3% at Wilson to 31% at Roosevelt.Portland residents identifying solely as Asian Americans account for 7.1% of the population; an additional 1.8% is partially of Asian heritage. Vietnamese Americans make up 2.2% of Portland's population, and make up the largest Asian ethnic group in the city, followed by Chinese (1.7%), Filipinos (0.6%), Japanese (0.5%), Koreans (0.4%), Laotians (0.4%), Hmong (0.2%), and Cambodians (0.1%). A small population of Iu Mien live in Portland. Portland has two Chinatowns, with New Chinatown in the 'Jade District' along SE 82nd Avenue with Chinese supermarkets, Hong Kong style noodle houses, dim sum, and Vietnamese phở restaurants.With about 12,000 Vietnamese residing in the city proper, Portland has one of the largest Vietnamese populations in America per capita. According to statistics, there are over 4,500 Pacific Islanders in Portland, making up 0.7% of the city's population. There is a Tongan community in Portland, who arrived in the area in the 1970s, and Tongans and Pacific Islanders as a whole are one of the fastest-growing ethnic groups in the Portland area.Portland's population has been and remains predominantly White. In 1940, Whites were over 98% of the city's population. In 2009, Portland had the fifth-highest percentage of White residents among the 40 largest U.S. metropolitan areas. A 2007 survey of the 40 largest cities in the U.S. concluded Portland's urban core has the highest percentage of White residents. Some scholars have noted the Pacific Northwest as a whole is "one of the last Caucasian bastions of the United States". While Portland's diversity was historically comparable to metro Seattle and Salt Lake City, those areas grew more diverse in the late 1990s and 2000s. Portland not only remains White, but migration to Portland is disproportionately White.The Oregon Territory banned African American settlement in 1849. In the 19th century, certain laws allowed the immigration of Chinese laborers but prohibited them from owning property or bringing their families. The early 1920s saw the rapid growth of the Ku Klux Klan, which became very influential in Oregon politics, culminating in the election of Walter M. Pierce as governor.The largest influxes of minority populations occurred during World War II, as the African American population grew by a factor of 10 for wartime work. After World War II, the Vanport flood in 1948 displaced many African Americans. As they resettled, redlining directed the displaced workers from the wartime settlement to neighboring Albina. There and elsewhere in Portland, they experienced police hostility, lack of employment, and mortgage discrimination, leading to half the black population leaving after the war.In the 1980s and 1990s, radical skinhead groups flourished in Portland. In 1988, Mulugeta Seraw, an Ethiopian immigrant, was killed by three skinheads. The response to his murder involved a community-driven series of rallies, campaigns, nonprofits and events designed to address Portland's racial history, leading to a city considered significantly more tolerant than in 1988 at Seraw's death.Portland has a substantial Roma population.76% of Latinos in Portland are of Mexican heritage.Italians and Russian Jews had a very visible presence in Portland.

Households
As of the 2010 census, there were 583,776 people living in the city, organized into 235,508 households. The population density was 4,375.2 people per square mile. There were 265,439 housing units at an average density of 1,989.4 per square mile (768.1/km2). Population growth in Portland increased 10.3% between 2000 and 2010. Population growth in the Portland metropolitan area has outpaced the national average during the last decade, and this is expected to continue over the next 50 years.Out of 223,737 households, 24.5% had children under the age of 18 living with them, 38.1% were married couples living together, 10.8% had a female householder with no husband present, and 47.1% were non-families. 34.6% of all households were made up of individuals, and 9% had someone living alone who was 65 years of age or older. The average household size was 2.3 and the average family size was 3. The age distribution was 21.1% under the age of 18, 10.3% from 18 to 24, 34.7% from 25 to 44, 22.4% from 45 to 64, and 11.6% who were 65 years of age or older. The median age was 35 years. For every 100 females, there were 97.8 males. For every 100 females age 18 and over, there were 95.9 males.
The median income for a household in the city was $40,146, and the median income for a family was $50,271. Males had a reported median income of $35,279 versus $29,344 reported for females. The per capita income for the city was $22,643. 13.1% of the population and 8.5% of families were below the poverty line. Out of the total population, 15.7% of those under the age of 18 and 10.4% of those 65 and older were living below the poverty line. Figures delineating the income levels based on race are not available at this time. According to the Modern Language Association, in 2010 80.9% (539,885) percent of Multnomah County residents ages 5 and over spoke English as their primary language at home. 8.1% of the population spoke Spanish (54,036), with Vietnamese speakers making up 1.9%, and Russian 1.5%.

Social
The Portland metropolitan area has historically had a significant LGBT population throughout the late 20th and early 21st century. In 2015, the city metro had the second highest percentage of LGBT residents in the United States with 5.4% of residents identifying as gay, lesbian, bisexual, or transgender, second only to San Francisco. In 2006, it was reported to have the seventh highest LGBT population in the country, with 8.8% of residents identifying as gay, lesbian, or bisexual, and the metro ranking fourth in the nation at 6.1%. The city held its first pride festival in 1975 on the Portland State University campus.

Religion
Portland has been cited as the least religious city in the United States with over 42% of residents identifying as religiously "unaffiliated", according to the nonpartisan and nonprofit Public Religion Research Institute's American Values Atlas.

Homelessness
A 2019 survey by the city's budget office showed that homelessness is perceived as the top challenge facing Portland, and was cited as a reason people move and do not participate in park programs. Calls to 911 concerning "unwanted persons" have significantly increased between 2013 and 2018, and the police are increasingly dealing with homeless and mentally ill. It is taking a toll on sense of safety among visitors and residents and business owners are adversely impacted. Even though homeless services and shelter beds have increased, as of 2020 homelessness is considered an intractable problem in Portland.The proposed budget for 2022–23 includes $5.8MM to buy land for affordable housing, and $36MM to equip and operate "safe rest villages". A 2022 initiative approved by the Portland city council makes homeless camping illegal, eventually requiring homeless individuals to move into mass shelters.

Crime
According to the Federal Bureau of Investigation's Uniform Crime Report in 2009, Portland ranked 53rd in violent crime out of the top 75 U.S. cities with a population greater than 250,000. The murder rate in Portland in 2013 averaged 2.3 murders per 100,000 people per year, which was lower than the national average. In 2011, 72% of arrested male subjects tested positive for illegal drugs and the city was dubbed the "deadliest drug market in the Pacific Northwest" due to drug related deaths. In 2010, ABC's Nightline reported that Portland is one of the largest hubs for child sex trafficking. Car theft rates in Portland are the fifth highest of any US metropolitan area as of 2023. According to the Los Angeles Times in 2023: "Shootings in the city have tripled" and "Lower-level crimes have spiked too: More than 11,000 vehicles were stolen in 2022, up from 6,500 in 2019."In the Portland Metropolitan statistical area which includes Clackamas, Columbia, Multnomah, Washington, and Yamhill Counties, OR and Clark and Skamania Counties, WA for 2017, the murder rate was 2.6, violent crime was 283.2 per 100,000 people per year. In 2017, the population within the city of Portland was 649,408 and there were 24 murders and 3,349 violent crimes.Portland's 101 homicides in 2022 set a new record. For 2021 year, Portland recorded 90 homicides, compared with 20 in 2016, and 27 in 2017.

Economy
Portland's location is beneficial for several industries. Relatively low energy cost, accessible resources, north–south and east–west Interstates, international air terminals, large marine shipping facilities, and both west coast intercontinental railroads are all economic advantages.
The city's marine terminals alone handle over 13 million tons of cargo per year, and the port is home to one of the largest commercial dry docks in the country. The Port of Portland is the third-largest export tonnage port on the west coast of the U.S., and being about 80 miles (130 km) upriver, it is the largest fresh-water port.The scrap steel industry's history in Portland predates World War II. By the 1950s, the scrap steel industry became the city's number one industry for employment. The scrap steel industry thrives in the region, with Schnitzer Steel Industries, a prominent scrap steel company, shipping a record 1.15 billion tons of scrap metal to Asia during 2003. Other heavy industry companies include ESCO Corporation and Oregon Steel Mills.Technology is a major component of the city's economy, with more than 1,200 technology companies existing within the metro. This high density of technology companies has led to the nickname Silicon Forest being used to describe the Portland area, a reference to the abundance of trees in the region and to the Silicon Valley region in Northern California. The area also hosts facilities for software companies and online startup companies, some supported by local seed funding organizations and business incubators. Computer components manufacturer Intel is the Portland area's largest employer, providing jobs for more than 15,000 people, with several campuses to the west of central Portland in the city of Hillsboro.The Portland metro area has become a business cluster for athletic/outdoor gear and footwear manufacturer's headquarters. Shoes are not manufactured in Portland. The area is home to the global, North American or U.S. headquarters of Nike (the only Fortune 500 company headquartered in Oregon), Adidas, Columbia Sportswear, LaCrosse Footwear, Dr. Martens, Li-Ning, Keen, and Hi-Tec Sports. While headquartered elsewhere, Merrell, Amer Sports and Under Armour have design studios and local offices in the Portland area.
Other notable Portland-based companies include industrial goods and metal fabrication company Precision Castparts, film animation studio Laika; commercial vehicle manufacturer Daimler Trucks North America; advertising firm Wieden+Kennedy; bankers Umpqua Holdings; child care and early childhood education provider KinderCare Learning Centers; and retailers Fred Meyer, New Seasons Market, Storables, and Powell's Books.
Breweries are another major industry in Portland, which is home to 139 breweries/microbreweries, the 7th most in the nation, as of December 2018. Additionally, the city boasts a robust coffee culture that now rivals Seattle and hosts over 20 coffee roasters.

Housing
In 2016, home prices in Portland grew faster than in any other city in the United States. Apartment rental costs in Portland reported in November 2019 was $1,337 for two bedroom and $1,133 for one bedroom.In 2017, developers projected an additional 6,500 apartments to be built in the Portland Metro Area over the next year. However, as of December 2019, the number of homes available for rent or purchase in Portland continues to shrink. Over the past year, housing prices in Portland have risen 2.5%. Housing prices in Portland continue to rise, the median price rising from $391,400 in November 2018 to $415,000 in November 2019. There has been a rise of people from out of state moving to Portland, which impacts housing availability. Because of the demand for affordable housing and influx of new residents, more Portlanders in their 20s and 30s are still living in their parents' homes. There is a considerable amount of "Airbnb type" rentals in the city. An audit in 2018 located around 4,600 listings, of which 80% were illegally operated.

Arts and culture
Music, film, and performing arts
Portland is home to a range of classical performing arts institutions including the Portland Opera, Portland Baroque Orchestra, Oregon Symphony and Portland Youth Philharmonic; the last of these, established in 1924, was the first youth orchestra established in the United States. The city is also home to several theaters and performing arts institutions including the Oregon Ballet Theatre, Northwest Children's Theatre, Portland Center Stage, Artists Repertory Theatre, Curious Comedy Theatre and Miracle Theatre.
In 2013, The Guardian named the city's music scene as one of the "most vibrant" in the United States. Portland is home to famous bands such as the Kingsmen and Paul Revere & the Raiders, both famous for their association with the song "Louie Louie" (1963). Other widely known musical groups include the Dandy Warhols, Quarterflash, Everclear, Pink Martini, Sleater-Kinney, Blitzen Trapper, the Decemberists, and the late Elliott Smith. More recently, Portugal. The Man, Modest Mouse, and the Shins have made their home in Portland. In the 1980s, the city was home to a burgeoning punk scene, which included bands such as the Wipers and Dead Moon. The city's now-demolished Satyricon nightclub was a punk venue notorious for being the place where Nirvana frontman Kurt Cobain first encountered his future wife and Hole frontwoman Courtney Love in 1990. Love was then a resident of Portland and started several bands there with Kat Bjelland, later of Babes in Toyland. Multi-Grammy award-winning jazz artist Esperanza Spalding is from Portland and performed with the Chamber Music Society of Oregon at a young age.A wide range of films have been shot in Portland, from various independent features to major big-budget productions. Director Gus Van Sant has notably set and shot many of his films in the city. The city has also been featured in various television programs, notably the IFC sketch comedy series Portlandia. The series, which ran for eight seasons from 2011 to 2018, was shot on location in Portland, and satirized the city as a hub of liberal politics, organic food, alternative lifestyles, and anti-establishment attitudes. MTV's long-time running reality show The Real World was also shot in Portland for the show's 29th season: The Real World: Portland premiered on MTV in 2013. Other television series shot in the city include Leverage, The Librarians, Under Suspicion, Grimm, and Nowhere Man.An unusual feature of Portland entertainment is the large number of movie theaters serving beer, often with second-run or revival films. Notable examples of these "brew and view" theaters include the Bagdad Theater and Pub, a former vaudeville theater built in 1927 by Universal Studios; Cinema 21; and the Laurelhurst Theater, in operation since 1923. Portland hosts the world's longest-running H. P. Lovecraft Film Festival at the Hollywood Theatre.

Museums and recreation
Portland is home to numerous museums and educational institutions, ranging from art museums to institutions devoted to science and wildlife. Among the science-oriented institutions are the Oregon Museum of Science and Industry (OMSI), which consists of five main halls and other ticketed attractions, such as the USS Blueback submarine, the ultra-large-screen Empirical Theater (which replaced an OMNIMAX theater in 2013), and the Kendall Planetarium. The World Forestry Center Discovery Museum, located in the city's Washington Park area, offers educational exhibits on forests and forest-related subjects. Also located in Washington Park are the Hoyt Arboretum, the International Rose Test Garden, the Japanese Garden, and the Oregon Zoo.
The Portland Art Museum owns the city's largest art collection and presents a variety of touring exhibitions each year and, with the recent addition of the Modern and Contemporary Art wing, it became one of the United States' 25 largest museums. The Oregon Historical Society Museum, founded in 1898, which has a variety of books, film, pictures, artifacts, and maps dating back throughout Oregon's history. It houses permanent and temporary exhibits about Oregon history, and hosts traveling exhibits about the history of the United States.Oaks Amusement Park, in the Sellwood district of Southeast Portland, is the city's only amusement park and is also one of the country's longest-running amusement parks. It has operated since 1905 and was known as the "Coney Island of the Northwest" upon its opening.

Cuisine and breweries
Food carts are extremely popular within the city, with over 600 licensed carts. The city is home to Stumptown Coffee Roasters as well as dozens of other micro-roasteries and cafes.
Portland has 58 active breweries within city limits, and 70+ within the surrounding metro area. and data compiled by the Brewers Association ranks Portland seventh in the United States as of 2018.Portland hosts a number of festivals throughout the year that celebrate beer and brewing, including the Oregon Brewers Festival, held in Tom McCall Waterfront Park. Held each summer during the last full weekend of July, it is the largest outdoor craft beer festival in North America, with over 70,000 attendees in 2008. Other major beer festivals throughout the calendar year include the Spring Beer and Wine Festival in April, the North American Organic Brewers Festival in June, the Portland International Beerfest in July, and the Holiday Ale Festival in December.

Sustainability
The city became a pioneer of state-directed metropolitan planning, a program which was instituted statewide in 1969 to compact the urban growth boundaries of the city. Portland was the first city to enact a comprehensive plan to reduce carbon dioxide emissions.

Free speech and public nudity
Strong free speech protections of the Oregon Constitution upheld by the Oregon Supreme Court in State v. Henry, specifically found that full nudity and lap dances in strip clubs are protected speech. Portland has the highest number of strip clubs per-capita in a city in the United States, and Oregon ranks as the highest state for per-capita strip clubs.In November 2008, a Multnomah County judge dismissed charges against a nude bicyclist arrested on June 26, 2008. The judge stated that the city's annual World Naked Bike Ride – held each year in June since 2004 – has created a "well-established tradition" in Portland where cyclists may ride naked as a form of protest against cars and fossil fuel dependence. The defendant was not riding in the official World Naked Bike Ride at the time of his arrest as it had occurred 12 days earlier that year, on June 14.

Protests
From November 10 to 12, 2016, protests in Portland turned into a riot, when a group broke off from a larger group of peaceful protesters who were opposed to the election of Donald Trump as president of the United States.

Public art
Sports
Portland is home to three major league sports franchises: the Portland Trail Blazers of the NBA, the Portland Timbers of Major League Soccer (MLS), and the Portland Thorns FC of the National Women's Soccer League. In 2015, the Timbers won the MLS Cup, which was the first male professional sports championship for a team from Portland since the Trail Blazers won the NBA championship in 1977. Despite being the 19th most populated metro area in the United States, Portland contains only one franchise from the NFL, NBA, NHL, or MLB, making it the United States' second most populated metro area with that distinction, behind San Antonio, which also has only one NBA  team (the Spurs). The city has been often rumored to receive an additional franchise, although efforts to acquire a team have failed due to stadium funding issues. An organization known as the Portland Diamond Project (PDP) has worked with MLB and local government, and there are plans to have an MLB stadium constructed in the industrial district of Portland. The PDP has not yet received the funding for this project.

Portland sports fans are characterized by their passionate support. The Trail Blazers sold out every home game between 1977 and 1995, a span of 814 consecutive games, the second-longest streak in American sports history. The Timbers joined MLS in 2011 and have sold out every home match since joining the league, a streak that has now reached 70+ matches. The Timbers season ticket waiting list has reached 10,000+, the longest waiting list in MLS. In 2015, they became the first team in the Northwest to win the MLS Cup. Player Diego Valeri marked a new record for fastest goal in MLS Cup history at 27 seconds into the game.
The annual Cambia Portland Classic women's golf tournament in September, now in its 50th year, is the longest-running non-major tournament on the LPGA Tour, plays in the southern suburb of West Linn.Two rival universities exist within Portland city limits: the University of Portland Pilots and the Portland State University Vikings, both of whom field teams in popular spectator sports including soccer, baseball, and basketball. Portland State also has a football team. Additionally, the University of Oregon Ducks (in Eugene) and the Oregon State University Beavers (in Corvallis) both receive substantial attention and support from many Portland residents, despite their campuses being 110 and 84 miles from the city, respectively.
Running is a popular activity in Portland, and every year the city hosts the Portland Marathon as well as parts of the Hood to Coast Relay, the world's largest long-distance relay race (by number of participants). Portland served as the center to an elite running group, the Nike Oregon Project until its 2019 disbandment following coach Alberto Salazar's ban due to doping violations.Historic Erv Lind Stadium is located in Normandale Park. It has been home to professional and college softball.
Portland also hosts numerous cycling events and has become an elite bicycle racing destination. The Oregon Bicycle Racing Association supports hundreds of official bicycling events every year. Weekly events at Alpenrose Velodrome and Portland International Raceway allow for racing nearly every night of the week from March through September. Cyclocross races, such as the Cross Crusade, can attract over 1,000 riders and spectators.

Parks and recreation
Parks and greenspace planning date back to John Charles Olmsted's 1903 Report to the Portland Park Board. In 1995, voters in the Portland metropolitan region passed a regional bond measure to acquire valuable natural areas for fish, wildlife, and people. Ten years later, more than 8,100 acres (33 km2) of ecologically valuable natural areas had been purchased and permanently protected from development.Portland is one of only four cities in the U.S. with extinct volcanoes within its boundaries (along with Pilot Butte in Bend, Oregon, Jackson Volcano in Jackson, Mississippi, and Diamond Head in Honolulu, Hawaii). Mount Tabor Park is known for its scenic views and historic reservoirs.Forest Park is the largest wilderness park within city limits in the United States, covering more than 5,000 acres (2,023 ha). Portland is also home to Mill Ends Park, the world's smallest park (a two-foot-diameter circle, the park's area is only about 0.3 m2). Washington Park is just west of downtown and is home to the Oregon Zoo, Hoyt Arboretum, the Portland Japanese Garden, and the International Rose Test Garden. Portland is also home to Lan Su Chinese Garden (formerly the Portland Classical Chinese Garden), an authentic representation of a Suzhou-style walled garden. Portland's east side has several formal public gardens: the historic Peninsula Park Rose Garden, the rose gardens of Ladd's Addition, the Crystal Springs Rhododendron Garden, the Leach Botanical Garden, and The Grotto.
Portland's downtown features two groups of contiguous city blocks dedicated for park space: the North and South Park Blocks. The 37-acre (15 ha) Tom McCall Waterfront Park was built in 1974 along the length of the downtown waterfront after Harbor Drive was removed; it now hosts large events throughout the year. The nearby historically significant Burnside Skatepark and five indoor skateparks give Portland a reputation as possibly "the most skateboard-friendly town in America."Tryon Creek State Natural Area is one of three Oregon State Parks in Portland and the most popular; its creek has a run of steelhead. The other two State Parks are Willamette Stone State Heritage Site, in the West Hills, and the Government Island State Recreation Area in the Columbia River near Portland International Airport.
Portland's city park system has been proclaimed one of the best in America. In its 2013 ParkScore ranking, the Trust for Public Land reported Portland had the seventh-best park system among the 50 most populous U.S. cities. In February 2015, the City Council approved a total ban on smoking in all city parks and natural areas and the ban has been in force since July 1, 2015. The ban includes cigarettes, vaping, as well as marijuana.

Government
City hall
The city of Portland is governed by the Portland City Council, which includes a mayor, four commissioners, and an auditor. Each is elected citywide to serve a four-year term. Each commissioner oversees one or more bureaus responsible for the day-to-day operation of the city. The mayor serves as chairman of the council and is principally responsible for allocating department assignments to his fellow commissioners. The auditor provides checks and balances in the commission form of government and accountability for the use of public resources. In addition, the auditor provides access to information and reports on various matters of city government. Portland is the only large city left in the United States with the commission form of government.
The city's Community & Civic Life (formerly Office of Neighborhood Involvement) serves as a conduit between city government and Portland's 95 officially recognized neighborhoods. Each neighborhood is represented by a volunteer-based neighborhood association which serves as a liaison between residents of the neighborhood and the city government. The city provides funding to neighborhood associations through seven district coalitions, each of which is a geographical grouping of several neighborhood associations. Most (but not all) neighborhood associations belong to one of these district coalitions.
Portland and its surrounding metropolitan area are served by Metro, the United States' only directly elected metropolitan planning organization. Metro's charter gives it responsibility for land use and transportation planning, solid waste management, and map development. Metro also owns and operates the Oregon Convention Center, Oregon Zoo, Portland Center for the Performing Arts, and Portland Metropolitan Exposition Center.
The Multnomah County government provides many services to the Portland area, as do Washington and Clackamas counties to the west and south.
Fire and emergency services are provided by Portland Fire & Rescue.
On November 8, 2022, Portland residents approved a charter reform ballot measure to replace the commission form of government with a 12-member council elected in four districts using the single transferable vote system, with a professional city manager appointed by a directly elected mayor. The city expects to hold the first election for this new system in 2024.

Courts and law enforcement
Law enforcement is provided by the Portland Police Bureau, whose headquarters are located in the Justice center building, along with the county jail.

State and national politics
Portland strongly favors the Democratic Party; registered Democrats (51.2%) outnumber Republicans (10.5%) nearly 5 to 1. All city offices are non-partisan. However, a Republican has not been elected as mayor since Fred L. Peterson in 1952, and has not served as mayor even on an interim basis since Connie McCready held the post from 1979 to 1980.
Portland is split among three U.S. congressional districts. Most of the city is in the 3rd District, represented by Earl Blumenauer (D-Portland), who served on the city council from 1986 until his election to Congress in 1996. Most of the city west of the Willamette River is part of the 1st District, represented by Suzanne Bonamici (D-Beaverton). A small portion of southeastern Portland is in the 5th District, formerly represented by Kurt Schrader (D-Canby) prior to losing his Democratic primary election to a more progressive candidate, but currently represented by the former mayor of Happy Valley, Republican Lori Chavez-DeRemer, who is the first Republican to represent a significant portion of the city in the U.S. House of Representatives since 1975. Both of Oregon's senators, Ron Wyden and Jeff Merkley, are from Portland and are progressive Democrats.
In the 2008 presidential election, Democratic candidate Barack Obama easily carried Portland, winning 245,464 votes from city residents to 50,614 for his Republican rival, John McCain. In the 2012 presidential election, Democratic candidate Barack Obama again easily carried Portland, winning 256,925 votes from Multnomah county residents to 70,958 for his Republican rival, Mitt Romney.Sam Adams, the former mayor of Portland, became the city's first openly gay mayor in 2009. In 2004, 59.7 percent of Multnomah County voters cast ballots against Oregon Ballot Measure 36, which amended the Oregon Constitution to prohibit recognition of same-sex marriages. The measure passed with 56.6% of the statewide vote. Multnomah County is one of two counties where a majority voted against the initiative; the other is Benton County, which includes Corvallis, home of Oregon State University. On April 28, 2005, Portland became the only city in the nation to withdraw from a Joint Terrorism Task Force. As of February 19, 2015, the Portland city council approved permanently staffing the JTTF with two of its city's police officers.

City planning and development
The city consulted with urban planners as far back as 1904, resulting in the development of Washington Park and the 40-Mile Loop greenway, which connects many of the city's parks. Portland is often cited as an example of a city with strong land use planning controls. This is largely the result of statewide land conservation policies adopted in 1973 under Governor Tom McCall, in particular the requirement for an urban growth boundary (UGB) for every city and metropolitan area. The opposite extreme, a city with few or no controls, is typically illustrated by Houston.
Oregon's 1973 "urban growth boundary" law limits the boundaries for large-scale development in each metropolitan area in Oregon. This limits access to utilities such as sewage, water and telecommunications, as well as coverage by fire, police and schools. Portland's urban growth boundary, adopted in 1979, separates urban areas (where high-density development is encouraged and focused) from traditional farm land (where restrictions on non-agricultural development are very strict). This was atypical in an era when automobile use led many areas to neglect their core cities in favor of development along interstate highways, in suburbs, and satellite cities.
The original state rules included a provision for expanding urban growth boundaries, but critics felt this was not being accomplished. In 1995, the State passed a law requiring cities to expand UGBs to provide enough undeveloped land for a 20-year supply of future housing at projected growth levels. In 2007, the legislature changed the law to require the maintenance of an estimated 50 years of growth within the boundary, as well as the protection of accompanying farm and rural lands. The growth boundary, along with efforts of the Portland Development Commission to create economic development zones, has led to the development of a large portion of downtown, a large number of mid- and high-rise developments, and an overall increase in housing and business density.Prosper Portland (formerly the Portland Development Commission) is a semi-public agency that plays a major role in downtown development; city voters created it in 1958 to serve as the city's urban renewal agency. It provides housing and economic development programs within the city and works behind the scenes with major local developers to create large projects. In the early 1960s, the Portland Development Commission led the razing of a large Italian-Jewish neighborhood downtown, bounded roughly by I-405, the Willamette River, 4th Avenue and Market street. Mayor Neil Goldschmidt took office in 1972 as a proponent of bringing housing and the associated vitality back to the downtown area, which was seen as emptying out after 5 pm. The effort has had dramatic effects in the 30 years since, with many thousands of new housing units clustered in three areas: north of Portland State University (between I-405, SW Broadway, and SW Taylor St.); the RiverPlace development along the waterfront under the Marquam (I-5) bridge; and most notably in the Pearl District (between I-405, Burnside St., NW Northrup St., and NW 9th Ave.).
Historically, environmental consciousness has weighed significantly in the city's planning and development efforts. Portland was one of the first cities in the United States to promote and integrate alternative forms of transportation, such as the MAX Light Rail and extensive bike paths. The Urban Greenspaces Institute, housed in Portland State University Geography Department's Center for Mapping Research, promotes better integration of the built and natural environments. The institute works on urban park, trail, and natural areas planning issues, both at the local and regional levels. In October 2009, the Portland City Council unanimously adopted a climate action plan that will cut the city's greenhouse gas emissions to 80% below 1990 levels by 2050.As of 2012, Portland was the largest city in the United States that did not add fluoride to its public water supply, and fluoridation has historically been a subject of controversy in the city. Portland voters have four times voted against fluoridation, in 1956, 1962, 1980 (repealing a 1978 vote in favor), and 2013. In 2012 the city council, responding to advocacy from public health organizations and others, voted unanimously to begin fluoridation by 2014. Fluoridation opponents forced a public vote on the issue, and on May 21, 2013, city voters again rejected fluoridation.

Education
Primary and secondary education
Nine public school districts and many private schools include sections of Portland. Portland Public Schools is the largest school district, operating 86 public schools. In addition to PPS, other school districts in Multnomah County that serve parts of the city include the Beaverton School District, Centennial School District, David Douglas School District, Parkrose School District, Reynolds School District, Riverdale School District, and Scappoose School District. Portions in Clackamas County are in the North Clackamas School District and Centennial School District. Portions in Washington County are in Portland Public Schools.David Douglas High School, in the Powellhurst neighborhood, has the largest enrollment of any public high school in the city. Other high schools include Benson, Cleveland, Franklin, Grant, Jefferson, Madison, Parkrose, Roosevelt, and Ida B Wells-Barnett (formerly Woodrow Wilson), and several suburban high schools which serve the city's outer areas. Established in 1869, Lincoln High School (formerly Portland High School) is the city's oldest public education institution, and is one of two of the oldest high schools west of the Mississippi River (after San Francisco's Lowell High School).Former public schools in the city included Washington High School, which operated from 1906 until 1981, as well as Adams and Jackson, which also closed the same year.
The area's private schools include The Northwest Academy, Portland Jewish Academy, Rosemary Anderson High School, Portland Adventist Academy, Portland Lutheran School, Trinity Academy, Catlin Gabel School, and Oregon Episcopal School.
The city and surrounding metropolitan area are also home to a large number of Roman Catholic-affiliated private schools, including St. Mary's Academy, an all-girls school; De La Salle North Catholic High School; the co-educational Jesuit High School; La Salle High School; and Central Catholic High School, the only archdiocesan high school in the Roman Catholic Archdiocese of Portland.

Higher education
Portland State University has the second-largest enrollment rate of any university in the state (after Oregon State University), with a student body of nearly 30,000. It has been named among the top fifteen percentile of American regional universities by The Princeton Review for undergraduate education, and has been internationally recognized for its degrees in Master of Business Administration and urban planning. The city is also home to the Oregon Health & Science University, as well as Portland Community College.
Notable private universities include the University of Portland, a Roman Catholic university affiliated with the Congregation of Holy Cross; Reed College, a liberal arts college, and Lewis & Clark College.
Other institutions of higher learning within the city are:

Media
The Oregonian is the only daily general-interest newspaper serving Portland. It also circulates throughout the state and in Clark County, Washington.

Smaller local newspapers, distributed free of charge in newspaper boxes and at venues around the city, include the Portland Tribune (general-interest paper published on Wednesdays), Willamette Week (general-interest alternative weekly published on Wednesdays), and The Portland Mercury (another alt-weekly, targeted at younger urban readers and published every other Thursday). The Portland area also has newspapers that are published for specific communities, including The Asian Reporter (a weekly covering Asian news, both international and local) and The Skanner (a weekly African-American newspaper covering both local and national news). The Portland Business Journal covers business-related news on a weekly basis, as does The Daily Journal of Commerce, its main competitor. Portland Monthly is a monthly news and culture magazine. The Bee, over 110 years old, is another neighborhood newspaper serving the inner southeast neighborhoods.

Infrastructure
Healthcare
Legacy Health, a non-profit healthcare system in Portland, operates multiple facilities in the city and surrounding suburbs. These include Legacy Emanuel, founded in 1912, in Northeast Portland; and Legacy Good Samaritan, founded in 1875, and in Northwest Portland. Randall's Children's Hospital operates at the Legacy Emanuel Campus. Good Samaritan has centers for breast health, cancer, and stroke, and is home to the Legacy Devers Eye Institute, the Legacy Obesity and Diabetes Institute, the Legacy Diabetes and Endocrinology Center, the Legacy Rehabilitation Clinic of Oregon, and the Linfield-Good Samaritan School of Nursing.The Catholic-affiliated Providence Health & Services operates Providence Portland Medical Center in the North Tabor neighborhood of the city. Oregon Health & Science University is a university hospital formed in 1974. The Veterans Affairs Medical Center operates next to the Oregon Health & Science University main campus. Adventist Medical Center also serves the city. Shriners Hospital for Children is a small children's hospital established in 1923.

Transportation
The Portland metropolitan area has transportation services common to major U.S. cities, though Oregon's emphasis on proactive land-use planning and transit-oriented development within the urban growth boundary means commuters have multiple well-developed options.
In 2008, 12.6% of all commutes in Portland were on public transit. TriMet operates most of the region's buses and the MAX (short for Metropolitan Area Express) light rail system, which connects the city and suburbs. The 1986-opened MAX system has expanded to five lines, with the latest being the Orange Line to Milwaukie, in service as of September 2015. WES Commuter Rail opened in February 2009 in Portland's western suburbs, linking Beaverton and Wilsonville.
The city-owned Portland Streetcar serves two routes in the Central City – downtown and adjacent districts. The first line, which opened in 2001 and was extended in 2005–07, operates from the South Waterfront District through Portland State University and north through the West End of downtown, to shopping areas and dense residential districts north and northwest of downtown. The second line that opened in 2012 added 3.3 miles (5.3 km) of tracks on the east side of the Willamette River and across the Broadway Bridge to a connection with the original line. The east-side line completed a loop to the tracks on the west side of the river upon completion of the new Tilikum Crossing in 2015, and, in anticipation of that, had been named the Central Loop line in 2012. However, it was renamed the Loop Service, with an A Loop (clockwise) and B Loop (counterclockwise), when it became a complete loop with the opening of the Tilikum Crossing bridge.
Fifth and Sixth avenues within downtown comprise the Portland Transit Mall, two streets devoted primarily to bus and light rail traffic with limited automobile access. Opened in 1977 for buses, the transit mall was renovated and rebuilt in 2007–09, with light rail added. Starting in 1975 and lasting nearly four decades, all transit service within downtown Portland was free, the area being known by TriMet as Fareless Square, but a need for minor budget cuts and funding needed for expansion prompted the agency to limit free rides to rail service only in 2010, and subsequently to discontinue the fare-free zone entirely in 2012.TriMet provides real-time tracking of buses and trains with its TransitTracker, and makes the data available to software developers so they can create customized tools of their own.
I-5 connects Portland with the Willamette Valley, Southern Oregon, and California to the south and with Washington to the north. I-405 forms a loop with I-5 around the central downtown area of the city and I-205 is a loop freeway route on the east side which connects to the Portland International Airport. U.S. 26 supports commuting within the metro area and continues to the Pacific Ocean westward and Mount Hood and Central Oregon eastward. U.S. 30 has a main, bypass, and business route through the city extending to Astoria to the west; through Gresham, Oregon, and the eastern exurbs, and connects to I-84, traveling towards Boise, Idaho.

Portland's main airport is Portland International Airport (PDX), about 20 minutes by car (40 minutes by MAX) northeast of downtown. Portland's airport has been named the best US airport for seven consecutive years (2013–2019). Portland is also home to Oregon's only public use heliport, the Portland Downtown Heliport.
Amtrak, the national passenger rail system, provides service to Portland at Union Station on three routes. Long-haul train routes include the Coast Starlight (with service from Los Angeles to Seattle) and the Empire Builder (with service to Chicago). The Amtrak Cascades state-supported trains operate between Vancouver, B.C., and Eugene, Oregon, and serve Portland several times daily. The city is also served by Greyhound Lines intercity bus service, which also operates BoltBus, an express bus service. The city's first airport was the Swan Island Municipal Airport, which was closed in the 1940s.

Portland is the only city in the United States that owns operating mainline steam locomotives, donated to the city in 1958 by the railroads that ran them. Spokane, Portland & Seattle 700 and the world-famous Southern Pacific 4449 can be seen several times a year pulling a special excursion train, either locally or on an extended trip. The "Holiday Express", pulled over the tracks of the Oregon Pacific Railroad on weekends in December, has become a Portland tradition over its several years running. These trains and others are operated by volunteers of the Oregon Rail Heritage Foundation, an amalgamation of rail preservation groups which collaborated on the finance and construction of the Oregon Rail Heritage Center, a permanent and publicly accessible home for the locomotives, which opened in 2012 adjacent to OMSI.In Portland, cycling is a significant mode of transportation. As the city has been particularly supportive of urban bicycling it now ranks highly among the most bicycle-friendly cities in the world.
Bicycles accounted for 6.3% of commuting in 2017. For its achievements in promoting cycling as an everyday means of transportation, Portland has been recognized by the League of American Bicyclists and other cycling organizations for its network of on-street bicycling facilities and other bicycle-friendly services, being one of only three U.S. cities to have earned a Platinum-level rating. A new bicycle-sharing system, Biketown, launched on July 19, 2016, with 100 stations in the city's central and eastside neighborhoods. The bikes were provided by Social Bicycles, and the system is operated by Motivate.
Car sharing through Zipcar, Getaround, and Uhaul Car Share is available to residents of the city and some inner suburbs. Portland has a commuter aerial cableway, the Portland Aerial Tram, which connects the South Waterfront district on the Willamette River to the Oregon Health & Science University campus on Marquam Hill above.
Portland abolished the requirement for parking minimum as well as Minneapolis and Austin.

Water
The main source of drinking water is the Bull Run Watershed consisting of 102 square miles (260 km2) of forested land on the western flank of Mount Hood The city also has 25 wells in an area which goes from near the airport at I-205 to Blue Lake Park which supplements the supply during the summer.

Notable people
Sister cities
Portland's sister cities are:

Portland also has a friendship city agreement with:

 Utrecht, Province of Utrecht, Netherlands (2012)

See also
1972 Portland–Vancouver tornado
Keep Portland Weird
List of hospitals in Portland, Oregon
List of sports venues in Portland, Oregon
Roman Catholic Archdiocese of Portland in Oregon
Roses in Portland, Oregon
USS Portland, 2 of 3 ships

Notes
References
Bibliography
Further reading
External links

Official website
Portland Maps Archived July 22, 2010, at the Wayback Machine (lot-level GIS)
Portland Business Alliance – Portland Chamber of Commerce
Portland's Visitor Association – official visitors' bureau website

Post-war

A post-war or postwar period is the interval immediately following the end of a war. The term usually refers to a varying period of time after World War II, which ended in 1945. A post-war period can become an interwar period or interbellum, when a war between the same parties resumes at a later date (such as the period between World War I and World War II). By contrast, a post-war period marks the cessation of armed conflict entirely.

Post–World War II
Chronology of the post–World War II era
The term "post-war" can have different meanings in different countries and refer to a period determined by local considerations based on the effect of the war there. Some examples of post-war events are in chronological order:

The Cold War (1947–1991)
The Cold War was a geopolitical conflict between the capitalist and liberal democratic United States of America, the authoritarian and Communist Marxist–Leninist Union of Soviet Socialist Republics, and their respective allies: NATO and the Western Bloc for the United States, and the Warsaw Pact and the Eastern Bloc for the Soviet Union. Although both sides did not fight each other directly, both engaged through various proxy wars. At the height of the cold war, both superpowers manufactured and deployed thousands of nuclear weapons to target each other's key economic, military, and political centers. Each superpower's buildup and demonstration of nuclear strike capabilities lead to an unofficial military doctrine known as mutually assured destruction (MAD). The doctrine of MAD prompted leaders on both sides to believe that victory following a full-scale nuclear exchange was simply impossible as the destruction on both sides would be insurmountable. Towards the end of the Cold War, a period of détente culminated in the easing of tensions, bans on nuclear testing, and the destruction of various quantities of nuclear stockpiles. The Cold War began to come to an end in 1989 with the overthrow of Communist governments across Eastern Europe in the Revolutions of 1989 which was followed shortly after by the collapse of the Soviet Union in 1991, leaving the United States the world's sole superpower.

Korean War (1950–1953)
On June 25, 1950, after years of tension between communist North Korea and democratic South Korea, North Korea coordinated a series of surprise attacks against strategic points between the 38th parallel. Soon US-led United Nations forces joined the war on behalf of South Korea, expelled the North Korean invasion, and then invaded and nearly captured North Korea. In response, Chinese forces entered the war on behalf of North Korea and pushed the US, South Korean, and UN forces back to the 38th parallel. After 3 years of advances and retreats nearly five million people died. To this very day there are still border disputes between the two Koreas.

Civil rights movement (1954–1968)
In the 1950s African Americans faced discrimination and segregation throughout the United States, especially in the south where many could not even vote. In 1954, the Supreme Court ruled unanimously in Brown v. Board of Education that racial segregation in public schools was unconstitutional. By the end of the 1950s, fewer than 10 percent of Black children in the South were attending integrated schools.

Vietnam war (1955–1975)
The Vietnam War was fought between the communist North Vietnam supported by the Soviet Union, China, and the Eastern Bloc and China and South Vietnam supported by the United States and SEATO. This war is especially brutal due to North Vietnamese regular forces and Viet Cong insurgents in South Vietnam adapting to guerrilla fighting and ambush tactics against the South Vietnamese military and the United States Armed Forces. Vietnam was one of the first wars to be broadcast to television. Many American civilians and soldiers were opposed to the war due to the condition and many thought the war was pointless. Finally after many protests the United States slowly withdrew from Vietnam due to public backlash.

United Kingdom

In Britain, "post-war":

culturally, is a term commonly used in the arts and architecture, as it is worldwide. It is primarily and especially before the ascendancy of Pop Art and overlapping "post-modernist" "1960s" movements. Its end is complex due to its archetypes of the 1950s contrasting with leading developments in avant-garde music genres and in popular art, becoming to some audiences mainstream, before 1960. Its movements such as continued functionalism and brutalism were overtaken by the, definitively raucous, counterculture of the 1960s, dominating as the decade wore on. Later resurgences to its stress on quite basic forms were common such as postmodernism and minimalism.
politically and economically
at its broadest, is the period from the election of Clement Attlee in 1945 general election to that of Margaret Thatcher in the 1979 general election, the so-called post-war consensus.
at its narrowest, usually with precise or contextual qualifiers, it is the war's direct aftermath; this prompted social solidarity, unprecedented high capital, particularly inheritance taxation, internationalism, the decolonization of the British Empire, the founding and endowing of the National Health Service all amid relative austerity particularly rationing. Hardships in capital taxation, and of rationing, faded due to global recovery, technological advances and consumerism enabled and encouraged from the late 1950s such as under the four-successive leader Conservative government, 1957–1964. These set a social norm for a majority of out-of-town journeys in private rather than public transport and private housing preferred over public housing, continued (with minor abatement) through alternating governments of the next two decades.

Cold War era
Considering the post-war era as equivalent to the Cold War era, post-war sometimes includes the 1980s, putting the end at 26 December 1991, with the Dissolution of the Soviet Union. The 1990s and the 21st century are extremely rarely described as part of the post-war era, with the more specific phrase "Post–Cold War era" being commonly used, instead.

See also

Interwar period
Pre-war (not a synonym for interwar when referring to World War I)
Aftermath of the September 11 attacks
Postbellum
Reconstruction era of the U.S.
Post–Cold War era


== References ==

Precarious work

Precarious work is a term that critics use to describe non-standard or temporary employment that may be poorly paid, insecure, unprotected, and unable to support a household. From this perspective, globalization, the shift from the manufacturing sector to the service sector, and the spread of information technology have created a new economy which demands flexibility in the workplace, resulting in the decline of the standard employment relationship, particularly for women. The characterization of temporary work as "precarious" is disputed by some scholars and entrepreneurs who see these changes as positive for individual workers.

Contrast with regular and temporary employment
The term "precarious work" is frequently associated with the following types of employment: Part-time jobs, self-employment, fixed-term work, temporary work, on-call work, and remote workers. Scholars and critics who use the term "precarious work" contrast it with the "standard employment relationship", which is the term they use to describe full-time, continuous employment where the employee works on their employer's premises or under the employer's supervision, under an employment contract of indefinite duration, with standardized working hours/weeks and social benefits such as pensions, unemployment benefits, and medical coverage. This "standard employment relationship" emerged after World War II, as men who completed their education would go on to work full-time for one employer their entire lives until their retirement at the age of 65. It did not typically describe women in the same time period, who would only work temporarily until they got married and had children, at which time they would withdraw from the workforce.
While many different kinds of part-time or limited-term jobs can be temporary, critics use the term "precarious" strictly to describe work that is uncertain, unpredictable, or offers little to no control over working hours or conditions. This characterization has been challenged by scholars focused on the agency that temporary work affords individual workers. However, many studies promoting individual agency focus on highly educated and skilled knowledge workers, rather than the full range of temporary workers.

Regulation
While increased flexibility in the marketplace and in employment relationships has created new opportunities for regulation, regulation intended explicitly to remediate precarious work often produces mixed results. The International Labour Organization (ILO) has developed standards for atypical and precarious employment, including the 1994 Convention Concerning Part-time Work, the 1996 Convention Concerning Home Work, and the 1999 "Decent Work" initiative.

See also
Contingent work
Flexicurity
Gig economy
Labour market flexibility
Precariat
Workforce casualisation
Zero-hour contract

References
Further reading
Andranik S. Tangian "Is flexible work precarious? A study based on the 4th European survey of working conditions 2005", WSI-Diskussionspapier Nr. 153, Hans-Böckler-Stiftung June 2007
Tangian, Andranik (2011). Flexicurity and political philosophy. New York: Nova. ISBN 978-1-61122-816-8.
Sonia McKay, Steve Jefferys, Anna Paraksevopoulou, Janoj Keles, "Study on Precarious work and social rights" Working Lives Research Institute, London Metropolitan University, April 2012
Kalleberg, Arne (2018). Precarious Lives: Job Insecurity and Well-Being in Rich Democracies. John Wiley & Sons. ISBN 9781509506538.

The Principles of Scientific Management

The Principles of Scientific Management (1911) is a monograph published by Frederick Winslow Taylor. This laid out Taylor's views on principles of scientific management, or industrial era organization and decision theory. Taylor was an American manufacturing manager, mechanical engineer, and then a management consultant in his later years. The term scientific management refers to coordinating the enterprise for everyone's benefit including increased wages for laborers although the approach is "directly antagonistic to the old idea that each workman can best regulate his own way of doing the work." His approach is also often referred to as Taylor's Principles, or Taylorism.

Contents
The monograph consisted of three sections: Introduction, Chapter 1: Fundamentals of Scientific Management, and Chapter 2: The Principles of Scientific Management.

Introduction
Taylor started this paper by quoting U.S. President Theodore Roosevelt: "The conservation of our national resources is only preliminary to the larger question of national efficiency". Taylor pointed out that while a large movement had started to conserve material resources, the less visible and less tangible effects of the wasted human effort were only vaguely appreciated. He argues the necessity of focusing on training rather than finding the "right man", stating "In the past, the man has been first; in the future, the system must be first", and the first goal of all good systems should be developing first-class men. He listed three goals for the work:

First. To point out, through a series of simple illustrations, the great loss which the whole country is suffering through inefficiency in almost all of our daily acts.Second. To try to convince the reader that the remedy for this inefficiency lies in systematic management, rather than in searching for some unusual or extraordinary man.Third. To prove that the best management is true science, resting upon clearly defined laws, rules, and principles, as a foundation. And further to show that the fundamental principles of scientific management are applicable to all kinds of human activities, from our simplest individual acts to the work of our great corporations, which call for the most elaborate cooperation. And, briefly, through a series of illustrations, to convince the reader that whenever these principles are correctly applied, results must follow which are truly astounding.

Lastly, Taylor noted that while the examples were chosen to appeal to engineers and managers, his principles could be applied to the management of any social enterprise, such as homes, farms, small businesses, churches, philanthropic institutions, universities, and government.

Chapter 1: Fundamentals of scientific management
Taylor argued that the principle object of management should be to secure the maximum prosperity for the employer, coupled with the maximum prosperity for each employee. He argued that the most important object of both the employee and the management should be the training and development of each individual in the establishment, so that he can do the highest class of work for which his natural abilities fit him. Taylor demonstrated that maximum prosperity can exist only as the result of maximum productivity, both for the shop and individual, and rebuked the idea that the fundamental interests of employees and employers are necessarily antagonistic.
Taylor described how workers deliberately work slowly, or “soldier”, to protect their interests. According to Taylor, there were three reasons for the inefficiency:

First. The fallacy, which has from time immemorial been almost universal among workmen, that a material increase in the output of each man or each machine in the trade would result in the end in throwing a large number of men out of work.Second. The defective systems of management which are in common use, and which make it necessary for each workman to soldier, or work slowly, in order that he may protect his own best interests.Third. The inefficient rule-of-thumb methods, which are still almost universal in all trades, and in practicing which our workmen waste a large part of their effort.

Taylor argued that the cheapening of any article in common use almost immediately results in a largely increased demand for that article, creating additional work and contradicting the first belief.
As to the second cause, Taylor pointed to quotes from 'Shop Management' to help explain how current management styles caused workers to soldier. He explained the natural tendency of men to take it easy as distinct from "systematic soldiering" due to thought and reasoning, and how bringing men together at a standard rate of pay exacerbated this problem. He described how under standard day, piece, or contract work it was in the workers' interest to work slowly and hide how fast work can actually be done, and the antagonism between workers and management must change.
For the third cause, Taylor noted the enormous saving of time and increase in output that could be obtained by eliminating unnecessary movements and substituting faster movements, which can only be realized after a motion and time study by a competent man. While there are perhaps "forty, fifty, or a hundred ways of doing each act in each trade", "there is always one method and one implement which is quicker and better than any of the rest".

Chapter 2: The Principles of Scientific Management
In this section, Taylor explained his principles of scientific management. He starts by describing what he considered the best system of management then in use, the system of "initiative and incentive". In this system, management gives incentives for better work, and workers give their best effort. The form of payment is practically the whole system, in contrast to scientific management. Taylor's scientific management consisted of four principles:
First. They develop a science for each element of a man's work, which replaces the old rule-of-thumb method.Second. They scientifically select and then train, teach, and develop the workman, whereas in the past he chose his own work and trained himself as best he could.Third. They heartily cooperate with the men so as to ensure all of the work being done is in accordance with the principles of the science which has been developed. 
Fourth. There is an almost equal division of the work and the responsibility between the management and the workmen. The management take over all work for which they are better fitted than the workmen, while in the past almost all of the work and the greater part of the responsibility were thrown upon the men.
Under the management of "initiative and incentive", the first three elements often exist in some form, but their importance is minor. However, under scientific management, they "form the very essence of the whole system". Taylor's summary of the fourth point is Under the management of "initiative and incentive" practically the whole problem is "up to the workman", while under scientific management fully one-half of the problem is "up to the management". It is up to the management to determine the best method to complete each task through a time and motion study, to train the worker in this method, and keep individual records for incentive based pay.
Taylor devotes most of the remainder of the work to providing case studies to support his case, including:

Moving pig iron at the  Bethlehem Steel Company, with the famous story of the "ox"-like worker "Schmidt".
Taylor's work at the Midvale Steel Company
Shoveling at Bethlehem Steel
Bricklaying, as studied by Frank B. Gilbreth
The inspection of small polished steel balls for bicycle bearing machine shop.Taylor warned about attempting to implement parts of scientific management without accepting the whole philosophy, stating that too fast of a change was often met with trouble, strikes, and failure.

See also
US labor law
Louis Brandeis
Frederick Winslow Taylor
Scientific management

Notes
References
E McGaughey, 'Behavioural Economics and Labour Law' (2014) LSE Legal Studies Working Paper No. 20/2014
Taylor, Frederick Winslow (1903), Shop Management, New York, NY, USA: American Society of Mechanical Engineers, OCLC 2365572. "Shop Management" began as an address by Taylor to a meeting of the ASME, which published it in pamphlet form. The link here takes the reader to a 1912 republication by Harper & Brothers. Also available from Project Gutenberg. {{citation}}: External link in |postscript= (help)CS1 maint: postscript (link)
Taylor, Frederick Winslow (1911), The Principles of Scientific Management, New York, NY, USA and London, UK: Harper & Brothers, LCCN 11010339, OCLC 233134. Also available from Project Gutenberg. {{citation}}: External link in |postscript= (help)CS1 maint: postscript (link)

External links
The Principles of Scientific Management – via Archive.org

Product lifecycle

In industry, product lifecycle management (PLM) is the process of managing the entire lifecycle of a product from its inception through the engineering, design and manufacture, as well as the service and disposal of manufactured products. PLM integrates people, data, processes, and business systems and provides a product information backbone for companies and their extended enterprises.

History
The inspiration for the burgeoning business process now known as PLM came from American Motors Corporation (AMC). The automaker was looking for a way to speed up its product development process to compete better against its larger competitors in 1985, according to François Castaing, Vice President for Product Engineering and Development. AMC focused its R&D efforts on extending the product lifecycle of its flagship products, particularly Jeeps, because it lacked the "massive budgets of General Motors, Ford, and foreign competitors." After introducing its compact Jeep Cherokee (XJ), the vehicle that launched the modern sport utility vehicle (SUV) market, AMC began development of a new model, that later came out as the Jeep Grand Cherokee. The first part in its quest for faster product development was computer-aided design (CAD) software system that made engineers more productive. The second part of this effort was the new communication system that allowed conflicts to be resolved faster, as well as reducing costly engineering changes because all drawings and documents were in a central database. The product data management was so effective that after AMC was purchased by Chrysler, the system was expanded throughout the enterprise connecting everyone involved in designing and building products. While an early adopter of PLM technology, Chrysler was able to become the auto industry's lowest-cost producer, recording development costs that were half of the industry average by the mid-1990s.

Forms
PLM systems help organizations in coping with the increasing complexity and engineering challenges of developing new products for the global competitive markets.Product lifecycle management (PLM) should be distinguished from 'product life-cycle management (marketing)' (PLCM). PLM describes the engineering aspect of a product, from managing descriptions and properties of a product through its development and useful life; whereas, PLCM refers to the commercial management of the life of a product in the business market with respect to costs and sales measures.
Product lifecycle management can be considered one of the four cornerstones of a manufacturing corporation's information technology structure. All companies need to manage communications and information with their customers (CRM-customer relationship management), their suppliers and fulfillment (SCM-supply chain management), their resources within the enterprise (ERP-enterprise resource planning) and their product planning and development (PLM).
One form of PLM is called people-centric PLM. While traditional PLM tools have been deployed only on the release or during the release phase, people-centric PLM targets the design phase.
As of 2009, ICT development (EU-funded PROMISE project 2004–2008) has allowed PLM to extend beyond traditional PLM and integrate sensor data and real-time 'lifecycle event data' into PLM, as well as allowing this information to be made available to different players in the total lifecycle of an individual product (closing the information loop). This has resulted in the extension of PLM into closed-loop lifecycle management (CL2M).

Benefits
Documented benefits of product lifecycle management include:
Reduced time to market
Increase full-price sales
Improved product quality and reliability
Reduced prototyping costs
More accurate and timely requests for quote generation
Ability to quickly identify potential sales opportunities and revenue contributions
Savings through the re-use of original data
A framework for product optimization
Reduced waste
Savings through the complete integration of engineering workflows
Documentation that can assist in proving compliance for RoHS or Title 21 CFR Part 11
Ability to provide contract manufacturers with access to a centralized product record
Seasonal fluctuation management
Improved forecasting to reduce material costs
Maximize supply chain collaboration

Overview of product lifecycle management
Within PLM there are five primary areas;

Systems engineering (SE) is focused on meeting all requirements, primarily meeting customer needs, and coordinating the systems design process by involving all relevant disciplines. An important aspect of lifecycle management is a subset within Systems Engineering called Reliability Engineering.
Product and portfolio management2 (PPM) are focused on managing resource allocation, tracking progress, planning for new product development projects that are in process (or in a holding status). Portfolio management is a tool that assists management in tracking progress on new products and making trade-off decisions when allocating scarce resources.
Product design (CAx) is the process of creating a new product to be sold by a business to its customers.
Manufacturing process management (MPM) is a collection of technologies and methods used to define how products are to be manufactured.
Product data management (PDM) is focused on capturing and maintaining information on products and/or services through their development and useful life. Change management is an important part of PDM/PLM.Note: While application software is not required for PLM processes, the business complexity and rate of change requires organizations to execute as rapidly as possible.

Introduction to development process
The core of PLM (product lifecycle management) is the creation and central management of all product data and the technology used to access this information and knowledge. PLM as a discipline emerged from tools such as CAD, CAM and PDM, but can be viewed as the integration of these tools with methods, people and the processes through all stages of a product's life. It is not just about software technology but is also a business strategy.
For simplicity, the stages described are shown in a traditional sequential engineering workflow.
The exact order of events and tasks will vary according to the product and industry in question but the main processes are:
Conceive
Specification
Concept design
Design
Detailed design
Validation and analysis (simulation)
Tool design
Realise
Plan manufacturing
Manufacture
Build/Assemble
Test (quality control)
Service
Sell and deliver
Use
Maintain and support
DisposeThe major key point events are:

Order
Idea
Kickoff
Design freeze
LaunchThe reality is however more complex, people and departments cannot perform their tasks in isolation and one activity cannot simply finish, and the next activity start. Design is an iterative process, often designs need to be modified due to manufacturing constraints or conflicting requirements. Whether a customer order fits into the timeline depends on the industry type and whether the products are, for example, built to order, engineered to order, or assembled to order.

Phases of product lifecycle and corresponding technologies
Many software solutions have been developed to organize and integrate the different phases of a product's lifecycle. PLM should not be seen as a single software product but as a collection of software tools and working methods integrated together to address either single stages of the lifecycle or connect different tasks or manage the whole process. Some software providers cover the whole PLM range while others have a single niche application. Some applications can span many fields of PLM with different modules within the same data model. An overview of the fields within PLM is covered here. The simple classifications do not always fit exactly; many areas overlap and many software products cover more than one area or do not fit easily into one category. It should also not be forgotten that one of the main goals of PLM is to collect knowledge that can be reused for other projects and to coordinate the simultaneous concurrent development of many products. It is about business processes, people, and methods as much as software application solutions. Although PLM is mainly associated with engineering tasks it also involves marketing activities such as product portfolio management (PPM), particularly with regard to new product development (NPD). There are several life-cycle models in each industry to consider, but most are rather similar. What follows below is one possible life-cycle model; while it emphasizes hardware-oriented products, similar phases would describe any form of product or service, including non-technical or software-based products:

Phase 1: Conceive
Imagine, specify, plan, innovate
The first stage is the definition of the product requirements based on customer, company, market, and regulatory bodies' viewpoints. From this specification, the product's major technical parameters can be defined. In parallel, the initial concept design work is performed defining the aesthetics of the product together with its main functional aspects. Many different media are used for these processes, from pencil and paper to clay models to 3D CAID computer-aided industrial design software.
In some concepts, the investment of resources into research or analysis-of-options may be included in the conception phase – e.g. bringing the technology to a level of maturity sufficient to move to the next phase. However, life-cycle engineering is iterative. It is always possible that something does not work well in any phase enough to back up into a prior phase – perhaps all the way back to conception or research. There are many examples to draw from.
The new product development process phase collects and evaluates both market and technical risks by measuring KPI and scoring model.

Phase 2: Design
Describe, define, develop, test, analyze and validate
This is where the detailed design and development of the product's form starts, progressing to prototype testing, from pilot release to full product launch. It can also involve redesign and ramp for improvement to existing products as well as planned obsolescence.
The main tool used for design and development is CAD. This can be simple 2D drawing/drafting or 3D parametric feature-based solid/surface modeling. Such software includes technology such as Hybrid Modeling, Reverse Engineering, KBE (knowledge-based engineering), NDT (Nondestructive testing), and Assembly construction.
This step covers many engineering disciplines including mechanical, electrical, electronic, software (embedded), and domain-specific, such as architectural, aerospace, automotive, … Along with the actual creation of geometry, there is the analysis of the components and product assemblies. Simulation, validation, and optimization tasks are carried out using CAE (computer-aided engineering) software either integrated into the CAD package or stand-alone. These are used to perform tasks such as Stress analysis, FEA (finite element analysis); kinematics; computational fluid dynamics (CFD); and mechanical event simulation (MES). CAQ (computer-aided quality) is used for tasks such as Dimensional tolerance (engineering) analysis. Another task performed at this stage is the sourcing of bought-out components, possibly with the aid of procurement systems.

Phase 3: Realize
Manufacture, make, build, procure, produce, sell and deliver
Once the design of the product's components is complete, the method of manufacturing is defined. This includes CAD tasks such as tool design; including the creation of CNC machining instructions for the product's parts as well as the creation of specific tools to manufacture those parts, using integrated or separate CAM (computer-aided manufacturing) software. This will also involve analysis tools for process simulation of operations such as casting, molding, and die-press forming.
Once the manufacturing method has been identified, CPM comes into play. This involves CAPE (computer-aided production engineering) or CAP/CAPP (computer-aided production planning) tools for carrying out factory, plant and facility layout, and production simulation e.g. press-line simulation, industrial ergonomics, as well as tool selection management.
After components are manufactured, their geometrical form and size can be checked against the original CAD data with the use of computer-aided inspection equipment and software. Parallel to the engineering tasks, sales product configuration, and marketing documentation work takes place. This could include transferring engineering data (geometry and part list data) to a web-based sales configurator and other desktop publishing systems.

Phase 4: Service
Use, operate, maintain, support, sustain, phase-out, retire, recycle and disposal
Another phase of the lifecycle involves managing "in-service" information. This can include providing customers and service engineers with the support and information required for repair and maintenance, as well as waste management or recycling. This can involve the use of tools such as Maintenance, Repair, and Overhaul Management (MRO) software.
An effective service consideration begins during and even prior to product design as an integral part of product lifecycle management. Service Lifecycle Management (SLM) has critical touchpoints at all phases of the product lifecycle that must be considered. Connecting and enriching a common digital thread will provide enhanced visibility across functions, improve data quality, and minimize costly delays and rework.
There is an end-of-life to every product. Whether it be the disposal or destruction of material objects or information, this needs to be carefully considered since it may be legislated and hence not free from ramifications.

Operational upgrades
During the operational phase, a product owner may discover components and consumables which have reached their individual end of life and for which there are Diminishing Manufacturing Sources or Material Shortages (DMSMS), or that the existing product can be enhanced for a wider or emerging user market easier or at less cost than a full redesign. This modernization approach often extends the product lifecycle and delays end-of-life disposal.

All phases: product lifecycle
Communicate, manage and collaborate
None of the above phases should be considered as isolated. In reality, a project does not run sequentially or separated from other product development projects, with information flowing between different people and systems. A major part of PLM is the coordination and management of product definition data. This includes managing engineering changes and release status of components; configuration product variations; document management; planning project resources as well as timescale and risk assessment.
For these tasks data of a graphical, textual, and meta nature – such as product bills of materials (BOMs) – needs to be managed. At the engineering departments level, this is the domain of Product Data Management (PDM) software, or at the corporate level Enterprise Data Management (EDM) software; such rigid level distinctions may not be consistently used, however, it is typical to see two or more data management systems within an organization. These systems may also be linked to other corporate systems such as SCM, CRM, and ERP. Associated with these systems are project management systems for project/program planning.
This central role is covered by numerous collaborative product development tools that run throughout the whole lifecycle and across organizations. This requires many technology tools in the areas of conferencing, data sharing, and data translation. This specialized field is referred to as product visualization which includes technologies such as DMU (digital mock-up), immersive virtual digital prototyping (virtual reality), and photo-realistic imaging.

User skills
The broad array of solutions that make up the tools used within a PLM solution-set (e.g., CAD, CAM, CAx...) were initially used by dedicated practitioners who invested time and effort to gain the required skills. Designers and engineers produced excellent results with CAD systems, manufacturing engineers became highly skilled CAM users, while analysts, administrators, and managers fully mastered their support technologies. However, achieving the full advantages of PLM requires the participation of many people of various skills from throughout an extended enterprise, each requiring the ability to access and operate on the inputs and output of other participants.
Despite the increased ease of use of PLM tools, cross-training all personnel on the entire PLM tool-set has not proven to be practical. Now, however, advances are being made to address ease of use for all participants within the PLM arena. One such advance is the availability of "role" specific user interfaces. Through tailorable user interfaces (UIs), the commands that are presented to users are appropriate to their function and expertise.
These techniques include:

Concurrent engineering workflow
Industrial design
Bottom–up design
Top–down design
Both-ends-against-the-middle design
Front-loading design workflow
Design in context
Modular design
NPD new product development
DFSS design for Six Sigma
DFMA design for manufacture / assembly
Digital simulation engineering
Requirement-driven design
Specification-managed validation
Configuration management

Concurrent engineering workflow
Concurrent engineering (British English: simultaneous engineering) is a workflow that, instead of working sequentially through stages, carries out a number of tasks in parallel. For example: starting tool design as soon as the detailed design has started, and before the detailed designs of the product are finished; or starting on detailed design solid models before the concept design surfaces models are complete. Although this does not necessarily reduce the amount of manpower required for a project, as more changes are required due to incomplete and changing information, it does drastically reduce lead times and thus time to market.Feature-based CAD systems have allowed simultaneous work on the 3D solid model and the 2D drawing by means of two separate files, with the drawing looking at the data in the model; when the model changes the drawing will associatively update. Some CAD packages also allow associative copying of geometry between files. This allows, for example, the copying of a part design into the files used by the tooling designer. The manufacturing engineer can then start work on tools before the final design freeze; when a design changes size or shape the tool geometry will then update.
Concurrent engineering also has the added benefit of providing better and more immediate communication between departments, reducing the chance of costly, late design changes. It adopts a problem-prevention method as compared to the problem-solving and re-designing method of traditional sequential engineering.

Bottom–up design
Bottom–up design (CAD-centric) occurs where the definition of 3D models of a product starts with the construction of individual components. These are then virtually brought together in sub-assemblies of more than one level until the full product is digitally defined. This is sometimes known as the "review structure" which shows what the product will look like. The BOM contains all of the physical (solid) components of a product from a CAD system; it may also (but not always) contain other 'bulk items' required for the final product but which (in spite of having definite physical mass and volume) are not usually associated with CAD geometry such as paint, glue, oil, adhesive tape, and other materials.
Bottom–up design tends to focus on the capabilities of available real-world physical technology, implementing those solutions to which this technology is most suited. When these bottom–up solutions have real-world value, bottom–up design can be much more efficient than top–down design. The risk of bottom–up design is that it very efficiently provides solutions to low-value problems. The focus of bottom–up design is "what can we most efficiently do with this technology?" rather than the focus of top–down which is "What is the most valuable thing to do?"

Top–down design
Top–down design is focused on high-level functional requirements, with relatively less focus on existing implementation technology. A top-level spec is repeatedly decomposed into lower-level structures and specifications until the physical implementation layer is reached. The risk of a top–down design is that it may not take advantage of more efficient applications of current physical technology, due to excessive layers of lower-level abstraction due to following an abstraction path that does not efficiently fit available components e.g. separately specifying sensing, processing, and wireless communications elements even though a suitable component that combines these may be available. The positive value of top–down design is that it preserves a focus on the optimum solution requirements.
A part-centric top–down design may eliminate some of the risks of top–down design. This starts with a layout model, often a simple 2D sketch defining basic sizes and some major defining parameters, which may include some Industrial design elements. Geometry from this is associatively copied down to the next level, which represents different subsystems of the product. The geometry in the sub-systems is then used to define more detail in the levels below. Depending on the complexity of the product, a number of levels of this assembly are created until the basic definition of components can be identified, such as position and principal dimensions. This information is then associatively copied to component files. In these files the components are detailed; this is where the classic bottom–up assembly starts.
The top–down assembly is sometimes known as a "control structure". If a single file is used to define the layout and parameters for the review structure it is often known as a skeleton file.
Defense engineering traditionally develops the product structure from the top down. The system engineering process prescribes a functional decomposition of requirements and then the physical allocation of product structure to the functions. This top down approach would normally have lower levels of the product structure developed from CAD data as a bottom–up structure or design.

Both-ends-against-the-middle design
Both-ends-against-the-middle (BEATM) design is a design process that endeavors to combine the best features of top–down design, and bottom–up design into one process. A BEATM design process flow may begin with an emergent technology that suggests solutions that may have value, or it may begin with a top–down view of an important problem that needs a solution. In either case, the key attribute of BEATM design methodology is to immediately focus on both ends of the design process flow: a top–down view of the solution requirements, and a bottom–up view of the available technology which may offer the promise of an efficient solution. The BEATM design process proceeds from both ends in search of an optimum merging somewhere between the top–down requirements, and bottom–up efficient implementation. In this fashion, BEATM has been shown to genuinely offer the best of both methodologies. Indeed, some of the best success stories from either top–down or bottom–up have been successful because of an intuitive, yet unconscious use of the BEATM methodology. When employed consciously, BEATM offers even more powerful advantages.

Front loading design and workflow
Front loading is taking top–down design to the next stage. The complete control structure and review structure, as well as downstream data such as drawings, tooling development, and CAM models, are constructed before the product has been defined or a project kick-off has been authorized. These assemblies of files constitute a template from which a family of products can be constructed. When the decision has been made to go with a new product, the parameters of the product are entered into the template model, and all the associated data is updated. Obviously, predefined associative models will not be able to predict all possibilities and will require additional work. The main principle is that a lot of the experimental/investigative work has already been completed. A lot of knowledge is built into these templates to be reused on new products. This does require additional resources "up front" but can drastically reduce the time between project kick-off and launch. Such methods do however require organizational changes, as considerable engineering efforts are moved into "offline" development departments. It can be seen as an analogy to creating a concept car to test new technology for future products, but in this case, the work is directly used for the next product generation.

Design in context
Individual components cannot be constructed in isolation. CAD and CAID models of components are created within the context of some or all of the other components within the product being developed. This is achieved using assembly modelling techniques. The geometry of other components can be seen and referenced within the CAD tool being used. The other referenced components may or may not have been created using the same CAD tool, with their geometry being translated from other collaborative product development (CPD) formats. Some assembly checking such as DMU is also carried out using product visualization software.

Product and process lifecycle management (PPLM)
Product and process lifecycle management (PPLM) is an alternate genre of PLM in which the process by which the product is made is just as important as the product itself. Typically, this is the life sciences and advanced specialty chemicals markets. The process behind the manufacture of a given compound is a key element of the regulatory filing for a new drug application. As such, PPLM seeks to manage information around the development of the process in a similar fashion that baseline PLM talks about managing information around the development of the product.
One variant of PPLM implementations are Process Development Execution Systems (PDES). They typically implement the whole development cycle of high-tech manufacturing technology developments, from initial conception, through development, and into manufacture. PDES integrates people with different backgrounds from potentially different legal entities, data, information and knowledge, and business processes.

Market size
After the Great Recession, PLM investments from 2010 onwards showed a higher growth rate than most general IT spending.Total spending on PLM software and services was estimated in 2020 to be $26 billion a year, with an estimated compound annual growth rate of 7.2% from 2021 to 2028. This was expected to be driven by a demand for software solutions for management functions, such as change, cost, compliance, data, and governance management.

Pyramid of production systems
According to Malakooti (2013), there are five long-term objectives that should be considered in production systems:

Cost: Which can be measured in terms of monetary units and usually consists of fixed and variable costs.
Productivity: Which can be measured in terms of the number of products produced during a period of time.
Quality: Which can be measured in terms of customer satisfaction levels for example.
Flexibility: Which can be considered the ability of the system to produce a variety of products for example.
Sustainability: Which can be measured in terms of ecological soundness i.e. biological and environmental impacts of a production system.The relation between these five objects can be presented as a pyramid with its tip associated with the lowest Cost, highest Productivity, highest Quality, most Flexibility, and greatest Sustainability. The points inside of this pyramid are associated with different combinations of five criteria. The tip of the pyramid represents an ideal (but likely highly unfeasible) system whereas the base of the pyramid represents the worst system possible.

See also
References
Further reading
Bergsjö, Dag (2009). Product Lifecycle Management – Architectural and Organisational Perspectives (PDF). Chalmers University of Technology. ISBN 9789173852579.
Grieves, Michael (2005). Product Lifecycle Management: Driving the Next Generation of Lean Thinking. McGraw-Hill. ISBN 9780071452304.
Saaksvuori, Antti (2008). Product Lifecycle Management. Springer. ISBN 9783540781738.

External links
 Media related to Product lifecycle management at Wikimedia Commons

Production flow analysis

In operations management and industrial engineering, production flow analysis refers to methods which share the following characteristics:

Classification of machines
Technological cycles information control
Generating a binary product-machines matrix (1 if a given product requires processing in a given machine, 0 otherwise)Methods differ on how they group together machines with products. These play an important role in designing manufacturing cells.

Rank Order Clustering
Given a binary product-machines n-by-m matrix 
  
    
      
        
          b
          
            i
            p
          
        
      
    
    {\displaystyle b_{ip}}
  , Rank Order Clustering is an algorithm characterized by the following steps:

For each row i compute the number 
  
    
      
        
          ∑
          
            p
            =
            1
          
          
            m
          
        
        
          b
          
            i
            p
          
        
        ∗
        
          2
          
            m
            −
            p
          
        
      
    
    {\displaystyle \sum _{p=1}^{m}b_{ip}*2^{m-p}}
  
Order rows according to descending numbers previously computed
For each column p compute the number 
  
    
      
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          b
          
            i
            p
          
        
        ∗
        
          2
          
            n
            −
            i
          
        
      
    
    {\displaystyle \sum _{i=1}^{n}b_{ip}*2^{n-i}}
  
Order columns according to descending numbers previously computed
If on steps 2 and 4 no reordering happened go to step 6, otherwise go to step 1
Stop

Similarity coefficients
Given a binary product-machines n-by-m matrix, the algorithm proceeds by the following steps:

Compute the similarity coefficient 
  
    
      
        
          s
          
            i
            j
          
        
        =
        
          n
          
            i
            j
          
        
        
          /
        
        (
        
          n
          
            i
            j
          
        
        +
        u
        )
      
    
    {\displaystyle s_{ij}=n_{ij}/(n_{ij}+u)}
   for all  with 
  
    
      
        
          n
          
            i
            j
          
        
      
    
    {\displaystyle n_{ij}}
   being the number of products that need to be processed on both machine i and machine j, u comprises the number of components which visit machine j but not k and vice versa.
Group together in cell k the tuple (i*,j*) with higher similarity coefficient, with k being the algorithm iteration index
Remove row i* and column j* from the original binary matrix and substitute for the row and column of the cell k, 
  
    
      
        
          s
          
            r
            k
          
        
        =
        m
        a
        x
        (
        
          s
          
            r
            i
            ∗
          
        
        ,
        
          s
          
            r
            j
            ∗
          
        
        )
      
    
    {\displaystyle s_{rk}=max(s_{ri*},s_{rj*})}
  
Go to step 2, iteration index k raised by oneUnless this procedure is stopped the algorithm eventually will put all machines in one single group.


== References ==

Production leveling

Production leveling, also known as production smoothing or – by its Japanese original term – heijunka (平準化), is a technique for reducing the mura (unevenness) which in turn reduces muda (waste). It was vital to the development of production efficiency in the Toyota Production System and lean manufacturing.  The goal is to produce intermediate goods at a constant rate so that further processing may also be carried out at a constant and predictable rate.
Where demand is constant, production leveling is easy, but where customer demand fluctuates, two approaches have been adopted: 1) demand leveling and 2) production leveling through flexible production.
To prevent fluctuations in production, even in outside affiliates, it is important to minimize fluctuation in the final assembly line. Toyota's final assembly line never assembles the same automobile model in a batch. Instead, they level production by assembling a mix of models in each batch and the batches are made as small as possible.

Production leveling by volume or by product type or mix
Production leveling can refer to leveling by volume, or leveling by product type or mix, although the two are closely related.

Leveling by volume
If for a family of products that use the same production process there is a demand that varies between 800 and 1,200 units then it might seem a good idea to produce the amount ordered. Toyota's view is that production systems that vary in the required output suffer from mura and muri with capacity being 'forced' in some periods. So their approach is to manufacture at the long-term average demand and carry an inventory proportional to the variability of demand, stability of the production process and the frequency of shipments. So for our case of 800–1,200 units, if the production process were 100% reliable and the shipments once a week, then the production would be with minimum standard inventory of 200 at the start of the week and 1,200 at the point of shipment. The advantage of carrying this inventory is that it can smooth production throughout the plant and therefore reduce process inventories and simplify operations which reduces costs.

Leveling by product
Most value streams produce a mix of products and therefore face a choice of production mix and sequence. It is here that the discussions on economic order quantities take place and have been dominated by changeover times and the inventory this requires. Toyota's approach resulted in a different discussion where it reduced the time and cost of changeovers so that smaller and smaller batches were not prohibitive and lost production time and quality costs were not significant. This meant that the demand for components could be leveled for the upstream sub-processes and therefore lead time and total inventories reduced along the entire value stream. To simplify leveling of products with different demand levels a related visual scheduling board known as a heijunka box is often used in achieving these heijunka style efficiencies. Other production leveling techniques based on this thinking have also been developed. Once leveling by product is achieved then there is one more leveling phase, that of "Just in Sequence" where leveling occurs at the lowest level of product production.
The use of production leveling as well as broader lean production techniques helped Toyota massively reduce vehicle production times as well as inventory levels during the 1980s.

Implementation
Even Toyota hasn't reached the final stage in this journey, single-piece flows, across all of their processes; indeed they recommend following their journey rather than trying to jump into an intermediate stage. The reason Toyota advocates this is that each production stage is accompanied by adjustments and adaptations to support services to production; if those services are not given these adaptation steps then major issues can arise.

Implement green stream/red stream or fixed sequence, fixed volume to establish the entry and exit criteria for products from these streams and establish the supporting disciplines in the support services. The cycle established will produce Every Product Every Cycle (EPEC). This is a specific form of Fixed Repeating Schedule. Green stream products are those with predictable demand, Red stream products are high value unpredictable demand products.
Faster fixed sequence with fixed volume keep the streams the same but use the now established familiarity with the streams to maximise learning and improve speed of production (economies of repetition). This will allow the shortening of the EPEC cycle so that the plant is now producing every product every 2 weeks instead of month and then later on repeating every week. This may require support services to speed up as well.
Fixed sequence with unfixed volume keep the stream sequences the same but now phase in allowing actual sales to influence volumes within those sequences. This affects inbound componentry as well as support services. This is a more generalised form of Fixed Repeating Schedule.
Unfixed sequence with fixed volume the stream sequences, and EPEC, can now be gradually flexed but move to small fixed batch sizes to make this more manageable.
Unfixed sequence with unfixed volume finally move to true single piece flow and pull by reducing batch sizes until they reach one.

Demand leveling
Demand leveling is the deliberate influencing of demand itself or the demand processes to deliver a more predictable pattern of customer demand. Some of this influencing is by manipulating the product offering, some by influencing the ordering process and some by revealing the demand amplification induced variability of ordering patterns. Demand levelling does not include influencing activities designed to clear existing stock.
Historically demand leveling evolved as subset of production levelling and has been approached in a variety of ways:

The first approach to demand levelling involves careful management of the sales pipeline. For this method of demand management it is instructive to look at Toyota in its home market, Japan. Toyota sales teams sell cars door-to-door whereby they build customer profiles and relationships. The sales process is low intensity but includes test drives, financing, insurance and trade-in deals. The sale itself is by special order placed with their representative. This means that orders can be predicted reasonably accurately in terms of vehicle numbers some way in advance. Finer specific vehicle details may only become known with the order. However, the order is often for delivery in the future so these details can usually be planned before build. Because the customer is getting the exact car they want there is less negotiation around price as indeed the fact that the build is to order removes the incentive of the manufacturer, or their agent, to discount existing stock. The aim of this system is to maximise the revenue from the customer in the long term. This leads to the sales team handling after-sales issues of diverse kinds for an extended period to keep customer loyalty and the relationship which will sell the next car. Between purchases the sales team remain in touch for all aspects of customer satisfaction with their cars including feedback for product design on changing customer preferences in the market. The Japanese market does not have the seasonal, promotional or other demand surges that are a characteristic of Western automotive markets. It is debated, for both markets, whether this is caused by manufacturers' behaviour or whether manufacturers' behaviour is a logical response to it.
A second approach to demand levelling is by deep understanding of the systems used to order products by retailers and other sellers from manufacturers. Even where this supply chain is very simple, customer-retailer-manufacturer, it is usually the case that orders are based on some form of economic order quantity (EOQ) calculation that aggregates actual customer demand over a certain period. This aggregation, and the other clever calculations that may be involved, often obscure the fact that actual demand for a product is close to flat, and for high volume products very close to flat. The demand pulsing effect is created by the ordering process and the more complex it is the greater this effect. The use of EPOS actual sales data can reveal this effect very clearly.
A third approach to demand management is to keep finished goods or nearly finished goods in stock to act as a buffer and thus isolate the production facility from actual demand.  This approach is widely used today but its weakness is becoming more and more evident as a growing variety of products is demanded. The cost of making, storing, managing and protecting finished goods stock can grow to be prohibitive depending upon product range and demand variability levels. This usually means that actually whilst stocks are kept they are insufficient to meet the stated aims and so customer dissatisfaction ensues along with distressed sales (reduced price) to eliminate stock levels seen as too high.

Implementation
If it is accepted that a large part of demand variability in high volume products can be substantially caused by sales and ordering process artifacts then analysis and leveling can be attempted.
The use of long delay supply chains to reduce manufacturing costs often means that production orders are placed long before customer demand can be realistically estimated. The much later arrival of forecast product demand volumes makes demand leveling irrelevant since the issue has now switched to disposal at best price possible products that are already created and possibly paid for. Demand leveling has only proven possible where build times have been made relatively low and production has been made relatively reliable and flexible. Examples of these are fast airborne supply chains (e.g. Apple iPod) or direct to customer selling through web sites allowing late customisation (e.g. NIKEiD custom shoes) or local manufacture (e.g. Timbuk2 custom courier bags).
Where actual build-delivery times can be brought within the same scale as customer time horizons then effort to modify impulse buying and make it somewhat planned can be successful. Reliable, flexible manufacturing will then mean that low stock levels (if any) do not interfere with customer satisfaction and that incentives to sell what has been produced eliminated.
Where demand follows a predictable pattern, e.g. flat, then regular deliveries of constant amounts can be agreed with variances in actual demand ignored unless it exceeds some agreed trigger level. Where this cannot be agreed then it can be simulated and the benefits gained through frequent deliveries and a market location.
The predictable pattern does not have to be flat and may, for example, be an annual pattern with higher volumes at particular periods. Here again the deliveries can be agreed to follow a simplified but similar pattern, perhaps one delivery volume for six months of the year and another for the other six months.

See also
Heijunka box


== References ==

Productivity

Productivity is the efficiency of production of goods or services expressed by some measure. Measurements of productivity are often expressed as a ratio of an aggregate output to a single input or an aggregate input used in a production process, i.e. output per unit of input, typically over a specific period of time. The most common example is the (aggregate) labour productivity measure, one example of which is GDP per worker. There are many different definitions of productivity (including those that are not defined as ratios of output to input) and the choice among them depends on the purpose of the productivity measurement and data availability. The key source of difference between various productivity measures is also usually related (directly or indirectly) to how the outputs and the inputs are aggregated to obtain such a ratio-type measure of productivity.Productivity is a crucial factor in the production performance of firms and nations. Increasing national productivity can raise living standards because more real income improves people's ability to purchase goods and services, enjoy leisure, improve housing, and education and contribute to social and environmental programs. Productivity growth can also help businesses to be more profitable.

Partial productivity
Productivity measures that use one class of inputs or factors, but not multiple factors, are called partial productivities. In practice, measurement in production means measures of partial productivity. Interpreted correctly, these components are indicative of productivity development, and approximate the efficiency with which inputs are used in an economy to produce goods and services. However, productivity is only measured partially – or approximately. In a way, the measurements are defective because they do not measure everything, but it is possible to interpret correctly the results of partial productivity and to benefit from them in practical situations. At the company level, typical partial productivity measures are such things as worker hours, materials or energy used per unit of production.Before the widespread use of computer networks, partial productivity was tracked in tabular form and with hand-drawn graphs. Tabulating machines for data processing began being widely used in the 1920s and 1930s and remained in use until mainframe computers became widespread in the late 1960s through the 1970s. By the late 1970s inexpensive computers allowed industrial operations to perform process control and track productivity. Today data collection is largely computerized and almost any variable can be viewed graphically in real time or retrieved for selected time periods.

Labour productivity
In macroeconomics, a common partial productivity measure is labour productivity. Labour productivity is a revealing indicator of several economic indicators as it offers a dynamic measure of economic growth, competitiveness, and living standards within an economy. It is the measure of labour productivity (and all that this measure takes into account) which helps explain the principal economic foundations that are necessary for both economic growth and social development. In general labour productivity is equal to the ratio between a measure of output volume (gross domestic product or gross value added) and a measure of input use (the total number of hours worked or total employment).

  
    
      
        
          labour productivity
        
        =
        
          
            output volume
            labor input use
          
        
      
    
    {\displaystyle {\text{labour productivity}}={\frac {\text{output volume}}{\text{labor input use}}}}
  The output measure is typically net output, more specifically the value added by the process under consideration, i.e. the value of outputs minus the value of intermediate inputs. This is done in order to avoid double-counting when an output of one firm is used as an input by another in the same measurement. In macroeconomics the most well-known and used measure of value-added is the gross domestic product or GDP. Increases in it are widely used as a measure of the economic growth of nations and industries. GDP is the income available for paying capital costs, labor compensation, taxes and profits. Some economists instead use gross value added (GVA); there is normally a strong correlation between GDP and GVA.The measure of input use reflects the time, effort and skills of the workforce. The denominator of the ratio of labour productivity, the input measure is the most important factor that influences the measure of labour productivity. Labour input is measured either by the total number of hours worked of all persons employed or total employment (head count). There are both advantages and disadvantages associated with the different input measures that are used in the calculation of labour productivity. It is generally accepted that the total number of hours worked is the most appropriate measure of labour input because a simple headcount of employed persons can hide changes in average hours worked and has difficulties accounting for variations in work such as a part-time contract, paid leave, overtime, or shifts in normal hours. However, the quality of hours-worked estimates is not always clear. In particular, statistical establishment and household surveys are difficult to use because of their varying quality of hours-worked estimates and their varying degree of international comparability.
GDP per capita is a rough measure of average living standards or economic well-being and is one of the core indicators of economic performance. GDP is, for this purpose, only a very rough measure. Maximizing GDP, in principle, also allows maximizing capital usage. For this reason, GDP is systematically biased in favour of capital intensive production at the expense of knowledge and labour-intensive production. The use of capital in the GDP-measure is considered to be as valuable as the production's ability to pay taxes, profits and labor compensation. The bias of the GDP is actually the difference between the GDP and the producer income.Another labour productivity measure, output per worker, is often seen as a proper measure of labour productivity, as here: "Productivity isn't everything, but in the long run it is almost everything. A country's ability to improve its standard of living over time depends almost entirely on its ability to raise its output per worker." This measure (output per worker) is, however, more problematic than the GDP or even invalid because this measure allows maximizing all supplied inputs, i.e. materials, services, energy and capital at the expense of producer income.

Multi-factor productivity
When multiple inputs are considered, the measure is called multi-factor productivity or MFP. Multi-factor productivity is typically estimated using growth accounting. If the inputs specifically are labor and capital, and the outputs are value added intermediate outputs, the measure is called total factor productivity (TFP]. TFP measures the residual growth that cannot be explained by the rate of change in the services of labour and capital. MFP replaced the term TFP used in the earlier literature, and both terms continue in use (usually interchangeably).TFP is often interpreted as a rough average measure of productivity, more specifically the contribution to economic growth made by factors such as technical and organisational innovation. The most famous description is that of Robert Solow's (1957): "I am using the phrase 'technical change' as a shorthand expression for any kind of shift in the production function. Thus slowdowns, speed ups, improvements in the education of the labor force and all sorts of things will appear as 'technical change' ." The original MFP model involves several assumptions: that there is a stable functional relation between inputs and output at the economy-wide level of aggregation, that this function has neoclassical smoothness and curvature properties, that inputs are paid the value of their marginal product, that the function exhibits constant returns to scale, and that technical change has the Hicks’n neutral form. In practice, TFP is "a measure of our ignorance", as Abramovitz (1956) put it, precisely because it is a residual. This ignorance covers many components, some wanted (like the effects of technical and organizational innovation), others unwanted (measurement error, omitted variables, aggregation bias, model misspecification) Hence the relationship between TFP and productivity remains unclear.

Total productivity
When all outputs and inputs are included in the productivity measure it is called total productivity. A valid measurement of total productivity necessitates considering all production inputs. If we omit an input in productivity (or income accounting) this means that the omitted input can be used unlimitedly in production without any impact on accounting results. Because total productivity includes all production inputs, it is used as an integrated variable when we want to explain income formation of the production process.
Davis has considered the phenomenon of productivity, measurement of productivity, distribution of productivity gains, and how to measure such gains. He refers to an article suggesting that the measurement of productivity shall be developed so that it ”will indicate increases or decreases in the productivity of the company and also the distribution of the ’fruits of production’ among all parties at interest”. According to Davis, the price system is a mechanism through which productivity gains are distributed, and besides the business enterprise, receiving parties may consist of its customers, staff and the suppliers of production inputs.
In the main article is presented the role of total productivity as a variable when explaining how income formation of production is always a balance between income generation and income distribution. The income change created by production function is always distributed to the stakeholders as economic values within the review period.

Benefits of productivity growth
Productivity growth is a crucial source of growth in living standards. Productivity growth means more value is added in production and this means more income is available to be distributed.
At a firm or industry level, the benefits of productivity growth can be distributed in a number of different ways:

to the workforce through better wages and conditions;
to shareholders and superannuation funds through increased profits and dividend distributions;
to customers through lower prices;
to the environment through more stringent environmental protection; and
to governments through increases in tax payments (which can be used to fund social and environmental programs).Productivity growth is important to the firm because it means that it can meet its (perhaps growing) obligations to workers, shareholders, and governments (taxes and regulation), and still remain competitive or even improve its competitiveness in the market place. Adding more inputs will not increase the income earned per unit of input (unless there are increasing returns to scale). In fact, it is likely to mean lower average wages and lower rates of profit. But, when there is productivity growth, even the existing commitment of resources generates more output and income. Income generated per unit of input increases. Additional resources are also attracted into production and can be profitably employed.

Drivers of productivity growth
In the most immediate sense, productivity is determined by the available technology or know-how for converting resources into outputs, and the way in which resources are organized to produce goods and services. Historically, productivity has improved through evolution as processes with poor productivity performance are abandoned and newer forms are exploited. Process improvements may include organizational structures (e.g. core functions and supplier relationships), management systems, work arrangements, manufacturing techniques, and changing market structure. A famous example is the assembly line and the process of mass production that appeared in the decade following commercial introduction of the automobile.Mass production dramatically reduced the labor in producing parts for and assembling the automobile, but after its widespread adoption productivity gains in automobile production were much lower. A similar pattern was observed with electrification, which saw the highest productivity gains in the early decades after introduction. Many other industries show similar patterns. The pattern was again followed by the computer, information and communications industries in the late 1990s when much of the national productivity gains occurred in these industries.There is a general understanding of the main determinants or drivers of productivity growth. Certain factors are critical for determining productivity growth. The Office for National Statistics (UK) identifies five drivers that interact to underlie long-term productivity performance: investment, innovation, skills, enterprise and competition.
Investment is in physical capital — machinery, equipment and buildings. The more capital workers have at their disposal, generally the better they are able to do their jobs, producing more and better quality output.
Innovation is the successful exploitation of new ideas. New ideas can take the form of new technologies, new products or new corporate structures and ways of working. Speeding up the diffusion of innovations can boost productivity.
Skills are defined as the quantity and quality of labour of different types available in an economy. Skills complement physical capital, and are needed to take advantage of investment in new technologies and organisational structures.
Enterprise is defined as the seizing of new business opportunities by both start-ups and existing firms. New enterprises compete with existing firms by new ideas and technologies increasing competition. Entrepreneurs are able to combine factors of production and new technologies forcing existing firms to adapt or exit the market.
Competition improves productivity by creating incentives to innovate and ensures that resources are allocated to the most efficient firms. It also forces existing firms to organise work more effectively through imitations of organisational structures and technology.

Individual and team productivity
Technology has enabled massive personal productivity gains—computers, spreadsheets, email, and other advances have made it possible for a knowledge worker to seemingly produce more in a day than was previously possible in a year. Environmental factors such as sleep and leisure play a significant role in work productivity and received wage.
Drivers of productivity growth for creative and knowledge workers include improved or intensified exchange with peers or co-workers, as more productive peers have a stimulating effect on one's own productivity. Productivity is influenced by effective supervision and job satisfaction. An effective or knowledgeable supervisor (for example a supervisor who uses the Management by objectives method) has an easier time motivating their employees to produce more in quantity and quality. An employee who has an effective supervisor, motivating them to be more productive is likely to experience a new level of job satisfaction thereby becoming a driver of productivity itself. There is also considerable evidence to support improved productivity through operant conditioning reinforcement, successful gamification engagement, research-based recommendations on principles and implementation guidelines for using monetary rewards effectively, and recognition, based on social cognitive theory, which builds upon self-efficacy.

Detrimental impact of bullying, incivility, toxicity and psychopathy
Workplace bullying results in a loss of productivity, as measured by self-rated job performance. Over time, targets of bullying will spend more time protecting themselves against harassment by bullies and less time fulfilling their duties. Workplace incivility has also been associated with diminished productivity in terms of quality and quantity of work.A toxic workplace is a workplace that is marked by significant drama and infighting, where personal battles often harm productivity. While employees are distracted by this, they cannot devote time and attention to the achievement of business goals. When toxic employees leave the workplace, it can improve the culture overall because the remaining staff become more engaged and productive. The presence of a workplace psychopath may have a serious detrimental impact on productivity in an organisation.In companies where the traditional hierarchy has been removed in favor of an egalitarian, team-based setup, the employees are often happier, and individual productivity is improved (as they themselves are better placed to increase the efficiency of the workfloor). Companies that have these hierarchies removed and have their employees work more in teams are called liberated companies or "Freedom Inc.'s". The Kaizen system of bottom-up, continuous improvement was first practiced by Japanese manufacturers after World War II, most notably as part of The Toyota Way.

Business productivity
Productivity is one of the main concerns of business management and engineering. Many companies have formal programs for continuously improving productivity, such as a production assurance program. Whether they have a formal program or not, companies are constantly looking for ways to improve quality, reduce downtime and inputs of labor, materials, energy and purchased services. Often simple changes to operating methods or processes increase productivity, but the biggest gains are normally from adopting new technologies, which may require capital expenditures for new equipment, computers or software. Modern productivity science owes much to formal investigations that are associated with scientific management. Although from an individual management perspective, employees may be doing their jobs well and with high levels of individual productivity, from an organizational perspective their productivity may in fact be zero or effectively negative if they are dedicated to redundant or value destroying activities. In office buildings and service-centred companies, productivity is largely influenced and affected by operational byproducts—meetings. The past few years have seen a positive uptick in the number of software solutions focused on improving office productivity. In truth, proper planning and procedures are more likely to help than anything else.

Productivity paradox
Overall productivity growth was relatively slow from the 1970s through the early 1990s, and again from the 2000s to 2020s. Although several possible causes for the slowdown have been proposed there is no consensus. The matter is subject to a continuing debate that has grown beyond questioning whether just computers can significantly increase productivity to whether the potential to increase productivity is becoming exhausted.

National productivity
In order to measure the productivity of a nation or an industry, it is necessary to operationalize the same concept of productivity as in a production unit or a company, yet, the object of modelling is substantially wider and the information more aggregate. The calculations of productivity of a nation or an industry are based on the time series of the SNA, System of National Accounts. National accounting is a system based on the recommendations of the UN (SNA 93) to measure the total production and total income of a nation and how they are used.International or national productivity growth stems from a complex interaction of factors. Some of the most important immediate factors include technological change, organizational change, industry restructuring and resource reallocation, as well as economies of scale and scope. A nation's average productivity level can also be affected by the movement of resources from low-productivity to high-productivity industries and activities. Over time, other factors such as research and development and innovative effort, the development of human capital through education, and incentives from stronger competition promote the search for productivity improvements and the ability to achieve them. Ultimately, many policy, institutional and cultural factors determine a nation's success in improving productivity.
At the national level, productivity growth raises living standards because more real income improves people's ability to purchase goods and services (whether they are necessities or luxuries), enjoy leisure, improve housing and education and contribute to social and environmental programs. Some have suggested that the UK's 'productivity puzzle' is an urgent issue for policy makers and businesses to address in order to sustain growth. Over long periods of time, small differences in rates of productivity growth compound, like interest in a bank account, and can make an enormous difference to a society's prosperity. Nothing contributes more to reduction of poverty, to increases in leisure, and to the country's ability to finance education, public health, environment and the arts’.Productivity is considered basic statistical information for many international comparisons and country performance assessments and there is strong interest in comparing them internationally. The OECD publishes an annual Compendium of Productivity Indicators that includes both labor and multi-factor measures of productivity.

See also
References
Citations
Sources
Further reading
External links
Field, Alexander J. (2008). "Productivity". In David R. Henderson (ed.). Concise Encyclopedia of Economics (2nd ed.). Indianapolis: Library of Economics and Liberty. ISBN 978-0865976658. OCLC 237794267.
Productivity and Costs – Bureau of Labor Statistics United States Department of Labor: contains international comparisons of productivity rates, historical and present
Productivity Statistics—Organisation for Economic Co-operation and Development
Greenspan Speech
OECD estimates of labour productivity levels

Handbooks
Measuring Productivity—OECD Manual
Bureau of Labor Statistics, Productivity Statistics (U.S.)

Programmable logic controller

A programmable logic controller (PLC) or programmable controller is an industrial computer that has been ruggedized and adapted for the control of manufacturing processes, such as assembly lines, machines, robotic devices, or any activity that requires high reliability, ease of programming, and process fault diagnosis.
PLCs can range from small modular devices with tens of inputs and outputs (I/O), in a housing integral with the processor, to large rack-mounted modular devices with thousands of I/O, and which are often networked to other PLC and SCADA systems. They can be designed for many arrangements of digital and analog I/O, extended temperature ranges, immunity to electrical noise, and resistance to vibration and impact. 
PLCs were first developed in the automobile manufacturing industry to provide flexible,  rugged and easily programmable controllers to replace hard-wired relay logic systems. Dick Morley who invented the first PLC, the Modicon 084, for General Motors in 1968, is considered the father of PLC.
A PLC is an example of a hard real-time system since output results must be produced in response to input conditions within a limited time, otherwise unintended operation may result. Programs to control machine operation are typically stored in battery-backed-up or non-volatile memory.

Invention and early development
PLC originated in the late 1960s in the automotive industry in the US and were designed to replace relay logic systems. Before, control logic for manufacturing was mainly composed of relays, cam timers, drum sequencers, and dedicated closed-loop controllers.The hard-wired nature made it difficult for design engineers to alter the automation process. Changes would require rewiring and careful updating of the documentation. If even one wire were out of place, or one relay failed, the whole system would become faulty. Often technicians would spend hours troubleshooting by examining the schematics and comparing them to existing wiring. When general-purpose computers became available, they were soon applied to control logic in industrial processes. These early computers were unreliable and required specialist programmers and strict control of working conditions, such as temperature, cleanliness, and power quality.The PLC provided several advantages over earlier automation systems. It tolerated the industrial environment better than computers and was more reliable, compact and required less maintenance than relay systems. It was easily extensible with additional I/O modules, while relay systems required complicated hardware changes in case of reconfiguration. This allowed for easier iteration over manufacturing process design. With a simple programming language focused on logic and switching operations, it was more user-friendly than computers using general-purpose programming languages. It also permitted its operation to be monitored.
Early PLCs were programmed in ladder logic, which strongly resembled a schematic diagram of relay logic.

Modicon
In 1968, GM Hydramatic (the automatic transmission division of General Motors) issued a request for proposals for an electronic replacement for hard-wired relay systems based on a white paper written by engineer Edward R. Clark. The winning proposal came from Bedford Associates from Bedford, Massachusetts. The result was the first PLC—built in 1969–designated the 084, because it was Bedford Associates' eighty-fourth project.Bedford Associates started a company dedicated to developing, manufacturing, selling, and servicing this new product, which they named Modicon (standing for modular digital controller). One of the people who worked on that project was Dick Morley, who is considered to be the "father" of the PLC. The Modicon brand was sold in 1977 to Gould Electronics and later to Schneider Electric, the current owner. About this same time, Modicon created Modbus, a data communications protocol used with its PLCs.  Modbus has since become a standard open protocol commonly used to connect many industrial electrical devices.One of the first 084 models built is now on display at Schneider Electric's facility in North Andover, Massachusetts. It was presented to Modicon by GM, when the unit was retired after nearly twenty years of uninterrupted service. Modicon used the 84 moniker at the end of its product range until the 984 made its appearance.

Allen-Bradley
In a parallel development Odo Josef Struger is sometimes known as the "father of the programmable logic controller" as well. He was involved in the invention of the Allen‑Bradley programmable logic controller and is credited with inventing the PLC initialism. Allen-Bradley (now a brand owned by Rockwell Automation) became a major PLC manufacturer in the United States during his tenure. Struger played a leadership role in developing IEC 61131-3 PLC programming language standards.

Early methods of programming
Many early PLCs were not capable of graphical representation of the logic, and so it was instead represented as a series of logic expressions in some kind of Boolean format, similar to Boolean algebra. As programming terminals evolved, it became more common for ladder logic to be used, because it was a familiar format used for electro-mechanical control panels. Newer formats, such as state logic and Function Block (which is similar to the way logic is depicted when using digital integrated logic circuits) exist, but they are still not as popular as ladder logic. A primary reason for this is that PLCs solve the logic in a predictable and repeating sequence, and ladder logic allows the person writing the logic to see any issues with the timing of the logic sequence more easily than would be possible in other formats.Up to the mid-1990s, PLCs were programmed using proprietary programming panels or special-purpose programming terminals, which often had dedicated function keys representing the various logical elements of PLC programs. Some proprietary programming terminals displayed the elements of PLC programs as graphic symbols, but plain ASCII character representations of contacts, coils, and wires were common. Programs were stored on cassette tape cartridges. Facilities for printing and documentation were minimal due to a lack of memory capacity. The oldest PLCs used non-volatile magnetic core memory.

Architecture
A PLC is an industrial microprocessor-based controller with programmable memory used to store program instructions and various functions. It consists of:

A processor unit (CPU) which interprets inputs, executes the control program stored in memory and sends output signals,
A power supply unit which converts AC voltage to DC,
A memory unit storing data from inputs and program to be executed by the processor,
An input and output interface, where the controller receives and sends data from/to external devices,
A communications interface to receive and transmit data on communication networks from/to remote PLCs.PLCs require programming device which is used to develop and later download the created program into the memory of the controller.Modern PLCs generally contain a real-time operating system, such as OS-9 or VxWorks.

Mechanical design
There are two types of mechanical design for PLC systems. A single box, or a brick is a small programmable controller that fits all units and interfaces into one compact casing, although, typically, additional expansion modules for inputs and outputs are available. Second design type – a modular PLC – has a chassis (also called a rack) that provides space for modules with different functions, such as power supply, processor, selection of I/O modules and communication interfaces – which all can be customized for the particular application. Several racks can be administered by a single processor and may have thousands of inputs and outputs. Either a special high-speed serial I/O link or comparable communication method is used so that racks can be distributed away from the processor, reducing the wiring costs for large plants. Options are also available to mount I/O points directly to the machine and utilize quick disconnecting cables to sensors and valves, saving time for wiring and replacing components.

Discrete and analog signals
Discrete (digital) signals can only take on or off value (1 or 0, true or false). Examples of devices providing a discrete signal include limit switches, photoelectric sensors and encoders.Analog signals can use voltage or current that is proportional to the size of the monitored variable and can take any value within their scale. Pressure, temperature, flow, and weight are often represented by analog signals. These are typically interpreted as integer values with various ranges of accuracy depending on the device and the number of bits available to store the data. For example, an analog 0 to 10 V or 4-20 mA current loop input would be converted into an integer value of 0 to 32,767. The PLC will take this value and transpose it into the desired units of the process so the operator or program can read it. Proper integration will also include filter times to reduce noise as well as high and low limits to report faults. Current inputs are less sensitive to electrical noise (e.g. from welders or electric motor starts) than voltage inputs. Distance from the device and the controller is also a concern as the maximum traveling distance of a good quality 0-10 V signal is very short compared to the 4-20 mA signal. The 4-20 mA signal can also report if the wire is disconnected along the path as a <4 mA signal would indicate an error.

Redundancy
Some special processes need to work permanently with minimum unwanted downtime. Therefore, it is necessary to design a system that is fault-tolerant and capable of handling the process with faulty modules. In such cases to increase the system availability in the event of hardware component failure, redundant CPU or I/O modules with the same functionality can be added to hardware configuration for preventing total or partial process shutdown due to hardware failure.  Other redundancy scenarios could be related to safety-critical processes, for example, large hydraulic presses could require that both PLCs turn on output before the press can come down in case one output does not turn off properly.

Programming
Programmable logic controllers are intended to be used by engineers without a programming background. For this reason, a graphical programming language called Ladder Diagram (LD, LAD) was first developed. It resembles the schematic diagram of a system built with electromechanical relays and was adopted by many manufacturers and later standardized in the IEC 61131-3 control systems programming standard. As of 2015, it is still widely used, thanks to its simplicity.As of 2015, the majority of PLC systems adhere to the IEC 61131-3 standard that defines 2 textual programming languages: Structured Text (ST; similar to Pascal) and Instruction List (IL); as well as 3 graphical languages: Ladder Diagram, Function Block Diagram (FBD) and Sequential Function Chart (SFC). Instruction List (IL) was deprecated in the third edition of the standard.Modern PLCs can be programmed in a variety of ways, from the relay-derived ladder logic to programming languages such as specially adapted dialects of BASIC and C.While the fundamental concepts of PLC programming are common to all manufacturers, differences in I/O addressing, memory organization, and instruction sets mean that PLC programs are never perfectly interchangeable between different makers. Even within the same product line of a single manufacturer, different models may not be directly compatible.

Programming device
PLC programs are typically written in a programming device, which can take the form of a desktop console, special software on a personal computer, or a handheld programming device. Then, the program is downloaded to the PLC directly or over a network. It is stored either in non-volatile flash memory or battery-backed-up RAM. In some programmable controllers, the program is transferred from a personal computer to the PLC through a programming board that writes the program into a removable chip, such as EPROM.
Manufacturers develop programming software for their controllers. In addition to being able to program PLCs in multiple languages, they provide common features like hardware diagnostics and maintenance, software debugging, and offline simulation.

Simulation
PLC simulation is a feature often found in PLC programming software. It allows for testing and debugging early in a project's development.
Incorrectly programmed PLC can result in lost productivity and dangerous conditions. Testing the project in simulation improves its quality, increases the level of safety associated with equipment and can save costly downtime during the installation and commissioning of automated control applications since many scenarios can be tried and tested before the system is activated.

Functionality
The main difference from most other computing devices is that PLCs are intended-for and therefore tolerant-of more severe conditions (such as dust, moisture, heat, cold), while offering extensive input/output (I/O) to connect the PLC to sensors and actuators. PLC input can include simple digital elements such as limit switches, analog variables from process sensors (such as temperature and pressure), and more complex data such as that from positioning or machine vision systems. PLC output can include elements such as indicator lamps, sirens, electric motors, pneumatic or hydraulic cylinders, magnetic relays, solenoids, or analog outputs. The input/output arrangements may be built into a simple PLC, or the PLC may have external I/O modules attached to a fieldbus or computer network that plugs into the PLC.
The functionality of the PLC has evolved over the years to include sequential relay control, motion control, process control, distributed control systems, and networking. The data handling, storage, processing power, and communication capabilities of some modern PLCs are approximately equivalent to desktop computers. PLC-like programming combined with remote I/O hardware, allows a general-purpose desktop computer to overlap some PLCs in certain applications. Desktop computer controllers have not been generally accepted in heavy industry because desktop computers run on less stable operating systems than PLCs, and because the desktop computer hardware is typically not designed to the same levels of tolerance to temperature, humidity, vibration, and longevity as the processors used in PLCs. Operating systems such as Windows do not lend themselves to deterministic logic execution, with the result that the controller may not always respond to changes of input status with the consistency in timing expected from PLCs. Desktop logic applications find use in less critical situations, such as laboratory automation and use in small facilities where the application is less demanding and critical.

Basic functions
The most basic function of a programmable controller is to emulate the functions of electromechanical relays. Discrete inputs are given a unique address, and a PLC instruction can test if the input state is on or off. Just as a series of relay contacts perform a logical AND function, not allowing current to pass unless all the contacts are closed, so a series of "examine if on" instructions will energize its output storage bit if all the input bits are on. Similarly, a parallel set of instructions will perform a logical OR. In an electromechanical relay wiring diagram, a group of contacts controlling one coil is called a "rung" of a "ladder diagram ", and this concept is also used to describe PLC logic.  Some models of PLC limit the number of series and parallel instructions in one "rung" of logic. The output of each rung sets or clears a storage bit, which may be associated with a physical output address or which may be an "internal coil" with no physical connection. Such internal coils can be used, for example, as a common element in multiple separate rungs.  Unlike physical relays, there is usually no limit to the number of times an input, output or internal coil can be referenced in a PLC program.
Some PLCs enforce a strict left-to-right, top-to-bottom execution order for evaluating the rung logic.  This is different from electro-mechanical relay contacts, which, in a sufficiently complex circuit, may either pass current left-to-right or right-to-left, depending on the configuration of surrounding contacts.  The elimination of these "sneak paths" is either a bug or a feature, depending on the programming style.
More advanced instructions of the PLC may be implemented as functional blocks, which carry out some operation when enabled by a logical input and which produce outputs to signal, for example, completion or errors, while manipulating variables internally that may not correspond to discrete logic.

Communication
PLCs use built-in ports, such as USB, Ethernet, RS-232, RS-485, or RS-422 to communicate with external devices (sensors, actuators) and systems (programming software, SCADA, HMI). Communication is carried over various industrial network protocols, like Modbus, or EtherNet/IP. Many of these protocols are vendor specific.
PLCs used in larger I/O systems may have peer-to-peer (P2P) communication between processors. This allows separate parts of a complex process to have individual control while allowing the subsystems to co-ordinate over the communication link. These communication links are also often used for HMI devices such as keypads or PC-type workstations.
Formerly, some manufacturers offered dedicated communication modules as an add-on function where the processor had no network connection built-in.

User interface
PLCs may need to interact with people for the purpose of configuration, alarm reporting, or everyday control. A human-machine interface (HMI) is employed for this purpose. HMIs are also referred to as man-machine interfaces (MMIs) and graphical user interfaces (GUIs). A simple system may use buttons and lights to interact with the user. Text displays are available as well as graphical touch screens. More complex systems use programming and monitoring software installed on a computer, with the PLC connected via a communication interface.

Process of a scan cycle
A PLC works in a program scan cycle, where it executes its program repeatedly. The simplest scan cycle consists of 3 steps:

Read inputs.
Execute the program.
Write outputs.The program follows the sequence of instructions. It typically takes a time span of tens of milliseconds for the processor to evaluate all the instructions and update the status of all outputs. If the system contains remote I/O—for example, an external rack with I/O modules—then that introduces additional uncertainty in the response time of the PLC system.As PLCs became more advanced, methods were developed to change the sequence of ladder execution, and subroutines were implemented.Special-purpose I/O modules may be used where the scan time of the PLC is too long to allow predictable performance. Precision timing modules, or counter modules for use with shaft encoders, are used where the scan time would be too long to reliably count pulses or detect the sense of rotation of an encoder. This allows even a relatively slow PLC to still interpret the counted values to control a machine, as the accumulation of pulses is done by a dedicated module that is unaffected by the speed of program execution.

Security
In his book from 1998, E. A. Parr pointed out that even though most programmable controllers require physical keys and passwords, the lack of strict access control and version control systems, as well as an easy-to-understand programming language make it likely that unauthorized changes to programs will happen and remain unnoticed.Prior to the discovery of the Stuxnet computer worm in June 2010, the security of PLCs received little attention. Modern programmable controllers generally contain a real-time operating systems, which can be vulnerable to exploits in a similar way as desktop operating systems, like Microsoft Windows. PLCs can also be attacked by gaining control of a computer they communicate with. Since 2011, these concerns have grown as networking is becoming more commonplace in the PLC environment connecting the previously separate plant floor networks and office networks.In February 2021, Rockwell Automation publicly disclosed a critical vulnerability affecting its Logix controllers family. Secret cryptographic key used to verify communication between the PLC and workstation can be extracted from Studio 5000 Logix Designer programming software and used to remotely change program code and configuration of connected controller. The vulnerability was given a severity score of 10 out of 10 on the CVSS vulnerability scale. At the time of writing, the mitigation of the vulnerability was to limit network access to affected devices.

Safety PLCs
Safety PLCs can be either a standalone model or a safety-rated hardware and functionality added to existing controller architectures (Allen-Bradley Guardlogix, Siemens F-series etc.). These differ from conventional PLC types by being suitable for safety-critical applications for which PLCs have traditionally been supplemented with hard-wired safety relays and areas of the memory dedicated to the safety instructions. The standard of safety level is the SIL. 
A safety PLC might be used to control access to a robot cell with trapped-key access, or to manage the shutdown response to an emergency stop on a conveyor production line. Such PLCs typically have a restricted regular instruction set augmented with safety-specific instructions designed to interface with emergency stops, light screens, and so forth. 
The flexibility that such systems offer has resulted in rapid growth of demand for these controllers.

PLC compared with other control systems
PLCs are well adapted to a range of automation tasks. These are typically industrial processes in manufacturing where the cost of developing and maintaining the automation system is high relative to the total cost of the automation, and where changes to the system would be expected during its operational life. PLCs contain input and output devices compatible with industrial pilot devices and controls; little electrical design is required, and the design problem centers on expressing the desired sequence of operations. PLC applications are typically highly customized systems, so the cost of a packaged PLC is low compared to the cost of a specific custom-built controller design. On the other hand, in the case of mass-produced goods, customized control systems are economical. This is due to the lower cost of the components, which can be optimally chosen instead of a "generic" solution, and where the non-recurring engineering charges are spread over thousands or millions of units.Programmable controllers are widely used in motion, positioning, or torque control. Some manufacturers produce motion control units to be integrated with PLC so that G-code (involving a CNC machine) can be used to instruct machine movements.

PLC Chip / Embedded Controller
For small machines with low or medium volume. PLCs that can execute PLC languages such as Ladder, Flow-Chart/Grafcet,... Similar to traditional PLCs, but their small size allows developers to design them into custom printed circuit boards like a microcontroller, without computer programming knowledge, but with a language that is easy to use, modify and maintain. It is between the classic PLC / Micro-PLC and the Microcontrollers.

Microcontrollers
A microcontroller-based design would be appropriate where hundreds or thousands of units will be produced and so the development cost (design of power supplies, input/output hardware, and necessary testing and certification) can be spread over many sales, and where the end-user would not need to alter the control. Automotive applications are an example; millions of units are built each year, and very few end-users alter the programming of these controllers. However, some specialty vehicles such as transit buses economically use PLCs instead of custom-designed controls, because the volumes are low and the development cost would be uneconomical.

Single-board computers
Very complex process control, such as those used in the chemical industry, may require algorithms and performance beyond the capability of even high-performance PLCs. Very high-speed or precision controls may also require customized solutions; for example, aircraft flight controls. Single-board computers using semi-customized or fully proprietary hardware may be chosen for very demanding control applications where the high development and maintenance cost can be supported. "Soft PLCs" running on desktop-type computers can interface with industrial I/O hardware while executing programs within a version of commercial operating systems adapted for process control needs.The rising popularity of single board computers has also had an influence on the development of PLCs. Traditional PLCs are generally closed platforms, but some newer PLCs (e.g. groov EPIC from Opto 22, ctrlX from Bosch Rexroth, PFC200 from Wago, PLCnext from Phoenix Contact, and Revolution Pi from Kunbus) provide the features of traditional PLCs on an open platform.

Programmable logic relays (PLR)
In more recent years, small products called programmable logic relays (PLRs) or smart relays, have become more common and accepted. These are similar to PLCs and are used in light industries where only a few points of I/O are needed, and low cost is desired. These small devices are typically made in a common physical size and shape by several manufacturers and branded by the makers of larger PLCs to fill their low-end product range. Most of these have 8 to 12 discrete inputs, 4 to 8 discrete outputs, and up to 2 analog inputs. Most such devices include a tiny postage-stamp-sized LCD screen for viewing simplified ladder logic (only a very small portion of the program being visible at a given time) and status of I/O points, and typically these screens are accompanied by a 4-way rocker push-button plus four more separate push-buttons, similar to the key buttons on a VCR remote control, and used to navigate and edit the logic. Most have a small plug for connecting via RS-232 or RS-485 to a personal computer so that programmers can use simple applications in general-purpose OS like MS Windows, macOS or Linux, that have user-friendly (G)UIs, for programming instead of being forced to use the tiny LCD and push-button set for this purpose. Unlike regular PLCs that are usually modular and greatly expandable, the PLRs are usually not modular or expandable, but their price can be two orders of magnitude less than a PLC, and they still offer robust design and deterministic execution of the logic.
A variant of PLCs, used in remote locations is the remote terminal unit or RTU. An RTU is typically a low power, ruggedized PLC whose key function is to manage the communications links between the site and the central control system (typically SCADA) or in some modern systems, "The Cloud". Unlike factory automation using high-speed Ethernet, communications links to remote sites are often radio-based and are less reliable. To account for the reduced reliability, RTU will buffer messages or switch to alternate communications paths. When buffering messages, the RTU will timestamp each message so that a full history of site events can be reconstructed. RTUs, being PLCs, have a wide range of I/O and are fully programmable, typically with languages from the IEC 61131-3 standard that is common to many PLCs, RTUs and DCSs. In remote locations, it is common to use an RTU as a gateway for a PLC, where the PLC is performing all site control and the RTU is managing communications, time-stamping events and monitoring ancillary equipment. On sites with only a handful of I/O, the RTU may also be the site PLC and will perform both communications and control functions.

See also
1-bit computing
Industrial control system
Industrial safety system
PLC technician

References
Bibliography
Further reading

Daniel Kandray, Programmable Automation Technologies, Industrial Press, 2010 ISBN 978-0-8311-3346-7, Chapter 8 Introduction to Programmable Logic Controllers
Walker, Mark John (2012-09-08). The Programmable Logic Controller: its prehistory, emergence and application (PDF) (PhD thesis). Department of Communication and Systems Faculty of Mathematics, Computing and Technology: The Open University. Archived (PDF) from the original on 2018-06-20. Retrieved 2018-06-20.

Public finance

Public finance is the study of the role of the government in the economy. It is the branch of economics that assesses the government revenue and government expenditure of the public authorities and the adjustment of one or the other to achieve desirable effects and avoid undesirable ones. 
The purview of public finance is considered to be threefold, consisting of governmental effects on:
The efficient allocation of available resources;
The distribution of income among citizens; and
The stability of the economy.Economist Jonathan Gruber has put forth a framework to assess the broad field of public finance. Gruber suggests public finance should be thought of in terms of four central questions: 

When should the government intervene in the economy? To which there are two central motivations for government intervention, Market failure and redistribution of income and wealth.
How might the government intervene? Once the decision is made to intervene the government must choose the specific tool or policy choice to carry out the intervention (for example public provision, taxation, or subsidization).
What is the effect of those interventions on economic outcomes? A question to assess the empirical direct and indirect effects of specific government intervention.
And finally, why do governments choose to intervene in the way that they do? This question is centrally concerned with the study of political economy, theorizing how governments make public policy.

Overview
One of the more traditional subfields of economics, public finance emphasizes the function and role of government in the economy. A region's inhabitants established a formal or informal entity known as the government to carry out a variety of tasks, including providing for social requirements like education and healthcare as well as protecting the populace's private property from outside threats.
The proper role of government provides a starting point for the analysis of public finance. In theory, under certain circumstances, private markets will allocate goods and services among individuals efficiently (in the sense that no waste occurs and that individual tastes are matching with the economy's productive abilities). If private markets were able to provide efficient outcomes and if the distribution of income were socially acceptable, then there would be little or no scope for government. In many cases, however, conditions for private market efficiency are violated. For example, if many people can enjoy the same good (the moment that good was produced and sold, it starts to give its utility to every one for free) at the same time (non-rival, non-excludable consumption), then private markets may supply too little of that good. National defense is one example of non-rival consumption, or of a public good."Market failure" occurs when private markets do not allocate goods or services efficiently. The existence of market failure provides an efficiency-based rationale for collective or governmental provision of goods and services. Externalities, public goods, informational advantages, strong economies of scale, and network effects can cause market failures. Public provision via a government or a voluntary association, however, is subject to other inefficiencies, termed "government failure."
Under broad assumptions, government decisions about the efficient scope and level of activities can be efficiently separated from decisions about the design of taxation systems (Diamond-Mirrlees separation). In this view, public sector programs should be designed to maximize social benefits minus costs (cost-benefit analysis), and then revenues needed to pay for those expenditures should be raised through a taxation system that creates the fewest efficiency losses caused by distortion of economic activity as possible. In practice, government budgeting or public budgeting is substantially more complicated and often results in inefficient practices. 
Government can pay for spending by borrowing (for example, with government bonds), although borrowing is a method of distributing tax burdens through time rather than a replacement for taxes. A deficit is the difference between government spending and revenues. The accumulation of deficits over time is the total public debt. Deficit finance allows governments to smooth tax burdens over time and gives governments an important fiscal policy tool. Deficits can also narrow the options of successor governments. There is also a difference between public and private finance, in public finance the source of income is indirect, e.g., various taxes (specific taxes, value added taxes), but in private finance sources of income is direct.

Public finance management
Collection of sufficient resources from the economy in an appropriate manner along with allocating and use of these resources efficiently and effectively constitute good financial management. Resource generation, resource allocation, and expenditure management (resource utilization) are the essential components of a public financial management system.
The following subdivisions form the subject matter of public finance.

Public expenditure
Public revenue
Public debt
Financial administration
Federal finance
fiscal policy

Government expenditures
Economists classify government expenditures into three main types. Government purchases of goods and services for current use are classed as government consumption. Government purchases of goods and services intended to create future benefits – such as infrastructure investment or research spending – are classed as government investment. Government expenditures that are not purchases of goods and services, and instead just represent transfers of money – such as social security payments – are called transfer payments.

Government operations
Government operations are those activities involved in the running of a state or a functional equivalent of a state (for example, tribes, secessionist movements or revolutionary movements) for the purpose of producing value for the citizens.  Government operations have the power to make, and the authority to enforce rules and laws within a civil, corporate, religious, academic, or other organization or group.

Income distribution
Income distribution – Some forms of government expenditure are specifically intended to transfer income from some groups to others. For example, governments sometimes transfer income to people that have suffered a loss due to natural disaster. Likewise, public pension programs transfer wealth from the young to the old. Other forms of government expenditure that represent purchases of goods and services also have the effect of changing the income distribution. For example, engaging in a war may transfer wealth to certain sectors of society. Public education transfers wealth to families with children in these schools. Public road construction transfers wealth from people that do not use the roads to those people that do (and to those that build the roads).
Income Security
Employment insurance
Health Care
Public financing of campaigns

Financing of government expenditures
Government expenditures are financed primarily in three ways:

Government revenue
Taxes
Non-tax revenue (revenue from government-owned corporations, sovereign wealth funds, sales of assets, or seigniorage)
Government borrowing
Money creationHow a government chooses to finance its activities can have important effects on the distribution of income and wealth (income redistribution) and on the efficiency of markets (effect of taxes on market prices and efficiency). The issue of how taxes affect income distribution is closely related to tax incidence, which examines the distribution of tax burdens after market adjustments are taken into account.  Public finance research also analyzes effects of the various types of taxes and types of borrowing as well as administrative concerns, such as tax enforcement.

Taxes
Taxation is the central part of modern public finance. Its significance arises not only from the fact that it is by far the most important of all revenues but also because of the gravity of the problems created by the present day tax burden. The main objective of taxation is raising revenue. A high level of taxation is necessary in a welfare State to fulfill its obligations. Taxation is used as an instrument of attaining certain social objectives, i.e., as a means of redistribution of wealth and thereby reducing inequalities. Taxation in a modern government is thus needed not merely to raise the revenue required to meet its expenditure on administration and social services, but also to reduce the inequalities of income and wealth. Taxation might also be needed to draw away money that would otherwise go into consumption and cause inflation to rise.A tax is a financial charge or other levy imposed on an individual or a legal entity by a state or a functional equivalent of a state (for example, tribes, secessionist movements or revolutionary movements). Taxes could also be imposed by a subnational entity. Taxes consist of direct tax or indirect tax, and may be paid in money or as corvée labor. A tax may be defined as a "pecuniary burden laid upon individuals or property to support the government [ . . .] a payment exacted by legislative authority."  A tax "is not a voluntary payment or donation, but an enforced contribution, exacted pursuant to legislative authority" and is "any contribution imposed by government [ . . .] whether under the name of toll, tribute, tallage, gabel, impost, duty, custom, excise, subsidy, aid, supply, or other name."
There are various types of taxes, broadly divided into two heads – direct (which is proportional) and indirect tax (which is differential in nature):
Stamp duty, levied on documents
Excise tax (tax levied on production for sale, or sale, of a certain good)
Sales tax (tax on business transactions, especially the sale of goods and services)
Value added tax (VAT) is a type of sales tax
Services taxes on specific services
Road tax; Vehicle excise duty (UK), Registration Fee (US), Regco  (Australia), Vehicle Licensing Fee (Brazil) etc.
Gift tax
Duties (taxes on importation, levied at customs)
Corporate income tax on corporations (incorporated entities)
Wealth tax
Personal income tax (may be levied on individuals, families such as the Hindu joint family in India, unincorporated associations, etc.)

Debt
Governments, like any other legal entity, can take out loans, issue bonds, and make financial investments. Government debt (also known as public debt or national debt) is money (or credit) owed by any level of government; either central or federal government, municipal government, or local government. Some local governments issue bonds based on their taxing authority, such as tax increment bonds or revenue bonds.
As the government represents the people, government debt can be seen as an indirect debt of the taxpayers. Government debt can be categorized as internal debt, owed to lenders within the country, and external debt, owed to foreign lenders. Governments usually borrow by issuing securities such as government bonds and bills. Less creditworthy countries sometimes borrow directly from commercial banks or international institutions such as the International Monetary Fund or the World Bank.
Most government budgets are calculated on a cash basis, meaning that revenues are recognized when collected and outlays are recognized when paid. Some consider all government liabilities, including future pension payments and payments for goods and services the government has contracted for but not yet paid, as government debt. This approach is called accrual accounting, meaning that obligations are recognized when they are acquired, or accrued, rather than when they are paid. This constitutes public debt.

Seigniorage
Seigniorage is the net revenue derived from the issuing of currency. It arises from the difference between the face value of a coin or banknote and the cost of producing, distributing and eventually retiring it from circulation. Seigniorage is an important source of revenue for some national banks, although it provides a very small proportion of revenue for advanced industrial countries.

Public finance through state enterprise
Public finance in centrally planned economies has differed in fundamental ways from that in market economies. Some state-owned enterprises generated profits that helped finance government activities.. In various mixed economies, the revenue generated by state-owned enterprises is used for various state endeavors; typically the revenue generated by state and government agencies.

Government finance statistics and methodology
Macroeconomic data to support public finance economics are generally referred to as fiscal or government finance statistics (GFS). The Government Finance Statistics Manual 2001 (GFSM 2001) is the internationally accepted methodology for compiling fiscal data.  It is consistent with regionally accepted methodologies such as the European System of Accounts 1995 and consistent with the methodology of the System of National Accounts (SNA1993) and broadly in line with its most recent update, the SNA2008.

Measuring the public sector
The size of governments, their institutional composition and complexity, their ability to carry out large and sophisticated operations, and their impact on the other sectors of the economy warrant a well-articulated system to measure government economic operations.
The GFSM 2001 addresses the institutional complexity of government by defining various levels of government. The main focus of the GFSM 2001 is the general government sector defined as the group of entities capable of implementing public policy through the provision of primarily non market  goods and services and the redistribution of income and wealth, with both activities supported mainly by compulsory levies on other sectors. The GFSM 2001 disaggregates the general government into subsectors: central government, state government, and local government (See Figure 1). The concept of general government does not include public corporations. The general government plus the public corporations comprise the public sector (See Figure 2).

The general government sector of a nation includes all non-private sector institutions, organisations and activities. The general government sector, by convention, includes all the public corporations that are not able to cover at least 50% of their costs by sales, and, therefore, are considered non-market producers.In the European System of Accounts, the sector "general government" has been defined as containing:

"All institutional units which are other non-market producers whose output is intended for individual and collective consumption, and mainly financed by compulsory payments made by units belonging to other sectors, and/or all institutional units principally engaged in the redistribution of national income and wealth".Therefore, the main functions of general government units are :

to organize or redirect the flows of money, goods and services, or other assets among corporations, among households, and between corporations and households; in the purpose of social justice, increased efficiency or other aims legitimized by the citizens -- examples are the redistribution of national income and wealth, the corporate income tax paid by companies to finance unemployment benefits, the social contributions paid by employees to finance the pension systems;
to produce goods and services to satisfy households' needs (e.g., state health care) or to collectively meet the needs of the whole community (e.g. defense, public order, and safety).The general government sector, in the European System of Accounts, has four sub-sectors:

central government
state government
local government
social security funds"Central government" consists of all administrative departments of the state and other central agencies whose responsibilities cover the whole economic territory of a country, except for the administration of social security funds.
"State government" is defined as the separate institutional units that exercise some government functions below those units at central government level and above those units at local government level, excluding the administration of social security funds.
"Local government" consists of all types of public administration whose responsibility covers only a local part of the economic territory, apart from local agencies of social security funds.
"Social security fund" is a central, state or local institutional unit whose main activity is to provide social benefits. It fulfils the two following criteria:

by law or regulation (except those about government employees), certain population groups must take part in the scheme and have to pay contributions;
general government is responsible for the management of the institutional unit, for the payment or approval of the level of the contributions and of the benefits, independent of its role as a supervisory body or employer.The GFSM 2001 framework is similar to the financial accounting of businesses. For example, it recommends that governments produce a full set of financial statements including the statement of government operations (akin to the income statement), the balance sheet, and a cash flow statement. Two other similarities between the GFSM 2001 and business financial accounting are the recommended use of accrual accounting as the basis of recording and the presentations of stocks of assets and liabilities at market value. It is an improvement on the prior methodology – Government Finance Statistics Manual 1986 – based on cash flows and without a balance sheet statement.

Users of GFS
The GFSM 2001 recommends standard tables including standard fiscal indicators that meet a broad group of users including policy makers, researchers, and investors in sovereign debt.
Government finance statistics should offer data for topics such as the fiscal architecture, the measurement of the efficiency and effectiveness of government expenditures, the economics of taxation, and the structure of public financing. The GFSM 2001 provides a blueprint for the compilation, recording, and presentation of revenues, expenditures, stocks of assets, and stocks of liabilities. The GFSM 2001 also defines some indicators of effectiveness in government's expenditures, for example the compensation of employees as a percentage of expense. The GFSM 2001 includes a functional classification of expense as defined by the Classification of Functions of Government (COFOG) .
This functional classification allows policy makers to analyze expenditures on categories such as health, education, social protection, and environmental protection.
The financial statements can provide investors with the necessary information to assess the capacity of a government to service and repay its debt, a key element determining sovereign risk, and risk premia. Like the risk of default of a private corporation, sovereign risk is a function of the level of debt, its ratio to liquid assets, revenues and expenditures, the expected growth and volatility of these revenues and expenditures, and the cost of servicing the debt. The government's financial statements contain the relevant information for this analysis.
The government's balance sheet presents the level of the debt; that is the government's liabilities. The memorandum items of the balance sheet provide additional information on the debt including its maturity and whether it is owed to domestic or external residents. The balance sheet also presents a disaggregated classification of financial and non-financial assets.
These data help estimate the resources a government can potentially access to repay its debt. The statement of operations  ("income statement") contains the revenue and expense accounts of the government. The revenue accounts are divided into subaccounts, including the different types of taxes, social contributions, dividends from the public sector, and royalties from natural resources. Finally, the interest expense account is one of the necessary inputs to estimate the cost of servicing the debt.

Fiscal data using the GFSM 2001 methodology
GFS can be accessible through several sources. The International Monetary Fund publishes GFS in two publications: International Financial Statistics and the Government Finance Statistics Yearbook. The World Bank gathers information on external debt. On a regional level, the Organization for Economic Co-operation and Development (Dibidami ) compiles general government account data for its members, and Eurostat, following a methodology compatible with the GFSM 2001, compiles GFS for the members of the European Union.

See also
Constitutional economics
Efficiency dividend
Fiscal incidence
Government budget
Henry George Theorem
Public economics
Public choice

Notes
References
Anthony B. Atkinson and Joseph E. Stiglitz (1980). Lectures in Public Economics, McGraw-Hill Economics Handbook Series
Alan S. Blinder, Robert M. Solow, et al. (1974).  The Economics of Public Finance, Brookings Institution. Table of Contents.
James M. Buchanan, ([1967] 1987). Public Finance in Democratic Process: Fiscal Institutions and Individual Choice, UNC Press.
_____ and Richard A. Musgrave (1999). Public Finance and Public Choice: Two Contrasting Visions of the State, MIT Press. Description and scrollable preview links.
Ferguson, E. James. The power of the purse: A history of American public finance, 1776-1790 (UNC Press Books, 1961).
Richard A. Musgrave, 1959. The Theory of Public Finance: A Study in Public Economy, McGraw-Hill. 1st-page reviews of J.M. Buchanan [1] & C.S. Shoup [2].
_____ (2008). "public finance", The New Palgrave Dictionary of Economics, 2nd Edition. Abstract.
_____ and Peggy B. Musgrave (1973). Public Finance in Theory and Practice, McGraw-Hill.
Richard A. Musgrave and Alan T. Peacock, ed. ([1958] 1994). Classics in the Theory of Public Finance, Palgrave Macmillan. Description and contents.
Edwin J. Perkins, American public finance and financial services, 1700-1815 (1994) pp 324–48. Complete text line free
Joseph E. Stiglitz (2000). Economics of the Public Sector, 3rd ed. Norton. Description.
Greene, Joshua E (2011). Public Finance: An International Perspective. Hackensack, New Jersey: World Scientific. p. 500. ISBN 978-981-4365-04-8.

External links

Taxation and Public Finance course at the Harris School of Public Policy Studies
IMF – Dissemination Standards Bulletin Board – Subscribing ... (see "fiscal sector")м
"Finance" . New International Encyclopedia. 1905.

Quick response manufacturing

Quick response manufacturing (QRM) is an approach to manufacturing which emphasizes the beneficial effect of reducing internal and external lead times.

Description
Shorter lead times improve quality, reduce cost and eliminate non-value-added waste within the organization while simultaneously increasing the organization's competitiveness and market share by serving customers better and faster.
The time-based framework of QRM accommodates strategic variability such as offering custom-engineered products while eliminating dysfunctional variability such as rework and changing due dates. For this reason, companies making products in low or varying volumes have used QRM as an alternative or to complement other strategies such as Lean Manufacturing, Total quality management, Six Sigma or Kaizen. However, the benefits of QRM are still mooted and contested by experts around. Many opposers of QRM criticize its approach being very "marketing-style" rather than academic or statistical.

History
Background
QRM is rooted in the concept of Time-based competition (TBC) pioneered by Japanese enterprises in the 1980s and first formulated by George Stalk Jr. in his 1988 article entitled Time – The Next Source of Competitive Advantage. Time-based competition is a broad-based competitive strategy emphasizing time as the major factor for achieving and maintaining a sustainable competitive advantage. It seeks to compress the time required to propose, develop, manufacture, market and deliver products.
QRM advocates a companywide focus on short lead times that include quick response to demand for existing products as well as new product and design changes. This combination has led to the implementation of QRM in many high-mix, low-volume companies.
Some argue that Quick Response Manufacturing differs from Quick Response (QR) methods used in the apparel industry and the fast fashion market.  QRM is a companywide management strategy applicable to a wide variety of businesses, whereas QR primarily stands for a specific business model in a particular industry. However, the important difference to note is that QR was a competitive industry initiative introduced in the US Textile Industry in 1984 as a means of improving efficiencies in manufacturing and supply chain processes and as such was one of the earliest pioneers of putting into practice time-based competition prior to Stalk's seminal article. Thus QR crossed the traditional boundaries of organization and was not limited to a single organizational efficiency improvement such as that advocated by proponents of QRM. In this respect the Textile Industry initiative was innovative and visionary in its application of QR techniques across the supply chain.

Development
The concept of Quick Response Manufacturing (QRM) was first developed in the late 1980s by Rajan Suri, at the time professor of Industrial and Systems Engineering at the University of Wisconsin-Madison. Combining growing academic research in Time-based Competition (TBC) with his own observations from various lead time reduction projects, Suri conceived QRM as a concept espousing a relentless emphasis on lead time reduction that has a long-term impact on every aspect of the company.In 1993, Suri, along with a few U.S. Midwest companies and academic colleagues at the University of Wisconsin-Madison, launched the Center for Quick Response Manufacturing, a consortium dedicated to the development and implementation of QRM principles in an industry setting. Proposed by Suri, the newly coined term "Quick Response Manufacturing" (QRM) signifies the new strategy.
QRM extends basic principles of time-based competition while including these new aspects:
Singular focus on lead time reduction
Focus on manufacturing enterprises
Clarification of the misunderstanding and misconceptions managers have about how to apply time-based strategies
Companywide approach reaching beyond shop floor to other areas such as office operations and the supply chain
Use of cellular organization structure throughout the business with more holistic and flexible cells
Inclusion of basic principles of systems dynamics to provide insight on how to best reorganize an enterprise to achieve quick response
New material planning and control approach (POLCA)
Specific QRM principles on how to rethink manufacturing process and equipment decisions
Novel performance measure
Focus on implementation and sustainability
Manufacturing Critical-path Time (MCT) metric to measure lead timesSuri's continued research into QRM through industry projects along with enthusiastic responses to various articles on lead time reduction issues led him to develop a comprehensive theory on implementing speed in a manufacturing company, covering all areas in the enterprise. He formulated his theory in the book Quick Response Manufacturing: A Companywide Approach to Reducing Lead Times (1998), providing a framework for the implementation of QRM in manufacturing companies.

QRM Strategies and Tools
Lead time as a management strategy
Traditionally, U.S. manufacturing firms have focused on scale and cost management strategies based on the division of labor practices formalized by Frederick Winslow Taylor and pioneered by Henry Ford.From the time-based perspective of QRM, the high degree of labor specialization and hierarchical department structures at purely cost-based organizations have these negative effects on lead times:
Products and product orders require long routes through numerous departments
Hierarchical communication structures involving various management levels require a significant amount of time to resolve even routine issues
Focus on efficiency and resource utilization encourages workers and managers to build backlogs, slowing the response to customer requests
Trying to minimize costly machine setups, managers and workers resort to running large batch sizes. Large batch sizes result in long run times, leaving other jobs waiting and increasing lead times
Making large product quantities to stock leads to high inventory, often prone to inventory obsolescence – when stored products have to be discarded because of market or engineering changes
Low skill levels lead to low quality and high levels of reworkAll these factors contribute to long lead times, ultimately resulting in waste throughout the enterprise such as excessive forecasting, planning, scheduling, expediting, work in progress (WIP), finished goods costs and obsolescence. These increase the overall costs and lower the organization's competitiveness.
QRM suggests that an enterprisewide focus on reducing lead times will result in improvements in both quality and cost. Eliminating the time-consuming – and often self-reinforcing – practices described above can lead to large cost savings while improving product quality and customer responsiveness. Hence, on a management level, QRM advocates a mindset change from cost-based to time-based thinking, making short lead times the yardstick for organizational success.

Manufacturing Critical-path Time (MCT)
QRM's strong focus on lead time reduction requires a comprehensive definition of lead time. To accomplish this, QRM introduces Manufacturing Critical-path Time (MCT). It is based on the standard critical path method; defined as the typical amount of calendar time from when a customer creates an order, until the first piece of that order is delivered to the customer.A metric designed to calculate waste and highlight opportunities for improvement, MCT gives an estimate of the time it takes to fulfill an order, quantifying the longest critical-path duration of order-fulfillment activities.

Organizational structure
QRM requires four fundamental structural changes to transform a company organized around cost-based management strategies to a time-based focus:
Functional to Cellular: Functional departments must be dissolved. In their place, QRM cells become the main organizational unit. QRM cells are more flexible and holistic in their implementation compared to other cell concepts, and can be applied outside the shop floor
Top-down Control to Team Ownership: Top-down control of processes by managers and supervisors in departments needs to be transformed to a decision-making structure in which QRM cells manage themselves and have ownership of the entire process within the cell
Specialized Workers to a Cross-trained Workforce: Workers need to be trained to perform multiple tasks
Efficiency/Utilization Goals to Lead Time Reduction: To support this new structure, companies must replace cost-based goals of efficiency and utilization with the overarching goal of lead time reduction

QRM Cell
The main building block of the QRM organization is the QRM cell. Extending the concept of cellular manufacturing, QRM cells are designed around a Focused Target Market Segment (FTMS) – a segment of the market in which shorter product lead times provide the company with maximum benefits. Resources in a cell are dedicated (only to be used for jobs in the cell), collocated (located in close proximity to each other) and multifunctional (cover different functions). QRM cells complete a sequence of operations ensuring that jobs leave the cell completed and do not need to return.The work organization in QRM cells is based on team ownership. Provided with a job and a completion deadline, teams can decide independently on how to complete the job. To ensure quick response to high-variety demand, workers in QRM cells need to go through cross training.The main performance measure for a QRM cell is lead time as defined by MCT. To measure MCT reduction, managers can use the QRM number, a metric designed to show management lead time trends for cells.

System Dynamics
In QRM, the product-focused cell structure has to be complemented by a thorough understanding of system dynamics in order to make better decisions to reduce lead times. Based on principles of system dynamics, QRM identifies high utilization of machines and labor as well as running large batch sizes as major obstacles to lead time reduction.

Create spare capacity
Many cost-based organizations aim for machines and labor to be utilized at close to 100% of capacity. QRM criticizes this approach as counterproductive to lead time reduction based on queuing theory, which shows that high utilization increases waiting times for products. In order to be able to handle high variability in demand and products, QRM advises companies to operate at 80 percent capacity on critical resources.

Optimize batch sizes
Common efficiency measures encourage production of parts in large batch sizes. From the QRM perspective, large batch sizes lead to long waiting times, high WIP and inventory, and ultimately long lead times. Long lead times in turn result in multiple forms of waste and increased cost as described above. Thus, QRM encourages enterprise to strive towards batch sizes that minimize lead times.

Enterprisewide Application
QRM emphasizes time-based thinking throughout the organization, creating a unified management strategy for the entire enterprise. Extending beyond traditional efforts to optimize shop floor operations, QRM applies time-based management principles to all other parts of the organization.

Office Operations
QRM identifies office operations such as quoting, engineering, scheduling and order processing as major contributors to lead times. To achieve short lead times in the office environment, QRM suggests implementing several changes according to the time-based approach described above.
The main requirement for reorganizing office operations in QRM is the formation of a Quick Response Office Cell (Q-ROC) around a Focus Target Market Segment (FTMS). In their focus on closed-loop, collocated, multifunctional, cross-trained teams, Q-ROCs are similar to QRM Cells. Q-ROCs, like QRM cells on the shop floor, break down functional departments and can complete jobs through multiple functional steps.

Material Planning
QRM criticizes commonly used material planning and scheduling systems such as Material Requirements Planning (MRP), Manufacturing resource planning (MRP II), and Enterprise resource planning (ERP) for not incorporating system dynamics in their analysis and not accounting for the cost of long lead times.QRM recommends simplifying existing MRP systems to a Higher Level MRP (HL/MRP) concerned with high-level planning and coordination of material and not with detailed scheduling of operations.

Production Control
To coordinate and control flow within the QRM structure of cells and HL/MRP, QRM utilizes POLCA (Paired-cell Overlapping Loops of Cards with Authorization). POLCA is a card-based shop floor control system, designed as the QRM alternative to Kanban.
POLCA differs from commonly used Kanban systems in the type of signal it sends to move jobs/material through the shop floor. POLCA constitutes a capacity signal, showing that a cell is ready to work on a new job, whereas Kanban systems rely on inventory signals designed to replenish a certain quantity of parts. For this reason, POLCA works well for low-volume and/or custom products. The first QRM shop floor control system was developed by PROPOS software. PROPOS software was also the first to develop a digital version of the POLCA card system. In March 2018 Rajan Suri published The Practitioner's Guide to POLCA: The Production Control System for High-Mix, Low-Volume and Custom Products] in which Suri describes a practical approach to POLCA to maximize production efficiency, reduce WIP (Work in Process) and prevent bottlenecks from forming. Suri also describes the use of PROPOS QRM software and digital POLCA, illustrated by a case at BOSCH Scharnieren. This Dutch manufacturer produces custom metal hinges and managed to greatly reduce lead times and optimize the production flow in their job shop using QRM and POLCA principles.

Supply Chain
QRM encourages companies to work with suppliers to reduce their MCT. Long supplier lead times can incur "hidden" costs such as high inventory, freight cost for rush shipments, unplanned engineering changes creating obsolete inventory, and reduced flexibility to respond to demand changes. QRM recommends that MCT be included as a significant factor in sourcing decisions.

New Product Introduction
QRM highlights strategic advantages of rapid New Product Introduction (NPI). Applying the MCT metric to the NPI process provides valuable information on the current NPI performance. Based on these findings, QRM encourages managers to rethink cost-based decisions in terms of their impact on the NPI MCT. For example, cost-based purchasing policies can result in long purchasing times for prototype materials, in turn delaying the NPI.

Implementation
QRM theory recommends following four common steps when implementing QRM:

Creating a QRM mindset
QRM implementation requires company personnel to embrace the strategy's time-based principles. In a first step, a team of management and employees trained in QRM principles should compile a list of wastes due to long MCT, creating awareness for the negative impact of long lead times on operations.
If the company decides to take action, QRM theory recommends the creation of an organizational framework for the implementation effort. In this framework, a high-level QRM Steering Committee oversees all QRM efforts, while a QRM Champion – an experienced employee with sound QRM training – is charged with driving and overseeing projects on a day-to-day basis.
With this structure in place, the Steering Committee can pick a set of products as the target for the first QRM project.

Changing of organizational structure
Following the general direction of the Steering Committee, a cross-functional planning team starts studying the project, including a detailed analysis of the MCT, product volumes, strategic needs and other factors. This analysis leads to the definition of the FTMS for the QRM project. Using QRM principles, the planning team designs a QRM cell for the FTMS.
With approval from management, an implementation team consisting of the people in the new cell and members of the planning team can start training activities, cross-training of operators and – if needed – relocation of equipment to launch the cell. After cell launching, the implementation team continues support for the new cell and measures MCT to monitor lead time changes.

Inclusion of system dynamics
During both design of the cell and its operation, the implementation team should reexamine policies on utilization to properly plan the loading of the cells and to maintain spare capacity.
Furthermore, cells teams should be encouraged to engage in a program of batch size reduction.

Enterprisewide expansion of QRM
After completing the initial project, the company needs to evaluate the results of these QRM efforts and publicize successes throughout the organization. Following the same pattern as described above, the company should identify additional FTMSs for other QRM projects and start the implementation process. As more cells are formed, restructuring of the MRP system and implementation of POLCA may become necessary.
To maximize benefits of a time-based management strategy, QRM projects should span across office operations, the shop floor and supply chain.

Practice
Quick Response Manufacturing is used by a variety of companies from different sectors worldwide. As an enterprisewide strategy, QRM has found applications in all areas of the company from shop floor to office operations to supply chain and beyond.  In the apparel industry, QRM has also become closely intertwined with the concepts of Fast fashion (Sweatshop) and Fast Fit, both of which are intended to reduce the timeframes typically associated with bringing catwalk style to the high street.
Many companies use QRM to address lead time issues in some parts of their organization or as an addition to existing continuous improvement efforts such as Lean, Six Sigma or others.
Another group of companies including Alexandria Extrusion, Omnipress, RenewAire and Phoenix Products have transformed their entire operation according to QRM principles making full use of QRM's enterprisewide reach.
In a 2008 article in Barron's magazine profiling the five companies most successful at boosting their sales and cash flow from among the 500 largest (by sales) publicly traded companies in the U.S. and Canada, Merrill Miller, chairman and CEO of National Oilwell Varco mentions improved manufacturing efficiencies based on QRM as a large part of NOV's growth.In recent years, QRM principles have also found applications in the healthcare and pharmaceutical sector.

Center for Quick Response Manufacturing
Founded in 1993 by Rajan Suri, along with a few U.S. Midwest companies and academic colleagues at the University of Wisconsin-Madison, the Center for Quick Response Manufacturing has been a driving force in the development and implementation of QRM.
Organized as a public-private consortium including faculty, students and company members, the Center has assisted more than 220 companies in applying QRM principles over the past 20 years.
The Center provides general information on QRM and hosts a variety of training events each year. Companies interested in implementing QRM can become members of the Center and take part in improvement projects conducted in cooperation with engineering students and university faculty.
Following the public-private partnership model, a new QRM Center at HAN University of Applied Sciences in Arnhem, Netherlands (founded 2010) is helping European companies implement QRM strategies.

References
Notes
Doherty, Jacqueline (12 May 2008). "Barron's 500". Barron's. New York.
Ericksen, Paul; Suri, Rajan; El-Jawhari, Bash'shar; Armstrong, Aaron (2005). "Filling the Gap". APICS Magazine. 15 (2): 27–31.
Hammond, Janice H.; Kelly, Maura G. (1990), Quick response in the apparel industry, Harvard Business School Note N9-690-038.
Krishnamurthy, Ananth; Suri, Rajan (2009). "Planning and implementing POLCA: a card-based control system for high variety or custom engineered products". Production Planning & Control. 20 (7): 596–610. doi:10.1080/09537280903034297. S2CID 109046909.
Stalk, George Jr. (1988). "Time – The Next Source of Competitive Advantage". Harvard Business Review. 66 (July/August): 41–51.
Suri, Rajan (1998a), Quick Response Manufacturing. A Companywide Approach to Reducing Lead Times, Productivity Press.
Suri, Rajan (1998b). "Don't Push or Pull - POLCA". APICS Magazine. 8 (11): 32–38.
Suri, Rajan (2010a), It's About Time. The Competitive Advantage of Quick Response Manufacturing, Productivity Press.
Suri, Rajan (2010b). "A Quick Response to Office Management". Industrial Management. 52 (1): 25–30.
Finken, Gerald; Krishnamurthy, Ananth (2010). "Quick Response Manufacturing: Taking The Pharmaceutical Industry Beyond Lean Six Sigma". Life Science Leader.

Book
T.C.E. Cheng, T.M. Choi (Eds.). Innovative Quick Response Programs in Logistics and Supply Chain Management, Springer, International Handbooks on Information Systems, 2010.

See also
Lean Manufacturing
Supply Chain Management
Business Process Reengineering

External links
Center for Quick Response Manufacturing
QRM Center Europe
QRM Danmark
Quick Response Enterprise France
QRM Center Brasil
axxelia Germany
Dr. Rajan Suri - Why QRM

Raw material

A raw material, also known as a feedstock, unprocessed material, or primary commodity, is a basic material that is used to produce goods, finished goods, energy, or intermediate materials that are feedstock for future finished products. As feedstock, the term connotes these materials are bottleneck assets and are required to produce other products.
The term raw material denotes materials in unprocessed or minimally processed states such as raw latex, crude oil, cotton, coal, raw biomass, iron ore, plastic, air, logs, and water. The term secondary raw material denotes waste material which has been recycled and injected back into use as productive material.

Raw material in supply chain
Supply chains typically begin with the acquisition or extraction of raw materials. For example, the European Commission notes that food supply chains commence in the agricultural phase of food production.A 2022 report on changes affecting international trade noted that improving sourcing of raw materials has become one of the main objectives of companies reconfiguring their supply chains.In a 2022 survey conducted by SAP, wherein 400 US-based leaders in logistics and supply chain were interviewed, 44% of respondents cited a lack of raw materials as a reason for their supply chain issues. Forecasting for 2023, 50% of respondents expect a reduced availability of raw materials in the US to drive supply chain disruptions.

Raw materials markets
Raw materials markets are affected by consumer behavior, supply chain uncertainty, manufacturing disruptions, and regulations, amongst other factors. This results in volatile raw materials markets that are difficult to optimize and manage. Companies can struggle when faced with raw material volatility due to a lack of understanding of market demands, poor or no visibility into the indirect supply chain, and the time lag of raw materials price changes.Volatility in the raw materials markets can also be driven by natural disasters and geopolitcal conflict. The COVID-19 pandemic disrupted the steel industry, and once demand rebounded, prices increased 250% in the US. The war in Ukraine caused the price of natural gas to increase by 50% in 2022.

Raw material processing
Ceramic
While pottery originated in many different points around the world, it is certain that it was brought to light mostly through the Neolithic Revolution. That is important because it was a way for the first agrarians to store and carry a surplus of supplies. While most jars and pots were fire-clay ceramics, Neolithic communities also created kilns that were able to fire such materials to remove most of the water to create very stable and hard materials. Without the presence of clay on the riverbanks of the Tigris and Euphrates in the Fertile Crescent, such kilns would have been impossible for people in the region to have produced. Using these kilns, the process of metallurgy was possible once the Bronze and Iron Ages came upon the people that lived there.

Metallic
Many raw metallic materials used in industrial purposes must first be processed into a usable state. Metallic ores are first processed through a combination of crushing, roasting, magnetic separation, flotation, and leaching to make them suitable for use in a foundry. Foundries then smelt the ore into usable metal that may be alloyed with other materials to improve certain properties. One metallic raw material that is commonly found across the world is iron, and combined with nickel, this material makes up over 35% of the material in the Earth's inner and outer core. The iron that was initially used as early as 4000 BC was called meteoric iron and was found on the surface of the Earth. This type of iron came from the meteorites that struck the Earth before humans appeared, and was in very limited supply. This type is unlike most of the iron in the Earth, as the iron in the Earth was much deeper than the humans of that time period were able to excavate. The nickel content of the meteoric iron made it not necessary to be heated up, and instead, it was hammered and shaped into tools and weapons.

Iron ore
Iron ore can be found in a multitude of forms and sources. The primary forms of iron ore today are Hematite and Magnetite. While iron ore can be found throughout the world, only the deposits in the order of millions of tonnes are processed for industrial purposes. The top five exporters of Iron ore are Australia, Brazil, South Africa, Canada, and Ukraine. One of the first sources of iron ore is bog iron. Bog iron takes the form of pea-sized nodules that are created under peat bogs at the base of mountains.

Conflicts of raw materials
Places with plentiful raw materials and little economic development often show a phenomenon known as "Dutch disease" or the "resource curse", which occurs when the economy of a country is mainly based upon its exports because of its method of governance. An example of this is the Democratic Republic of the Congo.

See also
References
Further reading
Elizabeth Kolbert, "Needful Things: The raw materials for the world we've built come at a cost" (largely based on Ed Conway, Material World: The Six Raw Materials That Shape Modern Civilization, Knopf, 2023; Vince Beiser, The World in a Grain; and Chip Colwell, So Much Stuff: How Humans Discovered Tools, Invented Meaning, and Made More of Everything, Chicago), The New Yorker, 30 October 2023, pp. 20–23. Kolbert mainly discusses the importance to modern civilization, and the finite sources of, six raw materials: high-purity quartz (needed to produce silicon chips), sand, iron,  copper, petroleum (which Conway lumps together with another fossil fuel, natural gas), and lithium. Kolbert summarizes archeologist Colwell's review of the evolution of technology, which has ended up giving the Global North a superabundance of "stuff," at an unsustainable cost to the world's environment and reserves of raw materials.
Karl Marx, Capital, Vol. 1, Part III, Chap. 7.

Reliability-centered maintenance

Reliability-centered maintenance (RCM) is a concept of maintenance planning to ensure that systems continue to do what their user require in their present operating context. Successful implementation of RCM will lead to increase in cost effectiveness, reliability, machine uptime, and a greater understanding of the level of risk that the organization is managing.

Context
It is generally used to achieve improvements in fields such as the establishment of safe minimum levels of maintenance, changes to operating procedures and strategies and the establishment of capital maintenance regimes and plans. Successful implementation of RCM will lead to increase in cost effectiveness, machine uptime, and a greater understanding of the level of risk that the organization is managing.
John Moubray characterized RCM as a process to establish the safe minimum levels of maintenance. This description echoed statements in the Nowlan and Heap report from United Airlines.
It is defined by the technical standard SAE JA1011, Evaluation Criteria for RCM Processes, which sets out the minimum criteria that any process should meet before it can be called RCM. This starts with the seven questions below, worked through in the order that they are listed:

1. What is the item supposed to do and its associated performance standards?
2. In what ways can it fail to provide the required functions?
3. What are the events that cause each failure?
4. What happens when each failure occurs?
5. In what way does each failure matter?
6. What systematic task can be performed proactively to prevent, or to diminish to a satisfactory degree, the consequences of the failure?
7. What must be done if a suitable preventive task cannot be found?Reliability centered maintenance is an engineering framework that enables the definition of a complete maintenance regimen. It regards maintenance as the means to maintain the functions a user may require of machinery in a defined operating context. As a discipline it enables machinery stakeholders to monitor, assess, predict and generally understand the working of their physical assets. This is embodied in the initial part of the RCM process which is to identify the operating context of the machinery, and write a Failure Mode Effects and Criticality Analysis (FMECA). The second part of the analysis is to apply the "RCM logic", which helps determine the appropriate maintenance tasks for the identified failure modes in the FMECA. Once the logic is complete for all elements in the FMECA, the resulting list of maintenance is "packaged", so that the periodicities of the tasks are rationalised to be called up in work packages; it is important not to destroy the applicability of maintenance in this phase. Lastly, RCM is kept live throughout the "in-service" life of machinery, where the effectiveness of the maintenance is kept under constant review and adjusted in light of the experience gained.
RCM can be used to create a cost-effective maintenance strategy to address dominant causes of equipment failure. It is a systematic approach to defining a routine maintenance program composed of cost-effective tasks that preserve important functions.
The important functions (of a piece of equipment) to preserve with routine maintenance are identified, their dominant failure modes and causes determined and the consequences of failure ascertained. Levels of criticality are assigned to the consequences of failure. Some functions are not critical and are left to "run to failure" while other functions must be preserved at all cost. Maintenance tasks are selected that address the dominant failure causes. This process directly addresses maintenance preventable failures. Failures caused by unlikely events, non-predictable acts of nature, etc. will usually receive no action provided their risk (combination of severity and frequency) is trivial (or at least tolerable). When the risk of such failures is very high, RCM encourages (and sometimes mandates) the user to consider changing something which will reduce the risk to a tolerable level.
The result is a maintenance program that focuses scarce economic resources on those items that would cause the most disruption if they were to fail.
RCM emphasizes the use of predictive maintenance (PdM) techniques in addition to traditional preventive measures.

Background
The term "reliability-centered maintenance" authored by Tom Matteson, Stanley Nowlan and Howard Heap of United Airlines (UAL) to describe a process used to determine the optimum maintenance requirements for aircraft (having left United Airlines to pursue a consulting career a few months before the publication of the final Nowlan-Heap report, Matteson received no authorial credit for the work). The US Department of Defense (DOD) sponsored the authoring of both a textbook (by UAL) and an evaluation report (by Rand Corporation) on Reliability-Centered Maintenance, both published in 1978. They brought RCM concepts to the attention of a wider audience.
The first generation of jet aircraft had a crash rate that would be considered highly alarming today, and both the Federal Aviation Administration (FAA) and the airlines' senior management felt strong pressure to improve matters. In the early 1960s, with FAA approval the airlines began to conduct a series of intensive engineering studies on in-service aircraft. The studies proved that the fundamental assumption of design engineers and maintenance planners—that every aircraft and every major component thereof (such as its engines) had a specific "lifetime" of reliable service, after which it had to be replaced (or overhauled) in order to prevent failures—was wrong in nearly every specific example in a complex modern jet airliner.
This was one of many astounding discoveries that have revolutionized the managerial discipline of physical asset management and have been at the base of many developments since this seminal work was published. Among some of the paradigm shifts inspired by RCM were:

an understanding that the vast majority of failures are not necessarily linked to the age of the asset
changing from efforts to predict life expectancies to trying to manage the process of failure
an understanding of the difference between the requirements of assets from a user perspective, and the design reliability of the asset
an understanding of the importance of managing assets on condition (often referred to as condition monitoring, condition based maintenance and predictive maintenance)
an understanding of four basic routine maintenance tasks
linking levels of tolerable risk to maintenance strategy developmentLater RCM was defined in the standard SAE JA1011, Evaluation Criteria for Reliability-Centered Maintenance (RCM) Processes. This sets out the minimum criteria for what is, and for what is not, able to be defined as RCM. The standard is a watershed event in the ongoing evolution of the discipline of physical asset management. Prior to the development of the standard many processes were labeled as RCM even though they were not true to the intentions and the principles in the original report that defined the term publicly.

Basic features
The RCM process described in the DOD/UAL report recognized three principal risks from equipment failures: threats

to safety,
to operations, and
to the maintenance budget.Modern RCM gives threats to the environment a separate classification, though most forms manage them in the same way as threats to safety.
RCM offers five principal options among the risk management strategies:

Predictive maintenance tasks,
Preventive Restoration or Preventive Replacement maintenance tasks,
Detective maintenance tasks,
Run-to-Failure, and
One-time changes to the "system" (changes to hardware design, to operations, or to other things).RCM also offers specific criteria to use when selecting a risk management strategy for a system that presents a specific risk when it fails. Some are technical in nature (can the proposed task detect the condition it needs to detect? does the equipment actually wear out, with use?). Others are goal-oriented (is it reasonably likely that the proposed task-and-task-frequency will reduce the risk to a tolerable level?). The criteria are often presented in the form of a decision-logic diagram, though this is not intrinsic to the nature of the process.

In use
After being created by the commercial aviation industry, RCM was adopted by the U.S. military (beginning in the mid-1970s) and by the U.S. commercial nuclear power industry (in the 1980s).
Starting in the late 1980s, an independent initiative led by John Moubray corrected some early flaws in the process, and adapted it for use in the wider industry. Moubray was also responsible for popularizing the method and for introducing it to much of the industrial community outside of the aviation industry. In the two decades since this approach (called by the author RCM2) was first released, industry has undergone massive change with advances in lean thinking and efficiency methods. At this point in time many methods sprung up that took an approach of reducing the rigour of the RCM approach. The result was the propagation of methods that called themselves RCM, yet had little in common with the original concepts. In some cases these were misleading and inefficient, while in other cases they were even dangerous. Since each initiative is sponsored by one or more consulting firms eager to help clients use it, there is still considerable disagreement about their relative dangers (or merits).The RCM standard (SAE JA1011, available from http://www.sae.org) provides the minimum criteria that processes must comply with if they are to be called RCM.
Although a voluntary standard, it provides a reference for companies looking to implement RCM to ensure they are getting a process, software package or service that is in line with the original report.
The Walt Disney Company introduced RCM to its parks in 1997, led by Paul Pressler and consultants McKinsey & Company, laying off a large number of maintenance workers and saving large amounts of money. Some people blamed the new cost-conscious maintenance culture for some of the Incidents at Disneyland Resort that occurred in the following years.

See also
Maintenance (technical)
RAMS

Notes
References
"Nowlan, F. Stanley, and Howard F. Heap. Reliability-Centered Maintenance. Report Number AD-A066579". United States Department of Defense. 1978. Archived from the original on 1 August 2013.
MSG-3: Operator/Manufacturer Scheduled Maintenance Development (Vol. 1 – Fixed Wing Aircraft and Vol. 2 – Rotorcraft). Revision 2018.1, Airlines for America, 2018
SAE JA1011, Evaluation Criteria for Reliability-Centered Maintenance (RCM) Processes, Society of Automotive Engineers, 1 August 1998
SAE JA1012, A Guide to the Reliability-Centered Maintenance (RCM) Standard, Society of Automotive Engineers, 1 January 2002
"MIL-P-24534A, Military Specification: Planned Maintenance System, Development of Maintenance Requirement Cards, Maintenance Index Pages, and Associated Documentation" (PDF). Naval Sea Systems Command. 7 May 1985.
"MIL-STD-2173, Military Standard: Reliability-Centered Maintenance (RCM) Requirements for Naval Aircraft, Weapons Systems, and Support Equipment (S/S By MIL-HDBK-2173)". United States Department of Defense. 21 January 1986. Archived from the original (PDF) on 6 November 2013.
"MIL-STD-3034, Military Standard: MIL-STD-3034, DEPARTMENT OF DEFENSE STANDARD PRACTICE: RELIABILITY-CENTERED MAINTENANCE (RCM) PROCESS" (PDF). United States Department of Defense. 21 January 2011.
"NASA Reliability Centered Maintenance (RCM) Guide for Facilities and Collateral Equipment" (PDF). NASA. February 2000.
"NAVAIR 00-25-403, Guidelines for the Naval Aviation Reliability-Centered Maintenance (RCM) Process)" (PDF). Naval Air Systems Command. 1 July 2005.
"NAVAIR S9081-AB-GIB-010, Reliability-Centered Maintenance (RCM) Handbook)". Naval Sea Systems Command. 18 April 2007. Archived from the original (PDF) on 4 December 2013.
"TM 5-698-2, Technical Manual: Reliability-Centered Maintenance (RCM) for Command, Control, Communications, Computer, Intelligence, Surveillance, and Reconnaissance (C4ISR) Facilities" (PDF). United States Army. 6 October 2006.

Further reading
[1] Standard To Define RCM (Part 1), Dana Netherton, Maintenance Technology (1998)
[2] Standard To Define RCM (Part 2), Dana Netherton, Maintenance Technology (1998)
[3] Standard RCM Process Requirements, Jesús R, Sifonte, Conscious Reliability (2017)
[4] What about RCM-R®? How does it stand when compared with SAE JA1011?, Jesús R, Sifonte, Conscious Reliability (2017)
[5] Reliability Centered Maintenance: 9 Principles of a Modern Preventive Maintenance Program, Erik Hupje, Road to Reliability™ (2020)

Return on capital

Return on capital (ROC), or return on invested capital (ROIC), is a ratio used in finance, valuation and accounting, as a measure of the profitability and value-creating potential of companies relative to the amount of capital invested by shareholders and other debtholders. It indicates how effective a company is at turning capital into profits.  
The ratio is calculated by dividing the after tax operating income (NOPAT) by the average book-value of the invested capital (IC).

Return on invested capital formula
ROIC = NOPAT/Average Invested CapitalThere are three main components of this measurement that are worth noting:
While ratios such as return on equity and return on assets use net income as the numerator, ROIC uses net operating income after tax (NOPAT), which means that after-tax expenses (income) from financing activities are added back to (deducted from) net income.
While many financial computations use market value instead of book value (for instance, calculating debt-to-equity ratios or calculating the weights for the weighted average cost of capital (WACC)), ROIC uses book values of the invested capital as the denominator. This procedure is done because, unlike market values which reflect future expectations in efficient markets, book values more closely reflect the amount of initial capital invested to generate a return.
The denominator represents the average value of the invested capital rather than the value of the end of the year. This is because the NOPAT represents a sum of money flows, while the value of the invested capital changes every day (e.g., the invested capital on December 31 could be 30% lower than the invested capital on December 30). Because the exact average is difficult to calculate, it is often estimated by taking the average between the IC at the beginning of the year and the IC at the end of the year.Some practitioners make an additional adjustment to the formula to add depreciation, amortization, and depletion charges back to the numerator. These charges are considered by some to be "non-cash expenses" which are often included as part of operating expenses. The practice of adding these back is said to more closely reflect the cash return of a firm over a given period of time. However, others (such as Warren Buffett) argue that depreciation should not be excluded seeing that it represents a real cash outflow. When a company purchases a depreciating asset, the cost is not immediately expensed on the income statement. Instead, it is capitalized on the balance sheet as an asset. Over time, the depreciation expenses on the income statement will reduce the asset value on the balance sheet. In turn, depreciation represents the delayed expensing of the initial cash outflow that purchased the asset, and is thus a rather liberal accounting practice.

Relationship with WACC
Because financial theory states that the value of an investment is determined by both the amount of and risk of its expected cash flows to an investor, it is worth noting ROIC and its relationship to the weighted average cost of capital (WACC). 
The cost of capital is the return expected from investors for bearing the risk that the projected cash flows of an investment deviate from expectations. It is said that for investments in which future cash flows are incrementally less certain, rational investors require incrementally higher rates of return as compensation for bearing higher degrees of risk. In corporate finance, WACC is a common measurement of the minimum expected weighted average return of all investors in a company given the riskiness of its future cash flows.
Since return on invested capital is said to measure the ability of a firm to generate a return on its capital, and since WACC is said to measure the minimum expected return demanded by the firm's capital providers, the difference between ROIC and WACC is sometimes referred to as a firm's "excess return", or "economic profit".

See also


== References ==

Supply chain finance

Supply chain financing  (or reverse factoring) is a form of financial transaction wherein a third party facilitates an exchange by financing the supplier on the customer's behalf. The term also refers to the techniques and practices used by banks and other financial institutions to manage the capital invested into the supply chain and reduce risk for the parties involved.Unlike traditional factoring (where a supplier wants to finance its receivables), supply chain financing is initiated by the ordering party (the customer) in order to help its suppliers to finance its receivables more easily and at a lower interest rate than what would normally be offered. In 2011, the reverse factoring market was still very small, accounting for less than 3% of the factoring market.

Method
The reverse factoring method, still rare, is similar to the factoring insofar as it involves three actors: the ordering party (customer), the supplier, and the factor. Just as basic factoring, the aim of the process is to finance the supplier's receivables by a financier (the factor), so the supplier can cash in the money for what they sold immediately (minus an interest the factor deducts to finance the advance of money).
Contrary to the basic factoring, the initiative is not from the supplier that would have presented invoices to the factor to be paid earlier. This time, it is the ordering party (customer) that starts the process – usually a large company – choosing invoices that they will allow to be paid earlier by the factor. And then, the supplier will themselves choose which of these invoices he will need to be paid by the factor. It is therefore a collaborative project between the ordering party, the supplier and the factor.
Because it is the ordering party that starts the process, it is their liability that is engaged and therefore the interest applied for the deduction is less than the one the supplier would have been given had they done it on their own. The ordering party will then benefit of a part of the benefit realized by the factor, because they are the one to allow this. And the financier for their part will make their profit and create a durable relation with both the supplier and the ordering party.
Reverse factoring is an effective cash flow optimization tool for companies outsourcing a large volume of services (e.g. clinical research activities by Pharmaceutical companies). The benefit to both parties is that the company providing the services can get the outstanding value of their invoices paid in 10 days or less vs. the normal 30- to 45-day payment terms while the ordering party can delay the actual payment of the invoices (which are paid to the bank) by 120–180 days thus increasing cash flow. After the initial period of cash flow optimization, it is unclear if this will remain of value to the ordering party because you will then be paying monthly invoices of approximately equal amounts assuming your outsourced services are stable/average across the year/future periods. The cost of the "money" is a set interest rate normally tied to an index plus a bps adjustment.

The concept
To fully understand how the reverse factoring process works, one needs to be familiar with trade discounts and factoring. Indeed, reverse factoring could be considered as a combination of these two methods, taking advantages of both in order to redistribute the benefits to all three actors. In order to better understand the process, it is necessary to look at the 8 individual aspects of those three financing methods:

Historic
The concept of reverse factoring started with automobile constructors - including Fiat in the 1980s - who used this kind of financing process for its suppliers in order to realise a better margin. The principle then spread to the retail industry because of the interest it represents for a sector where payment delays are at the heart of every negotiation.
In the 1990s and the early 2000s, reverse factoring was not used extensively due to economic contexts that did not allow it to be an efficient way of financing.

Advantages
For the supplier
The supplier has its invoices paid earlier; therefore it can more easily manage its cashflow, and reduce by the way the costs of receivables management. Moreover, as it is the ordering party that puts its liability at stake, it benefits from a better interest rate on the trade discount than the one that would have been obtained by going directly to a factoring company.
The reverse factoring is very useful for small companies that have large groups for clients, because it creates a more durable business relation as the big company helps the smaller one, and doing so gets some extra money.  This opinion does not account for the poor relations caused by unilateral changes to credit terms.  Smaller companies are generally not given a choice to accept the additional cost of finance imposed by this process.
In a factoring process, if there is any problem concerning the payment of the invoice, it is the supplier that is liable, and has to give back the money he received. In the reverse factoring process, as it concerns validated invoices, as soon as the supplier receives the payment from the factor, the company is protected. The factor will have to get its money from the ordering party.
Finally, in a trade discount system, the supplier is forced to be paid cash, regardless of its cash flow. Some reverse factoring platforms identified this problem, and therefore propose to the suppliers a more collaborative financing method: they choose themselves the invoices they want to receive cash, the others will be paid at due date.

For the ordering party (the buyer)
The reverse factoring permits all the suppliers to be gathered in one financier, and that way to pay one company instead of many, which eases the invoicing management. The relation with the suppliers benefiting of the reverse factoring is improved because they benefit from better financing, and their payment delays are reduced; for its part, the ordering party will gain some extra money reversed by the factor and pay her invoices to the due date. Making suppliers benefit from such advantages can be a powerful leverage in negotiation, and also ensure a more durable relationship with the suppliers. Moreover, it ensures that the suppliers will be able to find advantaging financing in case of cash flow problem: using reverse factoring assures that the suppliers will still be in business, and are reliable.
With the reverse factoring, instead of paying numerous suppliers, most of the invoices are centralized with the same factor; it is always better for the accounts department to deal with one company to pay than several. This can also be simplified and speeded by using a reverse factoring platform combined with digitalization of business transactions (i.e. EDI).

For the factor (the financier)
By taking part in the reverse factoring process, the factor realizes a benefit by making available cash to the supplier and supporting in its place the delays of payment. However, in opposition to the factoring, in this situation the factor is in a more durable business relation as everyone benefits from it.
Other advantage for the financier, is that he works directly with big ordering parties; it means that instead of going after each and every supplier of that company, he can reach faster and more easily all of the suppliers and do business with them. Therefore, the risk is less important: it passes from a lot of fragmented risks to one unique and less important.

Globalization
With the supply chain lengthening as a result of globalization and offshore production, many companies have experienced a reduction of capital availability. In addition, the pressure faced by companies to improve cash flow has resulted in increased pressure on their overseas suppliers. Specifically, suppliers receive pressure in the form of extended payment terms or increased working capital imposed on them by large buyers. The general trend toward open account from letters of credit has further contributed to the problem.
As a result, there is a need for global supply chain finance (GSCF). The market opportunity for a GSCF is significant. The total worldwide market for receivables management is US$1.3 trillion. Payables discounting and asset-based lending add an additional US$100 billion and $340 billion, respectively . Only a small percentage of companies are currently using supply chain finance techniques, but more than half have plans or are investigating options to improve supply chain finance techniques. In fact the market for supply chain finance has grown by 35% in volume in 2020 compared to 2019 reaching US$1,311bn. The amount of funds in use as at the end of 2020 is estimated at US$505bn, an increase of 42%.While buyers are extending payment terms to their suppliers, the suppliers often have limited access to short-term financing and, therefore, a higher cost of money. This cost-shifting to suppliers results in a financially unstable and higher-risk supply base. Overall, the benchmark report showed that companies should be pursuing three key areas of improvement: GSCF financing; GSCF technology; and GSCF visibility.

Benefits of GSCF
The role of GSCF is to optimize both the availability and cost of capital within a given buyer-supplier supply chain. It does this by aggregating, packaging, and utilizing information generated during supply chain activities and matching this information with the physical control of goods. The coupling of information and physical control enables lenders to mitigate financial risk within the supply chain. The mitigation of risk allows more capital to be raised, capital to be accessed sooner or capital to be raised at lower rates.
The need to increase capital or inject capital into the supply chain more quickly is being caused by several factors:
1.) Market trends with respect to the global supply chain have caused companies to demand an integrated approach to physical and financial supply chain challenges: 
a.) Buyers are looking to optimize their balance sheet by delaying inventory ownership. 
b.) Suppliers are looking to obtain funds earlier in the supply chain at favorable rates, given buyers’ desire to delay inventory ownership. 
c.) middle-market companies are looking to monetize non-US domiciled inventory to increase liquidity. 
d.) There is wide interest in integrated supply chain finance methods.
2.) Globalization of the United States and Western Europe’s manufacturing bases has resulted in fewer domestic assets that can be leveraged to generate working capital.
3.) Most small and medium suppliers to US and European businesses are located in countries that lack well-developed capital markets. Without access to efficient and cost-effective capital, production costs increase significantly or the suppliers go out of business.
4.) Letters of credit, a long-standing method of obtaining capital for suppliers in less developed countries, are on the decline as large buyers are forcing suppliers to move to open account.
5.) There is a desire to ensure stability of capital as supply chains elongate. Another Asian financial crisis (such as the one in 1997) would severely disrupt US buyers’ supply chains by making capital unavailable to their suppliers.

An improvement of business relations
In the basic invoicing or factoring framework, there are always some risks that threaten the invoices:

inaccurate invoice (fraudulent, erroneous calculation, or typographical error)
an underestimated time of payment delay
an incorrect estimation on the object of the invoice (a service poorly executed)
etc.By using the reverse factoring, these risks are materially reduced.

Market size
Given the competitive nature of the GSCF market (approved payable finance) and due to the fact that business undertaken is covered by customer and bank confidentiality, sources of information regarding market size and players are constrained and not widely available in the public domain. As a result, indications on the market size are based mainly on estimates. The current, global market size for Supply Chain Finance is estimated at US$275 billion of annual traded volume, which translates in approximately $46 billion in outstandings with an average of 60 days payment terms. It is still relatively small compared to the market size of other invoice finance methods such as factoring, which remains the largest trade finance segment and is primarily domestic in focus. The potential market for Supply Chain Finance for the OECD (Organization for Economic Co-operation and Development) countries is significant and is estimated at $1.3 trillion in annual traded volume. The market serving European supply chains is approximately $600 billion. Based on these figures, the potential Supply Chain Finance market size for the US is estimated to be approximately $600 billion in traded volume per annum. A recent comprehensive research paper estimated that currently there are 200 GSCF programs of scale in place. These programs are run both domestically and cross-border and in multiple currencies. Still, the market potential is far from its capacity. If examining spending of large organizations, such as Lowe's $33 billion in spend, it becomes apparent that Supply Chain Finance programs usually require a multi-bank platform due to the credit and capital issues associated with banks.

Market growth
Market experts estimate that only 10% of the global available marketplace has been satisfied with Supply Chain Finance, revealing a large potential market for growth. The market is expected to continue to expand strongly in the coming years at a rate of approximately 20-30% per annum and 10% per annum by 2020.  The highest growth of supply chain finance programs currently originates from the US and Western Europe. Asia - India and China in particular, are considered the markets with most potential in the coming years.
The driving forces behind the rapid growth of supply chain finance programs are: 

Globalisation has increased the risk in supply chains and the impact on the financials of corporations.
Working Capital Management has risen at the top of the CFOs’ and Treasurers’ agendas.
Strong interest from suppliers regarding the provision of liquidity and enabling lower financing costs.

Further growth potential - Challenges
Although Supply Chain Finance is experiencing significant growth in demand, financial institutions are focused mainly on the large buyer side of the trade equation. As structured finance has been traditionally engineered and provided by banks specifically for large international trading companies, they do not use common foundations. In order for Supply Chain Finance to take off on a broad scale, a fresh impetus is needed. A “tipping point” could easily be reached by solving the following challenges. 

On-boarding of Supplier. In a Supplier Financing program, the servicer needs to on board the buyer’s trading partners - the suppliers. The multitude of such platforms generates operational issues for suppliers wishing to benefit from various Supply Chain Finance offerings via their buyers’ funders.
Know-Your-Customer (KYC). Most funders require KYC checks to be performed on suppliers being enlisted as new trading partners. This procedure not only increases the total processing cost, but it also puts the business case for all parties including the service provider, funder, buyer and ultimately the supplier at risk.
Available Capital and Liquidity. With 90% of liquidity in Supply Chain Finance programs provided by large, global commercial banks, there is a large amount of trade assets, which cannot be covered by such financial institutions. Further regulations such as Basel III might impact the risk appetite and funding capacity of banks and make it more attractive for non-bank funders to step in and support Supply Chain Finance facilities. Limited to large buyers. Today’s Supply Chain Finance offerings are mainly addressing the large buyers with sound credit ratings whereas the real Supply Chain Finance opportunity extends to large suppliers too, in particular in terms of payment assurance and risk mitigation.
Proprietary legal documentation.  Current Supply Chain Finance offerings use proprietary legal documentation, which makes the signing of non-standard agreements a costly, complex and time consuming process for corporate clients and their suppliers. Therefore, the market is currently facing challenges related to the absence of interoperability and legal standards.
Standardized product definitions. The naming and definitions of the various Supply Chain Finance methods vary between suppliers, which makes it difficult for corporations to compare offerings and consider switching from one provider to another.

The role of the GSCF “translator”
Physical and informational control are the keys to a GSCF. There is a need for logistics providers and financial services firms to join together to develop precise visibility tools that provide CFOs and global supply chain managers with the data they need and lenders with the collateral security required to provide capital. In fact, according to a November 2006 study conducted by the Aberdeen Group, large companies are four times more likely to be planning to spend over US$500,000 in supply chain finance technology over the next 18 months. Once a robust information-based system is established, trading partners, logistics companies, and banks need to be able to access the information quickly and efficiently.
The starting point for information about goods being transported must be the entity that is transporting the goods – the supply chain services provider, transportation company, and/or logistics partner. These are the entities that have the physical control of the goods while in the supply chain. Access to this information is a must from a demand planning perspective. Knowing where the goods are in transit, the financial services provider can more confidently extend financing at various milestones within the supply chain.
There is a critical role missing in this equation, however, and that is the supply chain finance “translator” – the entity that is experienced in both logistics/transportation and financial services. The translator is the subject matter expert, if you will, that can bring all entities to the table – transportation and logistics; banks; buyers; and sellers and speak the various languages and understand the needs of each party. In addition to participating in the financial transaction, the translator can help bridge the information divide between the physical and financial worlds, providing critical analysis about the information being collected from the supply chain.
The following explains this translator role:

Global supply-chain finance sets
Some of the products that could be sold under the banner of Global Supply Chain Financing include, but are not limited to: 1.) Global asset-based lending (GABL) – Enables middle market companies to monetize off-shore or in-transit inventory. This results in increased liquidity to this class of borrower, 2.) Inventory finance – Enables companies that supply to large buyers to secure financing on inventory that they are required by buyers to hold. This results in an improvement in the net cash conversion cycle for the buyer while providing the supplier with capital at a reduced rate. 3.) Receivables management services – Provides third-party outsourcing of receivables management and collections process. It also provides financing of those receivables and guarantees on the payment of those receivables. 4.) Payables discounting -Provides third-party outsourcing of the payables process and leverages a buyer’s credit quality to obtain favorable financing rates for suppliers. This results in lower cost of capital for the supplier, a portion of which can be passed on to the buyer. 5) Insurance
– Further mitigates trade risk through cargo, credit, and transaction dispute insurance.
Because of the complexities surrounding the sharing and transferring of data, the need to physically control the goods, and to maintain visibility throughout the fulfillment supply chain, transportation and logistics providers such as UPS [UPS Corporation] have unique capabilities to support and provide SCF services to global organizations due to their access to the shipping data and capabilities as a lender. In these unique situations, UPS as a translator can participate in the lending as well as collaborate with other lenders in helping to extract costs from the supply chain and ensure that the physical and financial supply chains are synchronized.
Traditionally, dynamic payables discounting, the early payment of trade payables in advance of the invoice due date, has been only related to invoices that are already approved. Given these discounted payments are paid post-goods receipt and approval, they don't carry any transaction risk which is common in cross border trade. Given the complexities of modern financing and payment techniques, invoicement including invoice automation and discount management initiatives need a framework to ensure that programs are approached on a strategic basis which bridges the supply chain, purchasing, accounts payable and finance organizations. Examples of providers are Misys TI Plus, TradeCard, Demica and Manhattan Associates.
More recently, there has been a pivot back to Receivables Financing Factoring (finance) programs, primarily driven by enhancements in technology which has increased the efficiency and attractiveness of Receivables-based programs. In a Supplier Financing model there is a heavy administrative burden caused by the need to negotiate Receivables Purchase Agreements with each individual Supplier, whereas in Factoring (finance) programs only one RPA is required with the Vendor. In addition, traditionally there was a larger effort involved in calculating the Funders credit risk exposure in a Receivables-based program due to the many Buyers which needed to be credit evaluated on an ongoing basis. Technology has now automated much of the credit evaluation process allowing program Funders access to real-time Buyer risk ratings across their portfolio. This transparency generates increased credit appetites from program Funders and Credit Insurers and benefits the Vendor through attractive rates and increased program credit limits on their Buyers. Vendors can also use the receivables-based approach to increase competitiveness by extending payment terms to their Buyers Off-balance-sheet. Examples of providers include Global Supply Chain Finance Ltd.

Structures
Supply Chain Finance practices have been in place for over a decade. Three distinctive Supply Chain Finance structures have crystallized.  

Buyer managed platforms. In this structure the buyer owns and runs the Supply ChainFinance platform. Some large retailers such as Carrefour or Metro Group are using this structure and managing the finance program, supplier onboarding, and liquidity themselves.
Bank proprietary platforms. The Supply Chain Finance structure is managed by large commercial banks providing the technology platform, services and funding. This structure is used by several large buying organizations such as Carlsberg, Boeing, Marks & Spencer and Procter & Gamble.
Multi-bank platforms.  The structure that has exhibited the strongest growth rate is represented by independent third party supply chain finance providers offering multi-bank platforms. This structure separates the entities, which manage the platform – a specialised service provider, from the funding partner, which provides liquidity and takes the credit risk. Based on the fact that funding in Supply Chain Finance is uncommitted, no bank can fund in every jurisdiction or currency and due to the general limitations in terms of credit risk appetite and funder concentration risk.
Market share. In terms of market share, programs are serviced and funded by a handful of players including large commercial banks. Together they manage over 40% of the market share. The rest of the Supply Chain Finance is serviced and funded by a variety of local banks and smaller, independent service providers.

Optimization of the process
Often the reverse factoring is used with the dematerialization to speed the process. As the whole goal of it is to make money available to the supplier as fast as possible, a lot of companies decide to dematerialize their invoices when they start a reverse factoring system, because that way it saves few more days, plus all the advantages of the dematerialization (less expensive, and benefic to the environment). In average, it can shorten the delays by 10 to 15 days.

Reverse factoring choice
The core principle of the reverse factoring is to have a way that benefits to all the actors. That way, it is necessary to have good relations with the other actors. The principal risk in reverse factoring is that the supplier gets trapped in a system where he cannot decide which invoices he need paid immediately or not, and therefore he becomes the victim of that system. Therefore, it is necessary to choose a collaborative platform that would permit the supplier to select which invoices they will be paid early, and when they will be paid.

See also
Factoring (finance)
Paperless office


== References ==

Semantic Scholar

Semantic Scholar is a research tool powered by artificial intelligence for scientific literature. It was developed at the Allen Institute for AI and publicly released in November 2015. It uses advances in natural language processing to provide summaries for scholarly papers. The Semantic Scholar team is actively researching the use of artificial intelligence in natural language processing, machine learning, human–computer interaction, and information retrieval.Semantic Scholar began as a database for the topics of computer science, geoscience, and neuroscience. In 2017, the system began including biomedical literature in its corpus. As of September 2022, it includes over 200 million publications from all fields of science.

Technology
Semantic Scholar provides a one-sentence summary of scientific literature. One of its aims was to address the challenge of reading numerous titles and lengthy abstracts on mobile devices. It also seeks to ensure that the three million scientific papers published yearly reach readers, since it is estimated that only half of this literature are ever read.Artificial intelligence is used to capture the essence of a paper, generating it through an "abstractive" technique. The project uses a combination of machine learning, natural language processing, and machine vision to add a layer of semantic analysis to the traditional methods of citation analysis, and to extract relevant figures, tables, entities, and venues from papers.Another key AI-powered feature is Research Feeds, an adaptive research recommender that uses AI to quickly learn what papers users care about reading and recommends the latest research to help scholars stay up to date. It uses a state-of-the-art paper embedding model trained using contrastive learning to find papers similar to those in each Library folder.Semantic Scholar also offers Semantic Reader, an augmented reader with the potential to revolutionize scientific reading by making it more accessible and richly contextual. Semantic Reader provides in-line citation cards that allow users to see citations with TLDR summaries as they read and skimming highlights that capture key points of a paper so users can digest faster.
In contrast with Google Scholar and PubMed, Semantic Scholar is designed to highlight the most important and influential elements of a paper. The AI technology is designed to identify hidden connections and links between research topics. Like the previously cited search engines, Semantic Scholar also exploits graph structures, which include the Microsoft Academic Knowledge Graph, Springer Nature's SciGraph, and the Semantic Scholar Corpus.Each paper hosted by Semantic Scholar is assigned a unique identifier called the Semantic Scholar Corpus ID (abbreviated S2CID). The following entry is an example:

Liu, Ying; Gayle, Albert A; Wilder-Smith, Annelies; Rocklöv, Joacim (March 2020). "The reproductive number of COVID-19 is higher compared to SARS coronavirus". Journal of Travel Medicine. 27 (2). doi:10.1093/jtm/taaa021. PMID 32052846. S2CID 211099356.
Semantic Scholar is free to use and unlike similar search engines (i.e. Google Scholar) does not search for material that is behind a paywall.One study compared the index scope of Semantic Scholar to Google Scholar, and found that for the papers cited by secondary studies in computer science, the two indices had comparable coverage, each only missing a handful of the papers.

Number of users and publications
As of January 2018, following a 2017 project that added biomedical papers and topic summaries, the Semantic Scholar corpus included more than 40 million papers from computer science and biomedicine. In March 2018, Doug Raymond, who developed machine learning initiatives for the Amazon Alexa platform, was hired to lead the Semantic Scholar project. As of August 2019, the number of included papers metadata (not the actual PDFs) had grown to more than 173 million after the addition of the Microsoft Academic Graph records. In 2020, a partnership between Semantic Scholar and the University of Chicago Press Journals made all articles published under the University of Chicago Press available in the Semantic Scholar corpus. At the end of 2020, Semantic Scholar had indexed 190 million papers.In 2020,  of Semantic Scholar reached seven million users per month.

See also
Citation analysis – Examination of the frequency, patterns, and graphs of citations in documents
Citation index – Index of citations between publications
Knowledge extraction – Creation of knowledge from structured and unstructured sources
List of academic databases and search engines
Scientometrics – Study of measuring and analysing science, technology and innovation

References
External links

Official website

SCADA

Supervisory control and data acquisition (SCADA) is a control system architecture comprising computers, networked data communications and graphical user interfaces for high-level supervision of machines and processes. It also covers sensors and other devices, such as programmable logic controllers, which interface with process plant or machinery.

Explanation
The operator interfaces which enable monitoring and the issuing of process commands, like controller set point changes, are handled through the SCADA computer system. The subordinated operations, e.g. the real-time control logic or controller calculations, are performed by networked modules connected to the field sensors and actuators.
The SCADA concept was developed to be a universal means of remote-access to a variety of local control modules, which could be from different manufacturers and allowing access through standard automation protocols. In practice, large SCADA systems have grown to become very similar to distributed control systems in function, while using multiple means of interfacing with the plant. They can control large-scale processes that can include multiple sites, and work over large distances as well as small distance. It is one of the most commonly-used types of industrial control systems, in spite of concerns about SCADA systems being vulnerable to cyberwarfare/cyberterrorism attacks.

Control operations
The key attribute of a SCADA system is its ability to perform a supervisory operation over a variety of other proprietary devices.
The accompanying diagram is a general model which shows functional manufacturing levels using computerised control.
Referring to the diagram,

Level 0 contains the field devices such as flow and temperature sensors, and final control elements, such as control valves.
Level 1 contains the industrialised input/output (I/O) modules, and their associated distributed electronic processors.
Level 2 contains the supervisory computers, which collate information from processor nodes on the system, and provide the operator control screens.
Level 3 is the production control level, which does not directly control the process, but is concerned with monitoring production and targets.
Level 4 is the production scheduling level.Level 1 contains the programmable logic controllers (PLCs) or remote terminal units (RTUs).
Level 2 contains the SCADA to readings and equipment status reports that are communicated to level 2 SCADA as required. Data is then compiled and formatted in such a way that a control room operator using the HMI (Human Machine Interface) can make supervisory decisions to adjust or override normal RTU (PLC) controls. Data may also be fed to a historian, often built on a commodity database management system, to allow trending and other analytical auditing.
SCADA systems typically use a tag database, which contains data elements called tags or points, which relate to specific instrumentation or actuators within the process system. Data is accumulated against these unique process control equipment tag references.

Examples of use
Both large and small systems can be built using the SCADA concept. These systems can range from just tens to thousands of control loops, depending on the application. Example processes include industrial, infrastructure, and facility-based processes, as described below:

Industrial processes include manufacturing, process control, power generation, fabrication, and refining, and may run in continuous, batch, repetitive, or discrete modes.
Infrastructure processes may be public or private, and include water treatment and distribution, wastewater collection and treatment, oil and gas pipelines, electric power transmission and distribution, and wind farms.
Facility processes, including buildings, airports, ships, and space stations. They monitor and control heating, ventilation, and air conditioning systems (HVAC), access, and energy consumption.However, SCADA systems may have security vulnerabilities, so the systems should be evaluated to identify risks and solutions implemented to mitigate those risks.

System components
A SCADA system usually consists of the following main elements:

Supervisory computers
This is the core of the SCADA system, gathering data on the process and sending control commands to the field connected devices. It refers to the computer and software responsible for communicating with the field connection controllers, which are RTUs and PLCs, and includes the HMI software running on operator workstations.  In smaller SCADA systems, the supervisory computer may be composed of a single PC, in which case the HMI is a part of this computer. In larger SCADA systems, the master station may include several HMIs hosted on client computers, multiple servers for data acquisition, distributed software applications, and disaster recovery sites. To increase the integrity of the system the multiple servers will often be configured in a dual-redundant or hot-standby formation providing continuous control and monitoring in the event of a server malfunction or breakdown.

Remote terminal units
Remote terminal units, also known as (RTUs),  connect to sensors and actuators in the process, and are networked to the supervisory computer system.  RTUs have embedded control capabilities and often conform to the IEC 61131-3 standard for programming and support automation via ladder logic, a function block diagram or a variety of other languages. Remote locations often have little or no local infrastructure so it is not uncommon to find RTUs running off a small solar power system, using radio, GSM or satellite for communications, and being ruggedised to survive from -20C to +70C or even -40C to +85C without external heating or cooling equipment.

Programmable logic controllers
Also known as PLCs, these are connected to sensors and actuators in the process, and are networked to the supervisory system. In factory automation, PLCs typically have a high speed connection to the SCADA system. In remote applications, such as a large water treatment plant, PLCs may connect directly to SCADA over a wireless link, or more commonly, utilise an RTU for the communications management. PLCs are specifically designed for control and were the founding platform for the IEC 61131-3 programming languages. For economical reasons, PLCs are often used for remote sites where there is a large I/O count, rather than utilising an RTU alone.

Communication infrastructure
This connects the supervisory computer system to the RTUs and PLCs, and may use industry standard or manufacturer proprietary protocols.
Both RTUs and PLCs operate autonomously on the near-real time control of the process, using the last command given from the supervisory system. Failure of the communications network does not necessarily stop the plant process controls, and on resumption of communications, the operator can continue with monitoring and control. Some critical systems will have dual redundant data highways, often cabled via diverse routes.

Human-machine interface
The human-machine interface (HMI) is  the operator window of the supervisory system. It presents plant information to the operating personnel graphically in the form of mimic diagrams, which are a schematic representation of the plant being controlled, and alarm and event logging pages. The HMI is linked to the SCADA supervisory computer to provide live data to drive the mimic diagrams, alarm displays and trending graphs. In many installations the HMI is the graphical user interface for the operator, collects all data from external devices, creates reports, performs alarming, sends notifications, etc.
Mimic diagrams consist of line graphics and schematic symbols to represent process elements, or may consist of digital photographs of the process equipment overlain with animated symbols.
Supervisory operation of the plant is by means of the HMI, with operators issuing commands using mouse pointers, keyboards and touch screens. For example, a symbol of a pump can show the operator that the pump is running, and a flow meter symbol can show how much fluid it is pumping through the pipe. The operator can switch the pump off from the mimic by a mouse click or screen touch. The HMI will show the flow rate of the fluid in the pipe decrease in real time.
The HMI package for a SCADA system typically includes a drawing program that the operators or system maintenance personnel use to change the way these points are represented in the interface. These representations can be as simple as an on-screen traffic light, which represents the state of an actual traffic light in the field, or as complex as a multi-projector display representing the position of all of the elevators in a skyscraper or all of the trains on a railway.
A "historian", is a software service within the HMI which accumulates time-stamped data, events, and alarms in a database which can be queried or used to populate graphic trends in the HMI.  The historian is a client that requests data from a data acquisition server.

Alarm handling
An important part of most SCADA implementations is alarm handling. The system monitors whether certain alarm conditions are satisfied, to determine when an alarm event has occurred. Once an alarm event has been detected, one or more actions are taken (such as the activation of one or more alarm indicators, and perhaps the generation of email or text messages so that management or remote SCADA operators are informed). In many cases, a SCADA operator may have to acknowledge the alarm event; this may deactivate some alarm indicators, whereas other indicators remain active until the alarm conditions are cleared.
Alarm conditions can be explicit—for example, an alarm point is a digital status point that has either the value NORMAL or ALARM that is calculated by a formula based on the values in other analogue and digital points—or implicit: the SCADA system might automatically monitor whether the value in an analogue point lies outside high and low- limit values associated with that point.
Examples of alarm indicators include a siren, a pop-up box on a screen, or a coloured or flashing area on a screen (that might act in a similar way to the "fuel tank empty" light in a car); in each case, the role of the alarm indicator is to draw the operator's attention to the part of the system 'in alarm' so that appropriate action can be taken.

PLC/RTU programming
"Smart" RTUs, or standard PLCs, are capable of autonomously executing simple logic processes without involving the supervisory computer. They employ standardized control programming languages such as under, IEC 61131-3 (a suite of five programming languages including function block, ladder, structured text, sequence function charts and instruction list), is frequently used to create programs which run on these RTUs and PLCs.  Unlike a procedural language like C or FORTRAN, IEC 61131-3 has minimal training requirements by virtue of resembling historic physical control arrays. This allows SCADA system engineers to perform both the design and implementation of a program to be executed on an RTU or PLC.
A programmable automation controller (PAC) is a compact controller that combines the features and capabilities of a PC-based control system with that of a typical PLC. PACs are deployed in SCADA systems to provide RTU and PLC functions. In many electrical substation SCADA applications, "distributed RTUs" use information processors or station computers to communicate with digital protective relays, PACs, and other devices for I/O, and communicate with the SCADA master in lieu of a traditional RTU.

PLC commercial integration
Since about 1998, virtually all major PLC manufacturers have offered integrated HMI/SCADA systems, many of them using open and non-proprietary communications protocols.  Numerous specialized third-party HMI/SCADA packages, offering built-in compatibility with most major PLCs, have also entered the market, allowing mechanical engineers, electrical engineers and technicians to configure HMIs themselves, without the need for a custom-made program written by a software programmer.
The Remote Terminal Unit (RTU) connects to physical equipment. Typically, an RTU converts the electrical signals from the equipment to digital values. By converting and sending these electrical signals out to equipment the RTU can control equipment.

Communication infrastructure and methods
SCADA systems have traditionally used combinations of radio and direct wired connections, although SONET/SDH is also frequently used for large systems such as railways and power stations. The remote management or monitoring function of a SCADA system is often referred to as telemetry. Some users want SCADA data to travel over their pre-established corporate networks or to share the network with other applications. The legacy of the early low-bandwidth protocols remains, though.
SCADA protocols are designed to be very compact. Many are designed to send information only when the master station polls the RTU. Typical legacy SCADA protocols include Modbus RTU, RP-570, Profibus and Conitel. These communication protocols, with the exception of Modbus (Modbus has been made open by Schneider Electric), are all SCADA-vendor specific but are widely adopted and used. Standard protocols are IEC 60870-5-101 or 104, IEC 61850 and DNP3. These communication protocols are standardized and recognized by all major SCADA vendors. Many of these protocols now contain extensions to operate over TCP/IP. Although the use of conventional networking specifications, such as TCP/IP, blurs the line between traditional and industrial networking, they each fulfill fundamentally differing requirements. Network simulation can be used in conjunction with SCADA simulators to perform various 'what-if' analyses.
With increasing security demands (such as North American Electric Reliability Corporation (NERC) and critical infrastructure protection (CIP) in the US), there is increasing use of satellite-based communication. This has the key advantages that the infrastructure can be self-contained (not using circuits from the public telephone system), can have built-in encryption, and can be engineered to the availability and reliability required by the SCADA system operator.  Earlier experiences using consumer-grade VSAT were poor. Modern carrier-class systems provide the quality of service required for SCADA.RTUs and other automatic controller devices were developed before the advent of industry wide standards for interoperability. The result is that developers and their management created a multitude of control protocols. Among the larger vendors, there was also the incentive to create their own protocol to "lock in" their customer base. A list of automation protocols is compiled here.
An example of efforts by vendor groups to standardize automation protocols is the OPC-UA (formerly "OLE for process control" now Open Platform Communications Unified Architecture).

Architecture development
SCADA systems have evolved through four generations as follows:

First generation: "Monolithic"
Early SCADA system computing was done by large minicomputers. Common network services did not exist at the time SCADA was developed. Thus SCADA systems were independent systems with no connectivity to other systems. The communication protocols used were strictly proprietary at that time. The first-generation SCADA system redundancy was achieved using a back-up mainframe system connected to all the Remote Terminal Unit sites and was used in the event of failure of the primary mainframe system.  Some first generation SCADA systems were developed as "turn key" operations that ran on minicomputers such as the PDP-11 series.

Second generation: "Distributed"
SCADA information and command processing were distributed across multiple stations which were connected through a LAN. Information was shared in near real time. Each station was responsible for a particular task, which reduced the cost as compared to First Generation SCADA. The network protocols used were still not standardized. Since these protocols were proprietary, very few people beyond the developers knew enough to determine how secure a SCADA installation was. Security of the SCADA installation was usually overlooked.

Third generation: "Networked"
Similar to a distributed architecture, any complex SCADA can be reduced to the simplest components and connected through communication protocols. In the case of a networked design, the system may be spread across more than one LAN network called a process control network (PCN) and separated geographically. Several distributed architecture SCADAs running in parallel, with a single supervisor and historian, could be considered a network architecture. This allows for a more cost-effective solution in very large scale systems.

Fourth generation: "Web-based"
The growth of the internet has led SCADA systems to implement web technologies allowing users to view data, exchange information and control processes from anywhere in the world through web SOCKET connection. The early 2000s saw the proliferation of Web SCADA systems. Web SCADA systems use internet browsers such as Google Chrome and Mozilla Firefox as the graphical user interface (GUI) for the operators HMI. This simplifies the client side installation and enables users to access the system from various platforms with web browsers such as servers, personal computers, laptops, tablets and mobile phones.

Security issues
SCADA systems that tie together decentralized facilities such as power, oil, gas pipelines, water distribution and wastewater collection systems were designed to be open, robust, and easily operated and repaired, but not necessarily secure. The move from proprietary technologies to more standardized and open solutions together with the increased number of connections between SCADA systems, office networks and the Internet has made them more vulnerable to types of network attacks that are relatively common in computer security. For example, United States Computer Emergency Readiness Team (US-CERT) released a vulnerability advisory warning that unauthenticated users could download sensitive configuration information including password hashes from an Inductive Automation Ignition system utilizing a standard attack type leveraging access to the Tomcat Embedded Web server. Security researcher Jerry Brown submitted a similar advisory regarding a buffer overflow vulnerability in a Wonderware InBatchClient ActiveX control. Both vendors made updates available prior to public vulnerability release. Mitigation recommendations were standard patching practices and requiring VPN access for secure connectivity. Consequently, the security of some SCADA-based systems has come into question as they are seen as potentially vulnerable to cyber attacks.In particular, security researchers are concerned about

the lack of concern about security and authentication in the design, deployment and operation of some existing SCADA networks
the belief that SCADA systems have the benefit of security through obscurity through the use of specialized protocols and proprietary interfaces
the belief that SCADA networks are secure because they are physically secured
the belief that SCADA networks are secure because they are disconnected from the InternetSCADA systems are used to control and monitor physical processes, examples of which are transmission of electricity, transportation of gas and oil in pipelines, water distribution, traffic lights, and other systems used as the basis of modern society.  The security of these SCADA systems is important because compromise or destruction of these systems would impact multiple areas of society far removed from the original compromise.  For example, a blackout caused by a compromised electrical SCADA system would cause financial losses to all the customers that received electricity from that source.  How security will affect legacy SCADA and new deployments remains to be seen.
There are many threat vectors to a modern SCADA system. One is the threat of unauthorized access to the control software, whether it is human access or changes induced intentionally or accidentally by virus infections and other software threats residing on the control host machine. Another is the threat of packet access to the network segments hosting SCADA devices. In many cases, the control protocol lacks any form of cryptographic security, allowing an attacker to control a SCADA device by sending commands over a network. In many cases SCADA users have assumed that having a VPN offered sufficient protection, unaware that security can be trivially bypassed with physical access to SCADA-related network jacks and switches. Industrial control vendors suggest approaching SCADA security like Information Security with a defense in depth strategy that leverages common IT practices. Apart from that, research has shown that the architecture of SCADA systems has several other vulnerabilities, including direct tampering with RTUs, communication links from RTUs to the control center, and IT software and databases in the control center. The RTUs could, for instance, be targets of deception attacks injecting false data  or denial-of-service attacks. 
The reliable function of SCADA systems in our modern infrastructure may be crucial to public health and safety. As such, attacks on these systems may directly or indirectly threaten public health and safety. Such an attack has already occurred, carried out on Maroochy Shire Council's sewage control system in Queensland, Australia.  Shortly after a contractor installed a SCADA system in January 2000, system components began to function erratically. Pumps did not run when needed and alarms were not reported.  More critically, sewage flooded a nearby park and contaminated an open surface-water drainage ditch and flowed 500 meters to a tidal canal. The SCADA system was directing sewage valves to open when the design protocol should have kept them closed. Initially this was believed to be a system bug.  Monitoring of the system logs revealed the malfunctions were the result of cyber attacks. Investigators reported 46 separate instances of malicious outside interference before the culprit was identified.  The attacks were made by a disgruntled ex-employee of the company that had installed the SCADA system.  The ex-employee was hoping to be hired by the utility full-time to maintain the system.
In April 2008, the Commission to Assess the Threat to the United States from Electromagnetic Pulse (EMP) Attack issued a Critical Infrastructures Report which discussed the extreme vulnerability of SCADA systems to an electromagnetic pulse (EMP) event. After testing and analysis, the Commission concluded: "SCADA systems are vulnerable to an EMP event. The large numbers and widespread reliance on such systems by all of the Nation’s critical infrastructures represent a systemic threat to their continued operation following an EMP event. Additionally, the necessity to reboot, repair, or replace large numbers of geographically widely dispersed systems will considerably impede the Nation’s recovery from such an assault."Many vendors of SCADA and control products have begun to address the risks posed by unauthorized access by developing lines of specialized industrial firewall and VPN solutions for TCP/IP-based SCADA networks as well as external SCADA monitoring and recording equipment.
The International Society of Automation (ISA) started formalizing SCADA security requirements in 2007 with a working group, WG4. WG4 "deals specifically with unique technical requirements, measurements, and other features required to evaluate and assure security resilience and performance of industrial automation and control systems devices".The increased interest in SCADA vulnerabilities has resulted in vulnerability researchers discovering vulnerabilities in commercial SCADA software  and more general offensive SCADA techniques presented to the general security community.  In electric and gas utility SCADA systems, the vulnerability of the large installed base of wired and wireless serial communications links is addressed in some cases by applying bump-in-the-wire devices that employ authentication and Advanced Encryption Standard encryption rather than replacing all existing nodes.In June 2010, anti-virus security company VirusBlokAda reported the first detection of malware that attacks SCADA systems (Siemens' WinCC/PCS 7 systems) running on Windows operating systems. The malware is called Stuxnet and uses four zero-day attacks to install a rootkit which in turn logs into the SCADA's database and steals design and control files. The malware is also capable of changing the control system and hiding those changes. The malware was found on 14 systems, the majority of which were located in Iran.In October 2013 National Geographic released a docudrama titled American Blackout which dealt with an imagined large-scale cyber attack on SCADA and the United States' electrical grid.

See also
DNP3
IEC 60870
EPICS

References
External links

UK SCADA security guidelines
BBC NEWS | Technology | Spies 'infiltrate US power grid'

Scientific management

Scientific management is a theory of management that analyzes and synthesizes workflows. Its main objective is improving economic efficiency, especially labor productivity. It was one of the earliest attempts to apply science to the engineering of processes to management. Scientific management is sometimes known as Taylorism after its pioneer, Frederick Winslow Taylor.Taylor began the theory's development in the United States during the 1880s and 1890s within manufacturing industries, especially steel. Its peak of influence came in the 1910s. Although Taylor died in 1915, by the 1920s scientific management was still influential but had entered into competition and syncretism with opposing or complementary ideas.
Although scientific management as a distinct theory or school of thought was obsolete by the 1930s, most of its themes are still important parts of industrial engineering and management today. These include: analysis; synthesis; logic; rationality; empiricism; work ethic; efficiency through elimination of wasteful activities (as in muda, muri and mura); standardization of best practices; disdain for tradition preserved merely for its own sake or to protect the social status of particular workers with particular skill sets; the transformation of craft production into mass production; and knowledge transfer between workers and from workers into tools, processes, and documentation.

Name
Taylor's own names for his approach initially included "shop management" and "process management". However, "scientific management" came to national attention in 1910 when crusading attorney Louis Brandeis (then not yet Supreme Court justice) popularized the term. Brandeis had sought a consensus term for the approach with the help of practitioners like Henry L. Gantt and Frank B. Gilbreth. Brandeis then used the consensus of "SCIENTIFIC management" when he argued before the Interstate Commerce Commission (ICC) that a proposed increase in railroad rates was unnecessary despite an increase in labor costs; he alleged scientific management would overcome railroad inefficiencies (The ICC ruled against the rate increase, but also dismissed as insufficiently substantiated that concept the railroads were necessarily inefficient.) Taylor recognized the nationally known term "scientific management" as another good name for the concept, and adopted it in the title of his influential 1911 monograph.

History
The Midvale Steel Company, "one of America's great armor plate making plants," was the birthplace of scientific management. In 1877, Frederick W. Taylor started as a clerk in Midvale, but advanced to foreman in 1880. As foreman, Taylor was "constantly impressed by the failure of his [team members] to produce more than about one-third of [what he deemed] a good day's work". Taylor determined to discover, by scientific methods, how long it should take men to perform each given piece of work; and it was in the fall of 1882 that he started to put the first features of scientific management into operation.Horace Bookwalter Drury, in his 1918 work, Scientific management: A History and Criticism, identified seven other leaders in the movement,  most of whom learned of and extended scientific management from Taylor's efforts:
Henry L. Gantt (1861–1919)
Carl G. Barth (1860–1939)
Horace K. Hathaway (1878–1944)
Morris L. Cooke (1872–1960)
Sanford E. Thompson (1867–1949)
Frank B. Gilbreth (1868–1924). Gilbreth's independent work on "motion study" is on record as early as 1885; after meeting Taylor in 1906 and being introduced to scientific management, Gilbreth devoted his efforts to introducing scientific management into factories. Gilbreth and his wife Lillian Moller Gilbreth (1878–1972) performed micro-motion studies using stop-motion cameras as well as developing the profession of industrial/organizational psychology.
Harrington Emerson (1853–1931) began determining what industrial plants' products and costs were compared to what they ought to be in 1895. Emerson did not meet Taylor until December 1900, and the two never worked together.Emerson's testimony in late 1910 to the Interstate Commerce Commission brought the movement to national attention and instigated serious opposition. Emerson contended the railroads might save $1,000,000 a day by paying greater attention to efficiency of operation. By January 1911, a leading railroad journal began a series of articles denying they were inefficiently managed.When steps were taken to introduce scientific management at the government-owned Rock Island Arsenal in early 1911, it was opposed by Samuel Gompers, founder and President of the American Federation of Labor (an alliance of craft unions). When a subsequent attempt was made to introduce the bonus system into the government's Watertown Arsenal foundry during the summer of 1911, the entire force walked out for a few days. Congressional investigations followed, resulting in a ban on the use of time studies and pay premiums in Government service.Taylor's death in 1915 at age 59 left the movement without its original leader. In management literature today, the term "scientific management" mostly refers to the work of Taylor and his disciples ("classical", implying "no longer current, but still respected for its seminal value") in contrast to newer, improved iterations of efficiency-seeking methods. Today, task-oriented optimization of work tasks is nearly ubiquitous in industry.

Scientific Management Principles
Frederick Taylor tackled the challenge of making a business productive and profitable in his years of service and research in a steel company. He believed in a scientific solution. In his "Shop Management" article, Taylor explained that there were two facts that appeared "most noteworthy" in the field of management: (a) "Great unevenness": the lack of uniformity in what is called "the management", (b) The lack of relation between good (shop) management and the pay. He added, "The art of management has been defined, "as knowing exactly what you want men to do, and then seeing that they do it in the best and cheapest way"."
In this regard, he highlighted that although there is "no concise definition" for this art, "the relations between employers and men form without question the most important part of this art". He then continued that a good management must in long run give satisfaction to both managers and workers. Taylor emphasized that he was advocating "high wages" and "low labor cost" as "the foundation of the best management". Discussing the pays for different classes of workers and what he called a "first-class" workman, he compared different scenarios of workmanship and their pros and cons. For best management, he asserted with ample reasons that managers in an organization should follow the following guideline: (a) Each worker should be given the highest grade of work they are capable of.
(b) Each worker should be demanded the work that a first-grade worker can do and thrive.

(c) When each worker works at the pace of a first-grade worker, they should be paid 30% to 100% beyond the average of their class. While Taylor stated that sharing "the equitable division of the profits" is required in an organization, he believed that management could unite high wages with a low labor cost by application of the following principles:(a) A large daily task: Each worker in the organization, should have a clearly defined task.
(b) Standard Conditions: Each worker should be given standard conditions and appliances that will enable him to perform his tasks.
(c) High pay for success: Each worker should be rewarded when he accomplishes their task.

(d) Loss in case of failure: When a worker fails, he should know that he would share the loss.In Scientific Management, the responsibility of the success or failure of an organization is not solely on the shoulder of the workers, as it is in the old management systems. According to Scientific Management, the managers are taking half of the burden by being responsible for securing the proper work conditions for workers' prosperity.
In his book "Principles of Scientific Management", Taylor formally introduced his methodically investigated theory of Scientific Management. Although he explained the details of Scientific Management in his works, he did not provide its concise definition. Shortly before his death, Taylor approved the following summary and definition of Scientific Management that Hoxie prepared: "Scientific management is a system devised by industrial engineers for the purpose of serving the common interests of employers, workmen and society at large through the elimination of avoidable wastes, the general improvement of the processes and methods of production, and the just and scientific distribution of the product." Taylor indicated that Scientific Management consisted of four underlying principles:1) the development of a true science: We must scientifically analyze all parts of a job. This consists of examining the elements and steps that required to carry out the work, as well as measuring the optimum time for each task. We also need to know the working time per day for a qualified worker.
2) the scientific selection of the workers: The most suitable person for the job is selected.
3) the scientific education and training of the workers: There is a clear division of work and responsibility between managers and workers. While workers are carrying out the job with quality and workmanship, managers are responsible for planning, supervision, and proper training of the workers.

4) cooperation between managers and workers: Managers and workers scientific cooperation is required to ensure the proper and high-quality execution of the jobs.There are various tools that would enable us to serve these principles, such as time and motion study, functional foremanship, standardization of tools and movements of workers for each type of work, clear instructions for workers, and cost accounting.There are many other features, tools, and methods that Taylor developed and recommended during his job at the steel plant and research, which have footprints in other fields, such as accounting and Engineering. Some of his concepts, studies, and findings has led to intellectual revolution in organization management. Taylor made contributions to various fields such as work measurement, production planning and control, process design, quality control, ergonomics, and human engineering.

Pursuit of economic efficiency
Flourishing in the late 19th and early 20th century, scientific management built on earlier pursuits of economic efficiency. While it was prefigured in the folk wisdom of thrift, it favored empirical methods to determine efficient procedures rather than perpetuating established traditions. Thus it was followed by a profusion of successors in applied science, including time and motion study, the Efficiency Movement (which was a broader cultural echo of scientific management's impact on business managers specifically), Fordism, operations management, operations research, industrial engineering, management science, manufacturing engineering, logistics, business process management, business process reengineering, lean manufacturing, and Six Sigma. There is a fluid continuum linking scientific management with the later fields, and the different approaches often display a high degree of compatibility.
Taylor rejected the notion, which was universal in his day and still held today, that the trades, including manufacturing, were resistant to analysis and could only be performed by craft production methods. In the course of his empirical studies, Taylor examined various kinds of manual labor. For example, most bulk materials handling was manual at the time; material handling equipment as we know it today was mostly not developed yet. He looked at shoveling in the unloading of railroad cars full of ore; lifting and carrying in the moving of iron pigs at steel mills; the manual inspection of bearing balls; and others. He discovered many concepts that were not widely accepted at the time. For example, by observing workers, he decided that labor should include rest breaks so that the worker has time to recover from fatigue, either physical (as in shoveling or lifting) or mental (as in the ball inspection case). Workers were allowed to take more rests during work, and productivity increased as a result.Subsequent forms of scientific management were articulated by Taylor's disciples, such as Henry Gantt; other engineers and managers, such as Benjamin S. Graham; and other theorists, such as Max Weber. Taylor's work also contrasts with other efforts, including those of Henri Fayol and those of Frank Gilbreth, Sr. and Lillian Moller Gilbreth (whose views originally shared much with Taylor's but later diverged in response to Taylorism's inadequate handling of human relations).

Soldiering
Scientific management requires a high level of managerial control over employee work practices and entails a higher ratio of managerial workers to laborers than previous management methods. Such detail-oriented management may cause friction between workers and managers.
Taylor observed that some workers were more talented than others, and that even smart ones were often unmotivated. He observed that most workers who are forced to perform repetitive tasks tend to work at the slowest rate that goes unpunished. This slow rate of work has been observed in many industries and many countries and has been called by various terms. Taylor used the term "soldiering", a term that reflects the way conscripts may approach following orders, and observed that, when paid the same amount, workers will tend to do the amount of work that the slowest among them does. Taylor describes soldiering as "the greatest evil with which the working-people ... are now afflicted".This reflects the idea that workers have a vested interest in their own well-being, and do not benefit from working above the defined rate of work when it will not increase their remuneration. He, therefore, proposed that the work practice that had been developed in most work environments was crafted, intentionally or unintentionally, to be very inefficient in its execution. He posited that time and motion studies combined with rational analysis and synthesis could uncover one best method for performing any particular task, and that prevailing methods were seldom equal to these best methods. Crucially, Taylor himself prominently acknowledged that if each employee's compensation was linked to their output, their productivity would go up. Thus his compensation plans usually included piece rates. In contrast, some later adopters of time and motion studies ignored this aspect and tried to get large productivity gains while passing little or no compensation gains to the workforce, which contributed to resentment against the system.

Productivity, automation, and unemployment
Taylorism led to productivity increases, meaning fewer workers or working hours were needed to produce the same amount of goods. In the short term, productivity increases like those achieved by Taylor's efficiency techniques can cause considerable disruption. Labor relations often become contentious over whether the financial benefits will accrue to owners in the form of increased profits, or workers in the form of increased wages. As a result of decomposition and documentation of manufacturing processes, companies employing Taylor's methods might be able to hire lower-skill workers, enlarging the pool of workers and thus lowering wages and job security.In the long term, most economists consider productivity increases as a benefit to the economy overall, and necessary to improve the standard of living for consumers in general. By the time Taylor was doing his work, improvements in agricultural productivity had freed up a large portion of the workforce for the manufacturing sector, allowing those workers in turn to buy new types of consumer goods instead of working as subsistence farmers. In later years, increased manufacturing efficiency would free up large sections of the workforce for the service sector. If captured as profits or wages, the money generated by more-productive companies would be spent on new goods and services; if free market competition forces prices down close to the cost of production, consumers effectively capture the benefits and have more money to spend on new goods and services. Either way, new companies and industries spring up to profit from increased demand, and due to freed-up labor are able to hire workers. But the long-term benefits are no guarantee that individual displaced workers will be able to get new jobs that paid them as well or better as their old jobs, as this may require access to education or job training, or moving to different part of the country where new industries are growing.  Inability to obtain new employment due to mismatches like these is known as structural unemployment, and economists debate to what extent this is happening in the long term, if at all, as well as the impact on income inequality for those who do find jobs.
Though not foreseen by early proponents of scientific management, detailed decomposition and documentation of an optimal production method also makes automation of the process easier, especially physical processes that would later use industrial control systems and numerical control. Widespread economic globalization also creates opportunity for work to be outsourced to lower-wage areas, with knowledge transfer made easier if an optimal method is already clearly documented. Especially when wages or wage differentials are high, automation and offshoring can result in significant productivity gains and similar questions of who benefits and whether or not technological unemployment is persistent. Because automation is often best suited to tasks that are repetitive and boring, and can also be used for tasks that are dirty, dangerous, and demeaning, proponents believe that in the long run it will free up human workers for more creative, safer, and more enjoyable work.

Taylorism and unions
The early history of labor relations with scientific management in the U.S. was described by Horace Bookwalter Drury:

...for a long time there was thus little or no direct [conflict] between scientific management and organized labor... [However] One of the best known experts once spoke to us with satisfaction of the manner in which, in a certain factory where there had been a number of union men, the labor organization had, upon the introduction of scientific management, gradually disintegrated.
...From 1882 (when the system was started) until 1911, a period of approximately thirty years, there was not a single strike under it, and this in spite of the fact that it was carried on primarily in the steel industry, which was subject to a great many disturbances. For instance, in the general strike in Philadelphia, one man only went out at the Tabor plant [managed by Taylor], while at the Baldwin Locomotive shops across the street two thousand struck.

...Serious opposition may be said to have been begun in 1911, immediately after certain testimony presented before the Interstate Commerce Commission [by Harrington Emerson] revealed to the country the strong movement setting towards scientific management. National labor leaders, wide-awake as to what might happen in the future, decided that the new movement was a menace to their organization, and at once inaugurated an attack... centered about the installation of scientific management in the government arsenal at Watertown. 
In 1911, organized labor erupted with strong opposition to scientific management, including from Samuel Gompers, founder and president of the American Federation of Labor (AFL).
Once the time-and-motion men had completed their studies of a particular task, the workers had very little opportunity for further thinking, experimenting, or suggestion-making. Taylorism was criticized for turning the worker into an "automaton" or "machine", making work monotonous and unfulfilling by doing one small and rigidly defined piece of work instead of using complex skills with the whole production process done by one person. "The further 'progress' of industrial development... increased the anomic or forced division of labor," the opposite of what Taylor thought would be the effect. Some workers also complained about being made to work at a faster pace and producing goods of lower quality.

The Watertown Arsenal in Massachusetts provides an example of the application and repeal of the Taylor system in the workplace, due to worker opposition.  In the early 20th century, neglect in the Watertown shops included overcrowding, dim lighting, lack of tools and equipment, and questionable management strategies in the eyes of the workers. Frederick W. Taylor and Carl G. Barth visited Watertown in April 1909 and reported on their observations at the shops. Their conclusion was to apply the Taylor system of management to the shops to produce better results.  Efforts to install the Taylor system began in June 1909.  Over the years of time study and trying to improve the efficiency of workers, criticisms began to evolve.  Workers complained of having to compete with one another, feeling strained and resentful, and feeling excessively tired after work.  In June 1913, employees of the Watertown Arsenal petitioned to abolish the practice of scientific management there. A number of magazine writers inquiring into the effects of scientific management found that the "conditions in shops investigated contrasted favorably with those in other plants".A committee of the U.S. House of Representatives investigated and reported in 1912, concluding that scientific management did provide some useful techniques and offered valuable organizational suggestions, but that it also gave production managers a dangerously high level of uncontrolled power. After an attitude survey of the workers revealed a high level of resentment and hostility towards scientific management, the Senate banned Taylor's methods at the arsenal.Taylor had a largely negative view of unions, and believed they only led to decreased productivity. Efforts to resolve conflicts with workers included methods of scientific collectivism, making agreements with unions, and the personnel management movement.

Relationship to Fordism
It is often assumed that Fordism derives from Taylor's work. Taylor apparently made this assumption himself when visiting the Ford Motor Company's Michigan plants not too long before he died, but it is likely that the methods at Ford were evolved independently, and that any influence from Taylor's work was indirect at best. Charles E. Sorensen, a principal of the company during its first four decades, disclaimed any connection at all. There was a belief at Ford, which remained dominant until Henry Ford II took over the company in 1945, that the world's experts were worthless, because if Ford had listened to them, it would have failed to attain its great successes. Henry Ford felt that he had succeeded in spite of, not because of, experts, who had tried to stop him in various ways (disagreeing about price points, production methods, car features, business financing, and other issues). Sorensen thus was dismissive of Taylor and lumped him into the category of useless experts. Sorensen held the New England machine tool vendor Walter Flanders in high esteem and credits him for the efficient floorplan layout at Ford, claiming that Flanders knew nothing about Taylor. Flanders may have been exposed to the spirit of Taylorism elsewhere, and may have been influenced by it, but he did not cite it when developing his production technique. Regardless, the Ford team apparently did independently invent modern mass production techniques in the period of 1905–1915, and they themselves were not aware of any borrowing from Taylorism. Perhaps it is only possible with hindsight to see the zeitgeist that (indirectly) connected the budding Fordism to the rest of the efficiency movement during the decade of 1905–1915.

Adoption in planned economies
Scientific management appealed to managers of planned economies because central economic planning relies on the idea that the expenses that go into economic production can be precisely predicted and can be optimized by design.

Soviet Union
By 1913 Vladimir Lenin wrote that the "most widely discussed topic today in Europe, and to some extent in Russia, is the 'system' of the American engineer, Frederick Taylor"; Lenin decried it as merely a "'scientific' system of sweating" more work from laborers. Again in 1914, Lenin derided Taylorism as "man's enslavement by the machine". However, after the Russian Revolutions brought him to power, Lenin wrote in 1918 that the "Russian is a bad worker [who must] learn to work. The Taylor system... is a combination of the refined brutality of bourgeois exploitation and a number of the greatest scientific achievements in the field of analysing mechanical motions during work, the elimination of superfluous and awkward motions, the elaboration of correct methods of work, the introduction of the best system of accounting and control, etc. The Soviet Republic must at all costs adopt all that is valuable in the achievements of science and technology in this field."In the Soviet Union, Taylorism was advocated by Aleksei Gastev and nauchnaia organizatsia truda (the movement for the scientific organization of labor). It found support in both Vladimir Lenin and Leon Trotsky. Gastev continued to promote this system of labor management until his arrest and execution in 1939. In the 1920s and 1930s, the Soviet Union enthusiastically embraced Fordism and Taylorism, importing American experts in both fields as well as American engineering firms to build parts of its new industrial infrastructure. The concepts of the Five Year Plan and the centrally planned economy can be traced directly to the influence of Taylorism on Soviet thinking. As scientific management was believed to epitomize American efficiency, Joseph Stalin even claimed that "the combination of the Russian revolutionary sweep with American efficiency is the essence of Leninism."Sorensen was one of the consultants who brought American know-how to the USSR during this era, before the Cold War made such exchanges unthinkable. As the Soviet Union developed and grew in power, both sides, the Soviets and the Americans, chose to ignore or deny the contribution that American ideas and expertise had made: the Soviets because they wished to portray themselves as creators of their own destiny and not indebted to a rival, and the Americans because they did not wish to acknowledge their part in creating a powerful communist rival. Anti-communism had always enjoyed widespread popularity in America, and anti-capitalism in Russia, but after World War II, they precluded any admission by either side that technologies or ideas might be either freely shared or clandestinely stolen.

East Germany
By the 1950s, scientific management had grown dated, but its goals and practices remained attractive and were also being adopted by the German Democratic Republic as it sought to increase efficiency in its industrial sectors. Workers engaged in a state-planned instance of process improvement, pursuing the same goals that were contemporaneously pursued in capitalist societies, as in the Toyota Production System.

Criticism of rigor
Taylor believed that the scientific method of management included the calculations of exactly how much time it takes a man to do a particular task, or his rate of work. Critics of Taylor complained that such a calculation relies on certain arbitrary, non-scientific decisions such as what constituted the job, which men were timed, and under which conditions.  Any of these factors are subject to change, and therefore can produce inconsistencies. Some dismiss so-called "scientific management" or Taylorism as pseudoscience.
Others are critical of the representativeness of the workers Taylor selected to take his measurements.

Variations of scientific management after Taylorism
In the 1900s
Taylorism was one of the first attempts to systematically treat management and process improvement as a scientific problem, and Taylor is considered a founder of modern industrial engineering. Taylorism may have been the first "bottom-up" method and found a lineage of successors that have many elements in common. Later methods took a broader approach, measuring not only productivity but quality. With the advancement of statistical methods, quality assurance and quality control began in the 1920s and 1930s. During the 1940s and 1950s, the body of knowledge for doing scientific management evolved into operations management, operations research, and management cybernetics. In the 1980s total quality management became widely popular, growing from quality control techniques. In the 1990s "re-engineering" went from a simple word to a mystique. Today's Six Sigma and lean manufacturing could be seen as new kinds of scientific management, although their evolutionary distance from the original is so great that the comparison might be misleading. In particular, Shigeo Shingo, one of the originators of the Toyota Production System, believed that this system and Japanese management culture in general should be seen as a kind of scientific management. These newer methods are all based on systematic analysis rather than relying on tradition and rule of thumb.Other thinkers, even in Taylor's own time, also proposed considering the individual worker's needs, not just the needs of the process. Critics said that in Taylorism, "the worker was taken for granted as a cog in the machinery." James Hartness published The Human Factor in Works Management in 1912, while Frank Gilbreth and Lillian Moller Gilbreth offered their own alternatives to Taylorism. The human relations school of management (founded by the work of Elton Mayo) evolved in the 1930s as a counterpoint or complement of scientific management. Taylorism focused on the organization of the work process, and human relations helped workers adapt to the new procedures. Modern definitions of "quality control" like ISO-9000 include not only clearly documented and optimized manufacturing tasks, but also consideration of human factors like expertise, motivation, and organizational culture. The Toyota Production System, from which lean manufacturing in general is derived, includes "respect for people" and teamwork as core principles.
Peter Drucker saw Frederick Taylor as the creator of knowledge management, because the aim of scientific management was to produce knowledge about how to improve work processes. Although the typical application of scientific management was manufacturing, Taylor himself advocated scientific management for all sorts of work, including the management of schools, universities and government. For example, Taylor believed scientific management could be extended to "the work of our salesmen".  Shortly after his death, his acolyte Harlow S. Person began to lecture corporate audiences on the possibility of using Taylorism for "sales engineering" (Person was talking about what is now called sales process engineering—engineering the processes that salespeople use—not about what we call sales engineering today.) This was a watershed insight in the history of corporate marketing.

In the 2000s
Google's methods of increasing productivity and output can be seen to be influenced by Taylorism as well. The Silicon Valley company is a forerunner in applying behavioral science (such as the motivations of purpose, mastery, and autonomy set out by Daniel Pink in his 2009 book Drive: The Surprising Truth About What Motivates Us) to increase knowledge worker productivity. In classic scientific management as well as approaches like lean management where leaders facilitate and empower teams to continuously improve their standards and values. Leading high-tech companies use the concept of nudge management to increase productivity of employees. More and more business leaders start to make use of this new scientific management.Today's militaries employ all of the major goals and tactics of scientific management, if not under that name. Of the key points, all but wage incentives for increased output are used by modern military organizations. Wage incentives rather appear in the form of skill bonuses for enlistments.Scientific management has had an important influence in sports, where stop watches and motion studies rule the day. (Taylor himself enjoyed sports, especially tennis and golf. He and a partner won a national championship in doubles tennis. He invented improved tennis racquets and improved golf clubs, although other players liked to tease him for his unorthodox designs, and they did not catch on as replacements for the mainstream implements).Modern human resources can be seen to have begun in the scientific management era, most notably in the writings of Katherine M. H. Blackford.
Practices descended from scientific management are currently used in offices and in medicine (e.g. managed care) as well.In countries with a post-industrial economy, manufacturing jobs are a relatively few, with most workers in the service sector. One approach to efficiency in information work is called digital Taylorism, which uses software to monitor the performance of employees who use computers all day.

See also
American system of manufacturing
Cheaper by the Dozen
Hawthorne effect
Henry Louis Le Châtelier (1850–1936), industrial chemist and author of French language texts on Taylorism
Modern Times (film)
The Pajama Game
Pandora's Box
Hans Renold (1852–1943), credited with introducing Taylorism to Britain
Stakhanovism
Theory X and Theory Y
Henry R. Towne (1844–1924), ASME President and author of the seminal The Engineer as An Economist (1886)
Words per minute
Exploitation

Notes
References
Further reading
Gershon, Richard (2001), Telecommunications Management: Industry Structures and Planning Strategies, Mahwah, NJ, USA: Lawrence Erlbaum Associates, ISBN 978-0-8058-3002-6
Morf, Martin (1983) Eight Scenarios for Work in the Future. in Futurist, v17 n3 pp. 24–29 Jun 1983, reprinted in Cornish, Edward and World Future Society (1985) Habitats tomorrow: homes and communities in an exciting new era : selections from The futurist, pp. 14–19
Noble, David F. (1984), Forces of Production: A Social History of Industrial Automation, New York, New York, US: Knopf, ISBN 978-0-394-51262-4, LCCN 83048867.
Scheiber, Lukas (2012), Next Taylorism: A Calculus of Knowledge Work, Frankfurt am Main, BRD: Peter Lang, ISBN 978-3631624050
Taylor, Frederick Winslow (1903), Shop Management, New York, NY, US: American Society of Mechanical Engineers, ISBN 9780598777706, OCLC 2365572. "Shop Management" began as an address by Taylor to a meeting of the ASME, which published it in pamphlet form. The link here takes the reader to a 1912 republication by Harper & Brothers. Also available from Project Gutenberg.

External links

Special Collections: F.W. Taylor Collection. Stevens Institute of Technology has an extensive collection at its library.

Scientific method

The scientific method is an empirical method for acquiring knowledge that has characterized the development of science since at least the 17th century (with notable practitioners in previous centuries; see the article history of scientific method for additional detail.) It involves careful observation, applying rigorous skepticism about what is observed, given that cognitive assumptions can distort how one interprets the observation. It involves formulating hypotheses, via induction, based on such observations; the testability of hypotheses, experimental and the measurement-based statistical testing of deductions drawn from the hypotheses; and refinement (or elimination) of the hypotheses based on the experimental findings. These are principles of the scientific method, as distinguished from a definitive series of steps applicable to all scientific enterprises.Although procedures vary from one field of inquiry to another, the underlying process is frequently the same from one field to another. The process in the scientific method involves making conjectures (hypothetical explanations), deriving predictions from the hypotheses as logical consequences, and then carrying out experiments or empirical observations based on those predictions. A hypothesis is a conjecture, based on knowledge obtained while seeking answers to the question. The hypothesis might be very specific, or it might be broad. Scientists then test hypotheses by conducting experiments or studies. A scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment or observation that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.The purpose of an experiment is to determine whether observations  agree with or conflict with the expectations deduced from a hypothesis.: Book I, [6.54] pp.372, 408  Experiments can take place anywhere from a garage to a remote mountaintop to CERN's Large Hadron Collider. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, it represents rather a set of general principles. Not all steps take place in every scientific inquiry (nor to the same degree), and they are not always in the same order.

History
Important debates in the history of science concern skepticism that anything can be known for sure (such as views of Francisco Sanches), rationalism (especially as advocated by René Descartes), inductivism, empiricism (as argued for by Francis Bacon, then rising to particular prominence with Isaac Newton and his followers), and hypothetico-deductivism, which came to the fore in the early 19th century.
The term "scientific method" emerged in the 19th century, when a significant institutional development of science was taking place and terminologies establishing clear boundaries between science and non-science, such as "scientist" and "pseudoscience", appeared. Throughout the 1830s and 1850s, at which time Baconianism was popular, naturalists like William Whewell, John Herschel, John Stuart Mill engaged in debates over "induction" and "facts" and were focused on how to generate knowledge. In the late 19th and early 20th centuries, a debate over realism vs. antirealism was conducted as powerful scientific theories extended beyond the realm of the observable.

Problem-solving via scientific method
The term "scientific method" came into popular use in the twentieth century; Dewey's 1910 book, How We Think, inspired popular guidelines, popping up in dictionaries and science textbooks, although there was little consensus over its meaning. Although there was growth through the middle of the twentieth century, by the 1960s and 1970s numerous influential philosophers of science such as Thomas Kuhn and Paul Feyerabend had questioned the universality of the "scientific method" and in doing so largely replaced the notion of science as a homogeneous and universal method with that of it being a heterogeneous and local practice. In particular, Paul Feyerabend, in the 1975 first edition of his book Against Method, argued against there being any universal rules of science; Popper 1963, Gauch 2003, and Tow 2010 disagree with Feyerabend's claim; problem solvers, and researchers are to be prudent with their resources during their inquiry.Later stances include physicist Lee Smolin's 2013 essay "There Is No Scientific Method", in which he espouses two ethical principles, and historian of science Daniel Thurs' chapter in the 2015 book Newton's Apple and Other Myths about Science, which concluded that the scientific method is a myth or, at best, an idealization. As myths are beliefs, they are subject to the narrative fallacy as Taleb points out. Philosophers Robert Nola and Howard Sankey, in their 2007 book Theories of Scientific Method, said that debates over scientific method continue, and argued that Feyerabend, despite the title of Against Method, accepted certain rules of method and attempted to justify those rules with a meta methodology. 
Staddon (2017) argues it is a mistake to try following rules in the absence of an algorithmic scientific method; in that case, "science is best understood through examples". But algorithmic methods, such as disproof of existing theory by experiment have been used since Alhacen (1027) Book of Optics, and Galileo (1638) Two New Sciences, and The Assayer still stand as scientific method. They contradict Feyerabend's stance. 
The ubiquitous element in the scientific method is empiricism. This is in opposition to stringent forms of rationalism: the scientific method embodies the position that reason alone cannot solve a particular scientific problem. A strong formulation of the scientific method is not always aligned with a form of empiricism in which the empirical data is put forward in the form of experience or other abstracted forms of knowledge; in current scientific practice, however, the use of scientific modelling and reliance on abstract typologies and theories is normally accepted. The scientific method counters claims that revelation, political or religious dogma, appeals to tradition, commonly held beliefs, common sense, or currently held theories pose the only possible means of demonstrating truth.Different early expressions of empiricism and the scientific method can be found throughout history, for instance with the ancient Stoics, Epicurus, Alhazen, Avicenna, Al-Biruni, Roger Bacon, and William of Ockham. From the 16th century onwards, experiments were advocated by Francis Bacon, and performed by Giambattista della Porta, Johannes Kepler, and Galileo Galilei. There was particular development aided by theoretical works by Francisco Sanches, John Locke, George Berkeley, and David Hume.
A sea voyage from America to Europe afforded C. S. Peirce the distance to clarify his ideas, gradually resulting in the hypothetico-deductive model. Formulated in the 20th century, the model has undergone significant revision since first proposed (for a more formal discussion, see § Elements of the scientific method).

Overview
The scientific method is the process by which science is carried out. As in other areas of inquiry, science (through the scientific method) can build on previous knowledge and develop a more sophisticated understanding of its topics of study over time. This model can be seen to underlie the scientific revolution.

Process
The overall process involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions to determine whether the original conjecture was correct. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, these actions are better considered as general principles. Not all steps take place in every scientific inquiry (nor to the same degree), and they are not always done in the same order. As noted by scientist and philosopher William Whewell (1794–1866), "invention, sagacity, [and] genius" are required at every step.

Formulation of a question
The question can refer to the explanation of a specific observation, as in "Why is the sky blue?" but can also be open-ended, as in "How can I design a drug to cure this particular disease?" This stage frequently involves finding and evaluating evidence from previous experiments, personal scientific observations or assertions, as well as the work of other scientists. If the answer is already known, a different question that builds on the evidence can be posed. When applying the scientific method to research, determining a good question can be very difficult and it will affect the outcome of the investigation.

Hypothesis
A hypothesis is a conjecture, based on knowledge obtained while formulating the question, that may explain any given behavior. The hypothesis might be very specific; for example, Einstein's equivalence principle or Francis Crick's "DNA makes RNA makes protein", or it might be broad; for example, "unknown species of life dwell in the unexplored depths of the oceans". See § Hypothesis development
A statistical hypothesis is a conjecture about a given statistical population. For example, the population might be people with a particular disease. One conjecture might be that a new drug will cure the disease in some of the people in that population, as in a clinical trial of the drug. A null hypothesis would conjecture that the statistical hypothesis is false; for example, that the new drug does nothing, and that any cure in the population would be caused by chance (a random variable).
An alternative to the null hypothesis, to be falsifiable, must say that a treatment program with the drug does better than chance. To test the statement a treatment program with the drug does better than chance, an experiment is designed in which a portion of the population (the control group), is to be left untreated, while another, separate portion of the population is to be treated. t-Tests could then specify how large the treated groups, and how large the control groups are to be, in order to infer whether some course of treatment of the population has resulted in a cure of some of them, in each of the groups. The groups are examined, in turn by the researchers, in a protocol.Strong inference could alternatively propose multiple alternative hypotheses embodied in randomized controlled trials, treatments A, B, C, ... , (say in a blinded experiment with varying dosages, or with lifestyle changes, and so forth) so as not to introduce confirmation bias in favor of a specific course of treatment. Ethical considerations could be used, to minimize the numbers in the untreated groups, e.g., use almost every treatment in every group, but excluding A, B, C, ..., respectively as controls.

Prediction
The prediction step deduces the logical consequences of the hypothesis before the outcome is known. These predictions are expectations for the results of testing. If the result is already known, it is evidence that is ready to be considered in acceptance or rejection of the hypothesis. 
The evidence is also stronger if the actual result of the predictive test is not already known, as tampering with the test can be ruled out, as can hindsight bias (see postdiction). Ideally, the prediction must also distinguish the hypothesis from likely alternatives; if two hypotheses make the same prediction, observing the prediction to be correct is not evidence for either one over the other. (These statements about the relative strength of evidence can be mathematically derived using Bayes' Theorem).The consequence, therefore, is to be stated at the same time or briefly after the statement of the hypothesis, but before the experimental result is known.
Likewise, the test protocol is to be stated before execution of the test. These requirements become precautions against tampering, and aid the reproducibility of the experiment.

Testing
Suitable tests of a hypothesis compare the expected values from the tests of that hypothesis with the actual results of those tests. Scientists (and other people) can then secure, or discard, their hypotheses by conducting suitable experiments.

Analysis
An analysis determines, from the results of the experiment, the next actions to take. The expected values from the test of the alternative hypothesis are compared to the expected values resulting from the null hypothesis (that is, a prediction of no difference in the status quo). The difference between expected versus actual indicates which hypothesis better explains the resulting data from the experiment. In cases where an experiment is repeated many times, a statistical analysis such as a chi-squared test whether the null hypothesis is true, may be required.
Evidence from other scientists, and from experience are available for incorporation at any stage in the process. Depending on the complexity of the experiment, iteration of the process may be required to gather sufficient evidence to answer the question with confidence, or to build up other answers to highly specific questions, to answer a single broader question.
When the evidence has falsified the alternative hypothesis, a new hypothesis is required; if the evidence does not conclusively justify discarding the alternative hypothesis, other predictions from the alternative hypothesis might be considered. Pragmatic considerations, such as the resources available to continue inquiry, might guide the investigation's further course. When evidence for a hypothesis strongly supports that hypothesis, further questioning can follow, for insight into the broader inquiry under investigation.

DNA example
The basic elements of the scientific method are illustrated by the following example (which occurred from 1944 to 1953) from the discovery of the structure of DNA:

Question: Previous investigation of DNA had determined its chemical composition (the four nucleotides), the structure of each individual nucleotide, and other properties. DNA had been identified as the carrier of genetic information by the Avery–MacLeod–McCarty experiment in 1944, but the mechanism of how genetic information was stored in DNA was unclear.
Hypothesis: Linus Pauling, Francis Crick and James D. Watson hypothesized that DNA had a helical structure.
Prediction: If DNA had a helical structure, its X-ray diffraction pattern would be X-shaped. This prediction was determined using the mathematics of the helix transform, which had been derived by Cochran, Crick, and Vand (and independently by Stokes). This prediction was a mathematical construct, completely independent from the biological problem at hand.
 Experiment: Rosalind Franklin used pure DNA to perform X-ray diffraction to produce photo 51. The results showed an X-shape.
Analysis: When Watson saw the detailed diffraction pattern, he immediately recognized it as a helix. He and Crick then produced their model, using this information along with the previously known information about DNA's composition, especially Chargaff's rules of base pairing.The discovery became the starting point for many further studies involving the genetic material, such as the field of molecular genetics, and it was awarded the Nobel Prize in 1962. Each step of the example is examined in more detail later in the article.

Other components
The scientific method also includes other components required even when all the iterations of the steps above have been completed:

Replication
If an experiment cannot be repeated to produce the same results, this implies that the original results might have been in error. As a result, it is common for a single experiment to be performed multiple times, especially when there are uncontrolled variables or other indications of experimental error. For significant or surprising results, other scientists may also attempt to replicate the results for themselves, especially if those results would be important to their own work.
Replication has become a contentious issue in social and biomedical science where treatments are administered to groups of individuals. Typically an experimental group gets the treatment, such as a drug, and the control group gets a placebo. John Ioannidis in 2005 pointed out that the method being used has led to many findings that cannot be replicated.

External review
The process of peer review involves evaluation of the experiment by experts, who typically give their opinions anonymously. Some journals request that the experimenter provide lists of possible peer reviewers, especially if the field is highly specialized. Peer review does not certify the correctness of the results, only that, in the opinion of the reviewer, the experiments themselves were sound (based on the description supplied by the experimenter). If the work passes peer review, which occasionally may require new experiments requested by the reviewers, it will be published in a peer-reviewed scientific journal. The specific journal that publishes the results indicates the perceived quality of the work.

Data recording and sharing
Scientists typically are careful in recording their data, a requirement promoted by Ludwik Fleck (1896–1961) and others. Though not typically required, they might be requested to supply this data to other scientists who wish to replicate their original results (or parts of their original results), extending to the sharing of any experimental samples that may be difficult to obtain. See §Communication and community.

Instrumentation
Institutional researchers might acquire an instrument to institutionalize their tests. These instruments would use observations of the real world, which might agree with, or perhaps conflict with, their predictions deduced from their hypothesis. These institutions thereby reduce the research function to a cost/benefit, which is expressed as money, and the time and attention of the researchers to be expended, in exchange for a report to their constituents.Current large instruments, such as CERN's Large Hadron Collider (LHC), or LIGO, or the National Ignition Facility (NIF), or the International Space Station (ISS), or the James Webb Space Telescope (JWST), entail expected costs of billions of dollars, and timeframes extending over decades. These kinds of institutions affect public policy, on a national or even international basis, and the researchers would require shared access to such machines and their adjunct infrastructure. See Perceptual control theory, §Open-loop and closed-loop feedback

Elements of the scientific method
There are different ways of outlining the basic method used for scientific inquiry. The scientific community and philosophers of science generally agree on the following classification of method components. These methodological elements and organization of procedures tend to be more characteristic of experimental sciences than social sciences. Nonetheless, the cycle of formulating hypotheses, testing and analyzing the results, and formulating new hypotheses, will resemble the cycle described below.
The scientific method is an iterative, cyclical process through which information is continually revised. It is generally recognized to develop advances in knowledge through the following elements, in varying combinations or contributions:
Characterizations (observations, definitions, and measurements of the subject of inquiry)
Hypotheses (theoretical, hypothetical explanations of observations and measurements of the subject)
Predictions (inductive and deductive reasoning from the hypothesis or theory)
Experiments (tests of all of the above)Each element of the scientific method is subject to peer review for possible mistakes. These activities do not describe all that scientists do but apply mostly to experimental sciences (e.g., physics, chemistry, biology, and psychology). The elements above are often taught in the educational system as "the scientific method".The scientific method is not a single recipe: it requires intelligence, imagination, and creativity. In this sense, it is not a mindless set of standards and procedures to follow, but is rather an ongoing cycle, constantly developing more useful, accurate, and comprehensive models and methods. For example, when Einstein developed the Special and General Theories of Relativity, he did not in any way refute or discount Newton's Principia. On the contrary, if the astronomically massive, the feather-light, and the extremely fast are removed from Einstein's theories – all phenomena Newton could not have observed – Newton's equations are what remain. Einstein's theories are expansions and refinements of Newton's theories and, thus, increase confidence in Newton's work.
An iterative, pragmatic scheme of the four points above is sometimes offered as a guideline for proceeding:
Define a question
Gather information and resources (observe)
Form an explanatory hypothesis
Test the hypothesis by performing an experiment and collecting data in a reproducible manner
Analyze the data
Interpret the data and draw conclusions that serve as a starting point for a new hypothesis
Publish results
Retest (frequently done by other scientists)The iterative cycle inherent in this step-by-step method goes from point 3 to 6 back to 3 again.
While this schema outlines a typical hypothesis/testing method, many philosophers, historians, and sociologists of science, including Paul Feyerabend, claim that such descriptions of scientific method have little relation to the ways that science is actually practiced.

Characterizations
The scientific method depends upon increasingly sophisticated characterizations of the subjects of investigation. (The subjects can also be called unsolved problems or the unknowns.) For example, Benjamin Franklin conjectured, correctly, that St. Elmo's fire was electrical in nature, but it has taken a long series of experiments and theoretical changes to establish this. While seeking the pertinent properties of the subjects, careful thought may also entail some definitions and observations; the observations often demand careful measurements and/or counting.
The systematic, careful collection of measurements or counts of relevant quantities is often the critical difference between pseudo-sciences, such as alchemy, and science, such as chemistry or biology. Scientific measurements are usually tabulated, graphed, or mapped, and statistical manipulations, such as correlation and regression, performed on them. The measurements might be made in a controlled setting, such as a laboratory, or made on more or less inaccessible or unmanipulatable objects such as stars or human populations. The measurements often require specialized scientific instruments such as thermometers, spectroscopes, particle accelerators, or voltmeters, and the progress of a scientific field is usually intimately tied to their invention and improvement.

I am not accustomed to saying anything with certainty after only one or two observations.

Uncertainty
Measurements in scientific work are also usually accompanied by estimates of their uncertainty. The uncertainty is often estimated by making repeated measurements of the desired quantity. Uncertainties may also be calculated by consideration of the uncertainties of the individual underlying quantities used. Counts of things, such as the number of people in a nation at a particular time, may also have an uncertainty due to data collection limitations. Or counts may represent a sample of desired quantities, with an uncertainty that depends upon the sampling method used and the number of samples taken.

Definition
Measurements demand the use of operational definitions of relevant quantities. That is, a scientific quantity is described or defined by how it is measured, as opposed to some more vague, inexact, or "idealized" definition. For example, electric current, measured in amperes, may be operationally defined in terms of the mass of silver deposited in a certain time on an electrode in an electrochemical device that is described in some detail. The operational definition of a thing often relies on comparisons with standards: the operational definition of "mass" ultimately relies on the use of an artifact, such as a particular kilogram of platinum-iridium kept in a laboratory in France.
The scientific definition of a term sometimes differs substantially from its natural language usage. For example, mass and weight overlap in meaning in common discourse, but have distinct meanings in mechanics. Scientific quantities are often characterized by their units of measure which can later be described in terms of conventional physical units when communicating the work.
New theories are sometimes developed after realizing certain terms have not previously been sufficiently clearly defined. For example, Albert Einstein's first paper on relativity begins by defining simultaneity and the means for determining length. These ideas were skipped over by Isaac Newton with, "I do not define time, space, place and motion, as being well known to all." Einstein's paper then demonstrates that they (viz., absolute time and length independent of motion) were approximations. Francis Crick cautions us that when characterizing a subject, however, it can be premature to define something when it remains ill-understood. In Crick's study of consciousness, he actually found it easier to study awareness in the visual system, rather than to study free will, for example. His cautionary example was the gene; the gene was much more poorly understood before Watson and Crick's pioneering discovery of the structure of DNA; it would have been counterproductive to spend much time on the definition of the gene, before them.

DNA-characterizations
The history of the discovery of the structure of DNA is a classic example of the elements of the scientific method: in 1950 it was known that genetic inheritance had a mathematical description, starting with the studies of Gregor Mendel, and that DNA contained genetic information (Oswald Avery's transforming principle). But the mechanism of storing genetic information (i.e., genes) in DNA was unclear. Researchers in Bragg's laboratory at Cambridge University made X-ray diffraction pictures of various molecules, starting with crystals of salt, and proceeding to more complicated substances. Using clues painstakingly assembled over decades, beginning with its chemical composition, it was determined that it should be possible to characterize the physical structure of DNA, and the X-ray images would be the vehicle. ..2. DNA-hypotheses

Another example: precession of Mercury
The characterization element can require extended and extensive study, even centuries. It took thousands of years of measurements, from the Chaldean, Indian, Persian, Greek, Arabic, and European astronomers, to fully record the motion of planet Earth. Newton was able to include those measurements into the consequences of his laws of motion. But the perihelion of the planet Mercury's orbit exhibits a precession that cannot be fully explained by Newton's laws of motion (see diagram to the right), as Leverrier pointed out in 1859. The observed difference for Mercury's precession between Newtonian theory and observation was one of the things that occurred to Albert Einstein as a possible early test of his theory of General relativity. His relativistic calculations matched observation much more closely than did Newtonian theory. The difference is approximately 43 arc-seconds per century.

Hypothesis development
A hypothesis is a suggested explanation of a phenomenon, or alternately a reasoned proposal suggesting a possible correlation between or among a set of phenomena.
Normally hypotheses have the form of a mathematical model. Sometimes, but not always, they can also be formulated as existential statements, stating that some particular instance of the phenomenon being studied has some characteristic and causal explanations, which have the general form of universal statements, stating that every instance of the phenomenon has a particular characteristic.
Scientists are free to use whatever resources they have – their own creativity, ideas from other fields, inductive reasoning, Bayesian inference, and so on – to imagine possible explanations for a phenomenon under study. Albert Einstein once observed that "there is no logical bridge between phenomena and their theoretical principles." Charles Sanders Peirce, borrowing a page from Aristotle (Prior Analytics, 2.25) described the incipient stages of inquiry, instigated by the "irritation of doubt" to venture a plausible guess, as abductive reasoning.: II, p.290  The history of science is filled with stories of scientists claiming a "flash of inspiration", or a hunch, which then motivated them to look for evidence to support or refute their idea. Michael Polanyi made such creativity the centerpiece of his discussion of methodology.
William Glen observes that
the success of a hypothesis, or its service to science, lies not simply in its perceived "truth", or power to displace, subsume or reduce a predecessor idea, but perhaps more in its ability to stimulate the research that will illuminate ... bald suppositions and areas of vagueness.
In general scientists tend to look for theories that are "elegant" or "beautiful". Scientists often use these terms to refer to a theory that is following the known facts but is nevertheless relatively simple and easy to handle. Occam's Razor serves as a rule of thumb for choosing the most desirable amongst a group of equally explanatory hypotheses.
To minimize the confirmation bias which results from entertaining a single hypothesis, strong inference emphasizes the need for entertaining multiple alternative hypotheses.

DNA-hypotheses
Linus Pauling proposed that DNA might be a triple helix. This hypothesis was also considered by Francis Crick and James D. Watson but discarded. When Watson and Crick learned of Pauling's hypothesis, they understood from existing data that Pauling was wrong. and that Pauling would soon admit his difficulties with that structure. So, the race was on to figure out the correct structure (except that Pauling did not realize at the time that he was in a race) ..3. DNA-predictions

Predictions from the hypothesis
Any useful hypothesis will enable predictions, by reasoning including deductive reasoning. It might predict the outcome of an experiment in a laboratory setting or the observation of a phenomenon in nature. The prediction can also be statistical and deal only with probabilities.
It is essential that the outcome of testing such a prediction be currently unknown. Only in this case does a successful outcome increase the probability that the hypothesis is true. If the outcome is already known, it is called a consequence and should have already been considered while formulating the hypothesis.
If the predictions are not accessible by observation or experience, the hypothesis is not yet testable and so will remain to that extent unscientific in a strict sense. A new technology or theory might make the necessary experiments feasible. For example, while a hypothesis on the existence of other intelligent species may be convincing with scientifically based speculation, no known experiment can test this hypothesis. Therefore, science itself can have little to say about the possibility. In the future, a new technique may allow for an experimental test and the speculation would then become part of accepted science.

DNA-predictions
James D. Watson, Francis Crick, and others hypothesized that DNA had a helical structure. This implied that DNA's X-ray diffraction pattern would be 'x shaped'. This prediction followed from the work of Cochran, Crick and Vand (and independently by Stokes). The Cochran-Crick-Vand-Stokes theorem provided a mathematical explanation for the empirical observation that diffraction from helical structures produces x shaped patterns.
In their first paper, Watson and Crick also noted that the double helix structure they proposed provided a simple mechanism for DNA replication, writing, "It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material".  ..4. DNA-experiments

Another example: general relativity
Einstein's theory of general relativity makes several specific predictions about the observable structure of spacetime, such as that light bends in a gravitational field, and that the amount of bending depends in a precise way on the strength of that gravitational field. Arthur Eddington's observations made during a 1919 solar eclipse supported General Relativity rather than Newtonian gravitation.

Experiments
Once predictions are made, they can be sought by experiments. If the test results contradict the predictions, the hypotheses which entailed them are called into question and become less tenable. Sometimes the experiments are conducted incorrectly or are not very well designed when compared to a crucial experiment. If the experimental results confirm the predictions, then the hypotheses are considered more likely to be correct, but might still be wrong and continue to be subject to further testing. The experimental control is a technique for dealing with observational error. This technique uses the contrast between multiple samples, or observations, or populations, under differing conditions, to see what varies or what remains the same. We vary the conditions for the acts of measurement, to help isolate what has changed. Mill's canons can then help us figure out what the important factor is. Factor analysis is one technique for discovering the important factor in an effect.
Depending on the predictions, the experiments can have different shapes. It could be a classical experiment in a laboratory setting, a double-blind study or an archaeological excavation. Even taking a plane from New York to Paris is an experiment that tests the aerodynamical hypotheses used for constructing the plane.
Scientists assume an attitude of openness and accountability on the part of those experimenting. Detailed record-keeping is essential, to aid in recording and reporting on the experimental results, and supports the effectiveness and integrity of the procedure. They will also assist in reproducing the experimental results, likely by others. Traces of this approach can be seen in the work of Hipparchus (190–120 BCE), when determining a value for the precession of the Earth, while controlled experiments can be seen in the works of al-Battani (853–929 CE) and Alhazen (965–1039 CE).

DNA-experiments
Watson and Crick showed an initial (and incorrect) proposal for the structure of DNA to a team from King's College London – Rosalind Franklin, Maurice Wilkins, and Raymond Gosling. Franklin immediately spotted the flaws which concerned the water content. Later Watson saw Franklin's detailed X-ray diffraction images which showed an X-shape and was able to confirm the structure was helical. This rekindled Watson and Crick's model building and led to the correct structure. ..1. DNA-characterizations

Evaluation and improvement
The scientific method is iterative. At any stage, it is possible to refine its accuracy and precision, so that some consideration will lead the scientist to repeat an earlier part of the process. Failure to develop an interesting hypothesis may lead a scientist to re-define the subject under consideration. Failure of a hypothesis to produce interesting and testable predictions may lead to reconsideration of the hypothesis or of the definition of the subject. Failure of an experiment to produce interesting results may lead a scientist to reconsider the experimental method, the hypothesis, or the definition of the subject.
By 1027, Alhazen, based on his measurements of the refraction of light, was able to deduce that outer space was less dense than air, that is: "the body of the heavens is rarer than the body of air". In 1079 Ibn Mu'adh's Treatise On Twilight was able to infer that Earth's atmosphere was 50 miles thick, based on atmospheric refraction of the sun's rays.Other scientists may start their own research and enter the process at any stage. They might adopt the characterization and formulate their own hypothesis, or they might adopt the hypothesis and deduce their own predictions. Often the experiment is not done by the person who made the prediction, and the characterization is based on experiments done by someone else. Published results of experiments can also serve as a hypothesis predicting their own reproducibility.

DNA-iterations
After considerable fruitless experimentation, being discouraged by their superior from continuing, and numerous false starts, Watson and Crick were able to infer the essential structure of DNA by concrete modeling of the physical shapes of the nucleotides which comprise it. They were guided by the bond lengths which had been deduced by Linus Pauling and by Rosalind Franklin's X-ray diffraction images. ..DNA Example

Confirmation
Science is a social enterprise, and scientific work tends to be accepted by the scientific community when it has been confirmed. Crucially, experimental and theoretical results must be reproduced by others within the scientific community. Researchers have given their lives for this vision; Georg Wilhelm Richmann was killed by ball lightning (1753) when attempting to replicate the 1752 kite-flying experiment of Benjamin Franklin.To protect against bad science and fraudulent data, government research-granting agencies such as the National Science Foundation, and science journals, including Nature and Science, have a policy that researchers must archive their data and methods so that other researchers can test the data and methods and build on the research that has gone before. Scientific data archiving can be done at several national archives in the U.S. or the World Data Center.

Scientific inquiry
Scientific inquiry generally aims to obtain knowledge in the form of testable explanations that scientists can use to predict the results of future experiments. This allows scientists to gain a better understanding of the topic under study, and later to use that understanding to intervene in its causal mechanisms (such as to cure disease). The better an explanation is at making predictions, the more useful it frequently can be, and the more likely it will continue to explain a body of evidence better than its alternatives. The most successful explanations – those which explain and make accurate predictions in a wide range of circumstances – are often called scientific theories.Most experimental results do not produce large changes in human understanding; improvements in theoretical scientific understanding typically result from a gradual process of development over time, sometimes across different domains of science. Scientific models vary in the extent to which they have been experimentally tested and for how long, and in their acceptance in the scientific community. In general, explanations become accepted over time as evidence accumulates on a given topic, and the explanation in question proves more powerful than its alternatives at explaining the evidence. Often subsequent researchers re-formulate the explanations over time, or combined explanations to produce new explanations.
Tow sees the scientific method in terms of an evolutionary algorithm applied to science and technology. See Ceteris paribus, and Mutatis mutandis

Properties of scientific inquiry
Scientific knowledge is closely tied to empirical findings and can remain subject to falsification if new experimental observations are incompatible with what is found. That is, no theory can ever be considered final since new problematic evidence might be discovered. If such evidence is found, a new theory may be proposed, or (more commonly) it is found that modifications to the previous theory are sufficient to explain the new evidence. The strength of a theory relates to how long it has persisted without major alteration to its core principles (see invariant explanations).
Theories can also become subsumed by other theories. For example, Newton's laws explained thousands of years of scientific observations of the planets almost perfectly. However, these laws were then determined to be special cases of a more general theory (relativity), which explained both the (previously unexplained) exceptions to Newton's laws and predicted and explained other observations such as the deflection of light by gravity. Thus, in certain cases independent, unconnected, scientific observations can be connected, unified by principles of increasing explanatory power.Since new theories might be more comprehensive than what preceded them, and thus be able to explain more than previous ones, successor theories might be able to meet a higher standard by explaining a larger body of observations than their predecessors. For example, the theory of evolution explains the diversity of life on Earth, how species adapt to their environments, and many other patterns observed in the natural world; its most recent major modification was unification with genetics to form the modern evolutionary synthesis. In subsequent modifications, it has also subsumed aspects of many other fields such as biochemistry and molecular biology.

Beliefs and biases
Scientific methodology often directs that hypotheses be tested in controlled conditions wherever possible. This is frequently possible in certain areas, such as in the biological sciences, and more difficult in other areas, such as in astronomy.
The practice of experimental control and reproducibility can have the effect of diminishing the potentially harmful effects of circumstance, and to a degree, personal bias. For example, pre-existing beliefs can alter the interpretation of results, as in confirmation bias; this is a heuristic that leads a person with a particular belief to see things as reinforcing their belief, even if another observer might disagree (in other words, people tend to observe what they expect to observe).
[T]he action of thought is excited by the irritation of doubt, and ceases when belief is attained.
A historical example is the belief that the legs of a galloping horse are splayed at the point when none of the horse's legs touch the ground, to the point of this image being included in paintings by its supporters. However, the first stop-action pictures of a horse's gallop by Eadweard Muybridge showed this to be false, and that the legs are instead gathered together.Another important human bias that plays a role is a preference for new, surprising statements (see Appeal to novelty), which can result in a search for evidence that the new is true. Poorly attested beliefs can be believed and acted upon via a less rigorous heuristic.Goldhaber and Nieto published in 2010 the observation that if theoretical structures with "many closely neighboring subjects are described by connecting theoretical concepts, then the theoretical structure acquires a robustness which makes it increasingly hard – though certainly never impossible – to overturn". When a narrative is constructed its elements become easier to believe.Fleck 1979, p. 27 notes "Words and ideas are originally phonetic and mental equivalences of the experiences coinciding with them. ... Such proto-ideas are at first always too broad and insufficiently specialized. ... Once a structurally complete and closed system of opinions consisting of many details and relations has been formed, it offers enduring resistance to anything that contradicts it". Sometimes, these relations have their elements assumed a priori, or contain some other logical or methodological flaw in the process that ultimately produced them. Donald M. MacKay has analyzed these elements in terms of limits to the accuracy of measurement and has related them to instrumental elements in a category of measurement.

Models of scientific inquiry
Classical model
The classical model of scientific inquiry derives from Aristotle, who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy.

Hypothetico-deductive model
The hypothetico-deductive model or method is a proposed description of the scientific method. Here, predictions from the hypothesis are central: if you assume the hypothesis to be true, what consequences follow?
If a subsequent empirical investigation does not demonstrate that these consequences or predictions correspond to the observable world, the hypothesis can be concluded to be false.

Pragmatic model
In 1877, Charles Sanders Peirce (1839–1914) characterized inquiry in general not as the pursuit of truth per se but as the struggle to move from irritating, inhibitory doubts born of surprises, disagreements, and the like, and to reach a secure belief, the belief being that on which one is prepared to act. He framed scientific inquiry as part of a broader spectrum and as spurred, like inquiry generally, by actual doubt, not mere verbal or hyperbolic doubt, which he held to be fruitless. He outlined four methods of settling opinion, ordered from least to most successful:

The method of tenacity (policy of sticking to initial belief) – which brings comforts and decisiveness but leads to trying to ignore contrary information and others' views as if truth were intrinsically private, not public. It goes against the social impulse and easily falters since one may well notice when another's opinion is as good as one's own initial opinion. Its successes can shine but tend to be transitory.
The method of authority – which overcomes disagreements but sometimes brutally. Its successes can be majestic and long-lived, but it cannot operate thoroughly enough to suppress doubts indefinitely, especially when people learn of other societies' present and past.
The method of the a priori – which promotes conformity less brutally but fosters opinions as something like tastes, arising in conversation and comparisons of perspectives in terms of "what is agreeable to reason." Thereby it depends on fashion in paradigms and goes in circles over time. It is more intellectual and respectable but, like the first two methods, sustains accidental and capricious beliefs, destining some minds to doubt it.
The scientific method – the method wherein inquiry regards itself as fallible and purposely tests itself and criticizes, corrects, and improves itself.Peirce held that slow, stumbling ratiocination can be dangerously inferior to instinct and traditional sentiment in practical matters, and that the scientific method is best suited to theoretical research, which in turn should not be trammeled by the other methods and practical ends; reason's "first rule" is that, in order to learn, one must desire to learn and, as a corollary, must not block the way of inquiry. The scientific method excels the others by being deliberately designed to arrive – eventually – at the most secure beliefs, upon which the most successful practices can be based. Starting from the idea that people seek not truth per se but instead to subdue irritating, inhibitory doubt, Peirce showed how, through the struggle, some can come to submit to the truth for the sake of belief's integrity, seek as truth the guidance of potential practice correctly to its given goal, and wed themselves to the scientific method.For Peirce, rational inquiry implies presuppositions about truth and the real; to reason is to presuppose (and at least to hope), as a principle of the reasoner's self-regulation, that the real is discoverable and independent of our vagaries of opinion. In that vein, he defined truth as the correspondence of a sign (in particular, a proposition) to its object and, pragmatically, not as the actual consensus of some definite, finite community (such that to inquire would be to poll the experts), but instead as that final opinion which all investigators would reach sooner or later but still inevitably, if they were to push investigation far enough, even when they start from different points. In tandem he defined the real as a true sign's object (be that object a possibility or quality, or an actuality or brute fact, or a necessity or norm or law), which is what it is independently of any finite community's opinion and, pragmatically, depends only on the final opinion destined in a sufficient investigation. That is a destination as far, or near, as the truth itself to you or me or the given finite community. Thus, his theory of inquiry boils down to "Do the science." Those conceptions of truth and the real involve the idea of a community both without definite limits (and thus potentially self-correcting as far as needed) and capable of definite increase of knowledge. As inference, "logic is rooted in the social principle" since it depends on a standpoint that is, in a sense, unlimited.Paying special attention to the generation of explanations, Peirce outlined the scientific method as coordination of three kinds of inference in a purposeful cycle aimed at settling doubts, as follows (in §III–IV in "A Neglected Argument" except as otherwise noted):

Abduction (or retroduction). Guessing, inference to explanatory hypotheses for selection of those best worth trying. From abduction, Peirce distinguishes induction as inferring, based on tests, the proportion of truth in the hypothesis. Every inquiry, whether into ideas, brute facts, or norms and laws, arises from surprising observations in one or more of those realms (and for example at any stage of an inquiry already underway). All explanatory content of theories comes from abduction, which guesses a new or outside idea to account in a simple, economical way for a surprising or complicative phenomenon. Oftenest, even a well-prepared mind guesses wrong. But the modicum of success of our guesses far exceeds that of sheer luck and seems born of attunement to nature by instincts developed or inherent, especially insofar as best guesses are optimally plausible and simple in the sense, said Peirce, of the "facile and natural", as by Galileo's natural light of reason and as distinct from "logical simplicity". Abduction is the most fertile but least secure mode of inference. Its general rationale is inductive: it succeeds often enough and, without it, there is no hope of sufficiently expediting inquiry (often multi-generational) toward new truths. Coordinative method leads from abducing a plausible hypothesis to judging it for its testability and for how its trial would economize inquiry itself. Peirce calls his pragmatism "the logic of abduction". His pragmatic maxim is: "Consider what effects that might conceivably have practical bearings you conceive the objects of your conception to have. Then, your conception of those effects is the whole of your conception of the object". His pragmatism is a method of reducing conceptual confusions fruitfully by equating the meaning of any conception with the conceivable practical implications of its object's conceived effects – a method of experimentational mental reflection hospitable to forming hypotheses and conducive to testing them. It favors efficiency. The hypothesis, being insecure, needs to have practical implications leading at least to mental tests and, in science, lending themselves to scientific tests. A simple but unlikely guess, if uncostly to test for falsity, may belong first in line for testing. A guess is intrinsically worth testing if it has instinctive plausibility or reasoned objective probability, while subjective likelihood, though reasoned, can be misleadingly seductive. Guesses can be chosen for trial strategically, for their caution (for which Peirce gave as an example the game of Twenty Questions), breadth, and incomplexity. One can hope to discover only that which time would reveal through a learner's sufficient experience anyway, so the point is to expedite it; the economy of research is what demands the leap, so to speak, of abduction and governs its art.

Deduction. Two stages:
Explication. Unclearly premised, but deductive, analysis of the hypothesis in order to render its parts as clear as possible.
Demonstration: Deductive argumentation, Euclidean in procedure. Explicit deduction of hypothesis's consequences as predictions, for induction to test, about evidence to be found. Corollarial or, if needed, theorematic.

Induction. The long-run validity of the rule of induction is deducible from the principle (presuppositional to reasoning, in general,) that the real is only the object of the final opinion to which adequate investigation would lead; anything to which no such process would ever lead would not be real. Induction involving ongoing tests or observations follows a method which, sufficiently persisted in, will diminish its error below any predesignate degree. Three stages:
Classification. Unclearly premised, but inductive, classing of objects of experience under general ideas.
Probation: direct inductive argumentation. Crude (the enumeration of instances) or gradual (new estimate of the proportion of truth in the hypothesis after each test). Gradual induction is qualitative or quantitative; if qualitative, then dependent on weightings of qualities or characters; if quantitative, then dependent on measurements, or on statistics, or on countings.
Sentential Induction. "... which, by inductive reasonings, appraises the different probations singly, then their combinations, then makes self-appraisal of these very appraisals themselves, and passes final judgment on the whole result".

Invariant explanation
In a 2009 TED talk, Deutsch expounded a criterion for scientific explanation, which is to formulate invariants: "State an explanation [publicly, so that it can be dated and verified by others later] that remains invariant [in the face of apparent change, new information, or unexpected conditions]".
"A bad explanation is easy to vary.": minute 11:22 
"The search for hard-to-vary explanations is the origin of all progress": minute 15:05 
"That the truth consists of hard-to-vary assertions about reality is the most important fact about the physical world.": minute 16:15 Invariance as a fundamental aspect of a scientific account of reality had long been part of philosophy of science: for example, Friedel Weinert's book The Scientist as Philosopher (2004) noted the presence of the theme in many writings from around 1900 onward, such as works by Henri Poincaré (1902), Ernst Cassirer (1920), Max Born (1949 and 1953), Paul Dirac (1958), Olivier Costa de Beauregard (1966), Eugene Wigner (1967), Lawrence Sklar (1974), Michael Friedman (1983), John D. Norton (1992), Nicholas Maxwell (1993), Alan Cook (1994), Alistair Cameron Crombie (1994), Margaret Morrison (1995), Richard Feynman (1997), Robert Nozick (2001), and Tim Maudlin (2002).

Communication and community
Frequently the scientific method is employed not only by a single person but also by several people cooperating directly or indirectly. Such cooperation can be regarded as an important element of a scientific community. Various standards of scientific methodology are used within such an environment.

Peer review evaluation
Scientific journals use a process of peer review, in which scientists' manuscripts are submitted by editors of scientific journals to (usually one to three, and usually anonymous) fellow scientists familiar with the field for evaluation. In certain journals, the journal itself selects the referees; while in others (especially journals that are extremely specialized), the manuscript author might recommend referees. The referees may or may not recommend publication, or they might recommend publication with suggested modifications, or sometimes, publication in another journal. This standard is practiced to various degrees by different journals and can have the effect of keeping the literature free of obvious errors and generally improve the quality of the material, especially in the journals that use the standard most rigorously. The peer-review process can have limitations when considering research outside the conventional scientific paradigm: problems of "groupthink" can interfere with open and fair deliberation of some new research.

Documentation and replication
Sometimes experimenters may make systematic errors during their experiments, veer from standard methods and practices (Pathological science) for various reasons, or, in rare cases, deliberately report false results. Occasionally because of this then, other scientists might attempt to repeat the experiments to duplicate the results.

Archiving
Researchers sometimes practice scientific data archiving, such as in compliance with the policies of government funding agencies and scientific journals. In these cases, detailed records of their experimental procedures, raw data, statistical analyses, and source code can be preserved to provide evidence of the methodology and practice of the procedure and assist in any potential future attempts to reproduce the result. These procedural records may also assist in the conception of new experiments to test the hypothesis, and may prove useful to engineers who might examine the potential practical applications of a discovery.

Data sharing
When additional information is needed before a study can be reproduced, the author of the study might be asked to provide it. They might provide it, or if the author refuses to share data, appeals can be made to the journal editors who published the study or to the institution which funded the research.

Limitations
Since a scientist cannot record everything that took place in an experiment, facts selected for their apparent relevance are reported. This may lead, unavoidably, to problems later if some supposedly irrelevant feature is questioned. For example, Heinrich Hertz did not report the size of the room used to test Maxwell's equations, which later turned out to account for a small deviation in the results. The problem is that parts of the theory itself need to be assumed to select and report the experimental conditions. The observations are hence sometimes described as being 'theory-laden'.

Science of complex systems
Science applied to complex systems can involve elements such as transdisciplinarity, systems theory, control theory, and scientific modelling. The Santa Fe Institute studies such systems; Murray Gell-Mann interconnects these topics with message passing.Some biological systems, such those involved in proprioception, have been fruitfully modeled by engineering techniques.In general, the scientific method may be difficult to apply stringently to diverse, interconnected systems and large data sets. In particular, practices used within Big data, such as predictive analytics, may be considered to be at odds with the scientific method, as some of the data may have been stripped of the parameters which might be material in alternative hypotheses for an explanation; thus the stripped data would only serve to support the null hypothesis in the predictive analytics application. Fleck 1979, pp. 38–50 notes "a scientific discovery remains incomplete without considerations of the social practices that condition it".

Philosophy and sociology of science
Analytical philosophy
Philosophy of science looks at the underpinning logic of the scientific method, at what separates science from non-science, and the ethic that is implicit in science. There are basic assumptions, derived from philosophy by at least one prominent scientist, that form the base of the scientific method – namely, that reality is objective and consistent, that humans have the capacity to perceive reality accurately, and that rational explanations exist for elements of the real world. These assumptions from methodological naturalism form a basis on which science may be grounded. Logical positivist, empiricist, falsificationist, and other theories have criticized these assumptions and given alternative accounts of the logic of science, but each has also itself been criticized.
Thomas Kuhn examined the history of science in his The Structure of Scientific Revolutions, and found that the actual method used by scientists differed dramatically from the then-espoused method. His observations of science practice are essentially sociological and do not speak to how science is or can be practiced in other times and other cultures.
Norwood Russell Hanson, Imre Lakatos and Thomas Kuhn have done extensive work on the "theory-laden" character of observation. Hanson (1958) first coined the term for the idea that all observation is dependent on the conceptual framework of the observer, using the concept of gestalt to show how preconceptions can affect both observation and description. He opens Chapter 1 with a discussion of the Golgi bodies and their initial rejection as an artefact of staining technique, and a discussion of Brahe and Kepler observing the dawn and seeing a "different" sunrise despite the same physiological phenomenon. Kuhn and Feyerabend acknowledge the pioneering significance of Hanson's work.
Kuhn said the scientist generally has a theory in mind before designing and undertaking experiments to make empirical observations, and that the "route from theory to measurement can almost never be traveled backward". For Kuhn, this implies that how theory is tested is dictated by the nature of the theory itself, which led Kuhn to argue that "once it has been adopted by a profession ... no theory is recognized to be testable by any quantitative tests that it has not already passed" (revealing Kuhn's rationalist thinking style).

Post-modernism and science wars
Paul Feyerabend similarly examined the history of science, and was led to deny that science is genuinely a methodological process. In his book Against Method he argues that scientific progress is not the result of applying any particular method. In essence, he says that for any specific method or norm of science, one can find a historic episode where violating it has contributed to the progress of science. Thus, if believers in the scientific method wish to express a single universally valid rule, Feyerabend jokingly suggests, it should be 'anything goes'. However, this is uneconomic. Criticisms such as Feyerabend's led to the strong programme, a radical approach to the sociology of science.
The postmodernist critiques of science have themselves been the subject of intense controversy. This ongoing debate, known as the science wars, is the result of conflicting values and assumptions between the postmodernist and realist camps. Whereas postmodernists assert that scientific knowledge is simply another discourse (this term has special meaning in this context) and not representative of any form of fundamental truth, realists in the scientific community maintain that scientific knowledge does reveal real and fundamental truths about reality. Many books have been written by scientists which take on this problem and challenge the assertions of the postmodernists while defending science as a legitimate method of deriving truth.

Anthropology and sociology
In anthropology and sociology, following the field research in an academic scientific laboratory by Latour and Woolgar, Karin Knorr Cetina has conducted a comparative study of two scientific fields (namely high energy physics and molecular biology) to conclude that the epistemic practices and reasonings within both scientific communities are different enough to introduce the concept of "epistemic cultures", in contradiction with the idea that a so-called "scientific method" is unique and a unifying concept. Comparing 'epistemic cultures' with Fleck 1935, Thought collectives, (denkkollektiven): Entstehung und Entwicklung einer wissenschaftlichen Tatsache: Einfǖhrung in die Lehre vom Denkstil und DenkkollektivFleck 1979, p. xxvii recognizes that facts have lifetimes, flourishing only after incubation periods. His selected question for investigation (1934) was "HOW, THEN, DID THIS EMPIRICAL FACT ORIGINATE AND IN WHAT DOES IT CONSIST?". But by Fleck 1979, p.27, the thought collectives within the respective fields will have to settle on common specialized terminology, publish their results and further intercommunicate with their colleagues using the common terminology, in order to progress.

Relationship with mathematics
Science is the process of gathering, comparing, and evaluating proposed models against observables. A model can be a simulation, mathematical or chemical formula, or set of proposed steps. Science is like mathematics in that researchers in both disciplines try to distinguish what is known from what is unknown at each stage of discovery. Models, in both science and mathematics, need to be internally consistent and also ought to be falsifiable (capable of disproof). In mathematics, a statement need not yet be proved; at such a stage, that statement would be called a conjecture. But when a statement has attained mathematical proof, that statement gains a kind of immortality which is highly prized by mathematicians, and for which some mathematicians devote their lives.Mathematical work and scientific work can inspire each other. For example, the technical concept of time arose in science, and timelessness was a hallmark of a mathematical topic. But today, the Poincaré conjecture has been proved using time as a mathematical concept in which objects can flow (see Ricci flow).
Nevertheless, the connection between mathematics and reality (and so science to the extent it describes reality) remains obscure. Eugene Wigner's paper, The Unreasonable Effectiveness of Mathematics in the Natural Sciences, is a very well-known account of the issue from a Nobel Prize-winning physicist. In fact, some observers (including some well-known mathematicians such as Gregory Chaitin, and others such as Lakoff and Núñez) have suggested that mathematics is the result of practitioner bias and human limitation (including cultural ones), somewhat like the post-modernist view of science.
George Pólya's work on problem solving, the construction of mathematical proofs, and heuristic show that the mathematical method and the scientific method differ in detail, while nevertheless resembling each other in using iterative or recursive steps.

In Pólya's view, understanding involves restating unfamiliar definitions in your own words, resorting to geometrical figures, and questioning what we know and do not know already; analysis, which Pólya takes from Pappus, involves free and heuristic construction of plausible arguments, working backward from the goal, and devising a plan for constructing the proof; synthesis is the strict Euclidean exposition of step-by-step details of the proof; review involves reconsidering and re-examining the result and the path taken to it.
Building on Pólya's work, Imre Lakatos argued that mathematicians actually use contradiction, criticism, and revision as principles for improving their work. In like manner to science, where truth is sought, but certainty is not found, in Proofs and Refutations, what Lakatos tried to establish was that no theorem of informal mathematics is final or perfect. This means that we should not think that a theorem is ultimately true, only that no counterexample has yet been found. Once a counterexample, i.e. an entity contradicting/not explained by the theorem is found, we adjust the theorem, possibly extending the domain of its validity. This is a continuous way our knowledge accumulates, through the logic and process of proofs and refutations. (However, if axioms are given for a branch of mathematics, this creates a logical system —Wittgenstein 1921 Tractatus Logico-Philosophicus 5.13; Lakatos claimed that proofs from such a system were tautological, i.e. internally logically true, by rewriting forms, as shown by Poincaré, who demonstrated the technique of transforming tautologically true forms (viz. the Euler characteristic) into or out of forms from homology, or more abstractly, from homological algebra.)Lakatos proposed an account of mathematical knowledge based on Polya's idea of heuristics. In Proofs and Refutations, Lakatos gave several basic rules for finding proofs and counterexamples to conjectures. He thought that mathematical 'thought experiments' are a valid way to discover mathematical conjectures and proofs.Gauss, when asked how he came about his theorems, once replied "durch planmässiges Tattonieren" (through systematic palpable experimentation).

Relationship with statistics
When the scientific method employs statistics as a key part of its arsenal, there are mathematical and practical issues that can have a deleterious effect on the reliability of the output of scientific methods. This is described in a popular 2005 scientific paper "Why Most Published Research Findings Are False" by John Ioannidis, which is considered foundational to the field of metascience. Much research in metascience seeks to identify poor use of statistics and improve its use. See Preregistration (science)#Rationale
The particular points raised are statistical ("The smaller the studies conducted in a scientific field, the less likely the research findings are to be true" and "The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.") and economical ("The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true" and "The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.") Hence: "Most research findings are false for most research designs and for most fields" and "As shown, the majority of modern biomedical research is operating in areas with very low pre- and poststudy probability for true findings." However: "Nevertheless, most new discoveries will continue to stem from hypothesis-generating research with low or very low pre-study odds," which means that *new* discoveries will come from research that, when that research started, had low or very low odds (a low or very low chance) of succeeding. Hence, if the scientific method is used to expand the frontiers of knowledge, research into areas that are outside the mainstream will yield the newest discoveries. See: Expected value of sample information, False positives and false negatives, Test statistic, and Type I and type II errors

Role of chance in discovery
Somewhere between 33% and 50% of all scientific discoveries are estimated to have been stumbled upon, rather than sought out. This may explain why scientists so often express that they were lucky. Louis Pasteur is credited with the famous saying that "Luck favours the prepared mind", but some psychologists have begun to study what it means to be 'prepared for luck' in the scientific context. Research is showing that scientists are taught various heuristics that tend to harness chance and the unexpected. This is what Nassim Nicholas Taleb calls "Anti-fragility"; while some systems of investigation are fragile in the face of human error, human bias, and randomness, the scientific method is more than resistant or tough – it actually benefits from such randomness in many ways (it is anti-fragile). Taleb believes that the more anti-fragile the system, the more it will flourish in the real world.Psychologist Kevin Dunbar says the process of discovery often starts with researchers finding bugs in their experiments. These unexpected results lead researchers to try to fix what they think is an error in their method. Eventually, the researcher decides the error is too persistent and systematic to be a coincidence. The highly controlled, cautious, and curious aspects of the scientific method are thus what make it well suited for identifying such persistent systematic errors. At this point, the researcher will begin to think of theoretical explanations for the error, often seeking the help of colleagues across different domains of expertise.

See also
Armchair theorizing – practice of analyzing or synthesizing existing scholarship without new primary researchPages displaying wikidata descriptions as a fallback
Contingency – Status of propositions that are neither always true nor always false
Empirical limits in science – Idea that knowledge comes only/mainly from sensory experiencePages displaying short descriptions of redirect targets
Evidence-based practices – Pragmatic methodologyPages displaying short descriptions of redirect targets
Fuzzy logic – System for reasoning about vagueness
Information theory – Scientific study of digital information
Logic – Study of correct reasoning
Historical method – Techniques used by historians
Philosophical methodology – Study of the methods of philosophy
Scholarly method – Body of principles and practices used by scholars and academics to make their claims
Methodology – Study of research methods
Metascience – Scientific study of science
Operationalization – Part of the process of research design
Quantitative research – All procedures for the numerical representation of empirical facts
Rhetoric of science
Royal Commission on Animal Magnetism – 1784 French scientific bodies
Scientific law – Statement based on repeated empirical observations that describes some natural phenomenon
Social research – Research conducted by social scientists
Strong inference – Philosophy of science concept emphasizing the need for alternative hypotheses
Testability – Extent to which truthness or falseness of a hypothesis/declaration can be tested
Unsupervised learning – A paradigm in machine learning
Verificationism – Philosophical doctrine

Problems and issues
Descriptive science – Used to describe characteristics of a phenomenon being studiedPages displaying short descriptions of redirect targets
Design science – a systematic form of designingPages displaying wikidata descriptions as a fallback
Holism in science – approach to research that emphasizes the study of complex systemsPages displaying wikidata descriptions as a fallback
Junk science – Scientific data considered to be spurious
List of cognitive biases – Systematic patterns of deviation from norm or rationality in judgment
Normative science – Aspect of science
Philosophical skepticism – Philosophical views that question the possibility of knowledge or certainty
Poverty of the stimulus – Linguistic argument
Problem of induction – Question of whether inductive reasoning leads to definitive knowledge
Pseudoscience – Unscientific claims wrongly presented as scientific
Reference class problem – Issue when estimating a probability
Replication crisis – Observed inability to reproduce scientific studies
Skeptical hypotheses – Philosophical views that question the possibility of knowledge or certaintyPages displaying short descriptions of redirect targets
Underdetermination – Idea in the philosophy of science

History, philosophy, sociology
Baconian method – Investigative process
Epistemology – Branch of philosophy concerning knowledge
Epistemic truth – Attempts to analyze the concept of truth
Mertonian norms – Philosophical ideals for the practice of science
Normal science – Regular work of scientists
Post-normal science – Approach to the use of science on urgent issues involving uncertainty in facts and moral values
Science studies – Research area analyzing scientific expertise
Timeline of the history of scientific method

Notes
Notes: Problem-solving via scientific method
References
Sources
Further reading
External links

Andersen, Anne; Hepburn, Brian. "Scientific Method". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.
"Confirmation and Induction". Internet Encyclopedia of Philosophy.
Scientific method at PhilPapers
Scientific method at the Indiana Philosophy Ontology Project
An Introduction to Science: Scientific Thinking and a scientific method Archived 2018-01-01 at the Wayback Machine by Steven D. Schafersman.
Introduction to the scientific method at the University of Rochester
The scientific method from a philosophical perspective
Theory-ladenness by Paul Newall at The Galilean Library
Lecture on Scientific Method by Greg Anderson (archived 28 April 2006)
Using the scientific method for designing science fair projects
Scientific Methods an online book by Richard D. Jarrard
Richard Feynman on the Key to Science (one minute, three seconds), from the Cornell Lectures.
Lectures on the Scientific Method by Nick Josh Karean, Kevin Padian, Michael Shermer and Richard Dawkins (archived 21 January 2013).
"How Do We Know What Is True?" (animated video; 2:52)

Shigeo Shingo

Shigeo Shingo (新郷 重夫, Shingō Shigeo, 1909–1990) was a Japanese industrial engineer who was considered as the world’s leading expert on manufacturing practices and the Toyota Production System.

Life and work
After having worked as a technician specializing in fusions at the Taiwanese railways in Taipei, at the end of the World War II, in 1945, he started to work at the Japan Management Association (JMA) (ja:日本能率協会) in Tokyo, becoming a consultant focused on the improvement of factory management. Gathering tips from the improvement experiences in the field he had in 1950 at Toyo Ind. (now Mazda) and in 1957 at the sites in Hiroshima of the Mitsubishi Heavy Industries, since 1969 Shingō got involved in some actions in Toyota Motor Corporation (Toyota) for the reduction of set-up time (change of dies) of pressing machines which took him to the formulation of a specific technique based on operational analysis, which shortened set-up times from 1 to 2 hours (or even half a day) per each exchange of dies to a rapid setting of a few minutes. The method spread out under the English denomination Single Minute Exchange of Die, abbreviated as SMED.
Shingo may well be known better in the West than in Japan, as a result of his meeting Norman Bodek, an American entrepreneur and founder of Productivity Inc. In 1981 Bodek travelled to Japan to learn about the Toyota Production System, coming across books by Shingō, who as an external consultant had been teaching Industrial engineering courses at Toyota since 1955. Since 1947, Shingō had been involved all over Japan in the training of thousands of people, who joined his courses on the fundamental techniques of analysis and improvement of the operational activities in factories (among which the P-Course, or Production Course).Shingō had written his Study of the Toyota Production System in Japanese and had it translated into English in 1980. Bodek took as many copies of this book as he could to the USA and arranged to translate Shingo's other books into English, eventually having his original study re-translated. Bodek also brought Shingō to lecture in the USA and developed one of the first Western lean manufacturing consultancy practices with Shingō's support.
The relevance of his contribution has sometimes been doubted, but it is substantially confirmed by the opinions of his contemporaries, many saw him even as a contributor to the fundamental concepts of TPS, such as Just-in-time, and the “pull” production system, which were created by Toyota and Mr.Taiichi Ohno and still remain a strong logical and practical basis for the lean production and lean thinking management approaches.
The myth prevails that Shingo invented the Toyota Production System but what can be stated is that he did document the system. Shingo contributed to the formalization of some aspects of the management philosophy known as the Toyota Production System (TPS), developed and applied in Japan since the 1950s and later implemented in a huge number of companies in the world.
In 1988, the Jon M. Huntsman School of Business at Utah State University recognized Dr. Shingō for his lifetime accomplishments and created the Shingo Prize for Operational Excellence that recognizes world-class, lean organizations and operational excellence.
The theorist of important innovations related to Industrial engineering, such as Poka-yoke and the Zero Quality Control, Shingō could influence fields other than manufacturing.
For example, his concepts of SMED, mistake-proofing, and "zero quality control" (eliminating the need for inspection of results) have all been applied in the sales process engineeringShingo was the author of several books including: A Study of the Toyota Production System; Revolution in Manufacturing: The SMED System; Zero Quality Control: Source Inspection and the Poka-yoke System; The Sayings of Shigeo Shingo: Key Strategies for Plant Improvement; Non-Stock Production: The Shingo System for Continuous Improvement and The Shingo Production Management System: Improving Process Functions.

Education
Saga Technical High School
Yamanashi Technical College

Bibliography
Shigeo Shingo: A Revolution in Manufacturing: The Smed System, Productivity Press, 1985 (in English), ISBN 0-915299-03-8
Shigeo Shingo: A Study of the Toyota Production System, Productivity Press, 1981 (in Japanese), 1989 (in English), ISBN 0-915299-17-8.
Shigeo Shingo: Modern Approaches to Manufacturing Improvement: The Shingo System, Productivity Press, 1990 (in English), ISBN 0-915299-64-X
Shigeo Shingo: Quick Changeover for Operators: The SMED System, Productivity Press, 1996 (in English), ISBN 1-56327-125-7
Shigeo Shingo: The Sayings of Shigeo Shingo: Key Strategies for Plant Improvement, Productivity Press, 1987 (in English), ISBN 0-915299-15-1
Shigeo Shingo: Zero Quality Control: Source Inspection and the Poka-Yoke System, Productivity Press, 1986 (in English), ISBN 0-915299-07-0
Shigeo Shingo: Non-Stock Production: The Shingo System for Continuous Improvement, Productivity Press, 1988 (in English), ISBN 0-915299-30-5
Shigeo Shingo: Mistake-Proofing for Operators: The ZQC System, Productivity Press, 1997 (in English), ISBN 1-56327-127-3
Shigeo Shingo: The Shingo Production Management System: Improving Process Functions (Manufacturing & Production), Productivity Press, 1992 (in English), ISBN 0-915299-52-6
Shigeo Shingo: Enfoques Modernos Para la Mejora En la Fabricacion: El Sistema Shingo, Productivity Press, 1992 (in Spanish), ISBN 84-87022-77-4
Shigeo Shingo: Produccion Sin Stocks: El Sistema Shingo Para la Mejora Continua, Productivity Press, 1991 (in Spanish), ISBN 84-87022-74-X
Shigeo Shingo: Das Erfolgsgeheimnis der Toyota-Produktion, Verlag moderne industrie, 1992 (in German), ISBN 3-478-91062-5
Shigeo Shingo: Kaizen and The Art of Creative Thinking, Enna Product Corporation and PCS Inc, 2007 (in English), ISBN 1897363591
Shigeo Shingo: Fundamental Principles of Lean Manufacturing, Enna Product Corporation and PCS Inc, 2009 (in English), ISBN 9781926537078

See also
Taiichi Ohno (大野 耐一, Ōno Taiichi)
Just In Time (JIT)
Akira Kōdate
Shingo Prize
Taylorism
Toyota Production System

References
Further reading
Head, Simon: The New Ruthless Economy. Work and Power in the Digital Age, Oxford Oxford University Press 2005  - Head analyzes critically Shingo and the Toyota production system, ISBN 0-19-517983-8
Smalley, Art: Shigeo Shingo's Influence on TPS, in Superfactory, April 2006 (https://web.archive.org/web/20120630224714/http://www.superfactory.com/articles/featured/2006/0604-smalley-shingo-influence-tps.html)

External links
Shingo Prize
Concise Bio

Shortage

In economics, a shortage or excess demand is a situation in which the demand for a product or service exceeds its supply in a market. It is the opposite of an excess supply (surplus).

Definitions
In a perfect market (one that matches a simple microeconomic model), an excess of demand will prompt sellers to increase prices until demand at that price matches the available supply, establishing market equilibrium. In economic terminology, a shortage occurs when for some reason (such as government intervention, or decisions by sellers not to raise prices) the price does not rise to reach equilibrium.  In this circumstance, buyers want to purchase more at the market price than the quantity of the good or service that is available, and some non-price mechanism (such as "first come, first served" or a lottery) determines which buyers are served. So in a perfect market the only thing that can cause a shortage is price.
In common use, the term "shortage" may refer to a situation where most people are unable to find a desired good at an affordable price, especially where supply problems have increased the price.  "Market clearing" happens when all buyers and sellers willing to transact at the prevailing price are able to find partners.  There are almost always willing buyers at a lower-than-market-clearing price; the narrower technical definition doesn't consider failure to serve this demand as a "shortage", even if it would be described that way in a social or political context (which the simple model of supply and demand does not attempt to encompass).

Causes
Shortages (in the technical sense) may be caused by the following causes:

Price ceilings, a type of price control which involves a government-imposed limit on the price of a product or service.
Anti-price gouging laws.
Government ban on the sale of a product or service, such as prostitution or certain recreational drugs.
Decisions by suppliers not to raise prices, for example to maintain friendly relationships with potential future customers during a supply disruption.
Artificial scarcity.
Worker shortages in low-wage industries (hospitality and leisure, education, health care, rail transportation, aviation, retail, manufacturing, food, elderly care) caused by excessively low salaries (relative to the domestic cost of living) and adverse working conditions (excessive workload and working hours), which collectively lead to occupational burnout and attrition of existing workers, insufficient incentives to attract the inflow supply of workers (through a voluntary exchange), short-staffing at workplaces and further exacerbation (positive feedback) of staff shortages.

Effects
Decisions which result in a below-market-clearing price help some people and hurt others. In this case, shortages may be accepted because they theoretically enable a certain portion of the population to purchase a product that they couldn't afford at the market-clearing price. The cost is to those who are willing to pay for a product and either can't, or experience greater difficulty in doing so.
In the case of government intervention in the market, there is always a trade-off with positive and negative effects. For example, a price ceiling may cause a shortage, but it will also enable a certain percentage of the population to purchase a product that they couldn't afford at market costs. Economic shortages caused by higher transaction costs and opportunity costs (e.g., in the form of lost time) also mean that the distribution process is wasteful. Both of these factors contribute to a decrease in aggregate wealth.
Shortages may or will cause:
Black (illegal) and Grey (unregulated) markets in which products that are unavailable in conventional markets are sold, or in which products with excess demand are sold at higher prices than in the conventional market.
Artificial controls of demand, such as time (such as waiting in line at queues) and rationing.
Non-monetary bargaining methods, such as time (for example queuing), nepotism, or even violence.
Panic buying
Price discrimination.
The inability to purchase a product, and subsequent forced saving.
Increase in demand for substitute goods.
Deadweight loss due to artificial scarcity; a net loss of economic welfare to society occurs when an artificial limit of supply (by monopolies or oligopolies to maximise profits), limits the number of people who can enjoy the good.

Examples
Many regions around the world have experienced shortages in the past.

Food shortages have occurred in the United States during the Great Depression.
Rationing in the United Kingdom and the United States occurred mainly during and after the world wars
Potato shortages in the Netherlands triggered the 1917 Potato riots.
From 1920 to 1933 during prohibition in the United States, the creation of a black market for liquor was created due to the low supply of alcoholic beverages.
During the 1973 oil crisis, during which long lines and rationing was used to control demand.
In the former Soviet Union during the 1980s, prices were artificially low by fiat (i.e., high prices were outlawed). Soviet citizens waited in line for various price-controlled goods and services such as cars, apartments, or some types of clothing. From the point of view of those waiting in line, such goods were in perpetual "short supply"; some of them were willing and able to pay more than the official price ceiling, but were legally prohibited from doing so. This method for determining the allocation of goods in short supply is known as "rationing".
From the mid-2000s through the 2010s, shortages in Venezuela occurred, due to the Venezuelan government's economic policies; such as relying on foreign imports while creating strict foreign exchange controls, put price controls in place and having expropriations result with lower domestic production. As a result of such shortages, Venezuelans had to search for products, wait in lines for hours and rationing was initiated, with the government allowing the purchase of a certain amount of products when it's available, through fingerprint recognition.
Shortages in Sudan sparked a revolution in 2019 which ended President Omar al-Bashir's 30-year rule. They continued into 2020.
Panic buying due to the COVID-19 pandemic caused food and product shortages around the world.

Shortages and "longages"
Garrett Hardin emphasised that a shortage of supply can just as well be viewed as a "longage" of demand. For instance, a shortage of food can just as well be called a longage of people (overpopulation). By looking at it from this view, he felt the problem could be better dealt with.

Labour shortage
In its narrowest definition, a labour shortage is an economic condition in which employers believe there are insufficient qualified candidates (employees) to fill the marketplace demands for employment at a wage that is mostly employer-determined. Such a condition is sometimes referred to by economists as "an insufficiency in the labour force." An ageing population and a contracting workforce and a birth dearth may curb U.S. economic expansion for several decades, for example.In a wider definition, a widespread domestic labour shortage is caused by excessively low salaries (relative to the domestic cost of living) and adverse working conditions (excessive workload and working hours) in low-wage industries (hospitality and leisure, education, health care, rail transportation, aviation, retail, manufacturing, food, elderly care), which collectively lead to occupational burnout and attrition of existing workers, insufficient incentives to attract the inflow supply of domestic workers, short-staffing at workplaces and further exacerbation (positive feedback) of staff shortages.Labour shortages occur broadly across multiple industries within a rapidly expanding economy, whilst labour shortages often occur within specific industries (which generally offer low salaries) even during economic periods of high unemployment. In response to domestic labour shortages, business associations such as chambers of commerce would generally lobby to governments for an increase of the inward immigration of foreign workers from countries which are less developed and have lower salaries. In addition, business associations have campaigned for greater state provision of child care, which would enable more women to re-enter the labour workforce. However, as labour shortages in the relevant low-wage industries are often widespread globally throughout many countries in the world, immigration would only partially address the chronic labour shortages in the relevant low-wage industries in developed countries (whilst simultaneously discouraging local labour from entering the relevant industries) and in turn cause greater labour shortages in developing countries.

Wage factors
The Atlantic slave trade (which originated in the early 17th century but ended by the early 19th century) was said to have originated from perceived shortages of agricultural labour in the Americas (particularly in the Southern United States). It was thought that bringing African labor was the only means of malaria resistance available at the time. Ironically, malaria seems to itself have been introduced to the "New World" via the slave trade.

See also
References
Kornai, János, Socialist economy, Princeton University Press, 1992, ISBN 0-691-00393-9
Kornai, János, Economics of Shortage, Amsterdam: North Holland Press, Volume A, p. 27; Volume B, p. 196 .
Gomulka, Stanislaw: Kornai's Soft Budget Constraint and the Shortage Phenomenon: A Criticism and Restatement, in: Economics of Planning, Vol. 19. 1985. No. 1.
Planning Shortage and Transformation. Essays in Honor of Janos Kornai,  Cambridge, Massachusetts: MIT Press, 2000
Myant, Martin; Drahokoupil, Jan (2010), Transition Economies: Political Economy in Russia, Eastern Europe, and Central Asia, Wiley-Blackwell, ISBN 978-0-470-59619-7

External links

János Kornai Home Page at Harvard University
János Kornai Home Page at Collegium Budapest
Part 1 and Part 2 of COMPARING AND ASSESSING ECONOMIC SYSTEMS, Shortage and Inflation: The Phenomenon, PPT (PowerPoint file presentation) at West Virginia University
János Kornai 'The Soft Budget Constraint'
David Lipton and Jeffrey Sachs 'The Consequences of Central Planning in Eastern Europe'
On overview and critique of Kornai's account can be found in Myant, Martin; Jan Drahokoupil (2010). Transition Economies: Political Economy in Russia, Eastern Europe, and Central Asia. Hoboken, New Jersey: Wiley-Blackwell. pp. 19–23. ISBN 978-0-470-59619-7.
Planning For The Looming Labor Shortage - A Supply Chain Perspective by HK Systems
"America's New Immigrant Entrepreneurs"  - A Duke University Study
Criticism of high-tech shortage claims 
Disputation of High-tech Labor Shortage by Dr. Matloff
RAND Study on Alleged Shortage of Scientists
Shortage of skilled workers knocks red tape off top of business constraints league table - Grant Thornton IBR
The Real Science Gap - "It's not insufficient schooling or a shortage of scientists. It's a lack of job opportunities."

Single-minute exchange of die

Single-minute digit exchange of die (SMED) is one of the many lean production methods for reducing inefficiencies in a manufacturing process. It provides a rapid and efficient way of converting a manufacturing process from running the current product to running the next product. This is key to reducing production lot sizes, and reducing uneven flow (Mura), production loss, and output variability.
The phrase "single minute" does not mean that all changeovers and startups should only take one minute, rather, it should take less than 10 minutes ("single-digit minute"). A closely associated yet more difficult concept is one-touch exchange of die (OTED), which says changeovers can and should take less than 100 seconds.  A die is a tool used in manufacturing. However SMED's utility is not limited to manufacturing (see value stream mapping).

History
Frederick Winslow Taylor analyzed non-value-adding parts of setups in his 1911 book, Shop Management (page 171). However, he did not create any method or structured approach around it.
Frank Bunker Gilbreth studied and improved working processes in many different industries, from bricklaying to surgery. As part of his work, he also looked into changeovers. His book Motion Study (also from 1911) described approaches to reduce setup time.
Even Henry Ford's factories were using some setup reduction techniques. In the 1915 publication Ford Methods and Ford Shops, setup reduction approaches were clearly described. However, these approaches never became mainstream. For most parts during the 20th century, the economic order quantity was the gold standard for lot sizing.
The JIT workflow of Toyota had this problem of tools changeover took between two and eight hours, Toyota could neither afford the lost production time nor the enormous lot sizes suggested by the economic order quantity. Lot reduction and set up time reduction had actually been ongoing in TPS since 1945 when Taiichi Ohno became manager of the machine shops in Toyota. On a trip to the US in 1955, Taiichi Ohno observed Danly stamping presses with rapid die change capability. Subsequently, Toyota bought multiple Danly presses for the Motomachi plant. And Toyota started to work on improving the changeover time of their presses. This was known as Quick Die Change, or  QDC for short. They developed a structured approach based on a framework from the US World War II Training within Industry (TWI) program, called ECRS – Eliminate, Combine, Rearrange, and Simplify.
Over time they reduced these changeover times from hours to fifteen minutes by the 1960s, three minutes by the 1970s and then just 180 seconds by 1990s.
During the late 1970s, when Toyota's method was already well refined, Shigeo Shingo participated in one QDC workshop. After he started to publicize details of the Toyota Production System without permission, the business connection was terminated abruptly by Toyota. Shingo moved to the US and started to consult on lean manufacturing. Besides claiming to have invented this quick changeover method (among many other things), he renamed it Single Minute Exchange of Die or, in short, SMED. The Single Minute stands for a single digit minute (i.e., less than ten minutes). He promoted TPS and SMED in US.

Example
Toyota found that the most difficult tools to change were the dies on the large transfer-stamping machines that produce car vehicle body parts.  The dies – which must be changed for each model – weigh many tons, and must be assembled in the stamping machines with tolerances of less than a millimeter, otherwise the stamped metal will wrinkle, if not melt, under the intense heat and pressure.
When Toyota engineers examined the change-over, they discovered that the established procedure was to stop the line, let down the dies by an overhead crane, position the dies in the machine by human eyesight, and then adjust their position with crowbars while making individual test stampings.  The existing process took from twelve hours to almost three days to complete.
Toyota's first improvement was to place precision measurement devices on the transfer stamping machines, and record the necessary measurements for each model's die.  Installing the die against these measurements, rather than by human eyesight, immediately cut the change-over to a mere hour and a half.
Further observations led to further improvements – scheduling the die changes in a standard sequence (as part of FRS) as a new model moved through the factory, dedicating tools to the die-change process so that all needed tools were nearby, and scheduling use of the overhead cranes so that the new die would be waiting as the old die was removed.  Using these processes, Toyota engineers cut the change-over time to less than 10 minutes per die, and thereby reduced the economic lot size below one vehicle.
The success of this program contributed directly to just-in-time manufacturing which is part of the Toyota Production System. SMED makes Load balancing much more achievable by reducing economic lot size and thus stock levels.

Effects of Implementation
Shigeo Shingo, who created the SMED approach, claims that in his data from between 1975 and 1985 that average setup times he has dealt with have reduced to 2.5% of the time originally required; a 40 times improvement.
However, the power of SMED is that it has a lot of other effects which come from systematically looking at operations; these include:

Stockless production which drives inventory turnover rates,
Reduction in footprint of processes with reduced inventory freeing floor space
Productivity increases or reduced production time
Increased machine work rates from reduced setup times even if number of changeovers increases
Elimination of setup errors and elimination of trial runs reduces defect rates
Improved quality from fully regulated operating conditions in advance
Increased safety from simpler setups
Simplified housekeeping from fewer tools and better organization
Lower expense of setups
Operator preferred since easier to achieve
Lower skill requirements since changes are now designed into the process rather than a matter of skilled judgement
Elimination of unusable stock from model changeovers and demand estimate errors
Goods are not lost through deterioration
Ability to mix production gives flexibility and further inventory reductions as well as opening the door to revolutionized production methods (large orders ≠ large production lot sizes)
New attitudes on controllability of work process amongst staff

Implementation Techniques
Shigeo Shingo recognizes eight fundamental techniques that should be considered in implementing SMED.

Separate internal from external setup operations
Convert internal to external setup
Standardize function, not shape
Use functional clamps or eliminate fasteners altogether
Use intermediate jigs
Adopt parallel operations (see image below)
Eliminate adjustments
MechanizationNB External setup can be done without the line being stopped whereas internal setup requires that the line be stopped.
He suggests that SMED improvement should pass through four conceptual stages:
A) ensure that external setup actions are performed while the machine is still running,
B) separate external and internal setup actions, ensure that the parts all function and implement efficient ways of transporting the die and other parts,
C) convert internal setup actions to external,
D) improve all setup actions.

Formal method
There are seven basic steps  to reducing changeover using the SMED system:

OBSERVE the current methodology
Separate the INTERNAL and EXTERNAL activities. Internal activities are those that can only be performed when the process is stopped, while External activities can be done while the last batch is being produced, or once the next batch has started. For example, go and get the required tools for the job BEFORE the machine stops.
Convert (where possible) Internal activities into External ones (pre-heating of tools is a good example of this).
Streamline the remaining internal activities, by simplifying them. Focus on fixings – Shigeo Shingo observed that it's only the last turn of a bolt that tightens it—the rest is just movement.
Streamline the External activities, so that they are of a similar scale to the Internal ones (D).
Document the new procedure, and actions that are yet to be completed.
Do it all again: For each iteration of the above process, a 45% improvement in set-up times should be expected, so it may take several iterations to cross the ten-minute line.This diagram shows four successive runs with learning from each run and improvements applied before the next.

Run 1 illustrates the original situation.
Run 2 shows what would happen if more changeovers were included.
Run 3 shows the impact of the improvements in changeover times that come from doing more of them and building learning into their execution.
Run 4 shows how these improvements can get you back to the same production time but now with more flexibility in production capacity.
Run N (not illustrated) would have changeovers that take 1.5 minutes (97% reduction) and whole shift time reduced from 420 minutes to 368 minutes a productivity improvement of 12%.The SMED concept is credited to Shigeo Shingo, one of the main contributors to the consolidation of the Toyota Production System, along with Taiichi Ohno.

Key Elements to Observe
Look for:

Shortages, mistakes, inadequate verification of equipment causing delays and can be avoided by check tables, especially visual ones, and setup on an intermediary jig
Inadequate or incomplete repairs to equipment causing rework and delays
Optimization for least work as opposed to least delay
Unheated molds which require several wasted 'tests' before they will be at the temperature to work
Using slow precise adjustment equipment for the large coarse part of adjustment
Lack of visual lines or benchmarks for part placement on the equipment
Forcing a changeover between different raw materials when a continuous feed, or near equivalent, is possible
Lack of functional standardization, that is standardization of only the parts necessary for setup e.g. all bolts use same size spanner, die grip points are in the same place on all dies
Much operator movement around the equipment during setup
More attachment points than actually required for the forces to be constrained
Attachment points that take more than one turn to fasten
Any adjustments after initial setup
Any use of experts during setup
Any adjustments of assisting tools such as guides or switchesRecord All Necessary Data

Parallel operations using multiple operators By taking the 'actual' operations and making them into a network which contains the dependencies it is possible to optimise task attribution and further optimize setup time. Issues of effective communication between the operators must be managed to ensure safety is assured where potentially noisy or visually obstructive conditions occur.
Parallel operations using multiple operators By taking the 'actual' operations and making them into a network which contains the dependencies it is possible to optimise task attribution and further optimize setup time. Issues of effective communication between the operators must be managed to ensure safety is assured where potentially noisy or visually obstructive conditions occur.

See also
Changeover
Value stream mapping


== References ==

Six Sigma

Six Sigma (6σ) is a set of techniques and tools for process improvement. It was introduced by American engineer Bill Smith while working at Motorola in 1986.Six Sigma strategies seek to improve manufacturing quality by identifying and removing the causes of defects and minimizing variability in manufacturing and business processes. This is done by using empirical and statistical quality management methods and by hiring people who serve as Six Sigma experts. Each Six Sigma project follows a defined methodology and has specific value targets, such as reducing pollution or increasing customer satisfaction.
The term Six Sigma originates from statistical quality control, a reference to the fraction of a normal curve that lies within six standard deviations of the mean, used to represent a defect rate.

History
Motorola pioneered Six Sigma, setting a "six sigma" goal for its manufacturing business. It registered Six Sigma as a service mark on June 11, 1991 (U.S. Service Mark 1,647,704); on December 28, 1993, it registered Six Sigma as a trademark. In 2005 Motorola attributed over $17 billion in savings to Six Sigma.Honeywell and General Electric were also early adopters of Six Sigma. As GE's CEO, in 1995 Jack Welch made it central to his business strategy. In 1998 GE announced $350 million in cost savings thanks to Six Sigma, which was an important factor in the spread of Six Sigma (this figure later grew to more than $1 billion). By the late 1990s, about two thirds of the Fortune 500 organizations had begun Six Sigma initiatives with the aim of reducing costs and improving quality.In recent years, some practitioners have combined Six Sigma ideas with lean manufacturing to create a methodology named Lean Six Sigma. The Lean Six Sigma methodology views lean manufacturing, which addresses process flow and waste issues, and Six Sigma, with its focus on variation and design, as complementary disciplines aimed at promoting "business and operational excellence".In 2011, the International Organization for Standardization (ISO) published the first standard "ISO 13053:2011" defining a Six Sigma process. Other standards have been created mostly by universities or companies with Six Sigma first-party certification programs.

Etymology
The term Six Sigma comes from statistics, specifically from the field of statistical quality control, which evaluates process capability. Originally, it referred to the ability of manufacturing processes to produce a very high proportion of output within specification. Processes that operate with "six sigma quality" over the short term are assumed to produce long-term defect levels below 3.4 defects per million opportunities (DPMO).  The 3.4 dpmo is based on a "shift" of ± 1.5 sigma explained by Mikel Harry. This figure is based on the tolerance in the height of a stack of discs.Specifically, say that there are six standard deviations—represented by the Greek letter σ (sigma)—between the mean—represented by μ (mu)—and the nearest specification limit. As process standard deviation goes up, or the mean of the process moves away from the center of the tolerance, fewer standard deviations will fit between the mean and the nearest specification limit, decreasing the sigma number and increasing the likelihood of items outside specification. According to a calculation method employed in process capability studies, this means that practically no items will fail to meet specifications.One should also note that the calculation of sigma levels for a process data is independent of the data being normally distributed. In one of the criticisms of Six Sigma, practitioners using this approach spend a lot of time transforming data from non-normal to normal using transformation techniques. It must be said that sigma levels can be determined for process data that has evidence of non-normality.

Doctrine
Six Sigma asserts that:

Continuous efforts to achieve stable and predictable process results (e.g., by reducing process variation) are of vital importance to business success.
Manufacturing and business processes have characteristics that can be defined, measured, analyzed, improved, and controlled.
Achieving sustained quality improvement requires commitment from the entire organization, particularly from top-level management.Features that set Six Sigma apart from previous quality-improvement initiatives include:

Focus on achieving measurable and quantifiable financial returns
Emphasis on management leadership and support
Commitment to making decisions on the basis of verifiable data and statistical methods rather than assumptions and guessworkIn fact, lean management and Six Sigma share similar methodologies and tools, including the fact that both were influenced by Japanese business culture. However, lean management primarily focuses on eliminating waste through tools that target organizational efficiencies while integrating a performance improvement system, while Six Sigma focuses on eliminating defects and reducing variation. Both systems are driven by data, though Six Sigma is much more dependent on accurate data.Six Sigma's implicit goal is to improve all processes but not necessarily to the 3.4 DPMO level. Organizations need to determine an appropriate sigma level for each of their most important processes and strive to achieve these. As a result of this goal, it is incumbent on management of the organization to prioritize areas of improvement.

Methodologies
Six Sigma projects follow two project methodologies, inspired by W. Edwards Deming's Plan–Do–Study–Act Cycle, each with five phases.
DMAIC ("duh-may-ick", /də.ˈmeɪ.ɪk/) is used for projects aimed at improving an existing business process
DMADV ("duh-mad-vee", /də.ˈmæd.vi/) is used for projects aimed at creating new product or process designs

DMAIC
The DMAIC project methodology has five phases:

 Define the system, the voice of the customer and their requirements, and the project goals, specifically.
 Measure key aspects of the current process and collect relevant data; calculate the "as-is" process capability
 Analyze the data to investigate and verify cause and effect. Determine what the relationships are, and attempt to ensure that all factors have been considered. Seek out the root cause of the defect under investigation.
 Improve or optimize the current process based upon data analysis using techniques such as design of experiments, poka yoke or mistake proofing, and standard work to create a new, future state process. Set up pilot runs to establish process capability.
 Control the future state process to ensure that any deviations from the target are corrected before they result in defects. Implement control systems such as statistical process control, production boards, visual workplaces, and continuously monitor the process. This process is repeated until the desired quality level is obtained.Some organizations add a Recognize step at the beginning, which is to recognize the right problem to work on, thus yielding an RDMAIC methodology.

DMADV
Also known as DFSS ("Design For Six Sigma"), the DMADV methodology's five phases are:
 Define design goals that are consistent with customer demands and the enterprise strategy.
 Measure and identify CTQs (characteristics that are Critical To Quality), measure product capabilities, production process capability, and measure risks.
 Analyze to develop and design alternatives
 Design an improved alternative, best suited per analysis in the previous step
 Verify the design, set up pilot runs, implement the production process and hand it over to the process owner(s).

Professionalization
One key innovation of Six Sigma involves professionalizing quality management. Prior to Six Sigma, quality management was largely relegated to the production floor and to statisticians in a separate quality department. Formal Six Sigma programs adopt an elite ranking terminology similar to martial arts systems like judo to define a hierarchy (and career path) that spans business functions and levels.
Six Sigma identifies several roles for successful implementation:
Executive Leadership includes the CEO and other members of top management. They are responsible for setting up a vision for Six Sigma implementation. They also empower other stakeholders with the freedom and resources to transcend departmental barriers and overcome resistance to change.
Champions take responsibility for Six Sigma implementation across the organization. The Executive Leadership draws them from upper management. Champions also act as mentors to Black Belts.
Master Black Belts, identified by Champions, act as in-house coaches on Six Sigma. They devote all of their time to Six Sigma, assisting Champions and guiding Black Belts and Green Belts. In addition to statistical tasks, they ensure that Six Sigma is applied consistently across departments and job functions.
Black Belts operate under Master Black Belts to apply Six Sigma to specific projects. They also devote all of their time to Six Sigma. They primarily focus on Six Sigma project execution and special leadership with special tasks, whereas Champions and Master Black Belts focus on identifying projects/functions for Six Sigma.
Green Belts are the employees who take up Six Sigma implementation along with their other job responsibilities, operating under the guidance of Black Belts.According to proponents, special training is needed for all of these practitioners to ensure that they follow the methodology and use the data-driven approach correctly.Some organizations use additional belt colors, such as "yellow belts", for employees that have basic training in Six Sigma tools and generally participate in projects, and "white belts" for those locally trained in the concepts but do not participate in the project team. "Orange belts" are also mentioned to be used for special cases.

Certification
General Electric and Motorola developed certification programs as part of their Six Sigma implementation. Following this approach, many organizations in the 1990s started offering Six Sigma certifications to their employees. In 2008 Motorola University later co-developed with Vative and the Lean Six Sigma Society of Professionals a set of comparable certification standards for Lean Certification. Criteria for Green Belt and Black Belt certification vary; some companies simply require participation in a course and a Six Sigma project. There is no standard certification body, and different certifications are offered by various quality associations for a fee. The American Society for Quality, for example, requires Black Belt applicants to pass a written exam and to provide a signed affidavit stating that they have completed two projects or one project combined with three years' practical experience in the body of knowledge.

Tools and methods
Within the individual phases of a DMAIC or DMADV project, Six Sigma uses many established quality-management tools that are also used outside Six Sigma. The following table shows an overview of the main methods used.

Software
Role of the 1.5 sigma shift
Experience has shown that processes usually do not perform as well in the long term as they do in the short term. As a result, the number of sigmas that will fit between the process mean and the nearest specification limit may well drop over time, compared to an initial short-term study. To account for this real-life increase in process variation over time, an empirically based 1.5 sigma shift is introduced into the calculation.  Mikel Harry, the creator of Six Sigma, based the 1.5 sigma shift on the height of a stack of discs.  He called this "Benderizing". He claimed that based on his stack, all processes shift 1.5 sigma every 50 samples.  According to this idea, a process that fits 6 sigma between the process mean and the nearest specification limit in a short-term study will in the long term fit only 4.5 sigma – either because the process mean will move over time, or because the long-term standard deviation of the process will be greater than that observed in the short term, or both.Hence the widely accepted definition of a six sigma process is a process that produces 3.4 defective parts per million opportunities (DPMO). This is based on the fact that a process that is normally distributed will have 3.4 parts per million outside the limits, when the limits are six sigma from the "original" mean of zero and the process mean is then shifted by 1.5 sigma (and therefore, the six sigma limits are no longer symmetrical about the mean). The former six sigma distribution, when under the effect of the 1.5 sigma shift, is commonly referred to as a 4.5 sigma process. The failure rate of a six sigma distribution with the mean shifted 1.5 sigma is not equivalent to the failure rate of a 4.5 sigma process with the mean-centered on zero. This allows for the fact that special causes may result in a deterioration in process performance over time and is designed to prevent underestimation of the defect levels likely to be encountered in real-life operation.The role of the sigma shift is mainly academic. The purpose of six sigma is to generate organizational performance improvement. It is up to the organization to determine, based on customer expectations, what the appropriate sigma level of a process is. The purpose of the sigma value is as a comparative figure to determine whether a process is improving, deteriorating, stagnant or non-competitive with others in the same business. Six Sigma (3.4 DPMO) is not the goal of all processes.

Sigma levels
The table below gives long-term DPMO values corresponding to various short-term sigma levels.These figures assume that the process mean will shift by 1.5 sigma toward the side with the critical specification limit. In other words, they assume that after the initial study determining the short-term sigma level, the long-term Cpk value will turn out to be 0.5 less than the short-term Cpk value. So, now for example, the DPMO figure given for 1 sigma assumes that the long-term process mean will be 0.5 sigma beyond the specification limit (Cpk = –0.17), rather than 1 sigma within it, as it was in the short-term study (Cpk = 0.33). Note that the defect percentages indicate only defects exceeding the specification limit to which the process mean is nearest. Defects beyond the far specification limit are not included in the percentages.
The formula used here to calculate the DPMO is thus

In practice
Six Sigma mostly finds application in large organizations. According to industry consultants like Thomas Pyzdek and John Kullmann, companies with fewer than 500 employees are less suited to Six Sigma or need to adapt the standard approach to making it work for them. Six Sigma, however, contains a large number of tools and techniques that work well in small to mid-size organizations. The fact that an organization is not big enough to be able to afford black belts does not diminish its ability to make improvements using this set of tools and techniques. The infrastructure described as necessary to support Six Sigma is a result of the size of the organization rather than a requirement of Six Sigma itself.

Manufacturing
After its first application at Motorola in the late 1980s, other internationally recognized firms currently recorded high number of savings after applying Six Sigma. Examples include Johnson & Johnson, with $600 million of reported savings, Texas Instruments, which saved over $500 million as well as Telefónica, which reported €30 million in savings in the first 10 months; Sony and Boeing also reported successfully reducing waste.

Engineering and construction
Although companies have considered common quality control and process improvement strategies, there's still a need for more reasonable and effective methods as all the desired standards and client satisfaction have not always been reached. There is still a need for an essential analysis that can control the factors affecting concrete cracks and slippage between concrete and steel. After conducting a case study on Tinjin Xianyi Construction Technology, it was found that construction time and construction waste were reduced by 26.2% and 67% accordingly after adopting Six Sigma. Similarly, Six Sigma implementation was studied at one of the largest engineering and construction companies in the world: Bechtel Corporation, where after an initial investment of $30 million in a Six Sigma program that included identifying and preventing rework and defects, over $200 million were saved.

Finance
Six Sigma has played an important role by improving the accuracy of allocation of cash to reduce bank charges, automatic payments, improving the accuracy of reporting, reducing documentary credit defects, reducing check collection defects, and reducing variation in collector performance.
For example, Bank of America announced in 2004 that Six Sigma had helped it increase customer satisfaction by 10.4% and decrease customer issues by 24%; similarly, American Express eliminated non-received renewal credit cards. Other financial institutions that have adopted Six Sigma include GE Capital and JPMorgan Chase, where customer satisfaction was the main objective.

Supply chain
In the supply-chain field, it is important to ensure that products are delivered to clients at the right time while preserving high-quality standards. By changing the schematic diagram for the supply chain, Six Sigma can ensure quality control on products (defect-free) and guarantee delivery deadlines, the two main issues in the supply chain.

Healthcare
This is a sector that has been highly matched with this doctrine for many years because of the nature of zero tolerance for mistakes and potential for reducing medical errors involved in healthcare. The goal of Six Sigma in healthcare is broad and includes reducing the inventory of equipment that brings extra costs, altering the process of healthcare delivery in order to make it more efficient and refining reimbursements. A study at the MD Anderson Cancer Center, which recorded an increase in examinations with no additional machines of 45% and a reduction in patients' preparation time of 40 minutes; from 45 minutes to 5 minutes in multiple cases.Lean Six Sigma was adopted in 2003 at Stanford hospitals and was introduced at Red Cross hospitals in 2002.

Criticism
While there are many advocates for a Six Sigma approach for the reasons stated above, more than half of projects are unsuccessful: in 2010, the Wall Street Journal reported that more than 60% of projects fail.  A review of academic literature  found 34 common failure factors in 56 papers on Lean, Six Sigma, and LSS from 1995-2013. Among them are (summarized):

Lack of top management attitude, commitment, and involvement; lack of leadership and vision
Lack of training and education; lack of resources (financial, technical, human, etc.)
Poor project selection and prioritization; weak link to strategic objectives of the organization
Resistance to culture change; Poor communication; Lack of consideration of the human factors
Lack of awareness of the benefits of Lean/Six Sigma; Lack of technical understanding of tools, techniques, and practicesOthers have provided other criticisms.

Lack of originality
Quality expert Joseph M. Juran described Six Sigma as "a basic version of quality improvement", stating that "there is nothing new there. It includes what we used to call facilitators. They've adopted more flamboyant terms, like belts with different colors. I think that concept has merit to set apart, to create specialists who can be very helpful. Again, that's not a new idea. The American Society for Quality long ago established certificates, such as for reliability engineers."

Inadequate for complex manufacturing
Quality expert Philip B. Crosby pointed out that the Six Sigma standard does not go far enough—customers deserve defect-free products every time. For example, under the Six Sigma standard, semiconductors, which require the flawless etching of millions of tiny circuits onto a single chip, are all defective.

Role of consultants
The use of "Black Belts" as itinerant change agents has fostered an industry of training and certification. Critics have argued there is overselling of Six Sigma by too great a number of consulting firms, many of which claim expertise in Six Sigma when they have only a rudimentary understanding of the tools and techniques involved or the markets or industries in which they are acting.

Potential negative effects
A Fortune article stated that "of 58 large companies that have announced Six Sigma programs, 91% have trailed the S&P 500 since". The statement was attributed to "an analysis by Charles Holland of consulting firm Qualpro (which espouses a competing quality-improvement process)". The summary of the article is that Six Sigma is effective at what it is intended to do, but that it is "narrowly designed to fix an existing process" and does not help in "coming up with new products or disruptive technologies."

Over-reliance on statistics
More direct criticism is the "rigid" nature of Six Sigma with its over-reliance on methods and tools. In most cases, more attention is paid to reducing variation and searching for any significant factors, and less attention is paid to developing robustness in the first place (which can altogether eliminate the need for reducing variation). The extensive reliance on significance testing and use of multiple regression techniques increase the risk of making commonly unknown types of statistical errors or mistakes. A possible consequence of Six Sigma's array of p-value misconceptions is the false belief that the probability of a conclusion being in error can be calculated from the data in a single experiment without reference to external evidence or the plausibility of the underlying mechanism. One of the most serious but all-too-common misuses of inferential statistics is to take a model that was developed through exploratory model building and subject it to the same sorts of statistical tests that are used to validate a model that was specified in advance.Another comment refers to the oft-mentioned Transfer Function, which seems to be a flawed theory if looked at in detail. Since significance tests were first popularized many objections have been voiced by prominent and respected statisticians. The volume of criticism and rebuttal has filled books with language seldom used in the scholarly debate of a dry subject. Much of the first criticism was already published more than 40 years ago (see Statistical hypothesis testing § Criticism).
In a 2006 issue of USA Army Logistician an article critical of Six Sigma noted: "The dangers of a single paradigmatic orientation (in this case, that of technical rationality) can blind us to values associated with double-loop learning and the learning organization, organization adaptability, workforce creativity and development, humanizing the workplace, cultural awareness, and strategy making."Nassim Nicholas Taleb considers risk managers little more than "blind users" of statistical tools and methods. He states that statistics is fundamentally incomplete as a field as it cannot predict the risk of rare events—something Six Sigma is especially concerned with. Furthermore, errors in prediction are likely to occur as a result of ignorance of or distinction between epistemic and other uncertainties. These errors are the biggest in time variant (reliability) related failures.

1.5 sigma shift
The statistician Donald J. Wheeler has dismissed the 1.5 sigma shift as "goofy" because of its arbitrary nature. Its universal applicability is seen as doubtful.
The 1.5 sigma shift has also become contentious because it results in stated "sigma levels" that reflect short-term rather than long-term performance: a process that has long-term defect levels corresponding to 4.5 sigma performance is, by Six Sigma convention, described as a "six sigma process". The accepted Six Sigma scoring system thus cannot be equated to actual normal distribution probabilities for the stated number of standard deviations, and this has been a key bone of contention over how Six Sigma measures are defined. The fact that it is rarely explained that a "6 sigma" process will have long-term defect rates corresponding to 4.5 sigma performance rather than actual 6 sigma performance has led several commentators to express the opinion that Six Sigma is a confidence trick.

Stifling creativity in research
According to John Dodge, editor in chief of Design News, the use of Six Sigma is inappropriate in a research environment.  Dodge states "excessive metrics, steps, measurements and Six Sigma's intense focus on reducing variability water down the discovery process.  Under Six Sigma, the free-wheeling nature of brainstorming and the serendipitous side of discovery is stifled."  He concludes "there's general agreement that freedom in basic or pure research is preferable while Six Sigma works best in incremental innovation when there's an expressed commercial goal."
A BusinessWeek article says that James McNerney's introduction of Six Sigma at 3M had the effect of stifling creativity and reports its removal from the research function. It cites two Wharton School professors who say that Six Sigma leads to incremental innovation at the expense of blue skies research. This phenomenon is further explored in the book Going Lean, which describes a related approach known as lean dynamics and provides data to show that Ford's 6 Sigma program did little to change its fortunes.

Lack of documentation
One criticism voiced by Yasar Jarrar and Andy Neely from the Cranfield School of Management's Centre for Business Performance is that while Six Sigma is a powerful approach, it can also unduly dominate an organization's culture; and they add that much of the Six Sigma literature – in a remarkable way (six-sigma claims to be evidence, scientifically based) – lacks academic rigor:

One final criticism, probably more to the Six Sigma literature than concepts, relates to the evidence for Six Sigma’s success. So far, documented case studies using the Six Sigma methods are presented as the strongest evidence for its success. However, looking at these documented cases, and apart from a few that are detailed from the experience of leading organizations like GE and Motorola, most cases are not documented in a systemic or academic manner. In fact, the majority are case studies illustrated on websites, and are, at best, sketchy. They provide no mention of any specific Six Sigma methods that were used to resolve the problems. It has been argued that by relying on the Six Sigma criteria, management is lulled into the idea that something is being done about quality, whereas any resulting improvement is accidental (Latzko 1995). Thus, when looking at the evidence put forward for Six Sigma's success, mostly by consultants and people with vested interests, the question that begs to be asked is: are we making a true improvement with Six Sigma methods or just getting skilled at telling stories? Everyone seems to believe that we are making true improvements, but there is some way to go to document these empirically and clarify the causal relations.

See also
Design for Six Sigma – Approach to design supporting Six Sigma
DMAIC – Data-driven improvement cycle used for improving and optimizing business processes
Kaizen – Japanese concept referring to continuous improvement – a philosophical focus on continuous improvement of processes
Lean Six Sigma – Methodology of systematically removing waste
Lean manufacturing – Methodology used to improve production
Management fad – Pejorative term for organizational practice
Quality management – Business process to aid consistent product fitness
Total productive maintenance – Maintenance management methodology
Total quality management – Approach to business improvement.
W. Edwards Deming – American engineer and statistician (1900–1993)

References


== Further reading ==

Supply and demand

In microeconomics, supply and demand is an economic model of price determination in a market. It postulates that, holding all else equal, in a competitive market, the unit price for a particular good, or other traded item such as labor or liquid financial assets, will vary until it settles at a point where the quantity demanded (at the current price) will equal the quantity supplied (at the current price), resulting in an economic equilibrium for price and quantity transacted. The concept of supply and demand forms the theoretical basis of modern economics.  
In macroeconomics, as well, the aggregate demand-aggregate supply model has been used to depict how the quantity of total output and the aggregate price level may be determined in equilibrium.

Graphical representations
Supply schedule
A supply schedule, depicted graphically as a supply curve, is a table that shows the relationship between the price of a good and the quantity supplied by producers. Under the assumption of perfect competition, supply is determined by marginal cost: firms will produce additional output as long as the cost of producing an extra unit is less than the market price they receive.
A rise in the cost of raw materials would decrease supply, shifting the supply curve to the left because at each possible price a smaller quantity would be supplied. One may also think of this as a shift up in the supply curve, because the price must rise for producers to supply a given quantity.  A fall in production costs would increase supply, shifting the supply curve to the right and down. 
Mathematically, a supply curve is represented by a  supply function, giving the quantity supplied as a function of its price and as many other variables as desired to better explain quantity supplied. The two most common specifications are:
1) linear supply function, e.g.,  the slanted line 

  
    
      
        Q
        (
        P
        )
        =
        3
        P
        −
        6
      
    
    {\displaystyle Q(P)=3P-6}
  , and2) the constant-elasticity supply function (also called isoelastic or log-log or loglinear supply function), e.g., the smooth curve

  
    
      
        Q
        (
        P
        )
        =
        5
        
          P
          
            0.5
          
        
      
    
    {\displaystyle Q(P)=5P^{0.5}}
  which can be rewritten as 

  
    
      
        log
        ⁡
        Q
        (
        P
        )
        =
        log
        ⁡
        5
        +
        0.5
        log
        ⁡
        P
      
    
    {\displaystyle \log Q(P)=\log 5+0.5\log P}
  By its very nature, the concept of a supply curve assumes that firms are perfect competitors, having no influence over the market price. This is because each point on the supply curve answers the question, "If this firm is faced with this potential price, how much output will it sell?" If a firm has market power—in violation of the perfect competitor model—its decision on how much output to bring to market influences the market price. Thus the firm is not "faced with" any given price,  and a more complicated model, e.g., a monopoly or oligopoly or differentiated-product model, should be used.
Economists distinguish between the supply curve of an individual firm and the market supply curve.  The market supply curve shows the total quantity supplied by all firms, so it is the sum of the quantities supplied by all suppliers at each potential price  (that is, the individual firms' supply curves are added horizontally).
Economists  distinguish between  short-run and   long-run  supply curve.  Short run  refers to a time period during which one or more inputs are fixed (typically physical capital), and the number of firms in the industry is also fixed (if it is a market supply curve).  Long run refers to a time period during which new firms enter or existing firms exit  and all inputs can be adjusted fully to any price change.   Long-run supply curves are flatter than short-run counterparts (with quantity more sensitive to price, more elastic supply).
Common determinants of supply are:

Prices of inputs, including wages
The technology used, productivity
Firms' expectations about future prices
Number of suppliers (for a market supply curve)

Demand schedule
A demand schedule, depicted graphically as a demand curve, represents the amount of a certain good that buyers are willing and able to purchase at various prices, assuming all other determinants of demand are held constant, such as income, tastes and preferences, and the prices of substitute and complementary goods.   Generally, consumers will buy an additional unit as long as the marginal value of the extra unit is more than the market price they pay.  According to the law of demand, the demand curve is always downward-sloping, meaning that as the price decreases, consumers will buy more of the good.
Mathematically, a demand curve is represented by a demand function, giving the quantity demanded as a function of its price and as many other variables as desired to better explain  quantity demanded. The two most common specifications are linear demand, e.g.,  the slanted line 

  
    
      
        Q
        (
        P
        )
        =
        32
        −
        2
        P
      
    
    {\displaystyle Q(P)=32-2P}
  and the constant-elasticity demand function (also called isoelastic or log-log or loglinear demand function), e.g., the smooth curve

  
    
      
        Q
        (
        P
        )
        =
        3
        
          P
          
            −
            2
          
        
      
    
    {\displaystyle Q(P)=3P^{-2}}
  which can be rewritten as 

  
    
      
        log
        ⁡
        Q
        (
        P
        )
        =
        log
        ⁡
        3
        −
        2
        log
        ⁡
        P
      
    
    {\displaystyle \log Q(P)=\log 3-2\log P}
  As a matter of historical convention, a demand curve is drawn with price on the vertical y-axis and demand on the horizontal x-axis.  In keeping with modern convention, a demand curve would instead be drawn with price on the x-axis and demand on the y-axis, because price is the independent variable and demand is the variable that is dependent upon price.
Just as the supply curve parallels the marginal cost curve,  the demand curve parallels marginal utility, measured in dollars. Consumers will be willing to buy a given quantity of a good, at a given price, if the marginal utility of additional consumption is equal to the opportunity cost determined by the price, that is, the marginal utility of alternative consumption choices. The demand schedule is defined as the willingness and ability of a consumer to purchase a given product at a certain time.
The demand curve is generally downward-sloping, but for some goods it is upward-sloping. Two such types of goods have been given definitions and names that are in common use: Veblen goods, goods which because of fashion or  signalling  are  more attractive at higher prices, and Giffen goods, which, by virtue of being inferior goods that absorb a large part of a consumer's income (e.g., staples such as the classic example of potatoes in Ireland), may see an increase in quantity demanded when the price rises. The reason the law of demand is violated for Giffen goods is that the rise in the price of the good has a strong income effect, sharply reducing the purchasing power of the consumer so that he switches away from luxury goods to the Giffen good, e.g., when the price of potatoes rises, the Irish peasant can no longer afford meat and eats more potatoes to cover for the lost calories.
As with the supply curve, by its very nature  the concept of a demand curve requires that the purchaser be a perfect competitor—that is, that the purchaser have no influence over the market price.  This is true because each point on the demand curve answers the question, "If buyers are faced with this potential price, how much of the product will they purchase?" But, if  a buyer has market power (that is, the amount  he buys influences the price),  he is not "faced with" any given price, and  we must use a more complicated model, of monopsony.
As with supply curves, economists distinguish between the demand curve for an individual and the demand curve for a market.  The market demand curve is obtained by adding the  quantities from the individual demand curves at each price.
Common determinants of demand are:

Income
Tastes and preferences
Prices of related goods and services
Consumers' expectations about future prices and incomes
Number of potential consumers
Advertising

History of the curves
Since supply and demand can be considered as functions of price they have a natural graphical representation. Demand curves were first drawn by Augustin Cournot in his Recherches sur les Principes Mathématiques de la Théorie des Richesses (1838) – see Cournot competition. Supply curves were added by Fleeming Jenkin in The Graphical Representation of the Laws of Supply and Demand... of 1870. Both sorts of curve were popularised by Alfred Marshall who, in his Principles of Economics (1890), chose to represent price – normally the independent variable – by the vertical axis; a practice which remains common.
If supply or demand is a function of other variables besides price, it may be represented by a family of curves (with a change in the other variables constituting a shift between curves) or by a surface in a higher dimensional space.

Microeconomics
Equilibrium
Generally speaking,  an equilibrium is defined to be the price-quantity pair where the quantity demanded is equal to the quantity supplied. It is represented by the intersection of the demand and supply curves. The analysis of various equilibria is a fundamental aspect of microeconomics:
Market equilibrium: A situation in a market when the price is such that the quantity demanded by consumers is correctly balanced by the quantity that firms wish to supply. In this situation, the market clears.Changes in market equilibrium:
Practical uses of supply and demand analysis often center on the different variables that change equilibrium price and quantity, represented as shifts in the respective curves. Comparative statics of such a shift traces the effects from the initial equilibrium to the new equilibrium.
Demand curve shifts:

When consumers increase the quantity demanded at a given price, it is referred to as an increase in demand. Increased demand can be represented on the graph as the curve being shifted to the right. At each price point, a greater quantity is demanded, as from the initial curve D1 to the new curve D2. In the diagram, this raises the equilibrium price from P1 to the higher P2. This raises the equilibrium quantity from Q1 to the higher Q2.  (A movement along the curve is described as a "change in the quantity demanded" to distinguish it from a "change in demand," that is, a shift of the curve.) The increase in demand has caused an increase in (equilibrium) quantity.  The increase in demand could come from changing tastes and fashions, incomes, price changes in complementary and substitute goods, market expectations, and number of buyers. This would cause the entire demand curve to shift changing the equilibrium price and quantity.  Note in the diagram that the shift of the demand curve, by causing a new equilibrium price to emerge, resulted in movement along the supply curve from the point (Q1, P1) to the point (Q2, P2).
If the demand decreases, then the opposite happens: a shift of the curve to the left. If the demand starts at D2, and decreases to D1, the equilibrium price will decrease, and the equilibrium quantity will also decrease. The quantity supplied at each price is the same as before the demand shift, reflecting the fact that the supply curve has not shifted; but the equilibrium quantity and price are different as a result of the change (shift) in demand.
Supply curve shifts:

When technological progress occurs, the supply curve shifts.  For example, assume that someone invents a better way of growing wheat so that the cost of growing a given quantity of wheat decreases.   Otherwise stated, producers will be willing to supply more wheat at every price and this shifts the supply curve S1 outward, to S2—an increase in supply. This increase in supply causes the equilibrium price to decrease from P1 to P2. The equilibrium quantity increases from Q1 to Q2 as consumers move along the demand curve to the new lower price. As a result of a supply curve shift, the price and the quantity move in opposite directions.  If the quantity supplied decreases, the opposite happens. If the supply curve starts at S2, and shifts leftward to S1, the equilibrium price will increase and the equilibrium quantity will decrease as consumers move along the demand curve to the new higher price and associated lower quantity demanded. The quantity demanded at each price is the same as before the supply shift, reflecting the fact that the demand curve has not shifted. But due to the change (shift) in supply, the equilibrium quantity and price have changed.
The movement of the supply curve in response to a change in a non-price determinant of supply is caused by a change in the y-intercept, the constant term of the supply equation. The supply curve shifts up and down the y axis as non-price determinants of demand change.

Partial equilibrium
Partial equilibrium, as the name suggests, takes into consideration only a part of the market to attain equilibrium.
Jain proposes (attributed to George Stigler): "A partial equilibrium is one which is based on only a restricted range of data, a standard example is price of a single product, the prices of all other products being held fixed during the analysis."The supply-and-demand model is a partial equilibrium model of economic equilibrium, where the clearance on the market of some specific goods is obtained independently from prices and quantities in other markets. In other words, the prices of all substitutes and complements, as well as income levels of consumers are constant. This makes analysis much simpler than in a general equilibrium model which includes an entire economy.
Here the dynamic process is that prices adjust until supply equals demand. It is a powerfully simple technique that allows one to study equilibrium, efficiency and comparative statics.  The stringency of the simplifying assumptions inherent in this approach makes the model considerably more tractable, but may produce results which, while seemingly precise, do not effectively model real world economic
phenomena.
Partial equilibrium analysis examines the effects of policy action in creating equilibrium only in that particular sector or market which is directly affected, ignoring its effect in any other market or industry assuming that they being small will have little impact if any.
Hence this analysis is considered to be useful in constricted markets.
Léon Walras first formalized the idea of a one-period economic equilibrium of the general economic system, but it was French economist Antoine Augustin Cournot and English political economist Alfred Marshall who developed tractable models to analyze an economic system.

Other markets
The model of supply and demand also applies to various specialty markets.
The model is commonly applied to wages, in the market for labor. The typical roles of supplier and demander are reversed. The suppliers are individuals, who try to sell their labor for the highest price. The demanders of labor are businesses, which try to buy the type of labor they need at the lowest price. The equilibrium price for a certain type of labor is the wage rate. However, economist Steve Fleetwood revisited the empirical reality of supply and demand curves in labor markets and concluded that the evidence is "at best inconclusive and at worst casts doubt on their existence." For instance, he cites Kaufman and Hotchkiss (2006): "For adult men, nearly all studies find the labour supply curve to be negatively sloped or backward bending."In both classical and Keynesian economics, the money market is analyzed as a supply-and-demand system with interest rates being the price. The money supply may be a vertical supply curve, if the central bank of a country chooses to use monetary policy to fix its value regardless of the interest rate; in this case the money supply is totally inelastic. On the other hand, the money supply curve is a horizontal line if the central bank is targeting a fixed interest rate and ignoring the value of the money supply; in this case the money supply curve is perfectly elastic. The demand for money intersects with the money supply to determine the interest rate.According to some studies, the laws of supply and demand are applicable not only to the business relationships of people, but to the behaviour of social animals and to all living things that interact on the biological markets in scarce resource environments.
The model of supply and demand accurately describes the characteristic of metabolic systems: specifically, it explains how feedback inhibition allows metabolic pathways to respond to the demand for a metabolic intermediates while minimizing effects due to variation in the supply.

Empirical estimation
Demand and supply relations in a market can be statistically estimated from price, quantity, and other data with sufficient information in the model.  This can be done with simultaneous-equation methods of estimation in econometrics.  Such methods allow solving for the model-relevant "structural coefficients," the estimated algebraic counterparts of the theory. The Parameter identification problem is a common issue in "structural estimation." Typically, data on exogenous variables (that is, variables other than price and quantity, both of which are endogenous variables) are needed to perform such an estimation.  An alternative to "structural estimation" is reduced-form estimation, which regresses each of the endogenous variables on the respective exogenous variables.

Macroeconomic uses
Demand and supply have also been generalized to explain macroeconomic variables in a market economy, including the quantity of total output and the aggregate price level.  The aggregate demand-aggregate supply model may be the most direct application of supply and demand to macroeconomics, but other macroeconomic models also use supply and demand. Compared to microeconomic uses of demand and supply, different (and more controversial) theoretical considerations apply to such macroeconomic counterparts as aggregate demand and aggregate supply. Demand and supply are also used in macroeconomic theory to relate money supply and money demand to interest rates, and to relate labor supply and labor demand to wage rates.

History
The 256th couplet of Tirukkural, which was composed at least 2000 years ago, says that "if people do not consume a product or service, then there will not be anybody to supply that product or service for the sake of price".According to Hamid S. Hosseini, the power of supply and demand was understood to some extent by several early Muslim scholars, such as fourteenth-century Syrian scholar Ibn Taymiyyah, who wrote: "If desire for goods increases while its availability decreases, its price rises. On the other hand, if availability of the good increases and the desire for it decreases, the price comes down."
If desire for goods increases while its availability decreases, its price rises. On the other hand, if availability of the good increases and the desire for it decreases, the price comes down.
Shifting focus to the English etymology of the expression, it has been confirmed that the phrase 'supply and demand' was not used by English economics writers until after the end of the 17th century. 
In John Locke's 1691 work Some Considerations on the Consequences of the Lowering of Interest and the Raising of the Value of Money, Locke alluded to the idea of supply and demand, however, he failed to accurately label it as such and thus, he fell short in coining the phrase and conveying its true significance.  Locke wrote: “The price of any commodity rises or falls by the proportion of the number of buyer and sellers” and “that which regulates the price... [of goods] is nothing else but their quantity in proportion to [the] Vent.”  Locke's terminology drew criticism from John Law. Law argued that,"The Prices of Goods are not according to the quantity in proportion to the Vent, but in proportion to the Demand." From Law the demand part of the phrase was given its proper title and it began to circulate among "prominent authorities" in the 1730s. In 1755, Francis Hutcheson, in his A System of Moral Philosophy, furthered development toward the phrase by stipulating that, "the prices of goods depend on these two jointly, the Demand... and the Difficulty of acquiring."It was not until 1767 that the phrase "supply and demand" was first used by Scottish writer James Denham-Steuart in his Inquiry into the Principles of Political Economy. He originated the use of this phrase by effectively combining "supply" and "demand" together in a number of different occasions such as price determination and competitive analysis. In Steuart's chapter entitled "Of Demand", he argues that "The nature of Demand is to encourage industry; and when it is regularly made, the effect of it is, that the supply for the most part is found to be in proportion to it, and then the demand is simple". It is presumably from this chapter that the idea spread to other authors and economic thinkers. Adam Smith used the phrase after Steuart in his 1776 book The Wealth of Nations. In The Wealth of Nations, Smith asserted that the supply price was fixed but that its "merit" (value) would decrease as its "scarcity" increased, this idea by Smith was later named the law of demand. In 1803, Thomas Robert Malthus used the phrase "supply and demand" twenty times in the second edition of the Essay on Population. 
And David Ricardo in his 1817 work, Principles of Political Economy and Taxation, titled one chapter, "On the Influence of Demand and Supply on Price". In Principles of Political Economy and Taxation, Ricardo more rigorously laid down the idea of the assumptions that were used to build his ideas of supply and demand. In 1838, Antoine Augustin Cournot developed a mathematical model of supply and demand in his Researches into the Mathematical Principles of Wealth, it included diagrams. It is important to note  that the use of the phrase was still rare and only a few examples of more than 20 uses in a single work have been identified by the end of the second decade of the 19th century.During the late 19th century the marginalist school of thought emerged. The main innovators of this approach were Stanley Jevons, Carl Menger, and Léon Walras. The key idea was that the price was set by the subjective value of a good at the margin. This was a substantial change from Adam Smith's thoughts on determining the supply price.
In his 1870 essay "On the Graphical Representation of Supply and Demand", Fleeming Jenkin in the course of "introduc[ing] the diagrammatic method into the English economic literature"  published the first drawing of supply and demand curves in English, including comparative statics from a shift of supply or demand and application to the labor market. The model was further developed and popularized by Alfred Marshall in the 1890 textbook Principles of Economics.

Criticism
The philosopher Hans Albert has argued that the ceteris paribus conditions of the marginalist theory rendered the theory itself an empty tautology and completely closed to experimental testing. In essence, he argues, the supply and demand curves (theoretical functions which express the quantity of a product which would be offered or requested for a given price) are purely ontological.Piero Sraffa's critique focused on the inconsistency (except in implausible circumstances) of partial equilibrium analysis and the rationale for the upward slope of the supply curve in a market for a produced consumption good. The notability of Sraffa's critique is also demonstrated by Paul Samuelson's comments and engagements with it over many years, for example:

What a cleaned-up version of Sraffa (1926) establishes is how nearly empty are all of Marshall's partial equilibrium boxes. To a logical purist of Wittgenstein and Sraffa class, the Marshallian partial equilibrium box of constant cost is even more empty than the box of increasing cost.Modern Post-Keynesians criticize the supply and demand model for failing to explain the prevalence of administered prices, in which retail prices are set by firms, primarily based on a mark-up over normal average unit costs, and are not responsive to changes in demand up to capacity.Some economists criticize the conventional supply and demand theory for failing to explain or anticipate asset bubbles that can arise from a positive feedback loop. Conventional supply and demand theory assumes that expectations of consumers do not change as a consequence of price changes. In scenarios such as the United States housing bubble, an initial price change of an asset can increase the expectations of investors, making the asset more lucrative and contributing to further price increases until market sentiment changes, which creates a positive feedback loop and an asset bubble. Asset bubbles cannot be understood in the conventional supply and demand framework because the conventional system assumes a price change will be self-correcting and the system will snap back to equilibrium.
Paul Cockshott's critique focuses on the unfalsifiability of the neoclassical model.
In the linear examples given above we have four unknowns: the slope and intercept of both the supply curve and the demand curve.
But because we only have two knowns, price and quantity, any set of supply and demand curves that crosses the point 
  
    
      
        (
        Q
        ,
        P
        )
      
    
    {\displaystyle (Q,P)}
   could explain the data. Hence unfalsifiability.
Cockshott also points out that prices are negatively correlated with quantity due to economies of scale, not positively correlated as the theory suggests.
Finally, Cockshott argues that the theory is needlessly complicated when compared to the labour theory of value, and that having to introduce a notion of the curves shifting amounts to adding epicycles.

See also
References
Further reading
Foundations of Economic Analysis by Paul A. Samuelson
Price Theory and Applications by Steven E. Landsburg ISBN 0-538-88206-9
An Inquiry into the Nature and Causes of the Wealth of Nations, Adam Smith, 1776 [1]
Supply and Demand book by Hubert D. Henderson at Project Gutenberg.

External links

Nobel Prize Winner Prof. William Vickrey: 15 fatal fallacies of financial fundamentalism – A Disquisition on Demand Side Economics (William Vickrey)
Marshallian Cross Diagrams and Their Uses before Alfred Marshall: The Origins of Supply and Demand Geometry by Thomas M. Humphrey
By what is the price of a commodity determined?, a brief statement of Karl Marx's rival account
Supply and Demand by Fiona Maclachlan and Basic Supply and Demand by Mark Gillis, Wolfram Demonstrations Project.

Taiichi Ohno

Ohno Taiichi (大野耐一, Ōno Taiichi, February 29, 1912 – May 28, 1990) was a Japanese industrial engineer and businessman. He is considered to be the father of the Toyota Production System, which inspired Lean Manufacturing in the U.S. He devised the seven wastes (or muda in Japanese) as part of this system. He wrote several books about the system, including Toyota Production System: Beyond Large-Scale Production.

Life
Born in 1912 in Dalian, China, and a graduate of the Nagoya Technical High School (Japan), he joined the Toyoda family's Toyoda Spinning upon graduation in 1932 during the Great Depression thanks to the relations of his father to Kiichiro Toyoda, the son of Toyota's founding father Sakichi Toyoda. He moved to the Toyota motor company in 1943 where he worked as a shop-floor supervisor in the engine manufacturing shop of the plant, and gradually rose through the ranks to become an executive.

Influence
Ohno's principles influenced areas outside of manufacturing, and have been extended into the service arena.  For example, the field of sales process engineering has shown how the concept of Just In Time (JIT) can improve sales, marketing, and customer service processes.

Seven Wastes
Ohno was also instrumental in developing the way organizations identify waste, with his "Seven Wastes" model which have become core in many academic approaches. These wastes are:
1. Delay, waiting or time spent in a queue with no value being added 
2. Producing more than you need 
3. Over processing or undertaking non-value added activity 
4. Transportation 
5. Unnecessary movement or motion  
6. Inventory 
7. Defects in the Product.

Ten Precepts
Ohno is also known for his "Ten Precepts" to think and act to win.
You are a cost.  First reduce waste.
First say, "I can do it." And try before everything.
The workplace is a teacher.  You can find answers only in the workplace.
Do anything immediately. Starting something right now is the only way to win.
Once you start something, persevere with it. Do not give up until you finish it.
Explain difficult things in an easy-to-understand manner.  Repeat things that are easy to understand.
Waste is hidden.  Do not hide it.  Make problems visible.
Valueless motions are equal to shortening one's life.
Re-improve what was improved for further improvement.
Wisdom is given equally to everybody.  The point is whether one can exercise it.

See also
Shigeo Shingo (新郷 重雄, Shingō Shigeo)
Just In Time (JIT)
Lean manufacturing

Published works
Ohno, Taiichi (1988), Toyota Production System: Beyond Large-Scale Production, Productivity Press, ISBN 0-915299-14-3
Ohno, Taiichi (1988), Workplace Management, Productivity Press, ISBN 0-915299-19-4
Ohno, Taiichi  (2007), Workplace Management. Translated by Jon Miller, Gemba Press, ISBN 978-0-9786387-5-7, ISBN 0-9786387-5-1


== References ==

Takt time

Takt time, or simply takt, is a manufacturing term to describe the required product assembly duration that is needed to match the demand. Often confused with cycle time, takt time is a tool used to design work and it measures the average time interval between the start of production of one unit and the start of production of the next unit when items are produced sequentially. For calculations, it is the time to produce parts divided by the number of parts demanded in that time interval. The takt time is based on customer demand; if a process or a production line are unable to produce at takt time, either demand leveling, additional resources, or process re-engineering is needed to ensure on-time delivery.
For example, if the customer demand is 10 units per week, then, given a 40-hour workweek and steady flow through the production line, the average duration between production starts should be 4 hours, ideally. This interval is further reduced to account for things like machine downtime and scheduled employee breaks.

Etymology
Takt time is a borrowing of the Japanese word takuto taimu (タクトタイム), which in turn was borrowed from the German word Taktzeit, meaning 'cycle time'. The word was likely introduced to Japan by German engineers in the 1930s.The word originates from the Latin word "tactus" meaning "touch, sense of touch, feeling". Some earlier meanings include: (16th century) "beat triggered by regular contact, clock beat", then in music "beat indicating the rhythm" and (18th century) "regular unit of note values".

History
Takt time has played an important role in production systems even before the industrial revolution. From 16th-century shipbuilding in Venice, mass-production of Model T by Henry Ford, synchronizing airframe movement in the German aviation industry and many more. Cooperation between the German aviation industry and Mitsubishi brought takt to Japan, where Toyota incorporated it in the Toyota Production System (TPS).James P. Womack and Daniel T. Jones in The Machine That Changed the World (1990) and Lean Thinking (1996) introduced the world to the concept of "lean". Through this, Takt was connected to lean systems. In the Toyota Production System (TPS), takt time is a central element of the just-in-time pillar (JIT) of this production system.

Definition
Assuming a product is made one unit at a time at a constant rate during the net available work time, the takt time is the amount of time that must elapse between two consecutive unit completions in order to meet the demand.
Takt time can be first determined with the formula:

  
    
      
        T
        =
        
          
            
              T
              
                a
              
            
            D
          
        
      
    
    {\displaystyle T={\frac {T_{a}}{D}}}
  Where
T   = Takt time (or takt), e.g. [work time between two consecutive units]
Ta = Net time available to work during the period, e.g. [work time per period]
D = Demand (customer demand) during the period, e.g. [units required per period]
Net available time is the amount of time available for work to be done. This excludes break times and any expected stoppage time (for example scheduled maintenance, team briefings, etc.).

Example:If there are a total of 8 hours (or 480 minutes) in a shift (gross time) less 30 minutes lunch, 30 minutes for breaks (2 × 15 mins), 10 minutes for a team briefing and 10 minutes for basic maintenance checks, then the net Available Time to Work = 480 - 30 - 30 - 10 - 10 = 400 minutes.
If customer demand were 400 units a day and one shift was being run, then the line would be required to output at a minimum rate of one part per minute in order to be able to keep up with customer demand.

Takt time may be adjusted according to requirements within a company. For example, if one department delivers parts to several manufacturing lines, it often makes sense to use similar takt times on all lines to smooth outflow from the preceding station. Customer demand can still be met by adjusting daily working time, reducing down times on machines, and so on.

Implementation
Takt time is common in production lines that move a product along a line of stations that each performs a set of predefined tasks.

Manufacturing: casting of parts, drilling holes, or preparing a workplace for another task
Control tasks: testing of parts or adjusting machinery
Administration: answering standard inquiries or call center operation
Construction Management: scheduling process steps within a phase of the project

Takt in construction
With the adoption of lean thinking in the construction industry, takt time has found its way into the project-based production systems of the industry. Starting with construction methods that have highly repetitive products like bridge construction, tunnel construction, and repetitive buildings like hotels and residential high-rises, implementation of takt is increasing.According to Koskela (1992), an ideal production system  has continuous flow and creates value for the customer while transforming raw materials into products. Construction projects use critical path method (CPM) or program evaluation and review technique (PERT) for planning and scheduling. These methods do not generate flow in the production and tend to be vulnerable to variation in the system. Due to common cost and schedule overruns, industry professionals and academia have started to regard CPM and PERT as outdated methods that often fail to anticipate uncertainties and allocate resources accurately and optimally in a dynamic construction environment. This has led to increasing developments and implementation of takt.

Space scheduling
Takt, as used in takt planning or takt-time planning (TTP) for construction, is considered one of the several ways of planning and scheduling construction projects based on their utilization of space rather than just time, as done traditionally in the critical path method. Also, to visualize and create flow of work on a construction site, utilization of space becomes essential. Some other space scheduling methods include:

Linear scheduling method (LSM) and vertical production method (VPM) which are used to schedule horizontal and vertical repetitive projects respectively,
Line-of-balance (LOB) method used for any type of repetitive projects.
Location-based management system (LBMS) uses flowlines with the production rates of the crews, as they move through locations with an objective of optimizing work continuity.

Comparison with manufacturing
In manufacturing, the product being built keeps moving on the assembly line, while the workstations are stationary. On contrary, construction product, i.e. the building or infrastructure facilities being constructed, is stationary and the workers move from one location to another.Takt planning needs an accurate definition of work at each workstation, which in construction is done through defining spaces, called "zones". Due to the non-repetitive distribution of work in construction, achieving work completion within the defined takt for each zone, becomes difficult. Capacity buffer is used to deal with this variability in the system.The rationale behind defining these zones and setting the takt is not standardized and varies as per the style of the planner. Work density method (WDM) is one of the methods being used to assist in this process. Work density is expressed as a unit of time per unit of area. For a certain work area, work density describes how much time a trade will require to do their work in that area (zone), based on:
the product's design, i.e., what is in the construction project drawings and specifications
the scope of the trade's work,
the specific task in their schedule (depending on work already in place and work that will follow later in the same or another process),
the means and methods the trade will use (e.g., when prefabricating off-site, the work density on-site is expected to decrease),
while accounting for crew capabilities and size.

Benefits of takt time
Once a takt system is implemented there are a number of benefits:

The product moves along a line, so bottlenecks (stations that need more time than planned) are easily identified when the product does not move on in time.
Correspondingly, stations that don't operate reliably (suffer a frequent breakdown, etc.) are easily identified.
The takt leaves only a certain amount of time to perform the actual value added work. Therefore, there is a strong motivation to get rid of all non-value-adding tasks (like machine set-up, gathering of tools, transporting products, etc.)
Workers and machines perform sets of similar tasks, so they don't have to adapt to new processes every day, increasing their productivity.
There is no place in the takt system for removal of a product from the assembly line at any point before completion, so opportunities for shrink and damage in transit are minimized.

Problems of takt time
Once a takt system is implemented there are a number of problems:

When customer demand rises so much that takt time has to come down, quite a few tasks have to be either reorganized to take even less time to fit into the shorter takt time, or they have to be split up between two stations (which means another station has to be squeezed into the line and workers have to adapt to the new setup)
When one station in the line breaks down for whatever reason the whole line comes to a grinding halt, unless there are buffer capacities for preceding stations to get rid of their products and following stations to feed from. A built-in buffer of three to five percent downtime allows needed adjustments or recovery from failures.
Short takt time can put considerable stress on the "moving parts" of a production system or subsystem. In automated systems/subsystems, increased mechanical stress increases the likelihood of a breakdown, and in non-automated systems/subsystems, personnel face both increased physical stress (which increases the risk of repetitive motion (also "stress" or "strain") injury), intensified emotional stress, and lowered motivation, sometimes to the point of increased absenteeism.
Tasks have to be leveled to make sure tasks don't bulk in front of certain stations due to peaks in workload. This decreases the flexibility of the system as a whole.
The concept of takt time doesn't account for human factors such as an operator needing an unexpected bathroom break or a brief rest period between units (especially for processes involving significant physical labor).  In practice, this means that the production processes must be realistically capable of operation above peak takt and demand must be leveled in order to avoid wasted line capacity

See also
Turnaround time
Lean manufacturing
Toyota Production System
Muri
Lean construction
Factory Physics, a book on manufacturing management

References
External links
Lean Manufacturing site about Takt time
Six Sigma site about Takt time
On Line business processes simulator
Takt Time - a vision for Lean Manufacturing

Further reading
Monden, Yasuhiro (2011). Toyota Production System: An Integrated Approach to Just-In-Time. New York: Productivity Press. p. 566. ISBN 978-1-4398-2097-1.
Ohno, Taiichi, Toyota Production System: Beyond Large-Scale Production, Productivity Press (1988). ISBN 0-915299-14-3
Baudin, Michel, Lean Assembly: The Nuts and Bolts of Making Assembly Operations Flow, Productivity Press (2002). ISBN 1-56327-263-6
Ortiz, Chris A., Kaizen Assembly: Designing, Constructing, and Managing a Lean Assembly Line, CRC Press. ISBN 978-0-8493-7187-5

The Machine that Changed the World (book)



The Toyota Way

The Toyota Way is a set of principles defining the organizational culture of Toyota Motor Corporation. The company formalized the Toyota Way in 2001, after decades of academic research into the Toyota Production System and its implications for lean manufacturing as a methodology that could be adopted by other organizations. The two pillars of the Toyota Way are respect for people and continuous improvement. The philosophy was popularized by Jeffrey K. Liker in his 2004 book, The Toyota Way: 14 Management Principles from the World's Greatest Manufacturer. Subsequent research has explored the extent to which the Toyota Way can be applied in other contexts.

Background
The principles were first collated into a single document in the company's pamphlet "The Toyota Way 2001", to help codify the company's organizational culture. The philosophy was subsequently analyzed in the 2004 book The Toyota Way by industrial engineering researcher Jeffrey Liker, and has received attention in the fields of business administration education and corporate governance.

Principles
The principles of the Toyota Way are divided into the two broad categories of continuous improvement and respect for human resources. The standards for constant improvement include directives to set up a long-term vision, to engage in a step-by-step approach to challenges, to search for the root causes of problems, and to engage in ongoing innovation. The standards pertaining to respect for individuals incorporate ways of building appreciation and cooperation.
The system is summarized in 14 principles:
"Base your management decisions on a long-term philosophy, even at the expense of short-term financial goals."
"Create a continuous process flow to bring problems to the surface." Work processes are redesigned to eliminate waste (muda) such as overproduction and waiting times through the process of continuous improvement (kaizen).
"Use 'pull' systems to avoid overproduction." A pull system produces only required material after a subsequent operation signals a need for it.
"Level out the workload (heijunka). (Work like the tortoise, not the hare.)" This principle is aimed at avoiding overburdening people or equipment and creating uneven production levels (mura).
"Build a culture of stopping to fix problems, to get quality right the first time." Quality takes precedence (Jidoka). Any employee has the authority to stop the process to signal a quality issue.
"Standardized tasks and processes are the foundation for continuous improvement and employee empowerment."
"Use visual control so no problems are hidden." Included in this principle is the 5S Program, steps that are used to make all workspaces efficient and productive, help people share workstations, reduce time looking for needed tools, and improve the work environment.
"Use only reliable, thoroughly tested technology that serves your people and processes."
"Grow leaders who thoroughly understand the work, live the philosophy, and teach it to others." This principle argues that training and ingrained perspective are necessary for maintaining the organization.
"Develop exceptional people and teams who follow your company's philosophy."
"Respect your extended network of partners and suppliers by challenging them and helping them improve." In this principle, Toyota states its intention to apply the same principles to suppliers that are applied to employees.
"Go and see for yourself to thoroughly understand the situation (Genchi Genbutsu)." Toyota managers are expected to experience operations firsthand in order to see how they can be improved.
"Make decisions slowly by consensus, thoroughly considering all options; implement decisions rapidly (nemawashi)."
"Become a learning organization through relentless reflection (hansei) and continuous improvement (kaizen)." The general problem-solving technique to determine the root cause of a problem includes initial problem perception, clarification of the problem, locating the cause, root cause analysis, applying countermeasures, reevaluating, and standardizing.

Research findings
In 2004, Jeffrey Liker, a University of Michigan professor of industrial engineering, published The Toyota Way. In his book, Liker calls the Toyota Way "a system designed to provide the tools for people to continually improve their work." According to Liker, the 14 principles of The Toyota Way are organized into four sections: (1) long-term philosophy, (2) the right process will produce the right results, (3) add value to the organization by developing your people, and (4) continuously solving root problems drives organizational learning.

Long-term philosophy
The first principle involves managing with a long view rather than for short-term gain. It reflects a belief that people need a purpose to find motivation and establish goals.

Right process will produce right results
The next seven principles are focused on process with an eye towards a quality outcome. Following these principles, work processes are redesigned to eliminate waste (muda) through the process of continuous improvement — kaizen. The seven types of muda are (1) overproduction; (2) waiting, time on hand; (3) unnecessary transport or conveyance; (4) overprocessing or incorrect processing; (5) excess inventory; (6) motion; and (7) defects.
The principles in this section empower employees in spite of the bureaucratic processes of Toyota, as any employee in the Toyota Production System has the authority to stop production to signal a quality issue, emphasizing that quality takes precedence (Jidoka). The way the Toyota bureaucratic system is implemented allows for continuous improvement (kaizen) from the people affected by that system so that any employee may aid in the growth and improvement of the company.
Recognition of the value of employees is also part of the principle of measured production rate (heijunka), as a level workload helps avoid overburdening people and equipment (muri), but this is also intended to minimize waste (muda) and avoid uneven production levels (mura).
These principles are also designed to ensure that only essential materials are employed (to avoid overproduction), that the work environment is maintained efficiently (the 5S Program) to help people share workstations and to reduce time looking for needed tools, and that the technology used is reliable and thoroughly tested.

Value to organization by developing people
Human development is the focus of principles 9 through 11. Principle 9 emphasizes the need to ensure that leaders embrace and promote the corporate philosophy. This reflects, according to Liker, a belief that the principles have to be ingrained in employees to survive. The 10th principle emphasizes the need for individuals and work teams to embrace the company's philosophy, with teams of 4-5 people who are judged in success by their team achievements, rather than their individual efforts. Principle 11 looks to business partners, who are treated by Toyota much like they treat their employees. Toyota challenges them to do better and helps them achieve it, providing cross-functional teams to help suppliers discover and fix problems to become a stronger, better supplier.

Solving root problems drives organizational learning
The final principles embrace a philosophy of problem-solving that emphasizes thorough understanding, consensus-based solutions swiftly implemented and continual reflection (hansei) and improvement (kaizen). The 12th principle (Genchi Genbutsu) sets out the expectation that managers will personally evaluate operations so that they have a firsthand understanding of situations and problems. Principle 13 encourages thorough consideration of possible solutions through a consensus process, with rapid implementation of decisions once reached (nemawashi). The final principle requires that Toyota be a "learning organization", continually reflecting on its practices and striving for improvement. According to Liker, the process of becoming a learning organization involves criticizing every aspect of what one does.

Translating the principles
There is a question of uptake of the principles now that Toyota has production operations in many different countries around the world. As a New York Times article notes, while the corporate culture may have been easily disseminated by word of mouth when Toyota manufacturing was only in Japan, with worldwide production, many different cultures must be taken into account. Concepts such as "mutual ownership of problems", or "genchi genbutsu", (solving problems at the source instead of behind desks), and the "kaizen mind", (an unending sense of crisis behind the company's constant drive to improve), may be unfamiliar to North Americans and people of other cultures. A recent increase in vehicle recalls may be due, in part, to "a failure by Toyota to spread its obsession for craftsmanship among its growing ranks of overseas factory workers and managers." Toyota is attempting to address these needs by establishing training institutes in the United States and in Thailand.

Results
Toyota Way has been driven so deeply into the psyche of employees at all levels that it has morphed from a strategy into an important element of the company's culture. According to Masaki Saruta, author of several books on Toyota, "the real Toyota Way is a culture of control." The Toyota Way rewards intense company loyalty that at the same time invariably reduces the voice of those who challenge authority. "The Toyota Way of constructive criticism to reach a better way of doing things 'is not always received in good spirit at home.'" The Toyota Way management approach at the automaker "worked until it didn't."One consequence was when Toyota was given reports of sudden acceleration in its vehicles and the company faced a potential recall situation. There were questions if Toyota's crisis was caused by the company losing sight of its own principles. The Toyota Way, in this case, did not address the problem and provide direction on what the automaker would be doing, but managers instead protected the company and issued flat-out denials and placed the blame at others. The consequence of the automaker's actions led to the 2009–11 Toyota vehicle recalls. Although one of the Toyota Way principles is to "build a culture of stopping to fix problems to get quality right the first time," Akio Toyoda, President and CEO, stated during Congressional hearings that the reason for the problems was that his "company grew too fast." Toyota management had determined its goal was to become the world's largest automotive manufacturer. According to some management consultants, when the pursuit of growth took priority, the automaker "lost sight of the key values that gave it its reputation in the first place."

See also
Toyota Production System
Lean manufacturing
Kanban: a workflow management system also pioneered at Toyota

References
Further reading
Hino, Satoshi (2005). Inside the Mind of Toyota: Management Principles for Enduring Growth. Productivity Press. ISBN 9781563273001.
Liker, Jeffrey K.; Meier, David (2005). The Toyota Way Fieldbook: A Practical Guide for Implementing Toyota's 4Ps. McGraw-Hill. ISBN 9780071448932.

Theory of constraints

The theory of constraints (TOC) is a management paradigm that views any manageable system as being limited in achieving more of its goals by a very small number of constraints. There is always at least one constraint, and TOC uses a focusing process to identify the constraint and restructure the rest of the organization around it. TOC adopts the common idiom "a chain is no stronger than its weakest link". That means that organizations and processes are vulnerable because the weakest person or part can always damage or break them, or at least adversely affect the outcome.

History
The theory of constraints (TOC) is an overall management philosophy, introduced by Eliyahu M. Goldratt in his 1984 book titled The Goal, that is geared to help organizations continually achieve their goals. Goldratt adapted the concept to project management with his book Critical Chain, published in 1997.
An earlier propagator of a similar concept was Wolfgang Mewes in Germany with publications on power-oriented management theory (Machtorientierte Führungstheorie, 1963) and following with his Energo-Kybernetic System (EKS, 1971), later renamed Engpasskonzentrierte Strategie (Bottleneck-focused Strategy) as a more advanced theory of bottlenecks. The publications of Wolfgang Mewes are marketed through the FAZ Verlag, publishing house of the German newspaper Frankfurter Allgemeine Zeitung. However, the paradigm Theory of constraints was first used by Goldratt.

Key assumption
The underlying premise of the theory of constraints is that organizations can be measured and controlled by variations on three measures: throughput, operational expense, and inventory. Inventory is all the money that the system has invested in purchasing things which it intends to sell. Operational expense is all the money the system spends in order to turn inventory into throughput. Throughput is the rate at which the system generates money through sales.Before the goal itself can be reached, necessary conditions must first be met. These typically include safety, quality, legal obligations, etc.  For most businesses, the goal itself is to make profit.  However, for many organizations and non-profit businesses, making money is a necessary condition for pursuing the goal.  Whether it is the goal or a necessary condition, understanding how to make sound financial decisions based on throughput, inventory, and operating expense is a critical requirement.

The five focusing steps
Theory of constraints is based on the premise that the rate of goal achievement by a goal-oriented system (i.e., the system's throughput) is limited by at least one constraint.
The argument by reductio ad absurdum is as follows: If there was nothing preventing a system from achieving higher throughput (i.e., more goal units in a unit of time), its throughput would be infinite – which is impossible in a real-life system.
Only by increasing flow through the constraint can overall throughput be increased.Assuming the goal of a system has been articulated and its measurements defined, the steps are:

Identify the system's constraint(s).
Decide how to exploit the system's constraint(s).
Subordinate everything else to the above decision.
Elevate the system's constraint(s).
Warning! If in the previous steps a constraint has been broken, go back to step 1, but do not allow inertia to cause a system's constraint.The goal of a commercial organization is: "Make more money now and in the future", and its measurements are given by throughput accounting as: throughput, inventory, and operating expenses.
The five focusing steps aim to ensure ongoing improvement efforts are centered on the organization's constraint(s).  In the TOC literature, this is referred to as the process of ongoing improvement (POOGI).
These focusing steps are the key steps to developing the specific applications mentioned below.

Constraints
A constraint is anything that prevents the system from achieving its goal. There are many ways that constraints can show up, but a core principle within TOC is that there are not tens or hundreds of constraints. There is at least one, but at most only a few in any given system. Constraints can be internal or external to the system. An internal constraint is in evidence when the market demands more from the system than it can deliver. If this is the case, then the focus of the organization should be on discovering that constraint and following the five focusing steps to open it up (and potentially remove it). An external constraint exists when the system can produce more than the market will bear. If this is the case, then the organization should focus on mechanisms to create more demand for its products or services.
Types of (internal) constraints

Equipment: The way equipment is currently used limits the ability of the system to produce more salable goods/services.
People: Lack of skilled people limits the system. Mental models held by people can cause behaviour that becomes a constraint.
Policy: A written or unwritten policy prevents the system from making more.The concept of the constraint in Theory of Constraints is analogous to but differs from the constraint that shows up in mathematical optimization. In TOC, the constraint is used as a focusing mechanism for management of the system.  In optimization, the constraint is written into the mathematical expressions to limit the scope of the solution (X can be no greater than 5).
Please note: organizations have many problems with equipment, people, policies, etc. (A breakdown is just that – a breakdown – and is not a constraint in the true sense of the TOC concept). The constraint is the limiting factor that is preventing the organization from getting more throughput (typically, revenue through sales) even when nothing goes wrong.

Breaking a constraint
If a constraint's throughput capacity is elevated to the point where it is no longer the system's limiting factor, this is said to "break" the constraint. The limiting factor is now some other part of the system, or may be external to the system (an external constraint). This is not to be confused with a breakdown.

Buffers
Buffers are used throughout the theory of constraints. They often result as part of the exploit and subordinate steps of the five focusing steps. Buffers are placed before the governing constraint, thus ensuring that the constraint is never starved. Buffers are also placed behind the constraint to prevent downstream failure from blocking the constraint's output. Buffers used in this way protect the constraint from variations in the rest of the system and should allow for normal variation of processing time and the occasional upset (Murphy) before and behind the constraint.
Buffers can be a bank of physical objects before a work center, waiting to be processed by that work center. Buffers ultimately buy you time, as in the time before work reaches the constraint and are often verbalized as time buffers. There should always be enough (but not excessive) work in the time queue before the constraint and adequate offloading space behind the constraint.
Buffers are not the small queue of work that sits before every work center in a kanban system although it is similar if you regard the assembly line as the governing constraint. A prerequisite in the theory is that with one constraint in the system, all other parts of the system must have sufficient capacity to keep up with the work at the constraint and to catch up if time was lost. In a balanced line, as espoused by kanban, when one work center goes down for a period longer than the buffer allows, then the entire system must wait until that work center is restored. In a TOC system, the only situation where work is in danger is if the constraint is unable to process (either due to malfunction, sickness or a "hole" in the buffer – if something goes wrong that the time buffer can not protect).
Buffer management, therefore, represents a crucial attribute of the theory of constraints. There are many ways to apply buffers, but the most often used is a visual system of designating the buffer in three colors: green (okay), yellow (caution) and red (action required). Creating this kind of visibility enables the system as a whole to align and thus subordinate to the need of the constraint in a holistic manner. This can also be done daily in a central operations room that is accessible to everybody.

Plant types
There are four primary types of plants in the TOC lexicon. Draw the flow of material from the bottom of a page to the top, and you get the four types. They specify the general flow of materials through a system, and also provide some hints about where to look for typical problems. This type of analysis is known as VATI analysis as it uses the bottom-up shapes of the letters V, A, T, and I to describe the types of plants. The four types can be combined in many ways in larger facilities, e.g. "an A plant feeding a V plant".

V-plant: The general flow of material is one-to-many, such as a plant that takes one raw material and can make many final products.  Classic examples are meat rendering plants or a steel manufacturer. The primary problem in V-plants is "robbing," where one operation (A) immediately after a diverging point "steals" materials meant for the other operation (B). Once the material has been processed by A, it cannot come back and be run through B without significant rework.
A-plant: The general flow of material is many-to-one, such as in a plant where many sub-assemblies converge for a final assembly. The primary problem in A-plants is in synchronizing the converging lines so that each supplies the final assembly point at the right time.
T-plant: The general flow is that of an I-plant (or has multiple lines), which then splits into many assemblies (many-to-many). Most manufactured parts are used in multiple assemblies and nearly all assemblies use multiple parts. Customized devices, such as computers, are good examples. T-plants suffer from both synchronization problems of A-plants (parts aren't all available for an assembly) and the robbing problems of V-plants (one assembly steals parts that could have been used in another).
I-plant: Material flows in a sequence, such as in an assembly line. The primary work is done in a straight sequence of events (one-to-one). The constraint is the slowest operation.From the above list, one can deduce that for non-material systems one could draw the flow of work or the flow of processes, instead of physical flows, and arrive at similar basic V, A, T, or I structures. A project, for example, is an A-shaped sequence of work, culminating in a delivered product (i.e., the intended outcome of the project).

Applications
The focusing steps, this process of ongoing improvement, have been applied to manufacturing, project management, supply chain/distribution generated specific solutions. Other tools (mainly the "thinking process") also led to TOC applications in the fields of marketing and sales, and finance.  The solution as applied to each of these areas are listed below.

Operations
Within manufacturing operations and operations management, the solution seeks to pull materials through the system, rather than push them into the system. The primary methodology used is drum-buffer-rope (DBR) and a variation called simplified drum-buffer-rope (S-DBR).Drum-buffer-rope is a manufacturing execution methodology based on the fact the output of a system can only be the same as the output at the constraint of the system. Any attempt to produce more than what the constraint can process just leads to excess inventory piling up. The method is named for its three components. The drum is the rate at which the physical constraint of the plant can work: the work center or machine or operation that limits the ability of the entire system to produce more. The rest of the plant follows the beat of the drum. Schedule at the drum decides what the system should produce, in what sequence to produce and how much to produce. They make sure the drum has work and that anything the drum has processed does not get wasted.
The buffer protects the drum, so that it always has work flowing to it. Buffers in DBR provide the additional lead time beyond the required set up and process times, for materials in the product flow. Since these buffers have time as their unit of measure, rather than quantity of material, this makes the priority system operate strictly based on the time an order is expected to be at the drum. Each work order will have a remaining buffer status that can be calculated. Based on this buffer status, work orders can be color coded into Red, Yellow and Green. The red orders have the highest priority and must be worked on first, since they have penetrated most into their buffers followed by yellow and green. As time evolves, this buffer status might change and the color assigned to the particular work order change with it.Traditional DBR usually calls for buffers at several points in the system: the constraint, synchronization points and at shipping. S-DBR has a buffer at shipping and manages the flow of work across the drum through a load planning mechanism.The rope is the work release mechanism for the plant. Orders are released to the shop floor at one "buffer time" before they are due to be processed by the constraint. In other words, if the buffer is 5 days, the order is released 5 days before it is due at the constraint. Putting work into the system earlier than this buffer time is likely to generate too-high work-in-process and slow down the entire system.

High-speed automated production lines
Automated production lines achieve high throughput rates and output quantities by deploying automation solutions that are highly task-specific. Depending on their design and construction, these machines operate at different speeds and capacities and therefore have varying efficiency levels.
A prominent example is the use of automated production lines in the beverage industry. Filling systems usually have several machines executing parts of the complete bottling process, from filling primary containers to secondary packaging and palletisation.To be able to maximize the throughput, the production line usually has a designed constraint. This constraint is typically the slowest and often the most expensive machine on the line. The overall throughput of the line is determined by this machine. All other machines can operate faster and are connected by conveyors.
The conveyors usually have the ability to buffer product. In the event of a stoppage at a machine other than the constraint, the conveyor can buffer the product enabling the constraint machine to keep on running.
A typical line setup is such that in normal operation the upstream conveyors from the constraint machine are always run full to prevent starvation at the constraint and the downstream conveyors are run empty to prevent a back up at the constraint. The overall aim is to prevent minor stoppages at the machines from impacting the constraint.
For this reason as the machines get further from the constraint, they have the ability to run faster than the previous machine and this creates a V curve.

Supply chain and logistics
In general, the solution for supply chains is to create flow of inventory so as to ensure greater availability and to eliminate surpluses.
The TOC distribution solution is effective when used to address a single link in the supply chain and more so across the entire system, even if that system comprises many different companies. The purpose of the TOC distribution solution is to establish a competitive advantage based on extraordinary availability by reducing the damages caused when the flow of goods is interrupted by shortages and surpluses.
This approach uses several new rules to protect availability with less inventory than is conventionally required.

Inventory is held at an aggregation point(s) as close as possible to the source. This approach ensures smoothed demand at the aggregation point, requiring proportionally less inventory. The distribution centers holding the aggregated stock are able to ship goods downstream to the next link in the supply chain much more quickly than a make-to-order manufacturer can.
Following this rule may result in a make-to-order manufacturer converting to make-to-stock.  The inventory added at the aggregation point is significantly less than the inventory reduction downstream.
In all stocking locations, initial inventory buffers are set which effectively create an upper limit of the inventory at that location. The buffer size is equal to the maximum expected consumption within the average Replenishment Time ("RT"), plus additional stock to protect in case a delivery is late. In other words, there is no advantage in holding more inventory in a location than the amount that might be consumed before more could be ordered and received. Typically, the sum of the on hand value of such buffers are 25–75% less than currently observed average inventory levels
Replenishment Time (RT) is the sum of the delay, after the first consumption following a delivery, before an order is placed plus the delay after the order is placed until the ordered goods arrive at the ordering location.
Once buffers have been established, no replenishment orders are placed as long as the quantity inbound (already ordered but not yet received) plus the quantity on hand are equal to or greater than the buffer size. Following this rule causes surplus inventory to be bled off as it is consumed.
For any reason, when on hand plus inbound inventory is less than the buffer, orders are placed as soon as practical to increase the inbound inventory so that the relationship on Hand + Inbound = Buffer is maintained.
To ensure buffers remain correctly sized even with changes in the rates of demand and replenishment, a simple recursive algorithm called Buffer Management is used.  When the on hand inventory level is in the upper third of the buffer for a full RT, the buffer is reduced by one third (and don't forget rule 3).  Alternatively, when the on hand inventory is in the bottom one third of the buffer for too long, the buffer is increased by one third (and don't forget rule 4).  The definition of "too long" may be changed depending on required service levels, however, a rule of thumb is 20% of the RT. Moving buffers up more readily than down is supported by the usually greater damage caused by shortages as compared to the damage caused by surpluses.Once inventory is managed as described above, continuous efforts should be undertaken to reduce RT, late deliveries, supplier minimum order quantities (both per SKU and per order) and customer order batching. Any improvements in these areas will automatically improve both availability and inventory turns, thanks to the adaptive nature of Buffer Management.
A stocking location that manages inventory according to the TOC should help a non-TOC customer (downstream link in a supply chain, whether internal or external) manage their inventory according to the TOC process. This type of help can take the form of a vendor managed inventory (VMI). The TOC distribution link simply extends its buffer sizing and management techniques to its customers' inventories. Doing so has the effect of smoothing the demand from the customer and reducing order sizes per SKU. VMI results in better availability and inventory turns for both supplier and customer. The benefits to the non-TOC customers are sufficient to meet the purpose of capitalizing on the competitive edge by giving the customer a reason to be more loyal and give more business to the upstream link. When the end consumers buy more, the whole supply chain sells more.
One caveat should be considered. Initially and only temporarily, the supply chain or a specific link may sell less as the surplus inventory in the system is sold.  However, the sales lift due to improved availability is a countervailing factor. The current levels of surpluses and shortages make each case different.

Finance and accounting
Holistic thinking applied to the finance application has been termed throughput accounting. Throughput accounting suggests that one examine the impact of investments and operational changes in terms of the impact on the throughput of the business.  It is an alternative to cost accounting.
The primary measures for a TOC view of finance and accounting are: throughput, operating expense and investment. Throughput is calculated from sales minus "totally variable cost", where totally variable cost is usually calculated as the cost of raw materials that go into creating the item sold.: 13–14

Project management
Critical Chain Project Management (CCPM) are utilized in this area. CCPM is based on the idea that all projects look like A-plants: all activities converge to a final deliverable. As such, to protect the project, there must be internal buffers to protect synchronization points and a final project buffer to protect the overall project.

Marketing and sales
While originally focused on manufacturing and logistics, TOC has expanded into sales management and marketing. Its role is explicitly acknowledged in the field of sales process engineering. For effective sales management one can apply Drum Buffer Rope to the sales process similar to the way it is applied to operations (see Reengineering the Sales Process book reference below). This technique is appropriate when your constraint is in the sales process itself, or if you just want an effective sales management technique which includes the topics of funnel management and conversion rates.

Thinking processes
The thinking processes are a set of tools to help managers walk through the steps of initiating and implementing a project. When used in a logical flow, they help walk through a buy-in process:

Gain agreement on the problem
Gain agreement on the direction for a solution
Gain agreement that the solution solves the problem
Agree to overcome any potential negative ramifications
Agree to overcome any obstacles to implementationTOC practitioners sometimes refer to these in the negative as working through layers of resistance to a change.
Recently, the current reality tree (CRT) and future reality tree (FRT) have been applied to an argumentative academic paper.Despite its origins as a manufacturing approach (Goldratt & Cox, The Goal: A process of Ongoing Improvement, 1992), Goldratt's Theory of Constraints (TOC) methodology is now regarded as a systems methodology with strong foundations in the hard sciences (Mabin, 1999). Through its tools for convergent thinking and synthesis, the "Thinking processes", which underpin the entire TOC methodology, help identify and manage constraints and guide continuous improvement and change in organizations (Dettmer H. , 1998).
The process of change requires the identification and acceptance of core issues; the goal and the means to the goal. This comprehensive set of logical tools can be used for exploration, solution development and solution implementation for individuals, groups or organizations. Each tool has a purpose and nearly all tools can be used independently (Cox & Spencer, 1998). Since these thinking tools are designed to address successive "layers of resistance" and enable communication, it expedites securing "buy in" of groups. While CRT (current reality tree) represents the undesirable effects of the current situation, the FRT (the future reality tree), NBR (negative branch) help people plan and understand the possible results of their actions. The PRT (prerequisite tree) and TRT (transition tree) are designed to build collective buy in and aid in the Implementation phase. The logical constructs of these tools or diagrams are the necessary condition logic, the sufficient cause logic and the strict logic rules that are used to validate cause-effect relationships which are modelled with these tools (Dettmer W. , 2006).
A summary of these tools, the questions they help answer and the associated logical constructs used is presented in the table below.

TOC Thinking Process Tools:
Use of these tools are based on the fundamental beliefs of TOC that organizations a) are inherently simple (interdependencies exist in organizations) b) desire inherent harmony (win – win solutions are possible) c) are inherently good (people are good) and have inherent potential (people and organizations have potential to do better) (Goldratt E. , 2009). In the book "Through the clouds to solutions" Jelena Fedurko (Fedurko, 2013) states that the major areas for application of TP tools as:

To create and enhance thinking and learning skills
To make better decisions
To develop responsibility for one's own actions through understanding their consequences
To handle conflicts with more confidence and win-win outcomes
To correct behavior with undesirable consequences
Assist in evaluating conditions for achieving a desired outcome
To assist in peer mediation
To assist in relationship between subordinates and bosses

Development and practice
TOC was initiated by Goldratt, who until his death was still the main driving force behind the development and practice of TOC. There is a network of individuals and small companies loosely coupled as practitioners around the world. TOC is sometimes referred to as "constraint management". TOC is a large body of knowledge with a strong guiding philosophy of growth.

Criticism
Criticisms that have been leveled against TOC include:

Claimed suboptimality of drum-buffer-rope
While TOC has been compared favorably to linear programming techniques, D. Trietsch from University of Auckland argues that DBR methodology is inferior to competing methodologies. Linhares, from the Getulio Vargas Foundation, has shown that the TOC approach to establishing an optimal product mix is unlikely to yield optimum results, as it would imply that P=NP.

Unacknowledged debt
Duncan (as cited by Steyn)
says that TOC borrows heavily from systems dynamics developed by Forrester in the 1950s and from statistical process control which dates back to World War II. And Noreen Smith and Mackey, in their independent report on TOC, point out that several key concepts in TOC "have been topics in management accounting textbooks for decades.": 149  It is also claimed that Goldratt's books fail to acknowledge that TOC borrows from more than 40 years of previous management science research and practice, particularly from program evaluation and review technique/critical path method (PERT/CPM) and the just in time strategy.
A rebuttal to these criticisms is offered in Goldratt's "What is the Theory of Constraints and How Should it be Implemented?", and in his audio program, "Beyond The Goal". In these, Goldratt discusses the history of disciplinary sciences, compares the strengths and weaknesses of the various disciplines, and acknowledges the sources of information and inspiration for the thinking processes and critical chain methodologies. Articles published in the now-defunct Journal of Theory of Constraints referenced foundational materials. Goldratt published an article and gave talks with the title "Standing on the Shoulders of Giants" in which he gives credit for many of the core ideas of Theory of Constraints. Goldratt has sought many times to show the correlation between various improvement methods.
Goldratt has been criticized on lack of openness in his theories, an example being him not releasing the algorithm he used for the Optimum Performance Training system. Some view him as unscientific with many of his theories, tools and techniques not being a part of the public domain, rather a part of his own framework of profiting on his ideas.
According to Gupta and Snyder (2009), despite being recognized as a genuine management philosophy nowadays, TOC has yet failed to demonstrate its effectiveness in the academic literature and as such, cannot be considered academically worthy to be called a widely recognized theory.  TOC needs more case studies that prove a connection between implementation and improved financial performance.
Nave (2002) argues that TOC does not take employees into account and fails to empower them in the production process.  He also states that TOC fails to address unsuccessful policies as constraints.
In contrast, Mukherjee and Chatterjee (2007) state that much of the criticism of Goldratt's work has been focused on the lack of rigour in his work, but not of the bottleneck approach, which are two different aspects of the issue.

Certification and education
The Theory of Constraints International Certification Organization (TOCICO) is an independent not-for-profit incorporated society that sets exams to ensure a consistent standard of competence. It is overseen by a board of academic and industrial experts.  It also hosts an annual international conference. The work presented at these conferences constitutes a core repository of the current knowledge.

See also
Linear programming
List of Theory of Constraints topics
Industrial engineering
Limiting factor
Systems thinking – Critical systems thinking – Joint decision traps
Twelve leverage points by Donella Meadows
Constraint (disambiguation)
Thinklets
Throughput
Rate-determining step
Liebig's law of the minimum

References
Further reading
Cox, Jeff; Goldratt, Eliyahu M. (1986). The goal: a process of ongoing improvement. [Great Barrington, Massachusetts]: North River Press. ISBN 0-88427-061-0.
Dettmer, H. William. (2003). Strategic Navigation: A Systems Approach to Business Strategy. [Milwaukee, Wisconsin]: ASQ Quality Press. p. 302. ISBN 0-87389-603-3.
Dettmer, H. William. (2007). The Logical Thinking Process: A Systems Approach to Complex Problem Solving. [Milwaukee, Wisconsin]: ASQ Quality Press. p. 413. ISBN 978-0-87389-723-5.
Goldratt, Eliyahu M. (1994). It's not luck. [Great Barrington, Massachusetts]: North River Press. ISBN 0-88427-115-3.
Goldratt, Eliyahu M. (1997). Critical chain. [Great Barrington, Massachusetts]: North River Press. ISBN 0-88427-153-6.
Carol A. Ptak; Goldratt, Eliyahu M.; Eli Schragenheim (2000). Necessary But Not Sufficient. [Great Barrington, Massachusetts]: North River Press. ISBN 0-88427-170-6.
Goldratt, Eliyahu M. (1998). Essays on the Theory of Constraints. [Great Barrington, Massachusetts]: North River Press. ISBN 0-88427-159-5.
Goldratt, Eliyahu M. (1990). Theory of Constraints. [Great Barrington, Massachusetts]: North River Press. ISBN 0-88427-166-8.
Goldratt, Eliyahu M. Beyond the Goal : Eliyahu Goldratt Speaks on the Theory of Constraints (Your Coach in a Box). Coach Series. ISBN 1-59659-023-8.
Lisa Lang (January 2006). Achieving a Viable Vision: The Theory of Constraints Strategic Approach to Rapid Sustainable Growth. Throughput Publishing, Inc. ISBN 0-9777604-1-3.
Goldratt, Eliyahu M. (1990). The haystack syndrome: sifting information out of the data ocean. [Great Barrington, Massachusetts]: North River Press. ISBN 0-88427-089-0.
Fox, Robert; Goldratt, Eliyahu M. (1986). The race. [Great Barrington, Massachusetts]: North River Press. ISBN 0-88427-062-9.
Schragenheim, Eli. (1999). Management dilemmas. [Boca Raton, Florida]: St. Lucie Press. p. 209. ISBN 1-57444-222-8.
Schragenheim, Eli & Dettmer, H. William. (2000). Manufacturing at warp speed: optimizing supply chain financial performance. [Boca Raton, Florida]: St. Lucie Press. p. 342. ISBN 1-57444-293-7.
Schragenheim, Eli, Dettmer, H. William, and Patterson, J. Wayne. (2009). Supply chain management at warp speed: integrating the system from end to end. [Boca Raton, Florida]: CRC Press. p. 220. ISBN 978-1-42007-335-5.{{cite book}}:  CS1 maint: multiple names: authors list (link)
Lepore & Cohen, Domenico & Oded (1999). Deming and Goldratt: The Decalogue. Great Barrington (Massachusetts): North River Press. p. 179. ISBN 0884271633.
John Tripp TOC Executive Challenge A Goal Game. ISBN 0-88427-186-2
Goldratt, Eliyahu M. (2003). Production the TOC Way with Simulator. North River Press, Great Barrington, Massachusetts. ISBN 0-88427-175-7.
Stein, Robert E. (3 June 2003). Re-Engineering The Manufacturing System. Marcel Dekker. ISBN 0-8247-4265-6.
Stein, Robert E. (14 February 1997). The Theory of Constraints. Marcel Dekker. ISBN 0-8247-0064-3.
Jacob, Dee; Bergland, Suzan; Cox, Jeff (29 December 2009). Velocity: Combining Lean, Six Sigma and the Theory of Constraints to Achieve Breakthrough Performance. Free Pre. p. 320. ISBN 978-1439158920.
Dettmer, H (1998). Constraint Theory A Logic-Based Approach to System Improvement (PDF).
Fedurko, J. Through Clouds to Solutions: Working with UDEs and UDE clouds. Estonia: Ou Vali Press.

External links
A Guide to Implementing the Theory of Constraints
Five focusing Steps
Theory of Constraints Essentials
Theory of Constraints: A Research Database

Throughput (business)

Throughput is rate at which a product is moved through a production process and is consumed by the end-user, usually measured in the form of sales or use statistics. The goal of most organizations is to minimize the investment in inputs as well as operating expenses while increasing throughput of its production systems. Successful organizations which seek to gain market share strive to match throughput to the rate of market demand of its products.

Overview
In the business management theory of constraints, throughput is the rate at which a system achieves its goal. Oftentimes, this is monetary revenue and is in contrast to output, which is inventory that may be sold or stored in a warehouse. In this case, throughput is measured by revenue received (or not) at the point of sale—exactly the right time. Output that becomes part of the inventory in a warehouse may mislead investors or others about the organizations condition by inflating the apparent value of its assets. The theory of constraints and throughput accounting explicitly avoid that trap.
Throughput can be best described as the rate at which a system generates its products or services per unit of time. Businesses often measure their throughput using a mathematical equation known as Little's law, which is related to inventories and process time: time to fully process a single product.

Basic formula
Using Little's Law, one can calculate throughput with the equation:

  
    
      
        I
        =
        R
        ∗
        T
      
    
    {\displaystyle I=R*T}
  
where:

I is the number of units contained within the system, inventory;
T is the time it takes for all the inventory to go through the process, flow time;
R is the rate at which the process is delivering throughput, flow rate or throughput.If you solve for R, you will get:
  
    
      
        R
        =
        I
        
          /
        
        T
      
    
    {\displaystyle R=I/T}

References
Further reading
Goldratt, Eliyahu and Jeff Cox. The Goal. Croton-on-Hudson: North River Press, 2004.

Total productive maintenance

Total productive maintenance (TPM) started as a method of physical asset management, focused on maintaining and improving manufacturing machinery in order to reduce the operating cost to an organization. After the PM award was created and awarded to Nippon Denso in 1971, the JIPM (Japanese Institute of Plant Maintenance), expanded it to include 8 Activities of TPM that required participation from all areas of manufacturing and non-manufacturing in the concepts of lean manufacturing.
TPM is designed to disseminate the responsibility for maintenance and machine performance, improving employee engagement and teamwork within management, engineering, maintenance, and operations.
There are eight types of Activities in TPM  implementation process:

Kobetsu-Kaizen (Focused improvement) activities
Jishu-Hozen (autonomous maintenance activity)
Planned Maintenance activity (planned maintenance activity)
Hinshitsu-Hozen activity (quality maintenance activity)
Development Management activity (development Management activity)
Education and Training activity (education and training activity)
OTPM (office total productive maintenance, or office TPM)
Safety, Health and Environment Activity (safety, health and environment)

History
Total productive maintenance (TPM) was developed by Seiichi Nakajima in Japan between 1950 and 1970.  This experience led to the recognition that a leadership mindset engaging front line teams in small group improvement activity is an essential element of effective operation.  The outcome of his work was the application of the TPM process in 1971.  One of the first companies to gain from this was Nippondenso, a company that created parts for Toyota.  They became the first winner of the PM prize.  An internationally accepted TPM benchmark developed by the JIPM  Seiichi Nakajima is therefore regarded as the father of TPM.  The classic TPM process he developed consisting of 5 principles was later enhanced by the JIPM to incorporate many of the lessons of lean manufacturing and is referred to as Company-Wide TPM which consists of 8 principles/activities. The name "Pillar" is symbolically used as a structural support to the structure of TPM. The term "activities" is more appropriate since execution of these 8 activities is the process of TPM implementation.

Objectives
The goal of TPM is the improvement of equipment effectiveness through engaging those that impact on it in small group improvement activities. Total quality management (TQM) and total productive maintenance (TPM) are considered as the key operational activities of the quality management system. In order for TPM to be effective, the full participation of entire organisation from top to frontline operators is vital. This should result in accomplishing the goal of TPM: "Enhance the volume of the production, employee morals, and job satisfaction."The main objective of TPM is to increase the Overall Equipment Effectiveness (OEE) of plant equipment. TPM addresses the causes for accelerated deterioration and production losses while creating the correct environment between operators and equipment to create ownership.
OEE has three factors which are multiplied to give one measure called OEEPerformance x Availability x Quality = OEE
Each factor has two associated losses making 6 in total, these 6 losses are as follows:
Performance = (1) running at reduced speed – (2) Minor Stops
Availability = (3) Breakdowns – (4) Product changeover
Quality = (5) Startup rejects – (6) Running rejects
The objective finally is to identify then prioritize and eliminate the causes of the losses. This is done by self-managing teams that solve problems. Employing consultants to create this culture is a common practice.

Principles
The eight pillars of TPM are mostly focused on proactive and preventive techniques for improving equipment reliability:

Autonomous maintenance - Operators who use all of their senses to help identify causes for losses
Focused improvement - Scientific approach to problem solving to eliminate losses from the factory
Planned maintenance - Professional maintenance activities performed by trained mechanics and engineers
Quality maintenance - Scientific and statistical approach to identifying defects and eliminating the cause of them
Early/equipment management  - Scientific introduction of equipment and design concepts that eliminate losses and make it easier to make defect free production efficiently.
Education and training - Support to continuous improvement of knowledge of all workers and management
Administrative & office TPM - Using total productive maintenance tools to improve all the support aspects of a manufacturing plant including production scheduling, materials management and information flow, As well as increasing moral of individuals and offering awards to well deserving employees for increasing their morals.
Safety health environmental conditionsWith the help of these pillars, we can increase productivity.
Manufacturing support.

Implementation
Following are the steps involved by the implementation of TPM in an organization:
Initial evaluation of TPM level,
Introductory Education and Propaganda (IEP) for TPM,
Formation of TPM committee,
Development of a master plan for TPM implementation,
Stage by stage training to the employees and stakeholders on all eight pillars of TPM,
Implementation preparation process,
Establishing the TPM policies and goals and development of a road map for TPM implementation.According to Nicholas, the steering committee should consist of production managers, maintenance managers, and engineering managers. The committee should formulate TPM policies and strategies and give advice. This committee should be led by a top-level executive. Also a TPM program team must rise, this program team has oversight and coordination of implementation activities. As well, it's lacking some crucial activities, like starting with partial implementation. Choose the first target area as a pilot area, this area will demonstrate the TPM concepts. Lessons learned from early target areas/the pilot area can be applied further in the implementation process.

Difference from TQM
Total quality management and total productive maintenance are often used interchangeably. However, TQM and TPM share a lot of similarities but are considered as two different approaches in the official literature. TQM attempts to increase the quality of goods, services, and concomitant customer satisfaction by raising awareness of quality concerns across the organization.TQM is based on five cornerstones: The product, the process that allows the product to be produced, the organization that provides the proper environment needed for the process to work, the leadership that guides the organization, and commitment to excellence throughout the organization.In other words, TQM focuses on the quality of the product, while TPM focuses on the losses that impede the equipment used to produce the products. By preventing equipment break-down, improving the quality of the equipment and by standardizing the equipment (results in less variance, so better quality), the quality of the products increases. TQM and TPM can both result in an increase in quality. However, the way of going there is different. TPM can be seen as a way to help to achieve the goal of TQM.


== References ==

Total quality management

Total quality management (TQM) consists of organization-wide efforts to "install and make permanent 
climate where employees continuously improve their ability to provide on demand products and services that customers will find of particular value."  "Total" emphasizes that departments in addition to production (for example sales and marketing, accounting and finance, engineering and design) are obligated to improve their operations; "management" emphasizes that executives are obligated to actively manage quality through funding, training, staffing, and goal setting.  While there is no widely agreed-upon approach, TQM efforts typically draw heavily on the previously developed tools and techniques of quality control. TQM enjoyed widespread attention during the late 1980s and early 1990s before being overshadowed by ISO 9000, Lean manufacturing, and Six Sigma.

History
In the late 1970s and early 1980s, the developed countries of North America and Western Europe suffered economically in the face of stiff competition from Japan's ability to produce high-quality goods at competitive cost. For the first time since the start of the Industrial Revolution, the United Kingdom became a net importer of finished goods.  The United States undertook its own soul-searching, expressed most pointedly in the television broadcast of If Japan Can... Why Can't We?.  Firms began reexamining the techniques of quality control invented over the past 50 years and how those techniques had been so successfully employed by the Japanese. It was in the midst of this economic turmoil that TQM took root.
The exact origin of the term "total quality management" is uncertain.  It is almost certainly inspired by Armand V. Feigenbaum's multi-edition book Total Quality Control (OCLC 299383303) and Kaoru Ishikawa's What Is Total Quality Control? The Japanese Way (OCLC 11467749).  It may have been first coined in the United Kingdom by the Department of Trade and Industry during its 1983 "National Quality Campaign".  Or it may have been first coined in the United States by the Naval Air Systems Command to describe its quality-improvement efforts in 1985.

Development in the United States
In the spring of 1984, an arm of the United States Navy asked some of its civilian researchers to assess statistical process control and the work of several prominent quality consultants and to make recommendations as to how to apply their approaches to improve the Navy's operational effectiveness.  The recommendation was to adopt the teachings of W. Edwards Deming.  The Navy branded the effort "Total Quality Management" in 1985.From the Navy, TQM spread throughout the US Federal Government, resulting in the following:

The creation of the Malcolm Baldrige National Quality Award in August 1987
The creation of the Federal Quality Institute in June 1988
The adoption of TQM by many elements of government and the armed forces, including the United States Department of Defense, United States Army, and United States Coast GuardThe US Environmental Protection Agency's Underground Storage Tanks program, which was established in 1985, also employed Total Quality Management to develop its management style. The private sector followed suit, flocking to TQM principles not only as a means to recapture market share from the Japanese, but also to remain competitive when bidding for contracts from the Federal Government since "total quality" requires involving suppliers, not just employees, in process improvement efforts.

Features
There is no widespread agreement as to what TQM is and what actions it requires of organizations, however a review of the original United States Navy effort gives a rough understanding of what is involved in TQM.
The key concepts in the TQM effort undertaken by the Navy in the 1980s include:
"Quality is defined by customers' requirements."
"Top management has direct responsibility for quality improvement."
"Increased quality comes from systematic analysis and improvement of work processes."
"Quality improvement is a continuous effort and conducted throughout the organization."The Navy used the following tools and techniques:

The PDCA cycle to drive issues to resolution
Ad hoc cross-functional teams (similar to quality circles) responsible for addressing immediate process issues
Standing cross-functional teams responsible for the improvement of processes over the long term
Active management participation through steering committees
Use of the Seven Basic Tools of Quality to analyze quality-related issues

Notable definitions
While there is no generally accepted definition of TQM, several notable organizations have attempted to define it.  These include:

United States Department of Defense (1988)
"Total Quality Management (TQM) in the Department of Defense is a strategy for continuously improving performance at every level, and in all areas of responsibility. It combines fundamental management techniques, existing improvement efforts, and specialized technical tools under a disciplined structure focused on continuously improving all processes. Improved performance is directed at satisfying such broad goals as cost, quality, schedule, and mission need and suitability. Increasing user satisfaction is the overriding objective. The TQM effort builds on the pioneering work of Dr. W. E. Deming, Dr. J. M. Juran, and others, and benefits from both private and public sector experience with continuous process improvement."

British Standards Institution standard BS 7850-1:1992
"A management philosophy and company practices that aim to harness the human and material resources of an organization in the most effective way to achieve the objectives of the organization."

International Organization for Standardization standard ISO 8402:1994
"A management approach of an organisation centred on quality, based on the participation of all its members and aiming at long term success through customer satisfaction and benefits to all members of the organisation and society."

The American Society for Quality
"A term first used to describe a management approach to quality improvement. Since then, TQM has taken on many meanings. Simply put, it is a management approach to long-term success through customer satisfaction. TQM is based on all members of an organization participating in improving processes, products, services and the culture in which they work. The methods for implementing this approach are found in the teachings of such quality leaders as Philip B. Crosby, W. Edwards Deming, Armand V. Feigenbaum, Kaoru Ishikawa and Joseph M. Juran."

The Chartered Quality Institute
"TQM is a philosophy for managing an organization in a way which enables it to meet stakeholder needs and expectations efficiently and effectively, without compromising ethical values."

Baldrige Excellence Framework
In the United States, the Baldrige Award, created by Public Law 100–107, annually recognizes American businesses, education institutions, health care organizations, and government or nonprofit organizations that are role models for organizational performance excellence.  Organizations are judged on criteria from seven categories:
Leadership
Strategy
Customers
Measurement, analysis, and knowledge management
Workforce
Operations
ResultsExample criteria are:
How do you obtain information on your customers’ satisfaction relative to their satisfaction with your competitors?
How do you select, collect, align, and integrate data and information for tracking daily operations?
How do you manage your workforce, its needs, and your needs to ensure continuity, prevent workforce reductions, and minimize the impact of workforce reductions, if they do become necessary?Joseph M. Juran believed the Baldrige Award judging criteria to be the most widely accepted description of what TQM entails.: 650

Standards
During the 1990s, standards bodies in Belgium, France, Germany, Turkey, and the United Kingdom attempted to standardize TQM.  While many of these standards have since been explicitly withdrawn, they all are effectively superseded by ISO 9000:

Total Quality Management: Guide to Management Principles, London, England: British Standards Institution, 1992, ISBN 9780580211560, OCLC 655881602, BS 7850
Electronic Components Committee (1994), Guide to Total Quality Management (TQM) for CECC-Approved Organizations, Brussels, Belgium: European Committee for Electrotechnical Standardization, CECC 00 806 Issue 1
System zur Zukunftssicherung: Total Quality Management (TQM), Düsseldorf, Germany: Verein Deutscher Ingenieure, 1996, OCLC 632959402, VDI 5500
Total Quality and Marketing/Management Tools, Paris, France: AFNOR, 1998, FD X50-680
Total Quality Management: Guide to Management Principles, Turkish Standards Institution (TSE), 2006, TS 13133

Legacy
Interest in TQM as an academic subject peaked around 1993.The Federal Quality Institute was shuttered in September 1995 as part of the Clinton administration's efforts to streamline government.  The European Centre for Total Quality Management closed in August 2009.TQM, as a vaguely defined quality management approach, was largely supplanted by the ISO 9000 collection of standards and their formal certification processes in the 1990s.  Business interest in quality improvement under the TQM name also faded as Jack Welch's success attracted attention to Six Sigma and Toyota's success attracted attention to lean manufacturing, though the three share many of the same tools, techniques, and significant portions of the same philosophy.
TQM lives on in various national quality awards around the globe.

See also
Capability Maturity Model Integration CMMI
Lean manufacturing
List of national quality awards
Malcolm Baldrige National Quality Award
Outline of management
People Capability Maturity Model
Zero Defects

Explanatory footnotes
References
Further reading
Deming, W. Edwards (1986), Out of the Crisis, Cambridge, Massachusetts: Massachusetts Institute of Technology, ISBN 9780911379013, OCLC 13126265, retrieved 2013-12-07 (Originally published in 1982 as Quality, Productivity, and Competitive Position, OCLC 9234321)
Department of Defense (1990-02-15), Total Quality Management Guide: A Two Volume Guide for Defense Organizations, vol. 1—Key Features of the DoD Implementation, Washington, D.C.: United States Department of Defense, OCLC 26866911, ADA225196, archived from the original on December 11, 2013, retrieved 2013-12-07
Feigenbaum, Armand V. (1983), Total Quality Control (3 ed.), New York: McGraw-Hill, Inc., ISBN 9780070203532, OCLC 8552734
Ishikawa, Kaoru (1985), What Is Total Quality Control? The Japanese Way (1 ed.), Englewood Cliffs, New Jersey: Prentice-Hall, ISBN 9780139524332, OCLC 11467749
Office of the Deputy Under Secretary of Defense for Total Quality Management (1990-02-15), Total Quality Management Guide: A Two Volume Guide for Defense Organizations, vol. 2—A Guide to Implementation, Washington, D.C.: United States Department of Defense, OCLC 834271878, ADA230439, archived from the original on December 11, 2013, retrieved 2013-12-07
Rehder, Robert; Ralston, Faith (Summer 1984). "Total Quality Management: A Revolutionary Management Philosophy". S.A.M. Advanced Management Journal. 49 (3): 24–33. ISSN 0749-7075. OCLC 11220842.

External links

Example Baldrige Award criteria
The American Society for Quality resource page on TQM
The Chartered Quality Institute resource page on TQM Archived 2014-07-03 at the Wayback Machine
The Economist resource page on TQM

Toyota

Toyota Motor Corporation (Japanese: トヨタ自動車株式会社, Hepburn: Toyota Jidōsha kabushikigaisha, IPA: [toꜜjota], English: , commonly known as simply Toyota) is a Japanese multinational automotive manufacturer headquartered in Toyota City, Aichi, Japan. It was founded by Kiichiro Toyoda and incorporated on August 28, 1937. Toyota is one of the largest automobile manufacturers in the world, producing about 10 million vehicles per year.
The company was originally founded as a spinoff of Toyota Industries, a machine maker started by Sakichi Toyoda, Kiichiro's father. Both companies are now part of the Toyota Group, one of the largest conglomerates in the world. While still a department of Toyota Industries, the company developed its first product, the Type A engine, in 1934 and its first passenger car in 1936, the Toyota AA.
After World War II, Toyota benefited from Japan's alliance with the United States to learn from American automakers and other companies, which gave rise to The Toyota Way (a management philosophy) and the Toyota Production System (a lean manufacturing practice) that transformed the small company into a leader in the industry and was the subject of many academic studies.
In the 1960s, Toyota took advantage of the rapidly growing Japanese economy to sell cars to a growing middle-class, leading to the development of the Toyota Corolla, which became the world's all-time best-selling automobile. The booming economy also funded an international expansion that allowed Toyota to grow into one of the largest automakers in the world, the largest company in Japan and the ninth-largest company in the world by revenue, as of December 2020. Toyota was the world's first automobile manufacturer to produce more than 10 million vehicles per year, a record set in 2012, when it also reported the production of its 200 millionth vehicle. By September 2023, total production reached 300 million vehicles.Toyota was praised for being a leader in the development and sales of more fuel-efficient hybrid electric vehicles, starting with the introduction of the Toyota Prius in 1997. The company now sells more than 40 hybrid vehicle models around the world. More recently, the company has also been criticized for being slow to adopt all-electric vehicles and focusing on the development of hydrogen fuel cell vehicles, like the Toyota Mirai, a technology that is costlier and has fallen far behind electric batteries.
As of 2022, the Toyota Motor Corporation produces vehicles under four brands: Daihatsu, Hino, Lexus and the namesake Toyota. The company also holds a 20% stake in Subaru Corporation, a 5.1% stake in Mazda, a 4.9% stake in Suzuki, a 4.6% stake in Isuzu, a 3.8% stake in Yamaha Motor Corporation, and a 2.8% stake in Panasonic, as well as stakes in vehicle manufacturing joint-ventures in China (FAW Toyota and GAC Toyota), the Czech Republic (TPCA), India (Toyota Kirloskar) and the United States (MTMUS).
Toyota is listed on the London Stock Exchange, Nagoya Stock Exchange, New York Stock Exchange and on the Tokyo Stock Exchange, where its stock is a component of the Nikkei 225 and TOPIX Core30 indices.

History
1920s–1930s
In 1924, Sakichi Toyoda invented the Toyoda Model G Automatic Loom. The principle of jidoka, which means the machine stops itself when a problem occurs, became later a part of the Toyota Production System. Looms were built on a small production line. In 1929, the patent for the automatic loom was sold to the British company Platt Brothers, generating the starting capital for automobile development.Under the direction of the founder's son, Kiichiro Toyoda, Toyoda Automatic Loom Works established an Automobile Division on September 1, 1933, and formally declared its intention to begin manufacturing automobiles on January 29, 1934. A prototype Toyota Type A engine was completed on September 25, 1934, with the company's first prototype sedan, the A1, completed the following May. As Kiichiro had limited experience with automobile production, he initially focused on truck production; the company's first truck, the G1, was completed on August 25, 1935, and debuted on November 21 in Tokyo, becoming the company's first production model. Modeled on a period Ford truck, the G1 sold for ¥2,900, ¥200 cheaper than the Ford truck. A total of 379 G1 trucks were ultimately produced.In April 1936, Toyoda's first passenger car, the Model AA, was completed. The sales price was ¥3,350, ¥400 cheaper than Ford or GM cars. The company's plant at Kariya was completed in May. In July, the company filled its first export order, with four G1 trucks exported to northeastern China. On September 19, 1936, the Japanese imperial government officially designated Toyota Automatic Loom Works as an automotive manufacturer.
Vehicles were originally sold under the name "Toyoda" (トヨダ), from the family name of the company's founder, Kiichirō Toyoda. In September 1936, the company ran a public competition to design a new logo. Of 27,000 entries, the winning entry was the three Japanese katakana letters for "Toyoda" in a circle. However, Rizaburo Toyoda, who had married into the family and was not born with that name, preferred "Toyota" (トヨタ) because it took eight brush strokes (a lucky number) to write in Japanese, was visually simpler (leaving off the diacritic at the end), and with a voiceless consonant instead of a voiced one (voiced consonants are considered to have a "murky" or "muddy" sound compared to voiceless consonants, which are "clear").
Since toyoda literally means "fertile rice paddies", changing the name also prevented the company from being associated with old-fashioned farming. The newly formed word was trademarked and the company began trading on August 28, 1937, as the Toyota Motor Company Ltd. Kiichiro's brother-in-law Rizaburo Toyoda was appointed the firm's first president, with Kiichiro as vice-president. Toyota Automatic Loom Works formally transferred automobile manufacturing to the new entity on September 29.The Japanese government supported the company by preventing foreign competitors Ford and General Motors from importing automobiles into Japan.At the onset of World War II, Toyota almost exclusively produced standard-sized trucks for the Japanese Army, which paid one-fifth of the price in advance and the remainder in cash upon delivery.

1940s
Japan was heavily damaged in World War II and Toyota's plants, which were used for the war effort, were not spared. On August 14, 1945, one day before the surrender of Japan, Toyota's Koromo Plant was bombed by the Allied forces. After the surrender, the U.S.-led occupying forces banned passenger car production in Japan. However, automakers like Toyota were allowed to begin building trucks for civilian use, in an effort to rebuild the nation's infrastructure. The U.S. military also contracted with Toyota to repair its vehicles.By 1947, there was an emerging global Cold War between the Soviet Union and the U.S., who had been allies in World War II. U.S. priorities shifted (the "Reverse Course") from punishing and reforming Japan to ensuring internal political stability, rebuilding the economy, and, to an extent, remilitarizing Japan. Under these new policies, in 1949, Japanese automakers were allowed to resume passenger car production, but at the same time, a new economic stabilization program to control inflation plunged the automotive industry into a serious shortage of funds, while many truck owners defaulted on their loans. Ultimately, the Bank of Japan, the central bank of the country, bailed out the company, with demands that the company institute reforms.

1950s
As the 1950s began, Toyota emerged from its financial crisis a smaller company, closing factories and laying off workers. At about the same time, the Korean War broke out, and being located so close to the battlefront, the U.S. Army placed an order for 1,000 trucks from Toyota. The order helped to rapidly improve the struggling company's business performance. In 1950, company executives, including Kiichiro's cousin Eiji Toyoda, took a trip to the United States where they trained at the Ford Motor Company and observed the operations of dozens of U.S. manufacturers. The knowledge they gained during the trip, along with what the company learned making looms, gave rise to The Toyota Way (a management philosophy) and the Toyota Production System (a lean manufacturing practice) that transformed the company into a leader in the manufacturing industry.
Toyota started developing its first full-fledged passenger car, the Toyopet Crown, in January 1952. Prior to the Crown, Toyota had been outsourcing the design and manufacturing of auto bodies, which were then mounted on truck frames made by Toyota. The project was a major test for Toyota, who would need to build bodies and develop a new chassis that would be comfortable, but still stand up to the muddy, slow, unpaved roads common in Japan at the time. The project had been championed for many years by founder Kiichiro Toyoda, who died suddenly on March 27, 1952. The first prototypes were completed in June 1953 and began extensive testing, before the Crown went on sale in August 1955. The car was met with positive reviews from around the world.
After the introduction of the Crown, Toyota began aggressively expanding into the export market. Toyota began shipping Land Cruiser knock-down kits to Latin America in November 1955, sending complete Land Cruisers to Burma (now Myanmar) and the Philippines in 1956 as part of war reparations provided by the Japanese government, establishing a branch in Thailand in June 1957, and shipping Land Cruisers to Australia in August 1957. Toyota established a production facility in Brazil in 1958, the company's first outside of Japan.Toyota entered the United States market in July 1958, attempting to sell the Toyopet Crown. The company faced problems almost immediately, the Crown was a flop in the U.S. with buyers finding it overpriced and underpowered (because it was designed for the bad roads of Japan, not high-speed performance). In response, exports of the Crown to the United States were suspended in December 1960.After Kiichiro's death, his cousin Eiji Toyoda led the company for the two decades. Eiji helped establish the company's first plant independent from the Loom Works plant.

1960s–1970s
At the start of the 1960s, the Japanese economy was booming, a period that came to be known as the Japanese economic miracle. As the economy grew, so did the income of everyday people, who now could afford to purchase a vehicle. At the same time, the Japanese government heavily invested in improving road infrastructure. To take advantage of the moment, Toyota and other automakers started offering affordable economy cars like the Toyota Corolla, which became the world's all-time best-selling automobile.Toyota also found success in the United States in 1965 with the Toyota Corona compact car, which was redesigned specifically for the American market with a more powerful engine. The Corona helped increase U.S. sales of Toyota vehicles to more than 20,000 units in 1966 (a threefold increase) and helped the company become the third-best-selling import brand in the United States by 1967. Toyota's first manufacturing investment in the United States came in 1972 when the company struck a deal with Atlas Fabricators, to produce truck beds in Long Beach, in an effort to avoid the 25% "chicken tax" on imported light trucks. By importing the truck as an incomplete chassis cab (the truck without a bed), the vehicle only faced a 4% tariff. Once in the United States, Atlas would build the truck beds and attach them to the trucks. The partnership was successful and two years later, Toyota purchased Atlas.The energy crisis of the 1970s was a major turning point in the American auto industry. Before the crisis, large and heavy vehicles with powerful but inefficient engines were common. But in the years after, consumers started demanding high-quality and fuel-efficient small cars. Domestic automakers, in the midst of their malaise era, struggled to build these cars profitably, but foreign automakers like Toyota were well positioned. This, along with growing anti-Japanese sentiment, prompted the U.S. Congress to consider import restrictions to protect the domestic auto industry.
The 1960s also saw the slight opening of the Japanese auto market to foreign companies. In an effort to strengthen Japan's auto industry ahead of the market opening, Toyota purchased stakes in other Japanese automakers. That included a stake in Hino Motors, a manufacturer of large commercial trucks, buses and diesel engines, along with a 16.8 percent stake in Daihatsu, a manufacturer of kei cars, the smallest highway-legal passenger vehicles sold in Japan. That began what became a long-standing partnership between Toyota and the two companies. As part of the partnership, Daihatsu would supply kei cars for Toyota to sell and to a lesser extent Toyota would supply full-sized cars for Daihatsu to sell (a process known as rebadging), allowing both companies to sell a full line-up of vehicles.

1980s
After the successes of the 1970s, and the threats of import restrictions, Toyota started making additional investments in the North American market in the 1980s. In 1981, Japan agreed to voluntary export restraints, which limited the number of vehicles the nation would send to the United States each year, leading Toyota to establish assembly plants in North America. The U.S. government also closed the loophole that allowed Toyota to pay lower taxes by building truck beds in America.
Also in 1981, Eiji Toyoda stepped down as president and assumed the title of chairman. He was succeeded as president by Shoichiro Toyoda, the son of the company's founder. Within months, Shoichiro started to merge Toyota's sales and production organizations, and in 1982 the combined companies became the Toyota Motor Corporation. The two groups were described as "oil and water" and it took years of leadership from Shoichiro to successfully combine them into one organization.Efforts to open a Toyota assembly plant in the United States started in 1980, with the company proposing a joint-venture with the Ford Motor Company. Those talks broke down in July 1981. Eventually in 1984, the company struck a deal with General Motors (GM) to establish a joint-venture vehicle manufacturing plant called NUMMI (New United Motor Manufacturing, Inc.) in Fremont, California. GM saw the joint venture as a way to get access to a quality small car and an opportunity to learn about The Toyota Way and the Toyota Production System. For Toyota, the factory gave the company its first manufacturing base in North America allowing it to avoid any future tariffs on imported vehicles and saw GM as a partner who could show them how to navigate the American labor environment. The plant would be led by Tatsuro Toyoda, the younger brother of company president Shoichiro Toyoda. The first Toyota assembled in America, a white Corolla, rolled off the line at NUMMI on October 7, 1986.Toyota received its first Japanese Quality Control Award at the start of the 1980s and began participating in a wide variety of motorsports. Conservative Toyota held on to rear-wheel-drive designs for longer than most; while a clear first in overall production they were only third in production of front-wheel-drive cars in 1983, behind Nissan and Honda. In part due to this, Nissan's Sunny managed to squeeze by the Corolla in numbers built that year.
Before the decade was out, Toyota introduced Lexus, a new division that was formed to market and service luxury vehicles in international markets. Prior to the debut of Lexus, Toyota's two existing flagship models, the Crown and Century, both catered exclusively for the Japanese market and had little global appeal that could compete with international luxury brands such as Mercedes-Benz, BMW and Jaguar. The company had been developing the brand and vehicles in secret since August 1983, at a cost of over US$1 billion. The LS 400 flagship full-size sedan debuted in 1989 to strong sales, and was largely responsible for the successful launch of the Lexus marque.

1990s
In the 1990s, Toyota began to branch out from producing mostly compact cars by adding many larger and more luxurious vehicles to its lineup, including a full-sized pickup, the T100 (and later the Tundra), several lines of SUVs, a sport version of the Camry, known as the Camry Solara. They would also launch newer iterations of their sports cars, namely the MR2, Celica, and Supra during this era.
December 1997 saw the introduction of the first-generation Toyota Prius, the first mass-produced gasoline-electric hybrid car. The vehicle would be produced exclusively for the Japanese market for the first two years.
With a major presence in Europe, due to the success of Toyota Team Europe in motorsport, the corporation decided to set up Toyota Motor Europe Marketing and Engineering, TMME, to help market vehicles in the continent. Two years later, Toyota set up a base in the United Kingdom, TMUK, as the company's cars had become very popular among British drivers. Bases in Indiana, Virginia, and Tianjin were also set up.
Toyota also increased its ownership of Daihatsu during this period. In 1995, Toyota increased its shareholding in the company to 33.4 percent, giving Toyota the ability to veto shareholder resolutions at the annual meeting. In 1998, Toyota increased its holding in the company to 51.2 percent, becoming the majority shareholder.On September 29, 1999, the company decided to list itself on the New York and London Stock Exchanges.
The later half of the 1990s would also see the Toyoda brothers step back from the company their father had founded. In 1992, Shoichiro Toyoda would shift to become chairman, allowing his brother Tatsuro to become president, a job he held until his retirement in 1995. Shoichiro would step down as chairman in 1999. Both would retain honorary advisory roles in the company. Hiroshi Okuda would lead the company as president from 1995 until 1999 when he became chairman and the President's office would be filled by Fujio Cho.

2000s
In August 2000, exports began of the Prius. In 2001, Toyota acquired its long time partner, truck and bus manufacturer Hino Motors. In 2002, Toyota entered Formula One competition and established a manufacturing joint venture in France with French automakers Citroën and Peugeot. A youth-oriented marque for North America, Scion, was introduced in 2003. Toyota ranked eighth on Forbes 2000 list of the world's leading companies for the year 2005. Also in 2005, Fujio Cho would shift to become chairman of Toyota and would be replaced as president by Katsuaki Watanabe.
In 2007, Toyota released an update of its full-sized truck, the Tundra, produced in two American factories, one in Texas and one in Indiana. Motor Trend named the 2007 Toyota Camry "Car of the Year" for 2007. It also began the construction of two new factories, one in Woodstock, Ontario, Canada, and the other in Blue Springs, Mississippi, USA.
The company was number one in global automobile sales for the first quarter of 2008.Toyota was hit by the global financial crisis of 2008 as it was forced in December 2008 to forecast its first annual loss in 70 years. In January 2009, it announced the closure of all of its Japanese plants for 11 days to reduce output and stocks of unsold vehicles.In October 2009, Toyota announced that they were establishing an office in South Korea and launched the Camry sedan, Camry hybrid, Prius and the RAV4 during the launching event at the Grand Hyatt Seoul.Between 2009 and 2011, Toyota conducted recalls of millions of vehicles after reports that several drivers experienced unintended acceleration. The recalls were to prevent a front driver's side floor mat from sliding into the foot pedal well, causing the pedals to become trapped and to correct the possible mechanical sticking of the accelerator pedal. At least 37 were killed in crashes allegedly related to unintended acceleration, approximately 9 million cars and trucks were recalled, Toyota was sued for personal injuries and wrongful deaths, paid US$1 billion to settle a class action lawsuit to compensate owners for lost resale value, and paid a US$1.2 billion criminal penalty to the United States government over accusations that it had intentionally hid information about safety defects and had made deceptive statements to protect its brand image.
Amid the unintended acceleration scandal, Katsuaki Watanabe stepped down as company president. He was replaced by Akio Toyoda, grandson of company founder Kiichiro Toyoda, on June 23, 2009. Akio had been with Toyota since 1984, working jobs in production, marketing and product development, and took a seat on the board of directors in 2000. Akio's promotion by the board marked the return of a member of the Toyoda family to the top leadership role for the first time since 1999.

2010s
In 2011, Toyota, along with large parts of the Japanese automotive industry, suffered from a series of natural disasters. The 2011 Tōhoku earthquake and tsunami led to a severe disruption of the supplier base and a drop in production and exports. Severe flooding during the 2011 monsoon season in Thailand affected Japanese automakers that had chosen Thailand as a production base. Toyota is estimated to have lost production of 150,000 units to the tsunami and production of 240,000 units to the floods.
On February 10, 2014, it was announced that Toyota would cease manufacturing vehicles and engines in Australia by the end of 2017. The decision was based on the unfavourable Australian dollar making exports not viable, the high cost of local manufacture, and the high amount of competition in a relatively small local market. The company planned to consolidate its corporate functions in Melbourne by the end of 2017, and retain its Altona plant for other functions. The workforce is expected to be reduced from 3,900 to 1,300. Both Ford Motor Company and General Motors (Holden) followed suit, ending Australian production in 2016 and 2017 respectively.
The automaker narrowly topped global sales for the first half of 2014, selling 5.1 million vehicles in the six months ending June 30, 2014, an increase of 3.8% on the same period the previous year. Volkswagen AG, which recorded sales of 5.07 million vehicles, was close behind.In August 2014, Toyota announced it would be cutting its spare-parts prices in China by up to 35%. The company admitted the move was in response to a probe foreshadowed earlier in the month by China's National Development and Reform Commission of Toyota's Lexus spare-parts policies, as part of an industry-wide investigation into what the Chinese regulator considers exorbitantly high prices being charged by automakers for spare parts and after-sales servicing.In November 2015, the company announced that it would invest US$1 billion over the next 5 years into artificial intelligence and robotics research. In 2016, Toyota invested in Uber. In 2020, a corporate governance report showed that Toyota owns 10.25 million shares of Uber, which was valued at $292.46 million as of March 30, 2020. According to Reuters, this was roughly 0.6 per cent of Uber's outstanding shares.In March 2016, Toyota partnered with Yanmar to create a fiberglass pleasure boat using Yanmar outboard marine diesel engines or Toyota inboard engines.In August 2016, the company purchased all remaining assets of Daihatsu, making the manufacturer of small cars a wholly owned subsidiary of Toyota.On August 27, 2018, Toyota announced an investment of US$500 million in Uber's autonomous cars.In October 2019, Toyota backed the Trump Administration's proposal that federal authority should override California's ability to set its own emissions standards for automobiles. The proposal would reduce California's 2025 fuel efficiency standard from about 54.5 to 37 MPG. This shift by Toyota away from fuel efficiency damaged the company's reputation as a green brand.

2020s
By 2020, Toyota reclaimed its position as the largest automaker in the world, surpassing Volkswagen. It sold 9.528 million vehicles globally despite an 11.3% drop in sales due to the COVID-19 pandemic.  This includes subsidiaries Daihatsu and Hino Motors.On April 2, 2020, BYD and Toyota announced a new joint venture between the two companies called BYD Toyota EV Technology Co., Ltd., with the aim of "developing BEVs (Battery Electric Vehicles) that appeal to customers."In March 2021, Toyota, its subsidiary Hino, and Isuzu announced the creation of a strategic partnership between the three companies. Toyota acquired a 4.6% stake in Isuzu while the latter plans to acquire Toyota shares for an equivalent value. The three companies said they would form a new joint venture by April called Commercial Japan Partnership Technologies Corporation with the aim of developing fuel cell and electric light trucks. Toyota would own an 80% stake in the venture while Hino and Isuzu would own 10% each.In April 2021, Toyota said that it will buy Lyft's self-driving technology unit for $550 million and merge it with its newly created Woven Planet Holdings automation division.In June 2021, the company defended its donations to the United States Republican lawmakers after they voted against certifying the results of the 2020 presidential election, saying it did not believe it was "appropriate to judge members of Congress" for that one vote. A report by Axios found that Toyota was the top donor to 2020 election objectors, by a substantial margin. The company then reversed course in July 2021 and ceased donations to election objectors, releasing a statement saying it understood that its PAC's donations to those objectors, which far outpaced those of any other company, "troubled some stakeholders." Toyota resumed donations after a six-month pause.In December 2021, Toyota announced that it would invest ¥8,000,000,000,000 ($70 billion at 2021 exchange rate) in electric vehicles by 2030, launch 30 EV models worldwide by that year, and set a sales target of 3.5 million electric vehicles in 2030.Toyota will increase its software engineer intake to around 40% to 50% of all technical hires from the second quarter of 2022, the move plans to address a transformation to so-called CASE — connected, autonomous, shared and electric — technologies in an environment of intensifying global competition.In 2021, Toyota told some of its suppliers to increase their semiconductor inventory levels from the conventional three months to five months in response to the COVID-19 chip shortage. The "just-in-time" supply chain in which parts are only delivered when necessary, had already been revised after the March 11, 2011, earthquake and tsunami in Japan, lifting inventories across the entire procurement network. The time it takes Toyota to turn over its inventory increased by around 40% during the past ten years, to 36.36 days as of March 2021.In June 2022, Toyota recalled 2,700 of its first mass-produced all-electric vehicles due to worries that their wheels could fall off during driving. It was discovered that the bolts on the bZ4X's wheels could loosen up to the point where the wheel simply detaches from the car, causing a loss of control over the vehicle and possible accident.In August 2022, Toyota pledged up to $5.6 billion towards production of electric vehicle battery production and announced an increase in investment in its plant near Greensboro, North Carolina. Also in 2022, Toyota managed to maintain its position as the world's best-selling automaker for the third year in a row.In January 2023, Toyota CEO and President Akio Toyoda announced that he was stepping down and passing the position on to Koji Sato. Akio is the great-grandson of company founder Rizaburo Toyoda. Sato had previously run Lexus, Toyota's luxury car brand. The change is set to take effect on April 1, 2023.

Board of directors
Chairman: Akio Toyoda (since April 2023)
Vice chairman: Shigeru Hayakawa
President & CEO: Koji  Sato (since April 2023)
Members:
Takeshi Uchiyamada
James Kuffner
Kenta Kon
Masahiko Maeda
Ikuro Sugawara
Sir Philip Craven
Teiko Kudo

List of former chairmen
In 1950, Toyota was split into Toyota Motor Co. and Toyota Motor Sales Co. (sales arm of Toyota); the two companies merged in 1982 to create one unified company, with then-Toyota Motor Co. President Eiji Toyoda becoming chairman. Chairmen listed prior to 1982 below were for the pre-merger Toyota Motor Co. only.
Rizaburo Toyoda (1937–1948)
Taizo Ishida (1948–1952)
Shoichi Saito (1952–1959)
Masaya Hanai (1959–1982)
Eiji Toyoda (1982–1994)
Shoichiro Toyoda (1994–1999)
Hiroshi Okuda (1999–2006)
Fujio Cho (2006–2013)
Takeshi Uchiyamada (2013–2023)

List of former presidents
Similar to the chairman position, in 1982 the then-Toyota Motor Sales Co. President Shoichiro Toyoda becoming President. President's listed prior to 1982 below were for the pre-merger Toyota Motor Co. only.
Rizaburo Toyoda (1937–1941)
Kiichiro Toyoda (1941–1950)
Taizo Ishida (1950–1961)
Fukio Nakagawa (1961–1967)
Eiji Toyoda (1967–1982)
Shoichiro Toyoda (1982–1992)
Tatsuro Toyoda (1992–1995)
Hiroshi Okuda (1995–1999)
Fujio Cho (1999–2005)
Katsuaki Watanabe (2005–2009)
Akio Toyoda (2009–2023)

Product line
As of 2009, Toyota officially lists approximately 70 different models sold under its namesake brand, including sedans, coupes, vans, trucks, hybrids, and crossovers. Many of these models are produced as passenger sedans, which range from the subcompact Toyota Yaris, compact Corolla, to mid-size Camry and full-size Avalon. Minivans include the Innova, Alphard/Vellfire, Sienna, and others. Several small cars, such as the xB and tC, were sold under the Scion brand.

SUVs and crossovers
Toyota SUV and crossover line-up grew quickly in the late 2010s to 2020s due to the market shift to SUVs. Toyota crossovers range from the subcompact Yaris Cross and C-HR, compact Corolla Cross and RAV4, to midsize Harrier/Venza and Kluger/Highlander. Other crossovers include the Raize, Urban Cruiser. Toyota SUVs range from the midsize Fortuner to full-size Land Cruiser. Other SUVs include the Rush, Prado, FJ Cruiser, 4Runner, and Sequoia.

Pickup trucks
Toyota first entered the pickup truck market in 1947 with the SB that was only sold in Japan and limited Asian markets. It was followed in 1954 by the RK (renamed in 1959 as the Stout) and in 1968 by the compact Hilux. With continued refinement, the Hilux (simply known as the Pickup in some markets) became famous for being extremely durable and reliable. Extended cab and crew cab versions were eventually added, and Toyota continues to produce them today under various names depending on the market in various cab lengths, with gasoline or diesel engines, and 2WD and 4WD versions.
In North America, the Hilux became a major model for the company, leading the company to launch the Tacoma in 1995. The Tacoma was based on the Hilux, but with a design intended to better suit the needs of North American consumers who often use pickup trucks as personal vehicles. The design was a success and the Tacoma became the best-selling compact pickup in North America.
After the success of its compact Hilux pickups in North America, Toyota decided to enter the full-size pickup market, which was traditionally dominated by domestic automakers. The company introduced the T100 for the 1993 US model year. The T100 had a full-size 8-foot (2.4 m) long bed, but suspension and engine characteristics were similar to that of a compact pickup. Sales were disappointing and the T100 was criticized for having a small V6 engine (especially compared to the V8 engines common in American full-size trucks), lacking an extended-cab version, being too small, and too expensive (because of the 25% tariff on imported trucks). In 1995, Toyota added the more powerful V6 engine from the new Tacoma to the T100 and also added an extended cab version. In 1999, Toyota replaced the T100 with the larger Tundra, which would be built in the US with a V8 engine and styling that more closely matched other American full-size trucks.

Luxury vehicles
In the Japanese home market, Toyota has two flagship models: the Crown premium sedan and the Century limousine.
In the 1980s, Toyota wanted to expand its luxury car offerings but realized that existing Japanese-market flagship models had little global appeal and could not compete with established brands such as Mercedes-Benz, BMW and Jaguar or the Acura and Infiniti marquees being launched by Japanese competitors.
Before the decade was out, Toyota introduced Lexus, a new division that was formed to market and service luxury vehicles in markets outside of Japan. The company developed the brand and its vehicles in secret since August 1983, at a cost of over US$1 billion. The Lexus LS flagship full-size sedan debuted in 1989 to strong sales, and was largely responsible for the successful launch of the Lexus marque. Subsequently, the division added sedan, coupé, convertible and SUV models.
The Lexus brand was introduced to the Japanese market in 2005, previously all vehicles marketed internationally as Lexus from 1989 to 2005 were released in Japan under the Toyota marque.

Buses
The Toyota Coaster is a minibus introduced in 1969 that seats 17 passengers. The Coaster is widely used in Japan, Singapore, Hong Kong, and Australia, but also in the developing world for minibus operators in Africa, the Middle East, South Asia, the Caribbean, and South America to operate as public transportation.

Technology
Hybrid electric vehicles
Toyota is the world's leader in sales of hybrid electric vehicles, one of the largest companies to encourage the mass-market adoption of hybrid vehicles across the globe, and the first to commercially mass-produce and sell such vehicles, with the introduction of the Toyota Prius in 1997. The company's series hybrid technology is called Hybrid Synergy Drive, and it was later applied to many vehicles in Toyota's product lineup, starting first with the Camry and the technology was also brought to the luxury Lexus division.
As of January 2020, Toyota Motor Corporation sells 44 Toyota and Lexus hybrid passenger car models in over 90 countries and regions around the world, and the carmaker has sold over 15 million hybrid vehicles since 1997. The Prius family is the world's top-selling hybrid gasoline-electric vehicle nameplate with almost 4 million units sold worldwide as of January 2017.

Hydrogen fuel-cell
In 2002, Toyota began a development and demonstration program to test the Toyota FCHV, a hybrid hydrogen fuel cell vehicle based on the Toyota Highlander production SUV. Toyota also built a FCHV bus based on the Hino Blue Ribbon City low-floor bus. Toyota has built several prototypes/concepts of the FCHV since 1997, including the Toyota FCHV-1, FCHV-2, FCHV-3, FCHV-4, and Toyota FCHV-adv. The Toyota FCV-R fuel cell concept car was unveiled at the 2011 Tokyo Motor Show. The FCV-R sedan seats four and has a fuel cell stack including a 70 MPa high-pressure hydrogen tank, which can deliver a range of 435 mi (700 km) under the Japanese JC08 test cycle. Toyota said the car was planned for launch in about 2015.In August 2012, Toyota announced its plans to start retail sales of a hydrogen fuel-cell sedan in California in 2015. Toyota expects to become a leader in this technology. The prototype of its first hydrogen fuel cell vehicle will be exhibited at the November 2013 Tokyo Motor Show, and in the United States at the January 2014 Consumer Electronics Show.Toyota's first hydrogen fuel-cell vehicles to be sold commercially, the Toyota Mirai (Japanese for "future"), was unveiled at the November 2014 Los Angeles Auto Show. In January 2015, it was announced that production of the Mirai fuel cell vehicle would increase from 700 units in 2015 to approximately 2,000 in 2016 and 3,000 in 2017. Sales in Japan began on December 15, 2014, at a price of ¥6,700,000 (~US$57,400). The Japanese government plans to support the commercialization of fuel-cell vehicles with a subsidy of ¥2,000,000 (~US$19,600). Retail sales in the U.S. began in August 2015 at a price of US$57,500 before any government incentives. Initially, the Mirai will only be available in California. The market release in Europe is slated for September 2015, and initially will be available only in the UK, Germany, and Denmark, followed by other countries in 2017. Pricing in Germany starts at €60,000 (~US$75,140) plus VAT (€78,540).In 2015, Toyota released 5,600 patents for free use until 2020, hoping to promote global development of hydrogen fuel-cell technology.Since the mid-2010s, Toyota has increased its focus on building hydrogen powered trucks. It first showcased a heavy-duty semi-truck tractor in 2017 and in 2023 announced a kit to convert existing diesel-powered truck engines to use hydrogen. In late 2022, the company signed an £11.3 million government deal with the UK's Department for Business, Energy and Industrial Strategy to help it develop a hydrogen-powered Hilux pickup truck.

Plug-in hybrids
The Prius Plug-In Hybrid Concept was exhibited in late 2009, and shortly after, a global demonstration program involving 600 pre-production test cars began. The vehicles were leased to fleet and government customers, and were equipped with data tracking devices to allow Toyota to monitor the car's performance. The vehicle was based on the third-generation Toyota Prius and outfitted with two additional lithium-ion batteries beyond the normal hybrid battery pack. The additional batteries were used to operate the car with minimal use of the internal combustion engine until they are depleted, at which point they are disengaged from the system. They are not used in tandem with the main hybrid battery pack.
After the conclusion of the demonstration program, the production version of the Prius Plug-in Hybrid was unveiled in September 2011. The production Prius Plug-in had a maximum electric-only speed of 100 km/h (62 mph), and the United States Environmental Protection Agency (EPA) rated the vehicle as having an range of 18 kilometres (11 mi) in blended mode (mostly electric, but supplemented by the internal combustion engine). Toyota ultimately only did a small production run with 75,400 vehicles being produced between 2012 and 2016.The second-generation Prius Plug-in (renamed the Prius Prime in the US) was unveiled in early 2016. Unlike the prior generation, where the plug-in battery was limited by being added to the existing Prius, this model would be developed in tandem with the fourth-generation Prius, allowing Toyota to increase the range to 40 kilometres (25 mi), with a top speed of 135 km/h (84 mph), without needing the assistance of the internal combustion engine. The second-generation Prius Plug-in went on sale starting in late 2016, with Toyota expecting to sell up to 60,000 units globally per year.A second plug-in hybrid model, the Toyota RAV4 PHV (RAV4 Prime in the US) was unveiled in December 2019. The vehicle has an EPA-estimated 68 kilometres (42 mi) of all-electric range and generates a combined 225 kilowatts (302 hp), enabling it to be Toyota's second fastest car currently in production (behind the GR Supra 3.0 sports car). Sales started in mid-2020.

Battery electric vehicles
Toyota has been criticized for being slow to add battery electric vehicles to its lineup. It has been publicly skeptical of battery-electric vehicles, instead focusing on hybrid and hydrogen fuel cell vehicles, and actively lobbying against government mandates to transition to zero-emissions vehicles.As of 2023, only a small only a small proportion of the vehicles the company sells are of battery electric, which has prompted criticism by some environmental and public interest groups. The company plans to increase its sales of electric vehicles to 3.5 million per year by 2030. However, the company has stated that it believes other technologies, including hybrid and hydrogen fuel cell vehicles, will continue to play a role in the future of the company.Toyota created the first generation Toyota RAV4 EV (Electric Vehicle) as a compliance car after the California Air Resources Board mandated in the late 1990s that every automaker offer a zero-emissions vehicle. A total of 1,484 were leased and/or sold in California from 1997 to 2003, when the state dropped its mandate under legal pressure from lawsuits filed by automakers.A second generation of the RAV4 EV was developed in 2010 as part of a deal with Tesla. The production version was unveiled in August 2012, using battery pack, electronics and powertrain components from Tesla. The RAV4 EV had a limited production run with just under 3,000 vehicles being produced, before it was discontinued in 2014. According to Bloomberg News, the partnership between Tesla and Toyota was "marred by clashes between engineers".Starting in 2009, Toyota introduced three generations of concept electric vehicles called the FT-EV built on a modified Toyota iQ platform. In late-2012, the company announced plans build a production version of the car called the Toyota iQ EV (Scion iQ EV in the US, Toyota eQ in Japan), but ultimately production was cut back to 100 cars for special fleet use in Japan and the U.S. only.In late 2012, Toyota announced that it would back away from fully electric vehicles, after producing less than 5,000. At the time, the company's vice chairman, Takeshi Uchiyamada, said: "The current capabilities of electric vehicles do not meet society's needs, whether it may be the distance the cars can run, or the costs, or how it takes a long time to charge."
A shift in Toyota's formerly battery-agnostic posture could be seen as early as 2016, when Toyota's CFO Takahiko Ijichi “sent a strong signal that Toyota soon plans to jump on the battery bandwagon and make electric cars despite expressing skeptical views about their range and charging times,” as the Wall Street Journal wrote. Toyota said it would make and sell battery-electric vehicles if and where regulations and markets demand.
A year later, Toyota outlined its electric-vehicle plans for between 2020 and 2030 to the press in Tokyo, saying it would introduce "more than 10" battery-electric vehicles worldwide by the early 2020s, beginning in China, and later in Japan, Europe, the US and India.In April 2019, Toyota introduced the C-HR EV, its first mass-produced pure electric model in China along with an identical twin called the IZOA EV. It went on sale in April 2020 and May 2020 respectively. Nikkei reported in October 2020 that Toyota had only sold less than 2,000 units in the first eight months of the year.Toyota introduced the C+pod in late 2020, a 2-seater kei car with an estimated range of 100 kilometres (62 mi) and a top speed of 60 kilometres per hour (37 mph).In December 2020, Toyota CEO Akio Toyoda stated that electric cars are excessively "hyped" and that, in Japan, they would not necessarily reduce carbon dioxide emissions since electricity is mostly generated by burning coal and natural gas in the country. He also said that the infrastructure needed for Japan to switch fully to EVs would cost between $135 billion and $358 billion and switching only to EVs would cost millions of jobs and make cars less affordable.In April 2021, Toyota revealed the bZ4X, an electric crossover SUV that will be the first vehicle built on a dedicated electric platform called e-TNGA when it goes on sale in mid-2022. It is the first model of the bZ ("beyond Zero") series of battery electric vehicles. The company has also stated that there will be seven "bZ" models to be launched globally out of 15 BEV models by 2025.In June 2021, Transport & Environment ranked Toyota as the least ready OEM to transition to battery electric vehicles by 2030, stating: "Toyota has not set a target for 2030 and it plans to produce just 10% BEVs in 2025. It is expected to rely on polluting hybrid technologies."
In December 2021, Toyota announced in Tokyo plans for 30 battery-electric models by 2030, to sell 3.5 million BEVs per year by that date, and that its premium-brand Lexus will be 100% battery-operated by 2030 in North America, Europe, and China. The company announced investments of $70 billion into the company's electrification.In a session in parallel with the G-7 Meeting 2023 May, Akio Toyoda said that battery electric vehicles are not solely the future, instead offering a mix of battery electric, internal combustion engine-equipped and hydrogen cars.Toyota has been developing solid-state batteries in partnership with Panasonic, in which the company has more than a thousand patents covering solid-state batteries by late 2020. The technology has been implemented on the Toyota LQ concept. Toyota hopes the technology could increase efficiency of battery electric vehicles by 30 percent, which in turn would reduce battery costs by the same amount.

Autonomous vehicles
Toyota has is also regarded as lagging when it comes to developing smart car technology. Although the company unveiled its first self-driving test vehicle in 2017, and has been developing its own self-driving technology named "Chauffeur" (intended for full self-driving) and "Guardian" (a driver assist system), neither of these has been introduced into any production vehicles.The company had set up a large research and development operation by 2018, spending almost US$4 billion to start an autonomous vehicle research institute in California's Silicon Valley and another ¥300 billion on a similar research institute in Tokyo that would partner with fellow Toyota Group companies and automotive suppliers Aisin Seiki and Denso.Toyota has also been collaborating with autonomous vehicle technology developers and, in some cases, purchasing the companies. Toyota has acquired the autonomous vehicle division of ride-hailing service Lyft for $550 million, invested a total of US$1 billion in competing ride-hailing service Uber's self-driving vehicle division, invested $400 million in autonomous vehicle technology company Pony.ai, and announced a partnership with Chinese electronics e-commerce company Cogobuy to build a "Smart Car Ecosystem."In December 2020, Toyota showcased the 20-passenger "e-Palette" shared autonomous vehicle, which was used at the 2021 Tokyo Olympic Games.  Toyota has announced it intends to have the vehicle available for commercial applications before 2025.Since February 2021, Toyota has been building the "Woven City" which it calls a "175-acre high tech, sensor-laden metropolis" at the foot of Mount Fuji. When completed in 2024, the Woven City will be used to run tests on autonomous vehicles for deliveries, transport and mobile shops, with the city's residents participating in the living laboratory experiment.

Motorsports
Toyota has been involved in many global motorsports series, providing vehicles, engines and other auto parts under both the Toyota and Lexus brands.
Toyota Gazoo Racing (GR) is Toyota's performance brand that is used in many of the world's major motorsports contests. Toyota Gazoo Racing Europe, based in Cologne, Germany, competes in the FIA World Endurance Championship, while the Finland-based Toyota Gazoo Racing WRT participates in the FIA World Rally Championship. Toyota Gazoo Racing South Africa competes in the Dakar Rally. Between 2002 and 2009, the Toyota Racing team competed in Formula One. Toyota won the highest class of the 24 Hours of Le Mans 5 consecutive times from 2018 to 2022, competing with the Toyota TS050 Hybrid and Toyota GR010 Hybrid.
Toyota Racing Development USA (TRD USA) is responsible for participation in major motorsports contests in the United States including NASCAR, NHRA, Indy Racing League and Formula Drift.
Toyota also makes engines and other auto parts for other Japanese motorsports including Super Formula, Super GT, Formula 3, and Toyota Racing Series.

Non-automotive activities
Aerospace
Toyota is a minority shareholder in Mitsubishi Aircraft Corporation, having invested US$67.2 million in the new venture which will produce the Mitsubishi Regional Jet, slated for first deliveries in 2017. Toyota has also studied participation in the general aviation market and contracted with Scaled Composites to produce a proof of concept aircraft, the TAA-1, in 2002.

Pleasure boats
In 1997, building on a previous partnership with Yamaha Marine, Toyota created "Toyota Marine", building private ownership motorboats, currently sold only in Japan. A small network in Japan sells the luxury craft at 54 locations, called the "Toyota Ponam" series, and in 2017, a boat was labeled under the Lexus brand name starting May 26, 2017.

Philanthropy
Toyota supports a variety of philanthropic work in areas such as education, conservation, safety, and disaster relief.
Some of the organizations that Toyota has worked with in the US include the American Red Cross, the Boys and Girls Club, Leaders in Environmental Action for the Future (LEAF), and the National Center for Family Literacy.The Toyota USA Foundation exists to support education in the areas of science, technology, engineering, and mathematics.In addition, Toyota works with nonprofits to improve their processes and operations such as the Food Bank For New York City.Toyota also supports a variety of work in Japan.The Toyota Foundation takes a global perspective providing grants in the three areas of human and natural environments, social welfare, and education and culture.

Higher education
Toyota established the Toyota Technological Institute in 1981, as Sakichi Toyoda had planned to establish a university as soon as he and Toyota became successful. Toyota Technological Institute founded the Toyota Technological Institute at Chicago in 2003. Toyota is supporter of the Toyota Driving Expectations Program, Toyota Youth for Understanding Summer Exchange Scholarship Program, Toyota International Teacher Program, Toyota TAPESTRY, Toyota Community Scholars (scholarship for high school students), United States Hispanic Chamber of Commerce Internship Program, and Toyota Funded Scholarship. It has contributed to a number of local education and scholarship programs for the University of Kentucky, Indiana, and others.

Robotics
In 2004, Toyota showcased its trumpet-playing robot. Toyota has been developing multitask robots destined for elderly care, manufacturing, and entertainment. A specific example of Toyota's involvement in robotics for the elderly is the Brain Machine Interface. Designed for use with wheelchairs, it "allows a person to control an electric wheelchair accurately, almost in real-time", with his or her mind. The thought controls allow the wheelchair to go left, right, and forward with a delay between thought and movement of just 125 milliseconds. Toyota also played a part in the development of Kirobo, a 'robotic astronaut'.
In 2017, the company introduced T-HR3, a humanoid robot with the ability to be remotely controlled. The robot can copy the motions of a connected person. The 2017 version used wires for the connection but the 2018 version used 5G from a distance up to 10 km.

Agricultural biotechnology
Toyota invests in several small start-up businesses and partnerships in biotechnology, including:

P.T. Toyota Bio Indonesia in Lampung, Indonesia
Australian Afforestation Pty. Ltd. in Western Australia and Southern Australia
Toyota Floritech Co., Ltd. in Rokkasho-Mura, Kamikita District, Aomori Prefecture
Sichuan Toyota Nitan Development Co., Ltd. in Sichuan, China
Toyota Roof Garden Corporation in Miyoshi-Cho, Aichi Prefecture

Sewing machine brand
Aisin, another member of the Toyota Group of companies, uses the same Toyota wordmark logo to market its home-use sewing machines. Aisin was founded by Kiichiro Toyoda after he founded the Toyota Motor Corporation. According to Aisin, he was so pleased with the first sewing machine, he decided to apply the same Toyota branding as his auto business, despite the companies being independent from each other.

Carbon removal
Toyota Ventures, along with JetBlue Technology Ventures and Parley for the Oceans, is among the corporate investors that have invested $40 million in the Air Company, a carbon negative vodka distiller and perfume and hand sanitizer manufacturer that uses heterogeneous catalysis to convert captured carbon into ethanol.

Environmental initiatives
Toyota states it is committed to achieving carbon neutrality by 2050, and it has set a goal to reduce its overall carbon emissions by 90% by 2050, compared to 2010 levels.The company has invested heavily in solar energy, with a goal to install solar panels on the rooftops of all its dealerships worldwide by 2050. In addition, Toyota has partnered with various renewable energy companies to promote the use of wind and solar power, including a recent partnership with ENEOS Corporation to develop hydrogen refueling stations in Japan.Toyota has launched a program called "Global Environmental Challenge" 2050 which is a comprehensive initiative aimed at reducing the environmental impact of Toyota's operations. The challenge includes six environmental goals that Toyota aims to achieve by 2050, including reducing carbon emissions, minimizing water usage, promoting the recycling and reuse of materials, and promoting the use of renewable energy. The company has also implemented a variety of initiatives to promote sustainability across its supply chain, including efforts to reduce waste, water usage, and promote sustainable agriculture. For example, Toyota has implemented a zero-waste initiative at its manufacturing plants, aiming to send zero waste to landfills.

Controversies
Corrosion lawsuit
In November 2016, Toyota agreed to pay $3.4 billion to settle allegations that roughly one-and-a-half million of its Tacoma, Tundra, and Sequoia pickup trucks and SUVs had been outfitted with frames prone to corrosion and perforation. According to court papers, the corrosion could reach levels high enough to compromise the vehicle's structural integrity.

Death from overwork
On February 9, 2002, Kenichi Uchino, aged 30 years, a quality control manager, collapsed then died at work. On January 2, 2006, an unnamed chief engineer of the Camry Hybrid, aged 45 years, died from heart failure in his bed.

Fines for environmental breaches
In 2003, Toyota was fined $34 million for violating the United States Clean Air Act, as 2.2 million vehicles it sold had defective smog-control computers.In January 2021, Toyota was fined $180 million for delays in reporting emissions-related defects to the U.S. Environmental Protection Agency (EPA) between 2005 and 2015. The acting U.S. attorney said that the delays "likely led to delayed or avoided emissions-related recalls", although Toyota stated that despite the delays in reporting the issues to the EPA, it had notified customers and fixed the cars with the emissions defects. At the time, this was the biggest civil penalty ever levied for violating the EPA's emission reporting requirements.

2009–2011 unintended acceleration recalls
Between 2009 and 2011 Toyota, under pressure from the U.S. National Highway Traffic Safety Administration (NHTSA), conducted recalls of millions of vehicles after reports that several drivers experienced unintended acceleration. The first recall, in November 2009, was to prevent a front driver's side floor mat from sliding into the foot pedal well, causing the pedals to become trapped. The second recall, in January 2010, was begun after some crashes were shown not to have been caused by floor mats and may be caused by possible mechanical sticking of the accelerator pedal. Worldwide, approximately 9 million cars and trucks were impacted by the recalls.NHTSA received reports of a total of 37 deaths allegedly related to unintended acceleration, although an exact number was never verified. As a result of the problems, Toyota faced nearly 100 lawsuits from the families of those killed, drivers who were injured, vehicle owners who lost resale value, and investors who saw a drop in the value of their shares. While most of the personal injury and wrongful death lawsuits were settled confidentially, Toyota did spend more than US$1 billion to settle a class action lawsuit to compensate owners for lost resale value, and the company agreed to pay a US$1.2 billion criminal penalty to the United States government over accusations that it had intentionally hid information about safety defects from the public and had made deceptive statements to protect its brand image. The penalty was the largest ever levied against a car company.

Takata airbag recalls
Toyota was impacted by a recall of faulty airbag inflators made by Takata. The inflators can explode, shooting metal fragments into the vehicle cabin. Millions of vehicles produced between 2000 and 2014 were impacted by the recall, with some needing multiple repairs.

June 2010 Chinese labour strike
On June 21, 2010, a Chinese labor strike happened in Tianjin Toyoda Gosei Co, Tianjin. Workers demanded better wages and treatment.

Extremist usage
In 2015, U.S. officials asked Toyota how the Islamic State was in possession of so many Toyota trucks. Toyota cars have also been documented to have been used by other extremist organizations such as the Taliban, Hamas, Al-shabab and Somali pirates. Toyota representatives have said the company has a strict policy of to not sell vehicles to potential purchasers who may use or modify them for paramilitary or terrorist activities and that it cannot track aftermarket sales and that it has worked with the U.S Treasury to brief them on Toyota's supply chain on the Middle East.

Misleading marketing
In its marketing, Toyota has often referred to its non-plug-in hybrid vehicles as "self-charging hybrid" vehicles. The use of the term has caused some criticism that this is misleading, as some consumers were led to erroneously believe that these vehicles charge their batteries on their own when the vehicles are not used. Complaints about self-charging hybrid advertising were recorded in Ireland, although the complaints were rejected by the Advertising Standards Authority of Ireland. However, in 2020, the Norwegian Consumer Authority banned the adverts outright in Norway, stating: "It is misleading to give the impression that the power to the hybrid battery is free of charge, since the electricity produced by the car has consumption of gasoline as a necessary condition."Electric vehicle website IrishEVs criticised Toyota Ireland for paying University College Dublin to conduct a study of only seven examples of their hybrid cars over seven days to make a press release about the efficiency of the vehicles. The website also criticised Toyota Ireland for using opinion polls to substantiate a claim about their CO2 emissions on Twitter.

Rigging of crash test results
In April 2023, it was revealed that Toyota subsidiary Daihatsu had cheated by rigging some models to perform better in crash tests than actual production cars. The vehicles in question had a notch in the interior panel of the front door which avoided the possibility of the collision creating a sharp edge that could have injured an occupant when the side airbag deployed. This notch was present on the tested vehicles but not on vehicles sold to the public.The issue affected four models, the Toyota Yaris Ativ (also known as the Vios), Perodua Axia, Toyota Agya, and an undisclosed upcoming product. No recall was conducted over the issue.

Corporate affairs
Toyota is headquartered in the city of Toyota, which was named Koromo until 1951, when it changed its name to match the automaker. Toyota City is located in the Aichi Prefecture of Japan. The main headquarters of Toyota is located in a four-story building that has been described as "modest". In 2013, company CEO Akio Toyoda reported that it had difficulties retaining foreign employees at the headquarters due to the lack of amenities in the city.Surrounding the headquarters are the 14-story Toyota Technical Center and the Honsha plant (which was established in 1938). Toyota and its Toyota Group affiliates operate a total of 17 manufacturing facilities in Aichi Prefecture and a total of 32 plants in Japan.
Toyota also operates offices in Bunkyo, Tokyo, and Nakamura-ku, Nagoya.

Worldwide presence
Outside of Japan, as one of the world's largest automotive manufacturer by production volume, Toyota has factories in most parts of the world. The company assembles vehicles in Argentina, Belgium, Brazil, Canada, Colombia, the Czech Republic, France, Indonesia, Mexico, the Philippines, Poland, Russia, South Africa, Thailand, Turkey, the United Kingdom, the United States, and Venezuela.
Additionally, the company also has joint venture, licensed, or contract factories in China, France, India, Malaysia, Pakistan, Taiwan, the United States, and Vietnam.

North America
Toyota Motor North America is headquartered in Plano, Texas, and operates as a holding company for all operations of the Toyota Motor Corporation in Canada, Mexico, and the United States. Toyota's operations in North America began on October 31, 1957, and the current company was established in 2017 from the consolidation of three companies: Toyota Motor North America, Inc., which controlled Toyota's corporate functions; Toyota Motor Sales, U.S.A., Inc. which handled marketing, sales, and distribution in the United States; and Toyota Motor Engineering & Manufacturing North America which oversaw operations at all assembly plants in the region. While all three companies continue to exist in legal name, they operate as one company out of one headquarters campus.
Toyota has a large presence in the United States with six major assembly plants in Huntsville, Alabama, Georgetown, Kentucky, Princeton, Indiana, San Antonio, Texas, Buffalo, West Virginia, and Blue Springs, Mississippi. In 2018, Toyota and Mazda announced a joint venture plant that will produce vehicles in Huntsville, Alabama, starting in 2021.It has started producing larger trucks, such as the new Tundra, to go after the full-size pickup market in the United States. Toyota is also pushing hybrid electric vehicle in the US such as the Prius, Camry Hybrid, Highlander Hybrid, and various Lexus products. Currently, Toyota has no plans to offer diesel motor options in its North American products, including pickup trucks.Toyota Canada Inc., which is part of Toyota Motor North America, handles marketing, sales, and distribution in Canada. Toyota Motor Manufacturing Canada operates three assembly plants: two in Cambridge, Ontario and one in Woodstock, Ontario. In 2006, Toyota's subsidiary Hino Motors opened a heavy duty truck plant, also in Woodstock, employing 45 people and producing 2,000 trucks annually.

Europe/Western Asia
Toyota Motor Europe is headquartered in Brussels, Belgium, and oversees all operations of the Toyota Motor Corporation in Europe and western Asia. Toyota's operations in Europe began in 1963. Toyota has a significant presence in Europe with nine production facilities in Kolín, Czech Republic, Burnaston, England, Deeside, England, Onnaing, France, Jelcz-Laskowice, Poland, Wałbrzych, Poland, Ovar, Portugal, Saint Petersburg, Russia, and Arifiye, Turkey. Toyota also operates a joint venture plant with Citroën and Peugeot in Valenciennes, France.

Australia
In 1963, Australia was one of the first countries to assemble Toyotas outside Japan. However, in February 2014, Toyota was the last of Australia's major automakers to announce the end of production in Australia. The closure of Toyota's Australian plant was completed on October 3, 2017, and had produced a total 3,451,155 vehicles. At its peak in October 2007, Toyota manufactured 15,000 cars a month.  Before Toyota, Ford and GM's Holden had announced similar moves, all citing an unfavorable currency and attendant high manufacturing costs.

Financials
Toyota is publicly traded on the Tokyo, Osaka, Nagoya, Fukuoka, and Sapporo exchanges under company code TYO: 7203. In Japan, Toyota's stock is a component of the Nikkei 225 and TOPIX Core30 indices.
In addition, Toyota is foreign-listed on the New York Stock Exchange under NYSE: TM and on the London Stock Exchange under LSE: TYT.
Toyota has been publicly traded in Japan since 1949 and internationally since 1999.

Company strategy
The Toyota Way
The Toyota Way is a set of principles and behaviors that underlie the company's approach to management and production (which is further defined as the Toyota Production System).
The company has been developing its corporate philosophy since 1948 and passing it on as implicit knowledge to new employees, but as the company expanded globally, leaders officially identified and defined the Toyota Way in 2001. Toyota summarized it under two main pillars: continuous improvement and respect for people. Under the continuous improvement pillar are three principles: challenge (form a long-term vision), kaizen (a continual improvement process), and genchi genbutsu ("go and see" the process to make correct decisions). Under the respect for people pillar are two principles: respect and teamwork.In 2004, Dr. Jeffrey Liker, a University of Michigan professor of industrial engineering, published The Toyota Way. In his book, Liker calls the Toyota Way "a system designed to provide the tools for people to continually improve their work." According to Liker, there are 14 principles of The Toyota Way that can be organized into four themes: (1) long-term philosophy, (2) the right process will produce the right results, (3) add value to the organization by developing your people, and (4) continuously solving root problems drives organizational learning. The 14 principles are further defined in the Wikipedia article on The Toyota Way.

Toyota Production System
The Toyota Way also helped shape the company's approach to production, where it was an early pioneer of what would be come to be known as lean manufacturing. The company defines the Toyota Production System under two main pillars: just-in-time (make only what is needed, only when it is needed, and only in the amount that is needed) and Jidoka  (automation with a human touch).
The origin of the Toyota Production System is in dispute, with three stories of its origin: (1) that during a 1950 trip to train with the Ford Motor Company, company executives also studied the just-in-time distribution system of the grocery store company Piggly-Wiggly, (2) that they followed the writings of W. Edwards Deming, and (3) they learned the principles from a WWII US government training program (Training Within Industry).After developing the Toyota Production System in its own facilities, the company began teaching the system to its parts suppliers in the 1990s. Other companies were interested in the instruction, and Toyota later started offering training sessions. The company also has donated the training to non-profit groups to increase their efficiency and thus ability to serve people.

Logo and branding
In 1936, Toyota entered the passenger car market with its Model AA and held a competition to establish a new logo emphasizing speed for its new product line. After receiving 27,000 entries, one was selected that additionally resulted in a change of its moniker to "Toyota" from the family name "Toyoda", which means rice paddy. The new name was believed to sound better, and its eight-stroke count in the Japanese language was associated with wealth and good fortune. The original logo was a heavily stylized version of the katakana characters for Toyota (トヨタ).As the company started to expand internationally in the late 1950s, the katakana character logo was supplemented by various wordmarks with the English form of the company name in all capital letters, "TOYOTA."Toyota introduced a worldwide logo in October 1989 to commemorate the 50th year of the company, and to differentiate it from the newly released luxury Lexus brand. The logo consists of three ovals that combine to form the letter "T", which stands for Toyota. Toyota says that the overlapping of the two perpendicular ovals inside the larger oval represents the mutually beneficial relationship and trust between the customer and the company while the larger oval surrounding both of these inner ovals represents the "global expansion of Toyota's technology and unlimited potential for the future". The new logo started appearing on all printed material, advertisements, dealer signage, and most vehicles in 1990.
In countries or regions using traditional Chinese characters, e.g. Hong Kong and Taiwan, Toyota is known as "豐田". In countries using simplified Chinese characters (e.g. China and Singapore), Toyota is written as "丰田" (pronounced as Fēngtián in Mandarin Chinese and Hɔng Tshan in Minnanese). These are the same characters as the founding family's name "Toyoda" in Japanese.

Toyota still uses the katakana character logo as its corporate emblem in Japan, including on the headquarters building, and some special edition models still use the "TOYOTA" wordmark on the grille as a nod to the company's heritage.On July 15, 2015, the company was delegated its own generic top-level domain, .toyota.

Sports sponsorships
Toyota sponsors several teams and has purchased naming rights for several venues, and even competitions, including:

Toyota Alvark Tokyo, basketball team
Toyota Arena, Ontario, California
Toyota Cup
Toyota Center, Houston, Texas
Toyota Center, Kennewick, Washington
Toyota Field, San Antonio, Texas
Toyota Park, Bridgeview, Illinois
Toyota Sports Center, El Segundo, California
Toyota Stadium, Georgetown, Kentucky
Toyota Stadium, Frisco, TexasAs of 2017, Toyota is an official sponsor of Cricket Australia, the England and Wales Cricket Board and the AFL. In March 2015, Toyota became a sponsor partner for the Olympic Games, in the form of supplying vehicles and communications between vehicles until 2024.

See also
List of Toyota engines
List of Toyota manufacturing facilities
List of Toyota transmissions
List of Toyota vehicles
Nagoya Grampus, formerly the company's football club and still sponsored by them
Toyota model codes
Toyota Verblitz, the company's rugby team
Toyota War, a conflict between Libya and Chad which saw a heavy use of Toyota's pickup trucks.

References
External links

Official website 
Business data for Toyota Motor Corp:

Toyota Production System

The Toyota Production System (TPS) is an integrated socio-technical system, developed by Toyota, that comprises its management philosophy and practices. The TPS is a management system that organizes manufacturing and logistics for the automobile manufacturer, including interaction with suppliers and customers. The system is a major precursor of the more generic "lean manufacturing". Taiichi Ohno and Eiji Toyoda, Japanese industrial engineers, developed the system between 1948 and 1975.Originally called "just-in-time production", it builds on the approach created by the founder of Toyota, Sakichi Toyoda, his son Kiichiro Toyoda, and the engineer Taiichi Ohno. The principles underlying the TPS are embodied in The Toyota Way.

Goals
The main objectives of the TPS are to design out overburden (muri) and inconsistency (mura), and to eliminate waste (muda). The most significant effects on process value delivery are achieved by designing a process capable of delivering the required results smoothly; by designing out "mura" (inconsistency). It is also crucial to ensure that the process is as flexible as necessary without stress or "muri" (overburden) since this generates "muda" (waste). Finally the tactical improvements of waste reduction or the elimination of muda are very valuable. There are eight kinds of muda that are addressed in the TPS:
Waste of overproduction (largest waste)
Waste of time on hand (waiting)
Waste of transportation
Waste of processing itself
Waste of excess inventory
Waste of movement
Waste of making defective products
Waste of underutilized workers

Concept
Toyota Motor Corporation published an official description of TPS for the first time in 1992; this booklet was revised in 1998. In the foreword it was said: "The TPS is a framework for conserving resources by eliminating waste. People who participate in the system learn to identify expenditures of material, effort and time that do not generate value for customers and furthermore we have avoided a 'how-to' approach. The booklet is not a manual. Rather it is an overview of the concepts, that underlie our production system. It is a reminder that lasting gains in productivity and quality are possible whenever and wherever management and employees are united in a commitment to positive change". TPS is grounded on two main conceptual pillars:

Just-in-time – meaning "Making only what is needed, only when it is needed, and only in the amount that is needed"
Jidoka –  (Autonomation) meaning "Automation with a human touch"Toyota has developed various tools to transfer these concepts into practice and apply them to specific requirements and conditions in the company and business.

Origins
Toyota has long been recognized as a leader in the automotive manufacturing and production industry.Toyota received their inspiration for the system, not from the American automotive industry (at that time the world's largest by far), but from visiting a supermarket.  The idea of just-in-time production was originated by Kiichiro Toyoda, founder of Toyota. The question was how to implement the idea.  In reading descriptions of American supermarkets, Ohno saw the supermarket as the model for what he was trying to accomplish in the factory.  A customer in a supermarket takes the desired amount of goods off the shelf and purchases them.  The store restocks the shelf with enough new product to fill up the shelf space.  Similarly, a work-center that needed parts would go to a "store shelf" (the inventory storage point) for the particular part and "buy" (withdraw) the quantity it needed, and the "shelf" would be "restocked" by the work-center that produced the part, making only enough to replace the inventory that had been withdrawn.While low inventory levels are a key outcome of the System, an important element of the philosophy behind its system is to work intelligently and eliminate waste so that only minimal inventory is needed. Many Western businesses, having observed Toyota's factories, set out to attack high inventory levels directly without understanding what made these reductions possible. The act of imitating without understanding the underlying concept or motivation may have led to the failure of those projects.

Principles
The underlying principles, called the Toyota Way, have been outlined by Toyota as follows:

Continuous improvement
Challenge  (We form a long-term vision, meeting challenges with courage and creativity to realize our dreams.)
Kaizen  (We improve our business operations continuously, always driving for innovation and evolution.)
Genchi Genbutsu  (Go to the source to find the facts to make correct decisions.)

Respect for people
Respect (We respect others, make every effort to understand each other, take responsibility and do our best to build mutual trust.)
Teamwork  (We stimulate personal and professional growth, share the opportunities of development and maximize individual and team performance.)External observers have summarized the principles of the Toyota Way as:

The right process will produce the right results
Create continuous process flow to bring problems to the surface.
Use the "pull" system to avoid overproduction.
Level out the workload (heijunka). (Work like the tortoise, not the hare.)
Build a culture of stopping to fix problems, to get quality right from the start. (Jidoka)
Standardized tasks are the foundation for continuous improvement and employee empowerment.
Use visual control so no problems are hidden.
Use only reliable, thoroughly tested technology that serves your people and processes.

Add value to the organization by developing your people and partners
Grow leaders who thoroughly understand the work, live the philosophy, and teach it to others.
Develop exceptional people and teams who follow your company's philosophy.
Respect your extended network of partners and suppliers by challenging them and helping them improve.

Continuously solving root problems drives organizational learning
Go and see for yourself to thoroughly understand the situation (Genchi Genbutsu, 現地現物);
Make decisions slowly by consensus, thoroughly considering all options (Nemawashi, 根回し); implement decisions rapidly;
Become a learning organization through relentless reflection (Hansei, 反省) and continuous improvement and never stop (Kaizen, 改善).What this means is that it is a system for thorough waste elimination. Here, waste refers to anything which does not advance the process, everything that does not increase added value. Many people settle for eliminating the waste that everyone recognizes as waste. But much remains that simply has not yet been recognized as waste or that people are willing to tolerate.
People had resigned themselves to certain problems, had become hostage to routine and abandoned the practice of problem-solving. This going back to basics, exposing the real significance of problems and then making fundamental improvements, can be witnessed throughout the Toyota Production System.The principles of the Toyota Production System have been compared to production methods in the industrialization of construction.

Sharing
Toyota originally began sharing TPS with its parts suppliers in the 1990s. Because of interest in the program from other organizations, Toyota began offering instruction in the methodology to others. Toyota has even "donated" its system to charities, providing its engineering staff and techniques to non-profits in an effort to increase their efficiency and thus ability to serve people. For example, Toyota assisted the Food Bank For New York City to significantly decrease waiting times at soup kitchens, packing times at a food distribution center, and waiting times in a food pantry. Toyota announced on June 29, 2011 the launch of a national program to donate its Toyota Production System expertise towards nonprofit organizations with goal of improving their operations, extending their reach, and increasing their impact. By September, less than three months later, SBP, a disaster relief organization based out of New Orleans, reported that their home rebuilds had been reduced from 12 to 18 weeks, to 6 weeks. Additionally, employing Toyota methods (like kaizen) had reduced construction errors by 50 percent. The company included SBP among its first 20 community organizations, along with AmeriCorps.

Workplace Management
Taiichi Ohno's Workplace Management (2007) outlines in 38 chapters how to implement the TPS. Some important concepts are:

Chapter 1 Wise Mend Their Ways - See the Analects of Confucius for further information.
Chapter 4 Confirm Failures With Your Own Eyes
Chapter 11 Wasted Motion Is Not Work
Chapter 15 Just In Time - Phrase invented by Kiichiro Toyoda - the first president of Toyota. There is conflict on what the actual English translation of what "just in time" really means. Taiichi Ohno quoted from the book says " 'Just In Time' should be interpreted to mean that it is a problem when parts are delivered too early".
Chapter 23 How To Produce At A Lower Cost - "One of the main fundamentals of the Toyota System is to make 'what you need, in the amount you need, by the time you need it', but to tell the truth there is another part to this and that is 'at lower cost'.  But that part is not written down." World economies, events, and each individual job also play a part in production specifics.

Commonly used terminology
Andon (行灯) (English: A large lighted board used to alert floor supervisors to a problem at a specific station. Literally: Signboard)
Chaku-Chaku (着々 or 着着) (English: Load-Load)
Gemba (現場) (English: The actual place, the place where the real work is done; On site)
Genchi Genbutsu (現地現物) (English: Go and see for yourself)
Hansei (反省) (English: Self-reflection)
Heijunka (平準化) (English: Production Smoothing)
Jidoka (自働化) (English: Autonomation - automation with human intelligence)
Just-in-Time (ジャストインタイム) (JIT)
Kaizen (改善) (English: Continuous Improvement)
Kanban (看板, also かんばん) (English: Sign, Index Card)
Manufacturing supermarket where all components are available to be withdrawn by a process
Muda (無駄, also ムダ) (English: Waste)
Mura (斑 or ムラ) (English: Unevenness)
Muri (無理) (English: Overburden)
Nemawashi (根回し) (English: Laying the groundwork, building consensus, literally: Going around the roots)
Obeya (大部屋) (English: Manager's meeting. Literally: Large room, war room, council room)
Poka-yoke (ポカヨケ) (English:  fail-safing, bulletproofing - to avoid (yokeru) inadvertent errors (poka)
Seibi (English: To Prepare)
Seiri (整理) (English: Sort, removing whatever isn't necessary.)
Seiton (整頓) (English: Organize)
Seiso (清掃) (English: Clean and inspect)
Seiketsu (清潔) (English: Standardize)
Shitsuke (躾) (English: Sustain)

See also
Lean construction
W. Edwards Deming
Training Within Industry
Production flow analysis
Industrial engineering

References
Bibliography
Emiliani, B., with Stec, D., Grasso, L. and Stodder, J. (2007), Better Thinking, Better Results: Case Study and Analysis of an Enterprise-Wide Lean Transformation, second edition, The CLBM, LLC Kensington, Conn., ISBN 978-0-9722591-2-5
Liker, Jeffrey (2003), The Toyota Way: 14 Management Principles from the World's Greatest Manufacturer, First edition, McGraw-Hill, ISBN 0-07-139231-9.
Monden, Yasuhiro (1998), Toyota Production System, An Integrated Approach to Just-In-Time, Third edition,  Norcross, GA: Engineering & Management Press, ISBN 0-412-83930-X.
Ohno, Taiichi (1988), Just-In-Time for Today and Tomorrow, Cambridge, MA: Productivity Press, ISBN 0915299208
Ohno, Taiichi (1988), Toyota Production System: Beyond Large-Scale Production, Cambridge, MA: Productivity Press, ISBN 0915299143
Ohno, Taiichi (1988), Workplace Management, Cambridge, MA: Productivity Press, ISBN 0915299194
Shingo, Shigeo; Dillon, Andrew (1989), A study of the Toyota production system from an industrial engineering viewpoint (Produce What Is Needed, When It's Needed), Portland, OR: Productivity Press, ISBN 0-915299-17-8, OCLC 19740349
Spear, Steven, and Bowen, H. Kent (September 1999), "Decoding the DNA of the Toyota Production System," Harvard Business Review
Womack, James P. and Jones, Daniel T. (2003), Lean Thinking: Banish Waste and Create Wealth in Your Corporation, Revised and Updated, HarperBusiness, ISBN 0-7432-4927-5.
Womack, James P., Jones, Daniel T., and Roos, Daniel (1991), The Machine That Changed the World: The Story of Lean Production, HarperBusiness, ISBN 0-06-097417-6.

External links
Toyota Production System
History of the TPS at the Toyota Motor Manufacturing Kentucky Site Archived 2012-04-28 at the Wayback Machine
Toyota Production System Terms Archived 2012-04-27 at the Wayback Machine
Article: Lean Primer: Introduction

Value-driven maintenance

Value-driven maintenance (VDM) is a maintenance management methodology.

History
VDM was developed by Mark Haarman and Guy Delahay, both former chairmen of the Dutch Maintenance Association (NVDO)  and authors of a book entitled Value Driven Maintenance, New Faith in Maintenance published by Mainnovation in 2004.

Value drivers in maintenance
In financial literature, value (net present value) is defined as the sum of all free future cash flows, discounted to the present date.
A cash flow is the difference between income and expenditure. It is not the difference between turnover and costs, because this is easy to manipulate through accounting. Some companies may use  creative lease, depreciation and reservation techniques to keep book profits artificially high or low; this does not always contribute to shareholder value. Recent stock market scandals are a example of what may happen as a result of this. The second part of the definition concerns the knowledge that the value of a cash flow is time-related, given the term "present value". Future cash flows must be corrected or discounted to today. Managing by value necessitates maximizing future cash flows. Managing by value obliges companies constantly to search for new free cash flows. It's no longer enough for a company to go on doing what it is already doing, they must create value.
Now that we know what value is, we can translate the concept into maintenance. Within VDM, there are four axes along which maintenance can contribute to value creation within a company. The axes are also called the four value drivers.

Asset utilization
The first value driver, asset utilization seeks to increase the technical availability of a technical equipment. With higher technical availability, it is possible to produce and sell more products with the same invested capital, generating more income while the fixed costs remain the same. In other words, the free cash flows increase, which automatically means value creation. Maintenance can increase technical availability by preventing unwanted breakdowns, scheduling plant maintenance in a smarter way and performing repairs and inspections faster. A point to note is that higher technical availability produces value not only in growth markets. In markets where demand is stable or declining, greater availability can also create value. By making a plant more efficient, the number of shifts can be reduced or it may even be possible to close down sister plants. At corporate level, this does not generate more turnover, but it does significantly reduce costs, which is another way of creating value.

Safety, health, and environment
An increasingly important value driver for maintenance is safety, health, and environment (SHE) in VDM terminology. Compliance with legal directives covering SHE creates value in two ways. Firstly, it avoids the imposition of government penalties for breaches of legislation. Secondly, a good SHE policy has a positive effect on retention of the License to Operate. This is something else that has value, because it increases the likelihood of future cash flows actually materializing. Without a License to Operate, there will be no future cash flows and thus no value. Problem here is that it does not take the other error type into account (related to a lower false safety trip rate, but higher accident rate). It is very controversial to put a value on human life and therefore this VDM theory is just like the ALARP logic dangerous to use and might not be accepted by all legislation world-wide (specially in the US).
The importance of the SHE value driver becomes apparent when looking at the recent incident with the BP Oil Spill in the Gulf of Mexico; the Deepwater Horizon oil spill. Poor maintenance is believed to be the cause of one of the biggest oil spills in history, causing massive damage to the environment. Total accumulated consequence costs (both clean-up costs and loss of company value) are estimated on 12 billion.

Cost control
Even though maintenance is not a cost center in itself, it does gobble up a lot of money. The maintenance budget consists mainly of wage and training costs of technicians, managers and indirect personnel, the costs of spare parts and tools and the costs of contracted personnel and outsourced work. Savings on the maintenance budget automatically generate free cash flows in the future and, by consequence, value. The savings are achievable by having a smarter Preventive maintenance program, higher technician productivity, lower procurement prices for materials and services and the right ratio of the number of technicians, managers and indirect personnel. Controlling maintenance costs is called "Cost Control" in VDM.

Resource allocation
Finally, maintenance can create value through the smarter management of resources. This is called "Resource Allocation" in VDM. It is not about the consumption of resources, because that is already covered in the Cost Control value driver. Within VDM, a distinction is made between four types of resources: technicians, spare parts, contractors and knowledge. One need to think only of cash flows freed up as a result of smarter management or savings on warehouses, logistical employees, insurance and the avoidance of obsolete and surplus spare parts. In practise, it often turns out that the value potential of smart management of spare parts by far exceeds the value potential of the other resources.
As figure 1 shows, a natural tension exists between the four value drivers. That is why today's maintenance manager is continually searching for the right balance between these value drivers. It is important to know how they square up to each other. One can determine this instinctively but many corporate managers require a financial validation because, for example, it will be a decisive factor in investment decisions or because stakeholders (e.g. financial manager, production manager) are not prepared to accept a judgement based on instinct. This is a situation where one should use the VDM formula.

Formula
The VDM formula is derived from the net present value formula and can be used to calculate the value of maintenance. The VDM formula is:
PVmaintenance = Σ {FSHE,t x (CFAU,t + CFCC,t + CFRA,t + CFSHE,t) / (1+r)t}
where:
PVmaintenance = present value potential of maintenance
FSHE,t = SHE factor in year t
CFAU,t = future free cash flow in year t from Asset Utilization
CFCC,t = future free cash flow in year t from Cost Control
CFRA,t = future free cash flow in year t from Resource Allocation
CFSHE,t = future free cash flow in year t from Safety, Health & Environment
r = Discount rate
SHE stands out prominently in the formula. This factor shows how great the probability is that the License to Operate will be retained in the coming years and that the expected cash flows from all four value drivers will actually be attainable in the future. Consequently, the SHE factor is a probability factor with a value of 0 to 1. A SHE factor of 0 means 0% probability of retention of the License to Operate, for example in a fictitious case where a company decides to stop performing all maintenance for cost reasons. The free cash flow that this creates on Cost Control will be enormous. But because the company fails to satisfy SHE laws and this loses its License to Operate, this cash flow will not create any value. The VDM model can be simplified in certain situations. Assume that we work with Perpetuity. This means there will be indefinitely be a free cash flow that is the same year after year and the SHE factor is constant; the VDM formula thus becomes:
PVmaintenance = FSHE,t x (CFAU,t + CFCC,t + CFRA,t + CFSHE,t) / r

See also
Condition Based Maintenance
Corrective maintenance
Planned maintenance
Reliability Centered Maintenance

References
External links
Article on Value Driven Maintenance

Value-stream mapping

Value-stream mapping,  also known as material- and information-flow mapping, is a lean-management method for analyzing the current state and designing a future state for the series of events that take a product or service from the beginning of the specific process until it reaches the customer. A value stream map is a visual tool that displays all critical steps in a specific process and easily quantifies the time and volume taken at each stage. Value stream maps show the flow of both materials and information as they progress through the process.Whereas a value stream map represents a core business process that adds value to a material product, a value chain diagram shows an overview of all activities within a company. Other business activities may be represented in "value stream diagrams" and/or other kinds of diagram that represent business processes that create and use business data.

Purpose
The purpose of value-stream mapping is to identify and remove or reduce "waste" in value streams, thereby increasing the efficiency of a given value stream. Waste removal is intended to increase productivity by creating leaner operations which in turn make waste and quality problems easier to identify.

Applications
Value-stream mapping has supporting methods that are often used in Lean environments to analyze and design flows at the system level (across multiple processes).
Although value-stream mapping is often associated with manufacturing, it is also used in logistics, supply chain, service related industries, healthcare, software development, product development, project management, and administrative and office processes.

Identifying waste
Types of waste
Daniel T. Jones (1995) identifies seven commonly accepted types of waste. These terms are updated from Toyota's operating model "The Toyota Way" (Toyota Production System, TPS) original nomenclature (muda):
Faster-than-necessary pace: creating too much of a good or service that damages production flow, quality, and productivity. Previously referred to as overproduction, and leads to storage and lead time waste.
Waiting: any time goods are not being transported or worked on.
Conveyance: the process by which goods are moved around. Previously referred to as transport, and includes double-handling and excessive movement.
Processing: an overly complex solution for a simple procedure. Previously referred to as inappropriate processing, and includes unsafe production. This typically leads to poor layout and communication, and unnecessary motion.
Excess stock: an overabundance of inventory which results in greater lead times, increased difficulty identifying problems, and significant storage costs. Previously referred to as unnecessary inventory.
Unnecessary motion: ergonomic waste that requires employees to use excess energy such as picking up objects, bending, or stretching. Previously referred to as unnecessary movements, and usually avoidable.
Correction of mistakes: any cost associated with defects or the resources required to correct them.

Waste removal operations
Yasuhiro Monden (1994) identifies three types of operations:
Non-value adding operations (NVA): actions that should be eliminated, such as waiting.
Necessary but non-value adding (NNVA): actions that are wasteful but necessary under current operating procedures.
Value-adding (VA): conversion or processing of raw materials via manual labor.NNVA activities may also be referred to as "sustaining non-value adding", i.e. they have to be done, or they are necessary to sustain the business but do not contribute to customer requirements.For additional views on waste, see Lean manufacturing.

Using the method
There are two kinds of value stream maps, current state and future state. The current state value stream map is used to determine what the process currently looks like, the future state value stream map focuses on what the process will ideally look like after process improvements have occurred to the value stream.The current state value stream map must be created before the future state map and is created by observing the process and tracking the information and material flow. The value stream map is then created using the following symbols:
In a build-to-the-standard form, Shigeo Shingo suggests that the value-adding steps be drawn across the centre of the map and the non–value-adding steps be represented in vertical lines at right angles to the value stream. Thus, the activities become easily separated into the value stream, which is the focus of one type of attention, and the "waste" steps, another type. He calls the value stream the process and the non-value streams the operations. The thinking here is that the non–value-adding steps are often preparatory or tidying up to the value-adding step and are closely associated with the person or machine/workstation that executes that value-adding step. Therefore, each vertical line is the "story" of a person or workstation whilst the horizontal line represents the "story" of the product being created.
Value-stream mapping is a recognised method used as part of Lean Six Sigma methodologies.Value-stream mapping analyzes both material (artifact) and information flow. The following two resources exemplify the use of VSM in the context of software process improvement in industrial settings:

"Artifact analysis": analysis of software artifacts like requirements, use case, change request or defect report through the development process
"Information flow analysis": analysis of information flows in the development process

Associated analysis methods
Hines and Rich (1997) defined seven value-stream mapping tools. These are:

Process activity mapping: the initial step of constructing a map which consists of a study of process flows, waste identification, and business process re-engineering.
Supply chain response matrix: identifying critical bottlenecks for processes in a simple diagram.
Production variety funnel: helps draw connections to other industries that may have solutions to existing problems.
Forrester effect mapping: line graphs showing the customer demand and production, allowing visualisation of supply and demand and potential delays.
Quality filter mapping: locates product and service defects in the supply chain.
Decision point analysis: determines inflection points for push-and-pull demand in the supply chain.
Physical structure mapping: combined model that overviews supply chain from an industry level.

See also
Business process mapping
Lean manufacturing
Value-stream-mapping software
Value chain
Value stream

Citations
References
Project Management Institute (2021). A guide to the project management body of knowledge (PMBOK guide). Project Management Institute (7th ed.). Newtown Square, PA. ISBN 978-1-62825-664-2.{{cite book}}:  CS1 maint: location missing publisher (link)

External links
 Media related to Value stream mapping at Wikimedia Commons
Value-stream mapping – Lexicon term at the Lean Enterprise Institute website

Value (economics)

In economics, economic value is a measure of the benefit provided by a good or service to an economic agent. It is generally measured through units of currency, and the interpretation is therefore "what is the maximum amount of money a person is willing and able to pay for a good or service?”
Among the competing schools of economic theory there are differing theories of value.
Economic value is not the same as market price, nor is economic value the same thing as market value. If a consumer is willing to buy a good, it implies that the customer places a higher value on the good than the market price. The difference between the value to the consumer and the market price is called "consumer surplus". It is easy to see situations where the actual value is considerably larger than the market price: purchase of drinking water is one example.

Overview
The economic value of a good or service has puzzled economists since the beginning of the discipline. First, economists tried to estimate the value of a good to an individual alone, and extend that definition to goods that can be exchanged. From this analysis came the concepts value in use and value in exchange.
Value is linked to price through the mechanism of exchange. When an economist observes an exchange, two important value functions are revealed: those of the buyer and seller. Just as the buyer reveals what he is willing to pay for a certain amount of a good, so too does the seller reveal what it costs him to give up the good.
Additional information about market value is obtained by the rate at which transactions occur, telling observers the extent to which the purchase of the good has value over time.
Said another way, value is how much a desired object or condition is worth relative to other objects or conditions. Economic values are expressed as "how much" of one desirable condition or commodity will, or would be given up in exchange for some other desired condition or commodity. Among the competing schools of economic theory there are differing metrics for value assessment and the metrics are the subject of a theory of value. Value theories are a large part of the differences and disagreements between the various schools of economic theory.

Explanations
In neoclassical economics, the value of an object or service is often seen as nothing but the price it would bring in an open and competitive market. This is determined primarily by the demand for the object relative to supply in a perfectly competitive market. Many neoclassical economic theories equate the value of a commodity with its price, whether the market is competitive or not. As such, everything is seen as a commodity and if there is no market to set a price then there is no economic value.
In classical economics, the value of an object or condition is the amount of discomfort/labor saved through the consumption or use of an object or condition (Labor Theory of Value). Though exchange value is recognized, economic value is not, in theory, dependent on the existence of a market and price and value are not seen as equal. This is complicated, however, by the efforts of classical economists to connect price and labor value. Karl Marx, for one, saw exchange value as the "form of appearance" (This interpretation of Marx is along the lines of the Marxist thinker Michael Heinrich) [Erscheinungsform] of value, in his critique of political economy which implies that, although value is separate from exchange value, it is meaningless without the act of exchange.
In this tradition, Steve Keen makes the claim that "value" refers to "the innate worth of a commodity, which determines the normal ('equilibrium') ratio at which two commodities exchange." To Keen and the tradition of David Ricardo, this corresponds to the classical concept of long-run cost-determined prices, what Adam Smith called "natural prices" and Marx called "prices of production". It is part of a cost-of-production theory of value and price. Ricardo, but not Keen, used a "labor theory of price" in which a commodity's "innate worth" was the amount of labor needed to produce it.
"The value of a thing in any given time and place", according to Henry George, "is the largest amount of exertion that anyone will render in exchange for it. But as men always seek to gratify their desires with the least exertion this is the lowest amount for which a similar thing can otherwise be obtained."In another classical tradition, Marx distinguished between the "value in use" (use-value, what a commodity provides to its buyer), labor cost which he calls "value" (the socially-necessary labour time it embodies), and "exchange value" (how much labor-time the sale of the commodity can claim, Smith's "labor commanded" value). By most interpretations of his labor theory of value, Marx, like Ricardo, developed a "labor theory of price" where the point of analyzing value was to allow the calculation of relative prices. Others see values as part of his sociopolitical interpretation and critique of capitalism and other societies, and deny that it was intended to serve as a category of economics. According to a third interpretation, Marx aimed for a theory of the dynamics of price formation but did not complete it.
In 1860, John Ruskin published a critique of the economic concept of value from a moral point of view. He entitled the volume Unto This Last, and his central point was this: "It is impossible to conclude, of any given mass of acquired wealth, merely by the fact of its existence, whether it signifies good or evil to the nation in the midst of which it exists. Its real value depends on the moral sign attached to it, just as strictly as that of a mathematical quantity depends on the algebraic sign attached to it. Any given accumulation of commercial wealth may be indicative, on the one hand, of faithful industries, progressive energies, and productive ingenuities: or, on the other, it may be indicative of mortal luxury, merciless tyranny, ruinous chicanery." Gandhi was greatly inspired by Ruskin's book and published a paraphrase of it in 1908.Economists such as Ludwig von Mises asserted that "value" is a subjective judgment. Prices can only be determined by taking these subjective judgments into account, and that this is done through the price mechanism in the market. Thus, it was false to say that the economic value of a good was equal to what it cost to produce or to its current replacement cost.
Silvio Gesell denied value theory in economics. He thought that value theory is useless and prevents economics from becoming science and that a currency administration guided by value theory is doomed to sterility and inactivity.

Connected concepts
The theory of value is closely related to that of allocative efficiency, the quality by which firms produce those goods and services most valued by society. The market value of a machine part, for example, will depend upon a variety of objective facts involving its efficiency versus the efficiency of other types of part or other types of machine to make the kind of products that consumers will value in turn. In such a case, market value has both objective and subjective components.
Economy, efficiency and effectiveness, often referred to as the "Three Es", may be used as complementary factors contributing to an assessment of the value for money provided by a purchase, project or activity. The UK National Audit Office uses the following summaries to explain the meaning of each term:

Economy: minimising the cost of resources used or required (inputs) – spending less;
Efficiency: the relationship between the output from goods or services and the resources to produce them – spending well; and
Effectiveness: the relationship between the intended and actual results of public spending (outcomes) – spending wisely.Sometimes a fourth 'E', equity, is also added.In philosophy, economic value is a subcategory of a more general philosophical value, as defined in goodness and value theory or in the science of value.

See also


== References ==

Voice of the customer

In marketing and quality management, the voice of the customer (VOC) summarizes customers' expectations, preferences and aversions.
A widely used form of customer's voice market research produces a detailed set of customer wants and needs, organized into a hierarchical structure, and then prioritized in terms of relative importance and satisfaction with current alternatives. VOC studies typically consist of both qualitative and quantitative research steps and are generally conducted at the start of any new product, process, or service design initiative in order to better understand the customer's wants and needs, and as the key input for new product definition, quality function deployment (QFD), and the setting of detailed design specifications.

Data gathering
Much has been written about this process, and there are many possible ways to gather the information – focus groups, individual interviews, contextual inquiry, ethnographic techniques, conjoint analysis, etc.  All involve a series of structured in-depth interviews, which focus on the customers' experiences with current products or alternatives within the category under consideration.  Needs statements are then extracted, organized into a more usable hierarchy, and then prioritized by the customers.

Involvement
It is critical that the product development core team are involved in this process.  They must be the ones who take the lead in defining the topic, designing the sample (i.e. the types of customers to include), generating the questions for the discussion guide, either conducting or observing and analyzing the interviews, and extracting and processing the needs statements.

Target groups
According to American Production and Inventory Control Society (APICS) the definition of voice of the customer is: "Actual customer descriptions in words for the functions and features customers desire for goods and services". In the strict definition, as relates to quality function deployment, the term customer indicates the external customer of the supplying entity.

See also
User research
Sentiment analysis
ITIL
Conjoint analysis

References
Further reading
Voice of the Customer: Capture and Analysis. McGraw Hill Professional.
Acquiring, Processing, and Deploying: Voice of the Customer. CRC Press.
Satisfaction: How Every Great Company Listens to the Voice of the Customer. Penguin.

Wayback Machine

The Wayback Machine is a digital archive of the World Wide Web founded by the Internet Archive, a nonprofit based in San Francisco, California. Created in 1996 and launched to the public in 2001, it allows the user to go "back in time" to see how websites looked in the past. Its founders, Brewster Kahle and Bruce Gilliat, developed the Wayback Machine to provide "universal access to all knowledge" by preserving archived copies of defunct web pages.Launched on May 10, 1996, the Wayback Machine had saved more than 38.2 billion web pages at the end of 2009. As of 20 August 2023, the Wayback Machine has archived 832 billion web pages.

History
The Wayback Machine began archiving cached web pages in 1996. One of the earliest known pages was archived on May 10, 1996, at 2:08 p.m. (UTC).Internet Archive founders Brewster Kahle and Bruce Gilliat launched the Wayback Machine in San Francisco, California, in October 2001, primarily to address the problem of web content vanishing whenever it gets changed or when a website is shut down. The service enables users to see archived versions of web pages across time, which the archive calls a "three-dimensional index". Kahle and Gilliat created the machine hoping to archive the entire Internet and provide "universal access to all knowledge". The name "Wayback Machine" is a reference to a fictional time-traveling and translation device, the "Wayback Machine", used by the characters Mister Peabody and Sherman in the animated cartoon The Adventures of Rocky and Bullwinkle and Friends. In one of the cartoon's segments, "Peabody's Improbable History", the characters used the machine to witness, participate in, and often alter famous events in history.
From 1996 to 2001, the information was kept on digital tape, with Kahle occasionally allowing researchers and scientists to tap into the "clunky" database. When the archive reached its fifth anniversary in 2001, it was unveiled and opened to the public in a ceremony at the University of California, Berkeley. By the time the Wayback Machine launched, it already contained over 10 billion archived pages. The data is stored on the Internet Archive's large cluster of Linux nodes. It revisits and archives new versions of websites on occasion (see technical details below). Sites can also be captured manually by entering a website's URL into the search box, provided that the website allows the Wayback Machine to "crawl" it and save the data.On October 30, 2020, the Wayback Machine began fact-checking content. As of January 2022, domains of ad servers are disabled from capturing.In May 2021, for Internet Archive's 25th anniversary, the Wayback Machine introduced the "Wayforward Machine" which allows users to "travel to the Internet in 2046, where knowledge is under siege".

Technical information
The Wayback Machine's software has been developed to "crawl" the Web and download all publicly accessible information and data files on webpages, the Gopher hierarchy, the Netnews (Usenet) bulletin board system, and downloadable software. The information collected by these "crawlers" does not include all the information available on the Internet, since much of the data is restricted by the publisher or stored in databases that are not accessible. To overcome inconsistencies in partially cached websites, Archive-It.org was developed in 2005 by the Internet Archive as a means of allowing institutions and content creators to voluntarily harvest and preserve collections of digital content, and create digital archives.Crawls are contributed from various sources, some imported from third parties and others generated internally by the Archive. For example, crawls are contributed by the Sloan Foundation and Alexa, crawls run by Internet Archive on behalf of NARA and the Internet Memory Foundation, mirrors of Common Crawl. The "Worldwide Web Crawls" have been running since 2010 and capture the global Web.Documents and resources are stored with time stamp URLs such as 20231113210920. Pages' individual resources such as images and style sheets and scripts, as well as outgoing hyperlinks, are linked to with the time stamp of the currently viewed page, so they are redirected automatically to their individual captures that are the closest in time.The frequency of snapshot captures varies per website. Websites in the "Worldwide Web Crawls" are included in a "crawl list", with the site archived once per crawl. A crawl can take months or even years to complete, depending on size. For example, "Wide Crawl Number 13" started on January 9, 2015, and completed on July 11, 2016. However, there may be multiple crawls ongoing at any one time, and a site might be included in more than one crawl list, so how often a site is crawled varies widely.Starting in October 2019, users are limited to 15 archival requests and retrievals per minute.

Storage capacity and growth
As technology has developed over the years, the storage capacity of the Wayback Machine has grown. In 2003, after only two years of public access, the Wayback Machine was growing at a rate of 12 terabytes per month. The data is stored on PetaBox rack systems custom designed by Internet Archive staff. The first 100TB rack became fully operational in June 2004, although it soon became clear that they would need much more storage than that.The Internet Archive migrated its customized storage architecture to Sun Open Storage in 2009, and hosts a new data centre in a Sun Modular Datacenter on Sun Microsystems' California campus. As of 2009, the Wayback Machine contained approximately three petabytes of data and was growing at a rate of 100 terabytes each month.A new, improved version of the Wayback Machine, with an updated interface and a fresher index of archived content, was made available for public testing in 2011, where captures appear in a calendar layout with circles whose width visualizes the number of crawls each day, but no marking of duplicates with asterisks or an advanced search page. A top toolbar was added to facilitate navigating between captures. A bar chart visualizes the frequency of captures per month over the years. Features like "Changes", "Summary", and a graphical site map were added subsequently.
In March that year, it was said on the Wayback Machine forum that "the Beta of the new Wayback Machine has a more complete and up-to-date index of all crawled materials into 2010, and will continue to be updated regularly. The index driving the classic Wayback Machine only has a little bit of material past 2008, and no further index updates are planned, as it will be phased out this year." Also in 2011, the Internet Archive installed their sixth pair of PetaBox racks which increased the Wayback Machine's storage capacity by 700 terabytes.In January 2013, the company announced a ground-breaking milestone of 240 billion URLs.In October 2013, the company introduced the "Save a Page" feature which allows any Internet user to archive the contents of a URL, and quickly generates a permanent link unlike the preceding liveweb feature.
In December 2014, the Wayback Machine contained 435 billion web pages—almost nine petabytes of data, and was growing at about 20 terabytes a week.In July 2016, the Wayback Machine reportedly contained around 15 petabytes of data.In September 2018, the Wayback Machine contained over 25 petabytes of data.As of December 2020, the Wayback Machine contained over 70 petabytes of data.

Wayback Machine APIs
The Wayback Machine service offers three public APIs, SavePageNow, Availability, and CDX. SavePageNow can be used to archive web pages. Availability API for checking the archive availability status for a web page, checking whether an archive for the web page exists or not. CDX API is for complex querying, filtering, and analysis of captured data.

Website exclusion policy
Historically, the Wayback Machine has respected the robots exclusion standard (robots.txt) in determining if a website would be crawled – or if already crawled, if its archives would be publicly viewable. Website owners had the option to opt-out of Wayback Machine through the use of robots.txt. It applied robots.txt rules retroactively; if a site blocked the Internet Archive, any previously archived pages from the domain were immediately rendered unavailable as well. In addition, the Internet Archive stated that "Sometimes, a website owner will contact us directly and ask us to stop crawling or archiving a site. We comply with these requests." In addition, the website says: "The Internet Archive is not interested in preserving or offering access to Web sites or other internet documents of persons who do not want their materials in the collection."On April 17, 2017, reports surfaced of sites that had gone defunct and became parked domains that were using robots.txt to exclude themselves from search engines, resulting in them being inadvertently excluded from the Wayback Machine. The Internet Archive changed the policy to now require an explicit exclusion request to remove it from the Wayback Machine.

Oakland Archive Policy
Wayback's retroactive exclusion policy is based in part upon Recommendations for Managing Removal Requests and Preserving Archival Integrity published by the School of Information Management and Systems at University of California, Berkeley in 2002, which gives a website owner the right to block access to the site's archives.  Wayback has complied with this policy to help avoid expensive litigation.The Wayback retroactive exclusion policy began to relax in 2017, when it stopped honoring robots on U.S. government and military web sites for both crawling and displaying web pages. As of April 2017, Wayback is ignoring robots.txt more broadly, not just for U.S. government websites.

Uses
From its public launch in 2001, the Wayback Machine has been studied by scholars both for the ways it stores and collects data as well as for the actual pages contained in its archive. As of 2013, scholars had written about 350 articles on the Wayback Machine, mostly from the information technology, library science, and social science fields. Social science scholars have used the Wayback Machine to analyze how the development of websites from the mid-1990s to the present has affected the company's growth.When the Wayback Machine archives a page, it usually includes most of the hyperlinks, keeping those links active when they just as easily could have been broken by the Internet's instability. Researchers in India studied the effectiveness of the Wayback Machine's ability to save hyperlinks in online scholarly publications and found that it saved slightly more than half of them."Journalists use the Wayback Machine to view dead websites, dated news reports, and changes to website contents. Its content has been used to hold politicians accountable and expose battlefield lies." In 2014, an archived social media page of Igor Girkin, a separatist rebel leader in Ukraine, showed him boasting about his troops having shot down a suspected Ukrainian military airplane before it became known that the plane actually was a civilian Malaysian Airlines jet (Malaysia Airlines Flight 17), after which he deleted the post and blamed Ukraine's military for downing the plane. In 2017, the March for Science originated from a discussion on Reddit that indicated someone had visited Archive.org and discovered that all references to climate change had been deleted from the White House website. In response, a user commented, "There needs to be a Scientists' March on Washington".Furthermore, the site is used heavily for verification, providing access to references and content creation by Wikipedia editors.In September 2020, a partnership was announced with Cloudflare to automatically archive websites served via its "Always Online" service, which will also allow it to direct users to its copy of the site if it cannot reach the original host.

Limitations
In 2014 there was a six-month lag time between when a website was crawled and when it became available for viewing in the Wayback Machine. Currently, the lag time is 3 to 10 hours. The Wayback Machine offers only limited search facilities. Its "Site Search" feature allows users to find a site based on words describing the site, rather than words found on the web pages themselves.The Wayback Machine does not include every web page ever made due to the limitations of its web crawler. The Wayback Machine cannot completely archive web pages that contain interactive features such as Flash platforms and forms written in JavaScript and progressive web applications, because those functions require interaction with the host website. This means that, since approximately July 9, 2013, the Wayback Machine has been unable to display YouTube comments when saving videos' watch pages, as, according to the Archive Team, comments are no longer "loaded within the page itself." The Wayback Machine's web crawler has difficulty extracting anything not coded in HTML or one of its variants, which can often result in broken hyperlinks and missing images. Due to this, the web crawler cannot archive "orphan pages" that are not linked to by other pages. The Wayback Machine's crawler only follows a predetermined number of hyperlinks based on a preset depth limit, so it cannot archive every hyperlink on every page.

In legal evidence
Civil litigation
Netbula LLC v. Chordiant Software Inc.
In a 2009 case, Netbula, LLC v. Chordiant Software Inc., defendant Chordiant filed a motion to compel Netbula to disable the robots.txt file on its website that was causing the Wayback Machine to retroactively remove access to previous versions of pages it had archived from Netbula's site, pages that Chordiant believed would support its case.Netbula objected to the motion on the ground that defendants were asking to alter Netbula's website and that they should have subpoenaed Internet Archive for the pages directly. An employee of Internet Archive filed a sworn statement supporting Chordiant's motion, however, stating that it could not produce the web pages by any other means "without considerable burden, expense and disruption to its operations."Magistrate Judge Howard Lloyd in the Northern District of California, San Jose Division, rejected Netbula's arguments and ordered them to disable the robots.txt blockage temporarily in order to allow Chordiant to retrieve the archived pages that they sought.

Telewizja Polska USA, Inc. v. Echostar Satellite
In an October 2004 case, Telewizja Polska USA, Inc. v. Echostar Satellite, No. 02 C 3293, 65 Fed. R. Evid. Serv. 673 (N.D. Ill. October 15, 2004), a litigant attempted to use the Wayback Machine archives as a source of admissible evidence, perhaps for the first time. Telewizja Polska is the provider of TVP Polonia and EchoStar operates the Dish Network. Prior to the trial proceedings, EchoStar indicated that it intended to offer Wayback Machine snapshots as proof of the past content of Telewizja Polska's website. Telewizja Polska brought a motion in limine to suppress the snapshots on the grounds of hearsay and unauthenticated source, but Magistrate Judge Arlander Keys rejected Telewizja Polska's assertion of hearsay and denied TVP's motion in limine to exclude the evidence at trial. At the trial, however, District Court Judge Ronald Guzman, the trial judge, overruled Magistrate Keys' findings, and held that neither the affidavit of the Internet Archive employee nor the underlying pages (i.e., the Telewizja Polska website) were admissible as evidence. Judge Guzman reasoned that the employee's affidavit contained both hearsay and inconclusive supporting statements, and the purported web page, printouts were not self-authenticating.

Patent law
The United States patent office and the European Patent Office will accept date stamps from the Internet Archive as evidence of when a given Web page was accessible to the public. These dates are used to determine if a Web page is available as prior art for instance in examining a patent application.

Limitations of utility
There are technical limitations to archiving a website, and as a consequence, opposing parties in litigation can misuse the results provided by website archives. This problem can be exacerbated by the practice of submitting screenshots of web pages in complaints, answers, or expert witness reports when the underlying links are not exposed and therefore, can contain errors. For example, archives such as the Wayback Machine do not fill out forms and therefore, do not include the contents of non-RESTful e-commerce databases in their archives.

Legal status
In Europe, the Wayback Machine could be interpreted as violating copyright laws. Only the content creator can decide where their content is published or duplicated, so the Archive would have to delete pages from its system upon request of the creator. The exclusion policies for the Wayback Machine may be found in the FAQ section of the site.Some cases have been brought against the Internet Archive specifically for its Wayback Machine archiving efforts.

Archived content legal issues
Scientology
In late 2002, the Internet Archive removed various sites that were critical of Scientology from the Wayback Machine. An error message stated that this was in response to a "request by the site owner". Later, it was clarified that lawyers from the Church of Scientology had demanded the removal and that the site owners did not want their material removed.

Healthcare Advocates, Inc.
In 2003, Harding Earley Follmer & Frailey defended a client from a trademark dispute using the Archive's Wayback Machine. The attorneys were able to demonstrate that the claims made by the plaintiff were invalid, based on the content of their website from several years prior. The plaintiff, Healthcare Advocates, then amended their complaint to include the Internet Archive, accusing the organization of copyright infringement as well as violations of the DMCA and the Computer Fraud and Abuse Act. Healthcare Advocates claimed that, since they had installed a robots.txt file on their website, even if after the initial lawsuit was filed, the Archive should have removed all previous copies of the plaintiff website from the Wayback Machine, however, some material continued to be publicly visible on Wayback. The lawsuit was settled out of court after Wayback fixed the problem.

Suzanne Shell
Activist Suzanne Shell filed suit in December 2005, demanding Internet Archive pay her US$100,000 for archiving her website profane-justice.org between 1999 and 2004. Internet Archive filed a declaratory judgment action in the United States District Court for the Northern District of California on January 20, 2006, seeking a judicial determination that Internet Archive did not violate Shell's copyright. Shell responded and brought a countersuit against Internet Archive for archiving her site, which she alleges is in violation of her terms of service. On February 13, 2007, a judge for the United States District Court for the District of Colorado dismissed all counterclaims except breach of contract. The Internet Archive did not move to dismiss copyright infringement claims Shell asserted arising out of its copying activities, which would also go forward.On April 25, 2007, Internet Archive and Suzanne Shell jointly announced the settlement of their lawsuit. The Internet Archive said it "...has no interest in including materials in the Wayback Machine of persons who do not wish to have their Web content archived. We recognize that Ms. Shell has a valid and enforceable copyright in her Web site and we regret that the inclusion of her Web site in the Wayback Machine resulted in this litigation." Shell said, "I respect the historical value of Internet Archive's goal. I never intended to interfere with that goal nor cause it any harm."

Daniel Davydiuk
Between 2013 and 2016, a pornographic actor named Daniel Davydiuk tried to remove archived images of himself from the Wayback Machine's archive, first by sending multiple DMCA requests to the archive, and then by appealing to the Federal Court of Canada. The images were then finally removed from the website in 2017.

FlexiSpy
In 2018, archives of stalkerware application FlexiSpy's website were removed from the Wayback Machine. The company claimed to have contacted the Internet Archive, presumably to remove the archives of its website.

Censorship and other threats
Archive.org is blocked in China. After the Islamic State terrorist organization was banned, the Internet Archive had been blocked in its entirety in Russia as a host of an outreach video from that organization, for a short time in 2015–16. Since 2016, the website has been back, available in its entirety, although local commercial lobbyists are suing the Internet Archive in a local court to ban it on copyright grounds.In March 2015, it was published that security researchers became aware of the threat posed by the service's unintentional hosting of malicious binaries from archived sites.Alison Macrina, director of the Library Freedom Project, notes that "while librarians deeply value individual privacy, we also strongly oppose censorship".There is at least one case in which an article was removed from the archive shortly after it had been removed from its original website. A Daily Beast reporter had written an article that outed several gay Olympian athletes in 2016 after he had made a fake profile posing as a gay man on a dating app. The Daily Beast removed the article after it was met with widespread furor; not long after, the Internet Archive soon did as well, but emphatically stated that they did so for no other reason than to protect the safety of the outed athletes.Other threats include natural disasters, destruction (remote or physical), manipulation of the archive's contents (see also: cyberattack, backup), problematic copyright laws and surveillance of the site's users.Alexander Rose, executive director of the Long Now Foundation, suspects that in the long term of multiple generations "next to nothing" will survive in a useful way, stating, "If we have continuity in our technological civilization, I suspect a lot of the bare data will remain findable and searchable. But I suspect almost nothing of the format in which it was delivered will be recognizable" because sites "with deep back-ends of content-management systems like Drupal and Ruby and Django" are harder to archive.In an article reflecting on the preservation of human knowledge, The Atlantic has commented that the Internet Archive, which describes itself to be built for the long-term, "is working furiously to capture data before it disappears without any long-term infrastructure to speak of."

See also
References
External links

Official website 
Internet history is fragile. This archive is making sure it doesn't disappear. San Francisco: PBS Newshour. Archived from the original on January 6, 2022. Retrieved September 19, 2018.

Westinghouse Electric Corporation

The Westinghouse Electric Corporation was an American manufacturing company founded in 1886 by George Westinghouse. It was originally named "Westinghouse Electric & Manufacturing Company" and was renamed "Westinghouse Electric Corporation" in 1945. The company acquired the CBS television network in 1995 and was renamed "CBS Corporation" until being acquired by Viacom in 1999, a merger completed in April 2000. The CBS Corporation name was later reused for one of the two companies resulting from the split of Viacom in 2005.
The Westinghouse trademarks are owned by Westinghouse Electric Corporation, and were previously part of Westinghouse Licensing Corporation. The nuclear power business, Westinghouse Electric Company, was spun off from the Westinghouse Electric Corporation in 1999.

History
Beginnings
Westinghouse Electric was founded by George Westinghouse in Pittsburgh, Pennsylvania, on January 8, 1886. The firm became active in developing electric infrastructure throughout the United States. The company's largest factories were located in East Pittsburgh, Pennsylvania, Lester, Pennsylvania and Hamilton, Ontario, where they made turbines, generators, motors, and switch gear for the generation, transmission, and use of electricity. In addition to George Westinghouse, early engineers working for the company included Frank Conrad, Benjamin Garver Lamme, Bertha Lamme (first woman mechanical engineer in the United States), Oliver B. Shallenberger, William Stanley, Nikola Tesla, Stephen Timoshenko, and Vladimir Zworykin.
Early on, Westinghouse was a rival to Thomas Edison's electric company. In 1892, Edison was merged with Westinghouse's chief AC rival, the Thomson-Houston Electric Company, making an even bigger competitor, General Electric. Westinghouse Electric & Manufacturing Company changed its name to Westinghouse Electric Corporation in 1945.

Catastrophe
In 1990, Westinghouse experienced a financial catastrophe when the corporation lost over one billion dollars due to bad high-risk, high-fee, high-interest loans made by its Westinghouse Credit Corporation lending arm.In an attempt to revitalize the corporation, the board of directors appointed outside management in the form of CEO Michael H. Jordan, who brought in numerous consultants to help re-engineer the company in order to realize the potential that they saw in the broadcasting industry. Westinghouse reduced the workforce in many of its traditional industrial operations and made further acquisitions in broadcasting to add to its already substantial Group W network, including Infinity Broadcasting, TNN, CMT, American Radio Systems, and rights to NFL broadcasting. These investments cost the company over fifteen billion dollars. To recoup its costs, Westinghouse sold many other operations, including its defense electronics division, its metering and load control division (which was sold to ABB), its residential security division, the office furniture company Knoll, and Thermo King.Westinghouse purchased CBS Inc. in 1995 for $5.4 billion. Westinghouse Electric Corporation changed its name to and became the original "CBS Corporation" in 1997. Also in 1997, the Power Generation Business Unit, headquartered in Orlando, Florida, was sold to Siemens AG of Germany. A year later, CBS sold all of its commercial nuclear power businesses to British Nuclear Fuels Limited (BNFL). In connection with that sale, certain rights to use the Westinghouse trademarks were granted to the newly formed BNFL subsidiary, Westinghouse Electric Company. That company was sold to Toshiba in 2006.

Patents
During the 20th century, Westinghouse engineers and scientists were granted more than 28,000 U.S. patents, the third most of any company.

Products and sponsorships
Power generation: The company pioneered the power generation industry and in the fields of long-distance power transmission and high-voltage alternating-current transmission, unveiling the technology for lighting in Great Barrington, Massachusetts.
Steam turbine generator: The first commercial Westinghouse steam turbine-driven generator, a 1,500 kW unit, began operation at Hartford Electric Light Co. in 1901. The machine, nicknamed Mary-Ann, was the first steam turbine generator to be installed by an electric utility to generate electricity in the US. George Westinghouse had based his original steam turbine design on designs licensed from the English inventor Charles Parsons. Today a large proportion of steam turbine generators operating around the world, ranging to units as large as 1,500 MW (or 1,000 times the original 1901 unit) were supplied by Westinghouse from its factories in Lester, Pennsylvania, Charlotte, North Carolina, or Hamilton, Ont. or were built overseas under Westinghouse license. Major Westinghouse licensees or joint venture partners included Mitsubishi Heavy Industries of Japan and Harbin Turbine Co. and Shanghai Electric Co. of China.
Research: Westinghouse had 50,000 employees by 1900 and established a formal research and development department in 1906. While the company was expanding, it would experience internal financial difficulties. During the Panic of 1907, the Board of Directors forced George Westinghouse to take a six-month leave of absence. Westinghouse officially retired in 1909 and died several years later in 1914.
Electrical technology: Under new leadership, Westinghouse Electric diversified its business activities in electrical technology. It acquired the Copeman Electric Stove Company in 1914 and Pittsburgh High Voltage Insulator Company in 1921. Westinghouse also moved into radio broadcasting by establishing Pittsburgh's KDKA, the first commercial radio station, and WBZ in Springfield, Massachusetts in 1921. Westinghouse expanded into the elevator business, establishing the Westinghouse Elevator Company in 1928; it sold its elevator business to Schindler Group (forming the Schindler Elevator Corporation) in 1989. Throughout the decade, diversification engendered considerable growth; sales went from $43 million in 1914 to $216 million in 1929.
Aviation: Westinghouse produced the first operational American turbojet for the US Navy program in 1943.  After many successes, the ill-fated J40 project, started soon after World War Two, was abandoned in 1955 and led to Westinghouse exiting the aircraft engine business with the closure of the Westinghouse Aviation Gas Turbine Division (Kansas City) in 1960.
Gas turbines: During the late 1940s, Westinghouse applied its aviation gas turbine technology and experience to develop its first industrial gas turbine. A 2,000–horsepower model W21 was installed in 1948 at the Mississippi River Fuel Corp gas compression station in Wilmar, Arkansas. This was the starting point for the company to enter in industrial and utility gas turbine business, prior to the sale by Westinghouse of the power generation business to Siemens AG in 1997. Evolving from the Small Steam and Gas Turbine Division formed in the early 1950s, the Westinghouse Combustion Turbine Systems Division was located in Concordville, Pennsylvania, near Philadelphia and the old Lester, Pennsylvania plant, until it was relocated to Power Generation headquarters in Orlando, Florida in 1987.
Nuclear power: As a result of its participation in the US government's military program for nuclear energy applications (e.g., The Nuclear Navy) Westinghouse utilized that experience in the development and commercialization of nuclear energy systems for electric power generation.  This business currently operates as the Westinghouse Electric Company and is owned by Brookfield Business Partners of Canada. Electricite de France (EDF) a major global player in the nuclear power business, was a long-time licensee of the Westinghouse nuclear technology.
Industrial motors: Additional major industrial products in the widespread Westinghouse portfolio included electric motors of all sizes, elevators and escalators, controls, and lighting. The Large Motor Division, once headquartered in Buffalo, NY, entered a joint venture with Taiwan Electric Co. (TECO) in the 1970s and today operates as TECO-Westinghouse.  Much of Westinghouse's higher voltage power equipment was sold to ABB in 1989 and renamed the ABB Power T&D Company.
Rail transit: The Westinghouse Transportation Division (est. 1894) supplied equipment and controls for many North American interurban and streetcar lines, the San Francisco Bay Area Rapid Transit, Washington Metro, New York City Subway equipment from the 1890s elevated era to the R68A in 1988, among many other heavy rail and rail transit systems and built locomotives, often in partnership with Baldwin, Lima-Hamilton as well as supplying electrical and traction equipment for Fairbanks-Morse diesel locomotives.  The division designed and built Automated People Movers (APMs) at several major U.S. airports, including Sea-Tac. Tampa, Dallas-Ft. Worth and Orlando.  The Transportation Division was sold to AEG of Germany (1988), which merged into a joint venture of ABB and Daimler-Benz named AdTranz in 1996. Ultimately, the unit was acquired by Bombardier of Canada in 2001 and is still headquartered in Pittsburgh.
Consumer electrics: Westinghouse was also among the initial manufacturers to make household electrical products including radios, televisions, and other audio/video equipment. This also included both small and large electric appliances of all kinds, from hair dryers and electric irons to clothes washers and dryers, refrigerators and air conditioning units. After more than 50 years, and after playing a strong No. 2 to rival General Electric for most of that time, Westinghouse decided to exit the appliance business in the mid-1970s. White-Westinghouse was formed when White Consolidated Industries acquired the Westinghouse appliance unit in 1975.
World's Fair time capsules: The company is also known for its time capsule contributions during the 1939 New York World's Fair and 1964 New York World's Fair. They also participated in the St. Louis World's Fair in 1904. They sponsored the Westinghouse Auditorium at the fair, where they showed films documenting Westinghouse products and company plants.  Westinghouse was one of the original corporate sponsors and exhibitors at Walt Disney World's EPCOT attraction in Orlando, Florida.

Environmental incidents
There have been a number of Westinghouse-related environmental incidents in the US. Below is a short list of these. All of these are chemical pollution incidents; none of them involve nuclear reactors or nuclear pollution.

Sharon plant: The Westinghouse Sharon Plant was a 58-acre Westinghouse transformer production facility in Mercer County, Pennsylvania. The EPA's recent Five Year Review Report (2016) of this Superfund site determined that the Shenango River has been polluted due to Westinghouse operations in this area. Because of the findings, the state of Pennsylvania has issued a "Do Not Eat" advisory for fish around the Westinghouse site. This plant was no longer operational after 1984. Westinghouse submitted their final cleanup plan in 1998, and further action beyond their dissolution has been liable to CBS. The transformer business unit was sold to ABB in 1989. This site now houses a product design company.
Adams County plant: Westinghouse was fined $5.5 million in 1996 for polluting groundwater in over 100 wells, as well as other water sources, while operating its Westinghouse Elevator Company plant in Adams County, Pennsylvania.  Degreasers and other toxic chemicals were released over a 5-year period in the 1980s. This business unit was sold to Schindler in 1988. Future liability for cleanup has been directed to CBS following the dissolution of Westinghouse Electric Corporation in 1999.
Horseheads site: Westinghouse operated a cathode-ray tube plant in Horseheads, New York. They were deemed responsible for pollution at the Kentucky Avenue Wellfield Superfund site in Horseheads, New York.  Westinghouse polluted nearby soil, affecting the safety of a nearby aquifer and wells used by residents.  One phase of the cleanup effort describes Westinghouse Electric Corporation's facility, designated "Disposal Area F" and the "Former Runoff Basin Area," which are contaminated with volatile organic compounds (VOCs), polycyclic aromatic hydrocarbons (PAHs) and arsenic, will be cleaned up using a combination of soil excavation and soil vapor extraction. At Disposal Area F, the area of contamination is about 0.3 acres. At the Former Runoff Basin Area, the contaminated soils cover approximately 0.7 acres. Disposal of the excavated soils occurred at appropriate off-site facilities. The removal of the PAHs and arsenic contamination will protect site workers and employees at the Westinghouse facility and the cleanup of the VOCs will help restore the quality of the Newtown Creek Aquifer. In 1986, Westinghouse entered a joint venture at this plant with Toshiba to produce CRTs. In 1989, Toshiba became part owner of this plant and the Westinghouse CRT business unit. Future liability has been shifted to CBS.
Sunnyvale plant: Westinghouse operated a plant which manufactured electronics for military systems in Sunnyvale, California. Groundwater and soil near this plant are contaminated with PCBs, fuels, and volatile organic compounds (VOCs). Potential health threats to area residents include accidentally ingesting or coming into direct contact with site contaminants in soil or groundwater. There are municipal drinking water wells within 1⁄4-mile from this site, and 300,000 people get their drinking water from within three miles of the site. This business unit was sold to Northrop Grumman in 1996. Future liability for this action has been passed on to CBS.

Timeline of company evolution
1880s
1884 – George Westinghouse begins developing a DC electric lighting system
1885 – Westinghouse becomes aware of the new European transformer based alternating current systems when he reads about them in the UK technical journal Engineering
1885 – William Stanley, Jr., working for Westinghouse, develops the first practical AC transformer
1886 – Westinghouse Electric Company founded in East Pittsburgh
1886 – William Stanley, Jr. installs the world's first operational transformer based multiple voltage transmission system, a demonstration lighting system in Great Barrington, Massachusetts
1888 – development of an induction ampere-hour meter for alternating current developed by Oliver B. Shallenberger
1888 – licensing of Nikola Tesla's AC and Induction motor patents (Tesla was hired for one year as a consultant, but he quit after a few months)
1889 – renames itself the Westinghouse Electric & Manufacturing Company

1890s
1891 – built world's first industrial AC system (Ames Hydroelectric Generating Plant)
1893 – supplied electric lights and power for World's Columbian Exposition and generators for Gettysburg Electric Railway
1893 – hired Bertha Lamme Feicht, the company's first female engineer
1894 – transportation Division (rail equipment) founded
1895 – installed hydropower AC generators at Adams Power Plant, Niagara Falls which supplied power to Buffalo, New York, completed 1896
1898 – purchases Walker Mfg. Co of Cleveland, establishing main facility and plant in Cleveland which produces power-transmitting machinery, cable railway networks, castings and lighting
1899 – founded British Westinghouse Electric and Manufacturing Company

1900s to 1920s
1901 – acquires Bryant Electric Company of Bridgeport, Connecticut, which continues operation as a subsidiary
1901 – operation of first Westinghouse steam turbine generator installed at Hartford Electric Light Co.
1904 – with Baldwin, markets Baldwin-Westinghouse electric locomotives and AC electrification of railroads, particularly to the New Haven Railroad
1909 – introduces continuous-filament tungsten light bulb; ousts George Westinghouse as chairman during bankruptcy reorganization
1914 – acquires Copeman Electric Stove Company in Flint, Michigan from Lloyd Groff Copeman, moves it to Mansfield, Ohio and enters the home appliance market (sold in 1974 to White Consolidated Industries)
1914 – George Westinghouse dies, with a legacy including 361 patents and the founding of 60 companies
1915 – New England Westinghouse Company opens for business, first product is Mosin–Nagant rifles for the Russian Czar's army; within two years, the Bolsheviks cancel a previous order of over 1 million rifles after overthrowing the Russian Provisional Government, and facing bankruptcy, Westinghouse is rescued by the American Government when it purchases the rifles for use by the military
1917 – share of British Westinghouse purchased by a British holding company, which becomes Metropolitan-Vickers
1917 – builds steam turbine manufacturing plant in Lester, PA (Tinicum Township) near the Philadelphia airport
1919 – 8XE Pittsburgh experimental station goes on the air
1919 – creates RCA with GE, AT&T and United Fruit, buys the American division of Marconi
1920 – acquires International Radio Telegraph Company (formerly known as the National Electric Signaling Company)
1921 – acquires the Pittsburg High Voltage Insulator Company
1920s – enters the broadcasting industry, with stations like KDKA in Pittsburgh, Pennsylvania and WBZ in Springfield, Massachusetts, later moving to Boston.
1926 – in partnership with GE and RCA founds NBC Broadcasting

1930s and 1940s
1932 – announces Ignitron mercury-arc rectifier
1934 – opens its Home of Tomorrow in Mansfield, Ohio, to demonstrate Westinghouse home appliances
1935 – completes longest continuous electric steel annealing furnace in the world at Ford Motor Company, Dearborn, Michigan
1930s – funds invention of the magnetohydrodynamic generator
1937 – builds first "industrial atom smasher", a 5 MeV Van de Graaff electrostatic nuclear accelerator
1940s – enters aviation with airborne radar (defense electronics sold 1996), jet engine propulsion, and ground-based airport lighting, gets defense contract from U.S. military to produce plastic helmet liners for the M1 Helmet
1941 – after years of resistance to the unionization efforts of its employees and to the National Labor Relations Act, signs a national labor agreement with the United Electrical, Radio and Machine Workers of America after a United States Supreme Court decision that upheld the Act
1943 – purchased the lamp division of Kentucky-Radio Corporation (Ken-Rad) in Owensboro, Kentucky from Roy Burlew in exchange for 35,000 shares of Westinghouse stock valued at $1.6 million ($27.1 million todaywhen?)
1945 – renames itself the Westinghouse Electric Corporation and makes first automatic elevator
1945 – Westinghouse Aviation Gas Turbine Division (AGT) started
1948 – "You Can Be Sure... If It's Westinghouse" in Time magazine ad

1950s to 1970s
1951 – conducts first live network TV in U.S.
1952 – opens Cathode Ray Tube facility in Horseheads, New York; facility housed three divisions: Cathode Ray Tube, Electronic Tube, and Industrial and Government Tube
1954 – enters finance as Westinghouse Credit Corporation
1955 – buys KDKA-TV (then WDTV) in Pittsburgh and KYW-TV (then WNBK, now WKYC) and KYW Radio (originally, and currently WTAM) along with KYW-FM (then WTAM-FM, currently WMJI) in Cleveland;  KYW is now licensed to a TV and AM radio station in Philadelphia
1955 – Westinghouse J40 engine failure causes all F3H fighters using the engine to be grounded, and all other jets using it switch to other engines; Westinghouse is forced out of aircraft engine business
1957 – introduces first successful "cobra head" roadway luminaire, the OV-25, integrating both ballast and optics in a more streamlined modern design
1961 – acquires Thermo King (sold in 1997 to Ingersoll Rand)
1964 – begins Skybus project; beginning of automated mass transit
1965 – invention of the first MEMS device, buys Marketeer Electronic Vehicles
1966 – founds Cinema Center Films
1966 – starts housing and real estate development divisions
1966 – buys a toy manufacturer
1967 – lights America's first computer-controlled outdoor electric sign
1967 – makes the lowest bid for the BART project
1969 – buys 7 Up bottling
1973 – develops world's first AMLCD displays
1974 – sells well-known home appliance division to White Consolidated Industries, which becomes White-Westinghouse
1979 – withdraws from all oil related projects in the Middle East after Iranian Revolution

1980s
1981 – acquires both cable television operator TelePrompter (sold 1985), Muzak (sold September 1986), and 50% of Showtime for $576 million
1982 – acquires robot maker Unimation
1982 – sells street light division to Cooper Lighting
1983 – sells electric lamp division to Philips
1984 – buys Unimation robotics for $105 million
1986 – buys Los Angeles TV station
1986 – enters into joint venture with Airship Industries, Ltd. (London) to develop advanced lighter-than-air radar platforms and early warning surveillance airship for U.S. Navy in cooperation with its subsidiary TCOM Corp. located on the former Naval Air Station Weeksville in Elizabeth City, North Carolina
1987 – buys radio stations in Sacramento and Chicago
1987 – buys electrical equipment, engineering and waste disposal divisions
1988 – sells elevator/escalator division to Schindler Group, now known as Schindler Elevator Corporation
1988 – enters into joint venture with Taiwan Electric to build electric motors; Taiwan Electric eventually becomes sole owner of business as TECO Motor Company
1988 – spins Industrial and Government Tube Division off as Imaging and Sensing Technologies Corporation
1988 – closes the East Pittsburgh generator and Lester, PA turbine plants, which had once been the primary Westinghouse manufacturing facilities
1988 – Bryant Electric subsidiary closed, assets sold to Hubbell in 1991
1988 – Transportation Division, including railroad (locomotive and mass transit) equipment business sold to AEG, later merged into Adtranz in 1996, Bombardier Transportation in 2001, and Alstom in 2021
1989 – sells transmission and distribution business to ASEA Brown Boveri Group (ABB)
1989 – buys Shaw-Walker Furniture and Reff Furniture
1989 – buys Legacy Broadcasting

1990s to 2020s
1990 – buys Knoll International Furniture
1994 – buys United Technologies' Norden electronic systems
1994 – Cleveland operations and facilities purchased by Eaton Corporation for $1.6 billion; Cleveland Westinghouse facilities, as well as manufacturing plants converted into other commercial enterprises
1994–95 – separates IT and phone service sales into Westinghouse Communications division
1995 – under the leadership of Michael H. Jordan buys CBS for $5.4 billion ($10.4 billion today)
1996 – buys Infinity Broadcasting for $4.7 billion
1996 – sells Westinghouse Electronic Systems defense business to Northrop Grumman for $3 billion ($5.6 billion today), becoming Northrop Grumman Electronic Systems
1997 – sells Thermo King division to Ingersoll Rand
1997 – buys American Radio Systems for $2.6 billion, increasing station network to 175
1997 – sells most non-broadcast operations; renames itself CBS Corporation as of December 1
1997 – sells its non-nuclear power generation and energy units to Siemens AG, which operated under the name Siemens Westinghouse until 2003
1998 – CBS Corporation creates Westinghouse Licensing Corporation subsidiary to manage the Westinghouse brand
1999 – sells remaining manufacturing asset, its nuclear energy business, to BNFL
1999 – buys Outdoor Systems for $8.7 billion and King World Productions for $2.5 billion
2000 – CBS acquired by Viacom
2005 – Viacom is split into two companies in January, with a new Viacom being spun off of the old Viacom company, and the old Viacom being renamed as CBS Corporation, thus reviving Westinghouse's last name prior to sale, and reversing the 2000 Viacom-CBS merger
2019 – Viacom and CBS Corporation remerge to form Paramount Global (known as ViacomCBS until 2022)
2021 – Westinghouse Electric Corporation acquires the Westinghouse trademarks from ViacomCBS

Employees
CEOs
George Westinghouse, 1886–1909
Edwin Herr, 1911–1929
Frank Anderson Merrick, 1929 – February 1938
George Bucher, February 1938–1946
Gwilym Price, 1946–1957
Mark Cresap, Jr. 1957–1963
Don Burnham, 1963–1975
Robert Kirby, 1975–1983
Douglas Danforth, December 1983 – December 1987
John Marous, 1988 – June 29, 1990
Paul Lego, June 30, 1990 – January 1993
Gary Clark, January–July 1993
Michael Jordan, July 1993 – 1997

Other
Guy Tripp, a former Thomson-Houston employee who joined Westinghouse and became chairman of its board of directors in 1912, and served until his death in 1927.

Overseas subsidiaries
Westinghouse established subsidiary companies in several countries including British Westinghouse and Società Italiana Westinghouse in Vado Ligure, Italy. British Westinghouse became a subsidiary of Metropolitan-Vickers in 1919 and the Italian Westinghouse factory was taken over by Tecnomasio in 1921.

See also
List of Westinghouse locomotives
Siemens Westinghouse, also known as Siemens Power Generation, Inc.
War of the currents
Westinghouse Electric Company
Westinghouse Works, 1904
Westinghouse Broadcasting, also known as Group W
Westinghouse Lamp Plant
Westinghouse Combustion Turbine Systems Division
Westinghouse Aviation Gas Turbine Division
White-Westinghouse
Paramount Global
Westinghouse Licensing Corporation
Schindler Elevator Corporation

References
External links

Timeline of Westinghouse historical events
"Who Killed Westinghouse?" – March 1998 Pittsburgh Post-Gazette series detailing Westinghouse's history and break-up
The Westinghouse Legacy Pittsburgh Technology Council
"What Happened to Westinghouse?". Pittsburgh Technology Council. March 1999. Archived from the original on May 11, 2013. Retrieved October 3, 2012.
"The Westinghouse Electric Company". Antique Light Sockets. Retrieved July 10, 2010.
Assembling a Generator, Westinghouse Works, 1904
Westinghouse Electric Corporation Steam Division photograph collection (1898–1964) at Hagley Museum and Library
A Fact History of Westinghouse (for the Golden Jubilee)
Westinghouse Power Generation Business Unit, A booklet prepared in 1993 as a statement of commitment of the Power Generation Business Unit (PGBU) to the future of Westinghouse's leading position in the industry Archived April 5, 2016, at the Wayback Machine

Working capital

Working capital (WC) is a financial metric which represents operating liquidity available to a business, organisation, or other entity, including governmental entities. Along with fixed assets such as plant and equipment, working capital is considered a part of operating capital. Gross working capital is equal to current assets. Working capital is calculated as current assets minus current liabilities. If current assets are less than current liabilities, an entity has a working capital deficiency, also called a working capital deficit and negative working capital.A company can be endowed with assets and profitability but may fall short of liquidity if its assets cannot be readily converted into cash.  Positive working capital is required to ensure that a firm is able to continue its operations and that it has sufficient funds to satisfy both maturing short-term debt and upcoming operational expenses. The management of working capital involves managing inventories, accounts receivable and payable, and cash.

Calculation
Working capital is the difference between current assets and current liabilities. It is not to be confused with trade working capital (the latter excludes cash).
The basic calculation of working capital is based on the entity's gross current assets. 

  
    
      
        
          Working capital
        
        =
        
          Current assets
        
        −
        
          Current liabilities
        
      
    
    {\displaystyle {\text{Working capital}}={\text{Current assets}}-{\text{Current liabilities}}}

Inputs
Current assets and current liabilities include four accounts which are of special importance. These accounts represent the areas of the business where managers have the most direct impact:

cash and cash equivalents (current asset)
accounts receivable (current asset)
inventory (current asset), and
accounts payable (current liability)The current portion of debt (payable within 12 months) is critical because it represents a short-term claim to current assets and is often secured by long-term assets. Common types of short-term debt are bank loans and lines of credit.
An increase in net working capital indicates that the business has either increased current assets (that it has increased its receivables or other current assets) or has decreased current liabilities—for example has paid off some short-term creditors, or a combination of both.

Working capital cycle
Definition
The working capital cycle (WCC), also known as the cash conversion cycle, is the amount of time it takes to turn the net current assets and current liabilities into cash. The longer this cycle, the longer a business is tying up capital in its working capital without earning a return on it. Companies strive to reduce their working capital cycle by collecting receivables quicker or sometimes stretching accounts payable. Under certain conditions, minimizing working capital might adversely affect the company's ability to realize profitability, e.g. when unforeseen hikes in demand exceed inventories, or when a shortfall in cash restricts the company's ability to acquire trade or production inputs.

Meaning
A positive working capital cycle balances incoming and outgoing payments to minimize net working capital and maximize free cash flow. For example, a company that pays its financing is a carrying cost tinexpensive way to grow. Sophisticated buyers review closely a target's working capital cycle because it provides them with an idea of the management's effectiveness at managing their balance sheet and generating free cash flows.
As an absolute rule of funders, each of them wants to see a positive working capital because positive working capital implies there are sufficient current assets to meet current obligations. In contrast, companies risk being unable to meet current obligations with current assets when working capital is negative. While it is theoretically possible for a company to indefinitely show negative working capital on regularly reported balance sheets (since working capital may actually be positive between reporting periods), working capital will generally need to be non-negative for the business to be sustainable
Reasons why a business may show negative or low working capital over the long term while not indicating financial distress include:

Assets above or liabilities below their true economic value
Accrual basis accounting creating deferred revenue while the cost of goods sold is lower than the revenue to be generated
E.g. a software as a service business or newspaper receives cash from customers early on, but has to include the cash as a deferred revenue liability until the service is delivered. The cost of delivering the service or newspaper is usually lower than revenue thus, when the revenue is recognized, the business will generate gross income.

Working capital management
Decisions relating to working capital and short-term financing are referred to as working capital management. These involve managing the relationship between a firm's short-term assets and its short-term liabilities. The goal of working capital management is to ensure that the firm is able to continue its operations and that it has sufficient cash flow to satisfy both maturing short-term debt and upcoming operational expenses.
A managerial accounting strategy focusing on maintaining efficient levels of both components of working capital, current assets, and current liabilities, in respect to each other. Working capital management ensures a company has sufficient cash flow in order to meet its short-term debt obligations and operating expenses.

Decision criteria
By definition, working capital management entails short-term decisions—generally, relating to the next one-year period—which are "reversible".  These decisions are therefore not taken on the same basis as capital-investment decisions (NPV or related, as above); rather, they will be based on cash flows, or profitability, or both.

One measure of cash flow is provided by the cash conversion cycle—the net number of days from the outlay of cash for raw material to receiving payment from the customer.  As a management tool, this metric makes explicit the inter-relatedness of decisions relating to inventories, accounts receivable and payable, and cash.  Because this number effectively corresponds to the time that the firm's cash is tied up in operations and unavailable for other activities, management generally aims at a low net count.
In this context, the most useful measure of profitability is return on capital (ROC).  The result is shown as a percentage, determined by dividing relevant income for the 12 months by capital employed; return on equity (ROE) shows this result for the firm's shareholders.  Firm value is enhanced when, and if, the return on capital, which results from working-capital management, exceeds the cost of capital, which results from capital investment decisions as above.  ROC measures are therefore useful as a management tool, in that they link short-term policy with long-term decision making.  See economic value added (EVA).
Credit policy of the firm:  Another factor affecting working capital management is credit policy of the firm.  It includes buying of raw material and selling of finished goods either in cash or on credit.  This affects the cash conversion cycle.

Management of working capital
Guided by the above criteria, management will use a combination of policies and techniques for the management of working capital. The policies aim at managing the current assets (generally cash and cash equivalents, inventories and debtors) and the short-term financing, such that cash flows and returns are acceptable.

Cash management. Identify the cash balance which allows for the business to meet day to day expenses, but reduces cash holding costs.
Inventory management. Identify the level of inventory which allows for uninterrupted production but reduces the investment in raw materials—and minimizes reordering costs—and hence increases cash flow. Besides this, the lead times in production should be lowered to reduce Work in Process (WIP) and similarly, the Finished Goods should be kept on as low level as possible to avoid overproduction—see Supply chain management; Just In Time (JIT); Economic order quantity (EOQ); Economic quantity
Debtors management. Identify the appropriate credit policy, i.e. credit terms which will attract customers, such that any impact on cash flows and the cash conversion cycle will be offset by increased revenue and hence Return on Capital (or vice versa); see Discounts and allowances.
Short-term financing. Identify the appropriate source of financing, given the cash conversion cycle: the inventory is ideally financed by credit granted by the supplier; however, it may be necessary to utilize a bank loan (or overdraft), or to "convert debtors to cash" through "factoring".

See also
Cash conversion cycle
Overtrading
Quick ratio analysis
Sustainable growth rate
Trade finance
Working capital management

References
External links
How to Calculate Working Capital
Value Based Working Capital Management

Zero Defects

Zero Defects (or ZD) was a management-led program to eliminate defects in industrial production that enjoyed brief popularity in American industry from 1964 to the early 1970s.  Quality expert Philip Crosby later incorporated it into his "Absolutes of Quality Management" and it enjoyed a renaissance in the American automobile industry—as a performance goal more than as a program—in the 1990s.  Although applicable to any type of enterprise, it has been primarily adopted within supply chains wherever large volumes of components are being purchased (common items such as nuts and bolts are good examples).

Definition
"[...] Zero Defects is a management tool aimed at the reduction of defects through prevention. It is directed at motivating people to prevent mistakes by developing a constant, conscious desire to do their job right the first time.": vii  — Zero Defects: A New Dimension in Quality Assurance
Zero Defects seeks to directly reverse the attitude that the number of mistakes a worker makes doesn't matter since inspectors will catch them before they reach the customer.: 4   This stands in contrast to activities that affect the worker directly, such as receiving a paycheck in the correct amount.  Zero Defects involves reconditioning the worker "to take a personal interest in everything he does[,] by convincing him that his job is just as important as the task of the doctor or the dentist.": 4

History
The development of Zero Defects is credited to Philip B. Crosby, a quality control department manager on the Pershing missile program at the Martin Company, though at least one contemporary reference credits a small, unnamed group of Martin employees.Zero Defects was not the first application of motivational techniques to production: during World War II, the War Department's "E for Excellence" program sought to boost production and minimize waste.
The Cold War resulted in increased spending on the development of defense technology in the 1950s and 1960s. Because of the safety-critical nature of such technology, particularly weapons systems, the government and defense firms came to employ hundreds of thousands of people in inspection and monitoring of highly-complex products assembled from hundreds of thousands of individual parts.: 10  This activity routinely uncovered defects in design, manufacture, and assembly and resulted in an expensive, drawn out cycle of inspection, rework, reinspection, and retest.: 12  Additionally, reports of spectacular missile failures appearing in the press heightened the pressure to eliminate defects.
In 1961, the Martin Company's Orlando Florida facility embarked on an effort to increase quality awareness and specifically launched a program to drive down the number of defects in the Pershing missile to one half of the acceptable quality level in half a year's time.: 12   Subsequently, the Army asked that the missile be delivered a month earlier than the contract date in 1962.  Martin marshaled all of its resources to meet this challenge and delivered the system with no discrepancies in hardware and documentation and were able to demonstrate operation within a day of the start of setup.: 14¡V15   After reviewing how Martin was able to overachieve, its management came to the conclusion that while it had not insisted on perfection in the past, it had in this instance, and that was all that was needed to attain outstanding product quality.: 15 Management commissioned a team to examine the phenomenon and come up with an action plan, which became the organizing, motivating, and initiating elements of Zero Defects.: 15   The Department of Defense also took notice and in 1964, began to actively encourage its vendors to adopt Zero Defects programs.  Interest in the program from outside firms, including Litton Industries, Thiokol, Westinghouse, and Bendix Corporation,: 16  was keen and many made visits to Martin to learn about it.: 16   Their feedback was incorporated and rounded out the program.  In particular, General Electric suggested that error cause removal be included in the program.: 16 Martin claimed a 54% defect reduction in defects in hardware under government audit during the first two years of the program.  General Electric reported a $2 million reduction in rework and scrap costs, RCA reported 75% of its departments in one division were achieving Zero Defects, and Sperry Corporation reported a 54% defect reduction over a single year.: 17 During its heyday, it was adopted by General Electric, ITT Corporation, Montgomery Ward, the United States Army among other organizations.While Zero Defects began in the aerospace and defense industry, thirty years later it was regenerated in the automotive world. During the 1990s, large companies in the automotive industry cut costs by reducing their quality inspection processes and demanding that their suppliers dramatically improve the quality of their supplies.

Later developments
In 1979, Crosby penned Quality Is Free: The Art of Making Quality Certain which preserved the idea of Zero Defects in a Quality Management Maturity Grid, in a 14-step quality improvement program, and in the concept of the "Absolutes of Quality Management".  The quality improvement program incorporated ideas developed or popularized by others (for example, cost of quality (step 4), employee education (step 8), and quality councils (step 13)) with the core motivation techniques of booklets, films, posters, speeches, and the "ZD Day" centerpiece.

Absolutes of Quality Management
According to Crosby, there are four Absolutes:

1. "The definition of quality is conformance to requirements"
Newcomers to manufacturing bring their own vague impressions of what quality involves.  But in order to tackle quality-related problems, there must be widespread agreement on the specifics of what quality means for a particular product.  Customer needs and expectations must be reduced to measurable quantities like length, or smoothness, or roundness and a standard must be specified for each.  These become the requirements for a product and the organization must inspect, or measure what comes out of the production process against those standards to determine whether the product conforms to those requirements or not.: 17   An important implication of this is that if management does not specify these requirements workers invent their own which may not align with what management would have intended had they provided explicit requirements to begin with.: 78

2. "The system of quality is prevention"
Companies typically focus on inspection to ensure that defective product doesn't reach the customer.  But this is both costly and still lets nonconformances through.  Prevention, in the form of "pledging ourselves to make a constant conscious effort to do our jobs right the first time", is the only way to guarantee zero defects.  Beyond that, examining the production process for steps where defects can occur and mistake proofing them contributes to defect-free production.

3. "The performance standard is Zero Defects"
Workers, at least during the post–World War II economic expansion, had a lackadaisical attitude on the whole toward work.  Crosby saw statistical quality control and the MIL-Q-9858 standard as contributing to this through acceptable quality levels—a concept that allows a certain number of acceptable defects and reinforces the attitude that mistakes are inevitable.: 80 : 79–80   Another contributor is the self-imposed pressure to produce something to sell, even if that thing is defective.: 72–73   Workers must "make the attitude of Zero Defects [their] personal standard.": 172

4. "The measurement of quality is the price of nonconformance"
To convince executives to take action to resolve issues of poor quality, costs associated with poor quality must be measured in monetary terms.: 121   Crosby uses the term "the price of nonconformance" in preference to "the cost of quality" to overcome the misimpression that higher quality requires higher costs.  The point of writing Quality Is Free was to demonstrate that quality improvement efforts pay for themselves.  Crosby divides quality-related costs into the price of conformance and the price of nonconformance. The price of conformance includes quality-related planning, inspection, and auditing; the price of nonconformance includes scrap, rework, claims against warranty, unplanned service: 209

Criticisms
The main criticism is the amount of effort required to verify every person's performance in an organization.: 121   Confidence in the program, and therefore compliance with it, fades without this verification.: 118 Point 10 of Deming's 14 points ("Eliminate slogans, exhortations, and targets for the work force asking for zero defects and new levels of productivity.") is clearly aimed at ZD.  Joseph M. Juran was also critical of ZD.Another criticism is that Zero Defects is a motivational program aimed at encouraging employees to do better.  Crosby stated that "Motivation has nothing to do with it...It is merely setting performance standards that no one can misunderstand and then starting a two-way communications exercise to let everyone know about it."  He blamed management actions and attitudes for creating the opportunity for defects.

See also
Six Sigma
Total Quality Management

Notes
References
Further reading
American Management Association (1965). Zero Defects: Doing It Right the First Time. New York City: American Management Association. OCLC 244134.
A Guide to Zero Defects: Quality and Reliability Assurance Handbook. Washington, D.C.: Office of the Assistant Secretary of Defense (Manpower Installations and Logistics). 1965. OCLC 7188673. 4155.12-H. Archived from the original on May 29, 2014. Retrieved May 29, 2014.
Halpin, James F. (1966). Zero Defects: A New Dimension in Quality Assurance. New York City: McGraw-Hill. OCLC 567983091.
Riordan, John J., ed. (1968). Zero Defects: the Quest for Quality. Washington, D.C.: Office of the Assistant Secretary of Defense. OCLC 3396301. Technical Report TR9. Archived from the original on May 29, 2014. Retrieved May 29, 2014.

External links
Zero Defects at Lockheed Martin, the descendant of the Martin Company
Photo of Governor William W. Scranton of Pennsylvania speaking at a Zero Defects kickoff at Boeing Vertol in 1965

Wikipedia:Citation needed

To ensure that all Wikipedia content is verifiable, Wikipedia provides a means for anyone to question an uncited claim. If your work has been tagged, please provide a reliable source for the statement, and discuss if needed.
You can add a citation by selecting from the drop-down  menu at the top of the editing box. In markup, you can add a citation manually using ref tags. There are also more elaborate ways to cite sources.
In wiki markup, you can question an uncited claim by inserting a simple {{Citation needed}} tag, or a more comprehensive {{Citation needed|reason=Your explanation here|date=November 2023}}. Alternatively, {{fact}} and {{cn}} will produce the same result. These all display as: 

Example: 87 percent of statistics are made up on the spot. 
For information on adding citations in articles, see Help:Referencing for beginners. For information on when to remove this template messages, see Help:Maintenance template removal.

A "citation needed" tag is a request for another editor to supply a source for the tagged fact: a form of communication between members of a collaborative editing community. It is never, in itself, an "improvement" of an article. Though readers may be alerted by a "citation needed" that a particular statement is not supported, and even doubted by some, many readers don't fully understand the community's processes. Not all tags get addressed in a timely manner, staying in place for months or years, forming an ever-growing Wikipedia backlog—this itself can be a problem. Best practice recommends the following: 

Tag thoughtfully. Avoid "hit-and-run" or pointed tagging. Try to be courteous and consider the hypothetical fellow-editor who will, we hope, notice your tag and try to find the citation you have requested. When adding a tag, ask yourself: Is it clear just what information you want cited? Is the information probably factual? (If it is not, then it needs deletion or correction rather than citation!) Is the knowledge so self-evident that it really does not need to be cited at all? (Some things do not.)
Some tags are inserted by people well-placed to find a suitable citation themselves. If this is the case, consider adding these articles to your watchlist or a worklist so that you can revisit the article when you have the opportunity to fix any verifiability issues yourself.
Before adding a tag, at least consider the following alternatives, one of which may prove much more constructive:

Do not use this tag because you don't understand a statement, or feel that "non-expert" readers are likely to be confused. Use {{Clarify}}, {{Explain}}, {{Confusing}}, {{Examples}}, {{Why}} or {{Non sequitur}}, as appropriate, instead.
If the content is nonsense or is unlikely to be true,  be bold and delete it!
Do not tag controversial material about living people that is unsourced or poorly sourced. Remove it immediately!
Per WP:DIARY, do not tag excessively trivial claims. Remove them.
If you are sure the statement you want to tag is not factual, even if it does not come under either of the preceding headings, it may be more appropriate to simply remove the text (delete it!). Be sure to add a suitable edit summary, such as "Very doubtful – please add a citation if you return the content". If the original statement was accurate after all, this gives someone the chance to put it back, hopefully with a proper citation this time.
If a statement sounds plausible, and is consistent with other statements in the article, but you doubt that it is totally accurate, then consider making a reasonable effort to find a reference yourself. In the process, you may end up confirming that the statement needs to be edited or deleted to better reflect the best knowledge about the topic.
If an article, or a section within an article, is under-referenced, then consider adding an {{Unreferenced}}, {{Refimprove}}, or {{Unreferenced section}} tag to the article or section concerned – these tags allow you to indicate more systemic problems to the page.
A reference at the end of a paragraph typically refers to the whole paragraph, and similarly a reference at the end of a sentence may almost always be taken as referring to the whole sentence. If a particular part of a sentence or paragraph seems to require a separate citation, or looks as if it may have been inserted into the text at a sentence or paragraph level, try to check the original reference rather than adding tags to text that may already be well referenced. The extra parameters available in the {{Citation needed span}} template may allow you to indicate which section you want to refer to.
Do not insert a "Citation needed" tag to make a point, to "pay back" another editor, or because you "don't like" a subject, a particular article, or another editor.
If you can provide a reliable source for the claim, then please add it! If you are not sure how to do this, then give it your best try and replace the "Citation needed" template with enough information to locate the source. You may leave the copyediting or Wikifying to someone else, or learn more about citing sources on Wikipedia. This beginners' referencing guide for Wikipedia provides a brief introduction on how to reference Wikipedia articles.
If someone tagged your contributions with a "Citation needed" tag or tags, and you disagree, discuss the matter on the article's talk page. The most constructive thing to do in most cases is probably to supply the reference(s) requested, even if you feel the tags are "overdone" or unnecessary.
There are 505,457 articles with "Citation needed" statements. You can browse the whole list of these articles at Category:All articles with unsourced statements.
Frequently the authors of statements do not return to Wikipedia to support the statement with citations, so other Wikipedia editors have to do work checking those statements. With 505,457 statements that need WP:Verification, sometimes it's hard to choose which article to work on. The tool Citation Hunt makes that easier by suggesting random articles, which you can sort by topical category membership.

Template:Citation needed
Template:Citation needed span
Template:Verify source
Template:Unreferenced for an article (rather than an individual statement) which does not cite any references or sources.
Template:More citations needed for an article (rather than an individual statement) that has some citations, but not enough.
Template messages – Sources of articles
Inline verifiability and sources cleanup templates
Wikipedia:Verification methods – listing examples of the most common ways that citations are used in Wikipedia articles
Wikipedia:Citing sources/Example edits for different methods – showing comparative edit mode representations for different citation methods and techniques.
Wikipedia:Citation templates – a full listing of various styles for citing all sorts of materials
Wikipedia:WikiProject Reliability
Category:All articles with unsourced statements – list of all pages with {{citation needed}}
 Media related to Citation needed at Wikimedia Commons

Wikipedia:Citing sources

A citation, also called a reference, uniquely identifies a source of information, e.g.:

Wikipedia's verifiability policy requires inline citations for any material challenged or likely to be challenged, and for all quotations, anywhere in article space.
A citation or reference in an article usually has two parts. In the first part, each section of text that is either based on, or quoted from, an outside source is marked as such with an inline citation. This is usually displayed as a superscript footnote number: [1]  The second necessary part of the citation or reference is the list of full references, which provides complete, formatted detail about the source, so that anyone reading the article can find it and verify it.
This page explains how to place and format both parts of the citation. Each article should use one citation method or style throughout. If an article already has citations, preserve consistency by using that method or seek consensus on the talk page before changing it (the principle is reviewed at § Variation in citation methods). While you should try to write citations correctly, what matters most is that you provide enough information to identify the source. Others will improve the formatting if needed. See: "Help:Referencing for beginners", for a brief introduction on how to put references in Wikipedia articles; and cite templates in Visual Editor, about a graphical way for citation, included in Wikipedia.

An inline citation means any citation added close to the material it supports, for example after the sentence or paragraph, normally in the form of a footnote.
In-text attribution involves adding the source of a statement to the article text, such as Rawls argues that X.[5] This is done whenever a writer or speaker should be credited, such as with quotations, close paraphrasing, or statements of opinion or uncertain fact. The in-text attribution does not give full details of the source – this is done in a footnote in the normal way. See In-text attribution below.
A general reference is a citation that supports content, but is not linked to any particular piece of material in the article through an inline citation. General references are usually listed at the end of the article in a References section. They are usually found in underdeveloped articles, especially when all article content is supported by a single source. They may also be listed in more developed articles as a supplement to inline citations.

Short and full citations
A full citation fully identifies a reliable source and, where applicable, the place in that source (such as a page number) where the information in question can be found. For example: Rawls, John. A Theory of Justice. Harvard University Press, 1971, p. 1. This type of citation is usually given as a footnote, and is the most commonly used citation method in Wikipedia articles.A short citation is an inline citation that identifies the place in a source where specific information can be found, but without giving full details of the source. Some Wikipedia articles use it, giving summary information about the source together with a page number. For example, <ref>Rawls 1971, p. 1.</ref>, which renders as Rawls 1971, p. 1.. These are used together with full citations, which are listed in a separate "References" section or provided in an earlier footnote.Forms of short citations used include author-date referencing (APA style, Harvard style, or Chicago style), and author-title or author-page referencing (MLA style or Chicago style). As before, the list of footnotes is automatically generated in a "Notes" or "Footnotes" section, which immediately precedes the "References" section containing the full citations to the source. Short citations can be written manually, or by using either the {{sfn}} or {{harvnb}} templates or the {{r}} referencing template. (Note that templates should not be added without consensus to an article that already uses a consistent referencing style.) The short citations and full citations may be linked so that the reader can click on the short note to find full information about the source. See the template documentation for details and solutions to common problems. For variations with and without templates, see wikilinks to full references. For a set of realistic examples, see these.
This is how short citations look in the edit box:

This is how they look in the article:

Shortened notes using titles rather than publication dates would look like this in the article:

When using manual links it is easy to introduce errors such as duplicate anchors and unused references. The script User:Trappist the monk/HarvErrors will show many related errors. Duplicate anchors may be found by using the W3C Markup Validation Service.

By citing sources for Wikipedia content you enable users to verify that the cited information is supported by reliable sources – improving the credibility of Wikipedia while showing that the content is not original research. You also help users find additional information on the subject; and by giving attribution you avoid plagiarising the source of your words or ideas.
In particular, sources are needed for material that is challenged or likely to be challenged. If reliable sources cannot be found for challenged material, it is likely to be removed from the article. Sources are also required when quoting someone, with or without quotation marks, or closely paraphrasing a source. But the need to cite sources is not limited to those situations: editors are always encouraged to add or improve citations for any information in an article.
Citations are especially desirable for statements about living persons, particularly when the statements are contentious or potentially defamatory. In accordance with the biography of living persons policy, unsourced information of this type is likely to be removed on sight.

Multimedia
For an image or other media file, details of its origin and copyright status should appear on its file page. Image captions should be referenced as appropriate just like any other part of the article. A citation is not needed for descriptions such as alt text that are verifiable directly from the image itself, or for text that merely identifies a source (e.g., the caption "Belshazzar's Feast (1635)" for File:Rembrandt-Belsazar.jpg).

When not to cite
Citations are not used on disambiguation pages (sourcing for the information given there should be done in the target articles). Citations are often omitted from the lead section of an article, insofar as the lead summarizes information for which sources are given later in the article, although quotations and controversial statements, particularly if about living persons, should be supported by citations even in the lead. See WP:LEADCITE for more information.
Per WP:PAIC, citations should be placed at the end of the passage that they support. If one source alone supports consecutive sentences in the same paragraph, one citation of it at the end of the final sentence is sufficient. It is not necessary to include a citation for each individual consecutive sentence, as this is overkill. This does not apply to lists or tables, nor does it apply when multiple sources support different parts of a paragraph or passage. Citation requirements for WP:DYK may require a citation to be inserted (for the duration of the DYK listing) even within a passage completely cited to the same sources.

Consecutive cites of the same source
Material that is repeated multiple times in an article does not require an inline citation for every mention. If you say an elephant is a mammal more than once, provide one only at the first instance. Avoid cluttering text with redundant citations like this:

Elephants are large[1] land[2] mammals[3] ... Elephants' teeth[4] are very different[4] from those of most other mammals.[3][4] Unlike most mammals,[3] which grow baby teeth[5] and then replace them with a permanent set of adult teeth,[4] elephants have cycles of tooth[5] rotation throughout their entire[6] lives.[4]

Inline citations allow the reader to associate a given piece of material in an article with the specific reliable source(s) that support it. Inline citations are added using footnotes, long or short.

How to place an inline citation using ref tags
To create a footnote, use the <ref>...</ref> syntax at the appropriate place in the article text, for example:

Justice is a human invention.<ref>Rawls, John. ''A Theory of Justice''. Harvard University Press, 1971, p. 1.</ref> It&nbsp;...which will be displayed as something like:

Justice is a human invention.[1] It ...It will also be necessary to generate the list of footnotes (where the citation text is actually displayed); for this, see the previous section.
As in the above example, citation markers are normally placed after adjacent punctuation such as periods (full stops) and commas. For exceptions, see the WP:Manual of Style § Punctuation and footnotes. Note also that no space is added before the citation marker. Citations should not be placed within, or on the same line as, section headings.
The citation should be added close to the material it supports, offering text–source integrity. If a word or phrase is particularly contentious, an inline citation may be added next to that word or phrase within the sentence, but it is usually sufficient to add the citation to the end of the clause, sentence, or paragraph, so long as it's clear which source supports which part of the text.

Avoiding clutter
Inline references can significantly bloat the wikitext in the edit window and can become confusing and difficult to manage. There are two main methods to avoid clutter in the edit window:

Using list-defined references by collecting the full citation code within the reference list template {{reflist}}, and then inserting them in the text with a shortened reference tag, for example <ref name="Smith 2001, p99" />.
Inserting short citations (see below) that then refer to a full list of source textsAs with other citation formats, articles should not undergo large-scale conversion between formats without consensus to do so.
Note, however, that references defined in the reference list template can no longer be edited with the VisualEditor.

Repeated citations
For multiple use of the same inline citation or footnote, you can use the named references feature, choosing a name to identify the inline citation, and typing <ref name="name">text of the citation</ref>. Thereafter, the same named reference may be reused any number of times either before or after the defining use by typing the previous reference name, like this: <ref name="name" />. The use of the slash before the > means that the tag is self-closing, and the </ref> used to close other references must not be used in addition.
The text of the name can be almost anything‍—‌apart from being completely numeric. If spaces are used in the text of the name, the text must be placed within double quotes. Placing all named references within double quotes may be helpful to future editors who do not know that rule. To help with page maintenance, it is recommended that the text of the name have a connection to the inline citation or footnote, for example "author year page": <ref name="Smith 2005 p94">text of the citation</ref>.
Use straight quotation marks " to enclose the reference name. Do not use curly quotation marks “”. Curly marks are treated as another character, not as delimiters. The page will display an error if one style of quotation marks is used when first naming the reference, and the other style is used in a repeated reference, or if a mix of styles is used in the repeated references.

Citing multiple pages of the same source
When an article cites many different pages from the same source, to avoid the redundancy of many big, nearly identical full citations, most Wikipedia editors use one of these options:

Named references in conjunction with a combined list of page numbers using the |pages= parameter of the {{cite}} templates (most commonly used, but can become confusing for large number of pages)
Named references in conjunction with the {{rp}} or {{r}} templates to specify the page
Short citationsThe use of ibid., id., or similar abbreviations is discouraged, as they may become broken as new references are added (op. cit. is less problematic in that it should refer explicitly to a citation contained in the article; however, not all readers are familiar with the meaning of the terms). If the use of ibid is extensive, tag the article using the {{ibid}} template.

What information to include
Listed below is the information that a typical inline citation or general reference will provide, though other details may be added as necessary. This information is included in order to identify the source, assist readers in finding it, and (in the case of inline citations) indicate the place in the source where the information is to be found. (If an article uses short citations, then the inline citations will refer to this information in abbreviated form, as described in the relevant sections above.)
In general, the citation information should be cited as it appear in the original source. For example, the album notes from Hurts 2B Human should not be cited as being from the album Hurts to be Human, or an X user named "i😍dogs" should not be cited as "i[love]dogs". Retain the original special glyphs and spelling.

Examples
Books
Citations for books typically include:

name of author(s)
title of book
volume when appropriate
name of publisher
place of publication
date of publication of the edition
chapter or page numbers cited, if appropriate
edition, if not the first edition
ISBN (optional)Citations for individually authored chapters in books typically include:

name of author(s)
title of the chapter
name of book's editor
name of book and other details as above
chapter number or page numbers for the chapter (optional)In some instances, the verso of a book's title page may record, "Reprinted with corrections XXXX" or similar, where "XXXX" is a year. This is a different version of a book in the same way that different editions are different versions. Note this in your citation. See § Dates and reprints for how to further information.

Journal articles
Citations for journal articles typically include:

name of the author(s)
year and sometimes month of publication
title of the article
name of the journal
volume number, issue number, and page numbers (article numbers in some electronic journals)
DOI and/or other identifiers are optional and can often be used in place of a less stable URL (although URLs may also be listed in a journal citation)

Newspaper articles
Citations for newspaper articles typically include:

byline (author's name), if any
title of the article
name of the newspaper in italics
city of publication (if not included in name of newspaper)
date of publication
page number(s) are optional and may be substituted with negative number(s) on microfilm reels

Web pages
Citations for World Wide Web pages typically include:

URL of the specific web page where the referenced content can be found
name of the author(s)
title of the article
title or domain name of the website
publisher, if known
date of publication
page number(s) (if applicable)
the date you retrieved (or accessed) the web page (required if the publication date is unknown)

Sound recordings
Citations for sound recordings typically include:

name of the composer(s), songwriter(s), script writer(s) or the like
name of the performer(s)
title of the song or individual track
title of the album (if applicable)
name of the record label
year of release
medium (for example: LP, audio cassette, CD, MP3 file)
approximate time at which event or point of interest occurs, where appropriateDo not cite an entire body of work by one performer. Instead, make one citation for each work your text relies on.

Film, television, or video recordings
Citations for films, TV episodes, or video recordings typically include:

name of the director
name of the producer, if relevant
names of major performers
the title of a TV episode
title of the film or TV series
name of the studio
year of release
medium (for example: film, videocassette, DVD)
approximate time at which event or point of interest occurs, where appropriate

Wikidata
Wikidata is largely user-generated, and articles should not directly cite Wikidata as a source (just as it would be inappropriate to cite other Wikipedias' articles as sources). 
But Wikidata's statements can be directly transcluded into articles; this is usually done to provide external links or infobox data. For example, more than two million external links from Wikidata are shown through the {{Authority control}} template. There has been controversy over the use of Wikidata in the English Wikipedia due to vandalism and its own sourcing. While there is no consensus on whether information from Wikidata should be used at all, there is general agreement that any Wikidata statements that are transcluded need to be just as – or more – reliable compared to Wikipedia content. As such, Module:WikidataIB and some related modules and templates filter Wikidata statements not supported by a reference by default; however, other modules and templates, such as Module:Wikidata, do not.
To transclude an item from Wikidata, the QID (Q number) of an item in Wikidata needs to be known. QID can by found by searching for an item by the name or DOI in Wikidata. A book, a journal article, a musical recording, sheet music or any other item can be represented by a structured item in Wikidata.
The {{Cite Q}} template can be used to cite works whose metadata is held in Wikidata, provided the cited work meets Wikipedia's standards. As of December 2020, {{Cite Q}} does not support "last, first" or Vancouver-style author name lists, so it should not be used in articles in which "last, first" or Vancouver-style author names are the dominant citation style.

Other
See also:

{{cite album notes}}
{{cite comic}}
{{cite conference}} for conference reports or papers
{{cite court}} for court cases or legal decisions
{{cite act}} for a law or legal act
{{cite encyclopedia}}
{{cite episode}} for TV or radio series
{{cite mailing list}}
{{cite map}}
{{cite newsgroup}}
{{cite patent}} for patents
{{cite press release}}
{{cite report}}
{{cite thesis}}
{{cite video game}}

Identifying parts of a source
When citing lengthy sources, you should identify which part of a source is being cited.

Books and print articles
Specify the page number or range of page numbers. Page numbers are not required for a reference to the book or article as a whole. When you specify a page number, it is helpful to specify the version (date and edition for books) of the source because the layout, pagination, length, etc. can change between editions.
If there are no page numbers, whether in ebooks or print materials, then you can use other means of identifying the relevant section of a lengthy work, such as the chapter number or the section title.
In some works, such as plays and ancient works, there are standard methods of referring to sections, such as "Act 1, scene 2" for plays and Bekker numbers for Aristotle's works. Use these methods whenever appropriate.

Audio and video sources
Specify the time at which the event or other point of interest occurs. Be as precise as possible about the version of the source that you are citing; for example, movies are often released in different editions or "cuts". Due to variations between formats and playback equipment, precision may not be accurate in some cases. However, many government agencies do not publish minutes and transcripts but do post video of official meetings online; generally the subcontractors who handle audio-visual are quite precise.

Links and ID numbers
A citation ideally includes a link or ID number to help editors locate the source. If you have a URL (web page) link, you can add it to the title part of the citation, so that when you add the citation to Wikipedia the URL becomes hidden and the title becomes clickable. To do this, enclose the URL and the title in square brackets—the URL first, then a space, then the title. For example:

For web-only sources with no publication date, the "Retrieved" date (or the date you accessed the web page) should be included, in case the web page changes in the future. For example: Retrieved 15 July 2011 or you can use the access-date parameter in the automatic Wikipedia:refToolbar 2.0 editing window feature.
You can also add an ID number to the end of a citation. The ID number might be an ISBN for a book, a DOI (digital object identifier) for an article or some e-books, or any of several ID numbers that are specific to particular article databases, such as a PMID number for articles on PubMed. It may be possible to format these so that they are automatically activated and become clickable when added to Wikipedia, for example by typing ISBN (or PMID) followed by a space and the ID number.
If your source is not available online, it should be available in reputable libraries, archives, or collections. If a citation without an external link is challenged as unavailable, any of the following is sufficient to show the material to be reasonably available (though not necessarily reliable): providing an ISBN or OCLC number; linking to an established Wikipedia article about the source (the work, its author, or its publisher); or directly quoting the material on the talk page, briefly and in context.

Linking to pages in PDF files
Links to long PDF documents can be made more convenient by taking readers to a specific page with the addition of #page=n to the document URL, where n is the page number. For example, using https://www.domain.com/document.pdf#page=5 as the citation URL displays page five of the document in any PDF viewer that supports this feature. If the viewer or browser does not support it, it will display the first page instead.

Linking to Google Books pages
Google Books sometimes allows numbered book pages to be linked to directly.
Page links should only be added when the book is available for preview; they will not work with snippet view. Keep in mind that availability varies by location. No editor is required to add page links, but if another editor adds them, they should not be removed without cause; see the October 2010 RfC for further information.
These can be added in several ways (with and without citation templates):

Rawls, John. A Theory of Justice. Harvard University Press, 1971, p. 18.
Or with a template: Rawls, John (1971). A Theory of Justice. Harvard University Press. p. 18.
Rawls 1971, p. 18.
Rawls 1971, p. 18.
Rawls 1971, p. 18.
Rawls 1971, 18.In edit mode, the URL for p. 18 of A Theory of Justice can be entered like this using the {{Cite book}} template:

{{cite book |last=Rawls |first=John |date=1971 |title=A Theory of Justice |url=https://books.google.com/books?id=kvpby7HtAe0C&pg=PA18 |publisher=Harvard University Press |page=18}}
or like this, in the first of the above examples, formatted manually:

When the page number is a Roman numeral, commonly seen at the beginning of books, the URL looks like this for page xvii (Roman numeral 17) of the same book:     https://books.google.com/books?id=kvpby7HtAe0C&pg=PR17The &pg=PR17 indicates "page, Roman, 17", in contrast to the &pg=PA18, "page, Arabic, 18" the URL given earlier.
You can also link to a tipped-in page, such as an unnumbered page of images between two regular pages. (If the page contains an image that is protected by copyright, it will be replaced by a tiny notice saying "copyrighted image".) The URL for eleventh tipped-in page inserted after page 304 of The Selected Papers of Elizabeth Cady Stanton and Susan B. Anthony, looks like this:     https://books.google.com/books?id=dBs4CO1DsF4C&pg=PA304-IA11The &pg=PA304-IA11 can be interpreted as "page, Arabic, 304; inserted after: 11".

Note that some templates properly support links only in parameters specifically designed to hold URLs like |url= and |archive-url= and that placing links in other parameters may not link properly or will cause mangled COinS metadata output. However, the |page= and |pages= parameters of all Citation Style 1/Citation Style 2 citation templates, the family of {{sfn}}- and {{harv}}-style templates, as well as {{r}}, {{rp}} and {{ran}} are designed to be safe in this regard as well.Wikipedia DOI and Google Books Citation Maker or Citer may be helpful.
Users may also link the quotation on Google Books to individual titles, via a short permalink which ends with their related ISBN, OCLC or LCCN numerical code, e.g.:
https://books.google.com/books?vid=ISBN0521349931, a permalink to the Google book with the ISBN code 0521349931.
For further details, you may see How-to explanation on support.google.com.

Say where you read it
"Say where you read it" follows the practice in academic writing of citing sources directly only if you have read the source yourself. If your knowledge of the source is secondhand—that is, if you have read Jones (2010), who cited Smith (2009), and you want to use what Smith (2009) said—make clear that your knowledge of Smith is based on your reading of Jones.
When citing the source, write the following (this formatting is just an example):

John Smith (2009). Name of Book I Haven't Seen, Cambridge University Press, p. 99, cited in Paul Jones (2010). Name of Encyclopedia I Have Seen, Oxford University Press, p. 29.
Or if you are using short citations:

Smith (2009), p. 99, cited in Jones (2010), p. 29.
The same principle applies when indicating the source of images and other media files in an article.
Note: The advice to "say where you read it" does not mean that you have to give credit to any search engines, websites, libraries, library catalogs, archives, subscription services, bibliographies, or other sources that led you to Smith's book. If you have read a book or article yourself, that's all you have to cite. You do not have to specify how you obtained and read it.
So long as you are confident that you read a true and accurate copy, it does not matter whether you read the material using an online service like Google Books; using preview options at a bookseller's website like Amazon; through your library; via online paid databases of scanned publications, such as JSTOR; using reading machines; on an e-reader (except to the extent that this affects page numbering); or any other method.

Dates and reprints
Date a book that is identically reprinted or printed-on-demand to the first date in which the edition became available. For example, if an edition of a book was first released in 2005 with an identical reprinting in 2007, date it to 2005. If substantive changes were made in a reprint, sometimes marked on the verso with "Reprinted with corrections", note the edition and append the corrected reprint year to it (e.g. "1st ed. reprinted with corrections 2005").
Editors should be aware that older sources (especially those in the public domain) are sometimes republished with modern publication dates; treat these as new publications. When this occurs and the citation style being used requires it, cite both the new and original publication dates, e.g.:

Darwin, Charles (1964) [1859]. On the Origin of Species (facsimile of 1st ed.). Harvard University Press.This is done automatically in the {{citation}} and {{cite book}} templates when you use the |orig-date= parameter.
Alternately, information about the reprint can be appended as a textual note:

Boole, George (1854). An Investigation of the Laws of Thought on Which Are Founded the Mathematical Theories of Logic and Probabilities. Macmillan. Reprinted with corrections, Dover Publications, New York, NY, 1958.

Seasonal publication dates and differing calendar systems
Publication dates, for both older and recent sources, should be written with the goal of helping the reader find the publication and, once found, confirm that the correct publication has been located. For example, if the publication date bears a date in the Julian calendar, it should not be converted to the Gregorian calendar.
If the publication date was given as a season or holiday, such as "Winter" or "Christmas" of a particular year or two-year span, it should not be converted to a month or date, such as July–August or December 25. If a publication provided both seasonal and specific dates, prefer the specific one.

Additional annotation
In most cases it is sufficient for a citation footnote simply to identify the source (as described in the sections above); readers can then consult the source to see how it supports the information in the article. Sometimes, however, it is useful to include additional annotation in the footnote, for example to indicate precisely which information the source is supporting (particularly when a single footnote lists more than one source – see § Bundling citations and § Text–source integrity, below).
A footnote may also contain a relevant exact quotation from the source. This is especially helpful when the cited text is long or dense. A quotation allows readers to immediately identify the applicable portion of the reference. Quotes are also useful if the source is not easily accessible.
In the case of non-English sources, it may be helpful to quote from the original text and then give an English translation. If the article itself contains a translation of a quote from such a source (without the original), then the original should be included in the footnote. (See the WP:Verifiability § Non-English sources policy for more information.)

This section describes how to add footnotes and also describes how to create a list of full bibliography citations to support shortened footnotes.
The first editor to add footnotes to an article must create a dedicated citations' section where they are to appear. Any reasonable name may be chosen. The most frequent choice is "References". Other options in diminishing order of popularity are, "Notes", "Footnotes", or "Works cited", although these are more often used to distinguish between multiple end-matter sections or subsections.
For an example of headings of a notes section, see the article Tezcatlipoca.

General references
A general reference is a citation to a reliable source that supports content, but is not linked to any particular text in the article through an inline citation. General references are usually listed at the end of the article in a "References" section, and are usually sorted by the last name of the author or the editor. General reference sections are most likely to be found in underdeveloped articles, especially when all article content is supported by a single source. The disadvantage of general references is that text–source integrity is lost, unless the article is very short. They are frequently reworked by later editors into inline citations.
The appearance of a general references section is the same as those given above in the sections on short citations and parenthetical references. If both cited and uncited references exist, their distinction can be highlighted with separate section names, e.g., "References" and "General references".

How to create the list of citations
With some exceptions discussed below, citations appear in a single section containing only the <references /> tag or the {{Reflist}} template. For example:

References
{{Reflist}}

The footnotes will then automatically be listed under that section heading. Each numbered footnote marker in the text is a clickable link to the corresponding footnote, and each footnote contains a caret that links back to the corresponding point in the text.

Scrolling lists, or lists of citations appearing within a scroll box, should never be used. This is because of issues with readability, browser compatibility, accessibility, printing, and site mirroring.If an article contains a list of general references, this is usually placed in a separate section, titled, for example, "References". This usually comes immediately after the section(s) listing footnotes, if any. (If the general references section is called "References", then the citations section is usually called "Notes".)

Separating citations from explanatory footnotes
If an article contains both footnoted citations and other (explanatory) footnotes, then it is possible (but not necessary) to divide them into two separate lists using footnotes groups. The explanatory footnotes and the citations are then placed in separate sections, called (for example) "Notes" and "References", respectively.
Another method of separating explanatory footnotes from footnoted references is using {{efn}} for the explanatory footnotes. The advantage of this system is that the content of an explanatory footnote can in this case be referenced with a footnoted citation. When explanatory footnotes and footnoted references are not in separate lists, {{refn}} can be used for explanatory footnotes containing footnoted citations.

Duplicate citations
Combine precisely duplicated full citations, in keeping with the existing citation style (if any). In this context "precisely duplicated" means having the same content, not necessarily identical strings ("The New York Times" is the same as "NY Times"; different access-dates are not significant). Do not discourage editors, particularly inexperienced ones, from adding duplicate citations when the use of the source is appropriate, because a duplicate is better than no citation. But any editor should feel free to combine them, and doing so is the best practice on Wikipedia.
Citations to different pages or parts of the same source can also be combined (preserving the distinct parts of the citations), as described in the previous section. Any method that is consistent with the existing citation style (if any) may be used, or consensus can be sought to change the existing style. Some tools are linked below.

While citations should aim to provide the information listed above, Wikipedia does not have a single house style, though citations within any given article should follow a consistent style. A number of citation styles exist including those described in the Wikipedia articles for Citation, APA style, ASA style, MLA style, The Chicago Manual of Style, Author-date referencing, the Vancouver system and Bluebook.
Although nearly any consistent style may be used, avoid all-numeric date formats other than YYYY-MM-DD, because of the ambiguity concerning which number is the month and which the day. For example, 2002-06-11 may be used, but not 11/06/2002. The YYYY-MM-DD format should in any case be limited to Gregorian calendar dates where the year is after 1582. Because it could easily be confused with a range of years, the format YYYY-MM (for example: 2002-06) is not used.
For more information on the capitalization of cited works, see Wikipedia:Manual of Style/Capital letters § All caps and small caps.

Variation in citation methods
Editors should not attempt to change an article's established citation style, merely on the grounds of personal preference or to make it match other articles, without first seeking consensus for the change.As with spelling differences, it is normal practice to defer to the style used by the first major contributor or adopted by the consensus of editors already working on the page, unless a change in consensus has been achieved. If the article you are editing is already using a particular citation style, you should follow it; if you believe it is inappropriate for the needs of the article, seek consensus for a change on the talk page. If you are the first contributor to add citations to an article, you may choose whichever style you think best for the article. However, since 5 September 2020, inline parenthetical referencing is a deprecated citation style on English-language Wikipedia.
If all or most of the citations in an article consist of bare URLs, or otherwise fail to provide needed bibliographic data –  such as the name of the source, the title of the article or web page consulted, the author (if known), the publication date (if known), and the page numbers (where relevant) –  then that would not count as a "consistent citation style" and can be changed freely to insert such data. The data provided should be sufficient to uniquely identify the source, allow readers to find it, and allow readers to initially evaluate a source without retrieving it.

Generally considered helpful
The following are standard practice:

improving existing citations by adding missing information, such as by replacing bare URLs with full bibliographic citations: an improvement because it aids verifiability, and fights link rot;
replacing some or all general references with inline citations: an improvement because it provides more verifiable information to the reader, and helps maintain text–source integrity;
imposing one style on an article with inconsistent citation styles (e.g., some of the citations in footnotes and others as parenthetical references): an improvement because it makes the citations easier to understand and edit;
fixing errors in citation coding, including incorrectly used template parameters, and <ref> markup problems: an improvement because it helps the citations to be parsed correctly;
combining duplicate citations (see § Duplicate citations, above).
converting parenthetical referencing to an acceptable referencing style.
replacing opaque named reference names with conventional ones, such as "Einstein-1905" instead of ":27".

To be avoided
When an article is already consistent, avoid:

switching between major citation styles or replacing the preferred style of one academic discipline with another's – except when moving away from deprecated styles, such as parenthetical referencing;
adding citation templates to an article that already uses a consistent system without templates, or removing citation templates from an article that uses them consistently;
changing where the references are defined, e.g., moving reference definitions in the reflist to the prose, or moving reference definitions from the prose into the reflist.

Parenthetical referencing
Since September 2020, inline parenthetical referencing has been deprecated on Wikipedia. This includes short citations in parentheses placed within the article text itself, such as (Smith 2010, p. 1). This does not affect short citations that use <ref> tags, which are not inline parenthetical references; see the section on short citations above for that method. As part of the deprecation process in existing articles, discussion of how best to convert inline parenthetical citations into currently accepted formats should be held if there is objection to a particular method.
This is no longer in use:

As noted above under "What information to include", it is helpful to include hyperlinks to source material, when available. Here we note some issues concerning these links.

Avoid embedded links
Embedded links to external websites should not be used as a form of inline citation, because they are highly susceptible to linkrot. Wikipedia allowed this in its early years—for example by adding a link after a sentence, like this: [https://media.guardian.co.uk/site/story/0,14173,1601858,00.html], which is rendered as: [1]. This is no longer recommended. Raw links are not recommended in lieu of properly written out citations, even if placed between ref tags, like this <ref>[https://media.guardian.co.uk/site/story/0,14173,1601858,00.html]</ref>. Since any citation that accurately identifies the source is better than none, do not revert the good-faith addition of partial citations. They should be considered temporary, and replaced with more complete, properly formatted citations as soon as possible.
Embedded links should never be used to place external links in the content of an article, like this: "Example Inc. announced their latest product ...".

Convenience links
A convenience link is a link to a copy of your source on a web page provided by someone other than the original publisher or author. For example, a copy of a newspaper article no longer available on the newspaper's website may be hosted elsewhere. When offering convenience links, it is important to be reasonably certain that the convenience copy is a true copy of the original, without any changes or inappropriate commentary, and that it does not infringe the original publisher's copyright. Accuracy can be assumed when the hosting website appears reliable.
For academic sources, the convenience link is typically a reprint provided by an open-access repository, such as the author's university's library or institutional repository. Such green open access links are generally preferable to paywalled or otherwise commercial and unfree sources.
Where several sites host a copy of the material, the site selected as the convenience link should be the one whose general content appears most in line with Wikipedia:Neutral point of view and Wikipedia:Verifiability.

Indicating availability
If your source is not available online, it should be available in libraries, archives, or collections. If a citation without an external link is challenged as unavailable, any of the following is sufficient to show the material to be reasonably available (though not necessarily reliable): providing an ISBN or OCLC number; linking to an established Wikipedia article about the source (the work, its author, or its publisher); or directly quoting the material on the talk page, briefly and in context.

Links to sources
For a source available in hardcopy, microform, and/or online, omit, in most cases, which one you read. While it is useful to cite author, title, edition (1st, 2nd, etc.), and similar information, it generally is not important to cite a database such as ProQuest, EBSCOhost, or JSTOR (see the list of academic databases and search engines) or to link to such a database requiring a subscription or a third party's login. The basic bibliographic information you provide should be enough to search for the source in any of these databases that have the source. Don't add a URL that has a part of a password embedded in the URL. However, you may provide the DOI, ISBN, or another uniform identifier, if available. If the publisher offers a link to the source or its abstract that does not require a payment or a third party's login for access, you may provide the URL for that link. If the source only exists online, give the link even if access is restricted (see WP:PAYWALL).

Preventing and repairing dead links
To help prevent dead links, persistent identifiers are available for some sources. Some journal articles have a digital object identifier (DOI); some online newspapers and blogs, and also Wikipedia, have permalinks that are stable. When permanent links aren't available, consider making an archived copy of the cited document when writing the article; on-demand web archiving services such as the Wayback Machine (https://web.archive.org/save) or archive.today (https://archive.today) are fairly easy to use (see pre-emptive archiving).
Do not delete a citation merely because the URL is not working. Dead links should be repaired or replaced if possible. If you encounter a dead URL being used as a reliable source to support article content, follow these steps prior to deleting it:

Confirm status: First, check the link to confirm that it is dead and not temporarily down. Search the website to see whether it has been rearranged. The online service "Is it down right now?" can help to determine if a site is down, and any information known.
Check for a changed URL on the same Web site: Pages are frequently moved to different locations on the same site as they become archive content rather than news. The site's error page may have a "Search" box; alternatively, in both the Google and DuckDuckGo search engines – among others – the keyterm "site:" can be used. For instance: site:en.wikipedia.org "New Zealand police vehicle markings and livery".
Check for web archives: Many web archiving services exist (for a full list, see: Wikipedia:List of web archives on Wikipedia); link to their archive of the URL's content, if available. Examples:
Internet Archive has billions of archived web pages. See Wikipedia:Using the Wayback Machine.
archive.today See Wikipedia:Using archive.today
The UK Government Web Archive (https://www.nationalarchives.gov.uk/webarchive/) preserves 1500 UK central government websites.
The Mementos interface allows you to search multiple archiving services with a single request using the Memento protocol. Unfortunately, the Mementos web page interface removes any parameters which are passed with the URL. If the URL contains a "?" it is unlikely to work properly. When entering the URL into the Mementos interface manually, the most common change needed is to change "?" to "%3F". While making only this change will not be sufficient in all cases, it will work most of the time. The bookmarklet in the table below will properly encode URLs such that searches will work.If multiple archive dates are available, try to use one that is most likely to be the contents of the page seen by the editor who entered the reference on the |access-date=. If that parameter is not specified, a search of the article's revision history can be performed to determine when the link was added to the article.
For most citation templates, archive locations are entered using the |archive-url=, |archive-date= and |url-status= parameters. The primary link is switched to the archive link when |url-status=dead. This retains the original link location for reference.
If the web page now leads to a completely different website, set |url-status=usurped to hide the original website link in the citation.
Note: Some archives currently operate with a delay of ~18 months before a link is made public. As a result, editors should wait ~24 months after the link is first tagged as dead before declaring that no web archive exists. Dead URLs to reliable sources should normally be tagged with {{dead link|date=November 2023}}, so that you can estimate how long the link has been dead.
Bookmarklets to check common archive sites for archives of the current page:
Archive.org
javascript:void(window.open('https://web.archive.org/web/*/'+location.href))
archive.today / archive.is
javascript:void(window.open('https://archive.today/'+location.href))
Mementos interface
javascript:void(window.open('https://www.webarchive.org.uk/mementos/search/'+encodeURIComponent(location.href)+'?referrer='+encodeURIComponent(document.referrer)))Remove convenience links: If the material was published on paper (e.g., academic journal, newspaper article, magazine, book), then the dead URL is not necessary. Simply remove the dead URL, leaving the remainder of the reference intact.
Find a replacement source: Search the web for quoted text, the article title, and parts of the URL. Consider contacting the website/person that originally published the reference and asking them to republish it. Ask other editors for help finding the reference somewhere else, including the user who added the reference. Find a different source that says essentially the same thing as the reference in question.
Remove hopelessly-lost web-only sources: If the source material does not exist offline, and if there is no archived version of the web page (be sure to wait ~24 months), and if you cannot find another copy of the material, then the dead citation should be removed and the material it supports should be regarded as unverified if there is no other supporting citation. If it is material that is specifically required by policy to have an inline citation, then please consider tagging it with {{citation needed}}. It may be appropriate for you to move the citation to the talk page with an explanation, and notify the editor who added the now-dead link.

When using inline citations, it is important to maintain text–source integrity. The point of an inline citation is to allow readers and other editors to see which part of the material is supported by the citation; that point is lost if the citation is not clearly placed. The distance between material and its source is a matter of editorial judgment, but adding text without clearly placing its source may lead to allegations of original research, of violations of the sourcing policy, and even of plagiarism.

Keeping citations close
Editors should exercise caution when rearranging or inserting material to ensure that text–source relationships are maintained. References should not be moved if doing so might break the text–source relationship.
If a sentence or paragraph is footnoted with a source, adding new material that is not supported by the existing source to the sentence/paragraph, without a source for the new text, is highly misleading if placed to appear that the cited source supports it. When new text is inserted into a paragraph, make sure it is supported by the existing or a new source. For example, when editing text originally reading

The sun is pretty big.Notes

an edit that does not imply that the new material is supported by the same reference is

The sun is pretty big. The sun is also quite hot.Notes

Do not add other facts or assertions into a fully cited paragraph or sentence:

N
The sun is pretty big, but the moon is not so big. The sun is also quite hot.Notes

Include a source to support the new information. There are several ways to write this, including:

Y
The sun is pretty big, but the moon is not so big. The sun is also quite hot.Notes

Citation order
There is no consensus for a specific ordering of citations, and editors should not edit-war over it, nor make mass changes of ordering to suit personal preferences. In particular, references need not be moved solely to maintain the numerical order of footnotes as they appear in the article.

Bundling citations
Sometimes the article is more readable if multiple citations are bundled into a single footnote. For example, when there are multiple sources for a given sentence, and each source applies to the entire sentence, the sources can be placed at the end of the sentence, like this:[4][5][6][7] Or they can be bundled into one footnote at the end of the sentence or paragraph, like this:[4]Bundling is also useful if the sources each support a different portion of the preceding text, or if the sources all support the same text. Bundling has several advantages:

It helps readers and other editors see at a glance which source supports which point, maintaining text–source integrity;
It avoids the visual clutter of multiple clickable footnotes inside a sentence or paragraph;
It avoids the confusion of having multiple sources listed separately after sentences, with no indication of which source to check for each part of the text, such as this.[1][2][3][4]
It makes it less likely that inline citations will be moved inadvertently when text is re-arranged, because the footnote states clearly which source supports which point.To concatenate multiple citations for the same content into a single footnote, there are several layouts available, as illustrated below:

Simply using line breaks to separate list items breaches MOS:Accessibility § Nobreaks: "<br /> line breaks ... should not be used." {{Unbulleted list citebundle}} a.k.a. {{Multiref}} was made specifically for this purpose. Some other templates in the same vein are listed at the disambiguation page Template:Multiple references.
Within a given article only a single layout should generally be used, except that inline may always be appropriate for shortened references, often all for the same statement:

In-text attribution is the attribution inside a sentence of material to its source, in addition to an inline citation after the sentence. In-text attribution may need to be used with direct speech (a source's words between quotation marks or as a block quotation); indirect speech (a source's words modified without quotation marks); and close paraphrasing. It may also be used when loosely summarizing a source's position in your own words, and it should always be used for biased statements of opinion. It avoids inadvertent plagiarism and helps the reader see where a position is coming from. An inline citation should follow the attribution, usually at the end of the sentence or paragraph in question.
For example:

N To reach fair decisions, parties must consider matters as if behind a veil of ignorance.[2] 
Y John Rawls argues that, to reach fair decisions, parties must consider matters as if behind a veil of ignorance.[2] 
Y John Rawls argues that, to reach fair decisions, parties must consider matters as if "situated behind a veil of ignorance".[2] 
When using in-text attribution, make sure it doesn't lead to an inadvertent neutrality violation. For example, the following implies parity between the sources, without making clear that the position of Darwin is the majority view:

N Charles Darwin says that human beings evolved through natural selection, but John Smith writes that we arrived here in pods from Mars.
Y Humans evolved through natural selection, as first explained in Charles Darwin's The Descent of Man, and Selection in Relation to Sex.
Neutrality issues apart, there are other ways in-text attribution can mislead. The sentence below suggests The New York Times has alone made this important discovery:

N According to The New York Times, the sun will set in the west this evening.
Y The sun sets in the west each evening.
It is preferable not to clutter articles with information best left to the references. Interested readers can click on the ref to find out the publishing journal:

N In an article published in The Lancet in 2012, researchers announced the discovery of the new tissue type.[3] 
Y The discovery of the new tissue type was first published by researchers in 2012.[3] 
Simple facts such as this can have inline citations to reliable sources as an aid to the reader, but normally the text itself is best left as a plain statement without in-text attribution:

Y By mass, oxygen is the third most abundant element in the universe after hydrogen and helium.[4] 

If an article has no references at all, then:

If the entire article is "Patent Nonsense", tag it for speedy deletion using criterion G1.
If the article is a biography of a living person, it can be tagged with {{subst:prod blp}} to propose deletion. If it's a biography of a living person and is an attack page, then it should be tagged for speedy deletion using criterion G10, which will blank the page.
If the article doesn't fit into the above two categories, then consider finding references yourself, or commenting on the article talk page or the talk page of the article creator. You may also tag the article with the {{unreferenced}} template and consider nominating it for deletion.For individual claims in an article not supported by a reference:

If the article is a biography of a living person, then any contentious material must be removed immediately: see Biographies of living persons. If the material lacking reference is seriously inappropriate, it may need to be hidden from general view, in which case request admin assistance.
If the material added appears to be false or an expression of opinion, remove it and inform the editor who added the unsourced material. The {{uw-unsourced1}} template may be placed on their talk page.
In any other case consider finding references yourself, or commenting on the article talk page or the talk page of the editor who added the unsourced material. You may place a {{citation needed}} or {{dubious}} tag against the added text.

Citation templates can be used to format citations in a consistent way. The use of citation templates is neither encouraged nor discouraged: an article should not be switched between templated and non-templated citations without good reason and consensus – see "Variation in citation methods", above.
If citation templates are used in an article, the parameters should be accurate. It is inappropriate to set parameters to false values to cause the template to render as if it were written in some style other than the style normally produced by the template (e.g., MLA style).

Metadata
Citations may be accompanied by metadata, though it is not mandatory. Most citation templates on Wikipedia use the COinS standard. Metadata such as this allow browser plugins and other automated software to make citation data accessible to the user, for instance by providing links to their library's online copies of the cited works. In articles that format citations manually, metadata may be added manually in a span, according to the COinS specification.

Citation generation tools
The Wikipedia Visual Editor now helps users format, insert and edit sources by simply providing a DOI, URL, ISBN etc.
Citer is an all-purpose tool that generates complete scientific citations
User:Ark25/RefScript, a JavaScript bookmarklet – creates references in one click, works for many newspapers
User:V111P/js/WebRef, a script or bookmarklet automating the filling of the {{cite web}} template. You use the script on the page you want to cite.
User:Badgettrg, Biomedical citation maker. uses Pubmed ID (PMID) or DOI or PMCID or NCT. Adds links to ACP Journal Club and Evidence-Based Medicine comments if present.
WP:ReFill – adds titles to bare url references and other cleanup
Template:Ref info, which can aid evaluating what kind of citation style was used to write the article
Based on Citoid:
Cite templates in Visual Editor
User:Salix alba/Citoid a client for the mw:citoid server which generates Citation Style 1 templates from urls.
RefTag:
Reftag for DOI
Reftag for New York Times
Wikipedia DOI and Google Books Citation Maker
Hosted on tools.wmflabs.org:
Wikipedia:refToolbar 2.0, used in the Source Editor
Citation bot
Yadkard: A web-based tool for generating shortened footnotes and citation using Google Books URLs, DOI or ISBN. Also supports some news websites.
Wikipedia template filling – generates Vancouver style citations from PMIDs (PubMed IDs).

Duplicate reference finders
Finding duplicate references by examining reference lists is difficult. There are some tools that can help:

AutoWikiBrowser (AWB) will identify and (usually) correct exact duplicates between <ref>...</ref> tags. See the documentation.
URL Extractor For Web Pages and Text can identify Web citations with the exact same URL but otherwise possibly different. Most differences are not significant, but sometimes different page numbers from the same URL are cited. Occasionally references to the same Web page might be followed by different non-significant tracking parameters (?utm ..., #ixzz...), and will not be listed as duplicates.
Step 1: click "Enter URL", enter (paste) the URL of the Wikipedia article and click "Load",
Step 2: tick "Only Display duplicate URL addresses" (which unticks "Remove duplicate addresses")
Optional: Tick the radio button "Do not show", tick the box at the beginning of its line, and enter (paste) into the box web.archive.org,wikipedia,wikimedia,wikiquote,wikidata
Step 3: Click Extract.
Then the duplicates will be listed, and must be manually merged. There will often be false positives; web.archive.org URLs, in particular, are a nuisance as they contain the original URLs, which show as duplicates. The optional part of Step 2 eliminates the archive URLs, but unfortunately the list of duplicates includes the archived pages. The wiki* URLs are less of a problem as they can just be ignored.

Programming tools
Wikicite is a free program that helps editors to create citations for their Wikipedia contributions using citation templates. It is written in Visual Basic .NET, making it suitable only for users with the .NET Framework installed on Windows, or, for other platforms, the Mono alternative framework. Wikicite and its source code is freely available; see the developer's page for further details.
User:Richiez has tools to automatically handle citations for a whole article at a time. Converts occurrences of {{pmid|XXXXXXXX}} (PubMed) or {{isbn|XXXXXXXXXX}} to properly formatted footnote or Harvard-style references. Written in Ruby and requires a working installation with basic libraries.
pubmed2wikipedia.xsl an XSL stylesheet transforming the XML output of PubMed to Wikipedia refs.

Reference management software
Reference management software can output formatted citations in several styles, including BibTeX, RIS, or Wikipedia citation template styles.

Comparison of reference management software – side-by-side comparison of various reference management software
Wikipedia:Citing sources with Zotero – essay on using Zotero to quickly add citations to articles. Zotero (by Roy Rosenzweig Center for History and New Media; license: Affero GPL) is open-source software with local reference database which can be synchronized between several computers over the online database (up to 300 MB without payment).
EndNote (by Thomson Reuters; license: proprietary)
Mendeley (by Elsevier; license: proprietary)
Paperpile (by Paperpile, LLC; license: proprietary)
Papers (by Springer; license: proprietary)
How to cite

Wikipedia:References dos and don'ts – a concise summary of some of the most important guidance on this page
Help:Referencing for beginners – a simple practical guide to getting started
Help:How to mine a source – how-to guide on getting maximum information from cited material
Wikipedia:Verification methods – listing examples of the most common ways that citations are used in Wikipedia articles
Wikipedia:Improving referencing efforts – essay on why references are important
Wikipedia:Citation templates – a full listing of various styles for citing all sorts of materials
Wikipedia:Citing sources/Example edits for different methods – showing comparative edit mode representations for different citation methods and techniques
Wikipedia:Citing sources/Further considerations – additional considerations for citing sources
Wikipedia:Inline citation – more information on inline citations
Wikipedia:Nesting footnotes – how-to guide on "nesting" footnotes
Wikipedia:Manual of Style/Layout § Further reading – for information about the "Further reading" section
Wikipedia:External links – for information about the "External links" section
Wikipedia:Plagiarism § Public-domain sources – guideline covering the inclusion of material in the public domain
Wikipedia:Scientific citation guidelines – guidelines for dealing with scientific and mathematical articles
Wikipedia:WikiProject Resource Exchange/Shared Resources – project guide on finding resources
MediaWiki:Extension:Cite – details of the software which support the <ref> parser hooksCitation problems

Template:Irrelevant citation – inline template to note source simply is not relevant to the material
Template:More citations needed – template to add to article (or section) where citations are insufficient
Template:Text-source – template to add to article (or section) where text–source integrity is questioned
Wikipedia:Citation needed – explanation of template to tag statements that need a citation
Wikipedia:Citation overkill – why too many citations on one fact can be a bad thing
Wikipedia:Copyright problems – in case of text that has been copied verbatim inappropriately
Wikipedia:Link rot – guide to preventing link rot
Wikipedia:More seasoning doesn't mean more flavor – an essay about how less detail doesn't always mean less info
Wikipedia:You don't need to cite that the sky is blue – an essay advising: do not cite already obvious information
Wikipedia:You do need to cite that the sky is blue – an essay advising: just because something appears obvious to you does not mean it is obvious to everyone
Wikipedia:Video links – an essay discussing the use of citations linking to YouTube and other user-submitted video sites
Wikipedia:WikiProject Citation cleanup – a group of people devoted to cleaning up citations
Wikipedia:Reference database – essay/proposalChanging citation style formats

WP:CITEVAR

"Online Style Guide". New Oxford Style Manual. Oxford, UK: Oxford University Press. 2016. ISBN 978-0-19-876725-1.
The Chicago Manual of Style (17th ed.). Chicago: University of Chicago Press. 2017. ISBN 978-0-226-28705-8.
"Academic Writing: Citing Sources". Writers Workshop. University of Illinois.
"Citation Style Guides & Management Tools". Library Guides. LIU Post.
"Citing: Help & how-to". Concordia University Library.
"Citation Help". Subject Guides. University of Iowa.
"Guide to Citation Style Guides". Journalism Resources. University of Iowa.
"Library: Citing Sources & Citation Generators". Capital Community College.
"Research and Citation Resources". Online Writing Lab. Purdue University.
"The Writer's Handbook: Documentation". Writing Center. University of Wisconsin–Madison.
"ACS Style Guide". Research Guides. University of Wisconsin–Madison.
"Samples of Formatted References for Authors of Journal Articles". MEDLINE and PubMed: The Resources Guide. United States National Library of Medicine. 26 April 2018.

"reFill". Toolforge. WP:ReFill. – tool that expands bare references semi-automatically
Wikipedia editing basics: Citing sources (part 1) (YouTube). Wikimedia Foundation.
Wikipedia editing basics: Citing sources (part 2) (YouTube). Wikimedia Foundation.

Wikipedia:Criticism

All encyclopedic content on Wikipedia must adhere to neutral point of view (NPOV), which means representing fairly, proportionately, and, as far as possible, without editorial bias, all the significant views that have been published by reliable sources on a topic. If significant views include negative criticism of the article subject published in reliable sources, then they should be incorporated into the article content in an appropriate and neutral way. 
In most cases separate sections devoted to criticism, controversies, or the like should be avoided in an article because these sections call undue attention to negative viewpoints. Articles should present the prevailing viewpoints from reliable sources, whether positive or negative. Segregation of text or other content into different subsections, based solely on the apparent POV of the content itself, may result in an unencyclopedic structure, such as a back-and-forth dialogue between proponents and opponents. There is no requirement to include criticism or controversies in an article. 
Wikipedia's policy on biographies of living persons requires exercising special care in presenting negative viewpoints about living persons.

Neutrality and verifiability
Most problems with negative material can be avoided by adhering to standard WP policies, such as using good sources, balancing the content carefully, and writing in an unbiased way. When including negative material in an article, some things to check for include:

Ensure that the material is supported by reliable sources
Do not present the material in a way that emphasizes beyond the emphasis given in reliable sources.
The prominence and proportion of coverage on negative or positive materials should reflect those of what is published in reliable sources. Prominence among Wikipedia editors or the general public are irrelevant.
Always present positive viewpoints along with any negative information to give balance in proportion to reliable sources without giving undue weight to one viewpoint.
When presenting negative material, it is often best to name the source of the criticism within the paragraph or sentence, so that the criticism is not presented in the encyclopedia's voice.
Integrate negative material into sections that cover all viewpoints of the event, product, or policy that is being criticized, rather than in a dedicated "criticism" section.

Living persons
Negative material about living persons may violate privacy policies or damage the person's reputation; therefore, strict rules are in place to govern such information.  See Biographies of living persons for details.

Amount and presentation of criticism: other related guidance
No undue weight should be given to criticism. Some policies and guidelines that help determine the amount and presentation (or: weight) of criticism in an article:

WP:BALASPS: the weight a Wikipedia article gives to criticism of its subject should be proportionate to the overall weight of such criticisms in reliable sources on the subject of the article.
WP:POVFORK: don't split off articles with the purpose of purging a Wikipedia article of its legitimate criticism.
WP:ABOUTSELF and WP:PRIMARY: even if third-party reliable sources are generally negative about a topic this shouldn't impede devoting sufficient space to a fair description of the topic, for instance (partially) based on primary or self-published sources, within the limits of policy.
Specific guidelines like WP:FRINGE may instruct how to handle criticism in certain areas.The list of suggestions above is not comprehensive, it shows a few directions where additional guidance may be found.

Avoid sections and articles focusing on criticisms or controversies
An article dedicated to negative criticism of a topic, as well as one dedicated to accolades and praises is usually discouraged because it tends to be a point-of-view fork, which is generally prohibited by the neutral point-of-view policy.
Likewise, the article structure must protect neutrality. Sections within an article dedicated to negative criticisms are normally also discouraged.   Topical or thematic sections are frequently superior to sections devoted to criticism.  Other than for articles about particular worldviews, philosophies or religious topics etc. where different considerations apply (see below), best practice is to incorporate positive and negative material into the same section.  For example, if a politician received significant criticism about their public image, create a section entitled "Public image" or "Public profile", and include all related information—positive and negative—within that section.  If a book was heavily criticized, create a section in the book's article called "Reception", and include positive and negative material in that section. 
Articles on artists and works by artists often include material describing the opinions of critics, peers, and reviewers.  Although the term "criticism" can, in that context, include both positive and negative  assessment, the word  "Criticism" or "Accolades" should be avoided in section titles because it may convey a biased connotation to many readers. Alternative section titles which avoid a negative connotation include "Reception", "Reviews", "Responses", "Reactions", "Critiques", and "Assessments".
In some situations the term "criticism" may be appropriate in an article or section title, for example, if there is a large body of critical material, and if independent secondary sources comment, analyze or discuss the critical material. 
Sections or article titles should generally not include the word "controversies". Instead, titles should simply name the event, for example,  "2009 boycott" or "Hunting incident". The word "controversy" should not appear in the title except in the rare situations when it has become part of the commonly accepted name for the event, such as Creation–evolution controversy.
Criticisms and controversies are two distinct concepts, and they should not be commingled. Criticisms are specific appraisals or assessments, whereas controversies are protracted public disputes, with opposing opinions rather than universal disapproval. Thus, sections such as "Criticisms and controversies" are generally inappropriate.

When an article gets too large
The best approach to including negative criticism is to integrate it into the primary article on the topic.  Sometimes that will cause the article to get too large, in which case the article should be split using the WP splitting guidelines. The preferred way to split an article is as a content fork into subarticles, using a "main" template to link to the new subarticles (related guidance: WP:SPINOFF). Generally, new subarticles should not be devoted to criticism, controversies, or other specific viewpoints but should instead focus on topical themes.

Organizations and corporations
Many organizations and corporations are involved in well-documented controversies or may be subject to significant criticism. If reliable sources – other than the critics themselves – provide substantial coverage devoted to the controversies or criticisms, then sections and subarticles about them may be justified, but only within the limitations of WP:BLPGROUPS. 
Example: the sources that discuss the 2008 Summer Olympics often describe its controversies in detail, as an independent topic. As the main article is very long and it was deemed unpractical to integrate all the controversy material into the main article: the summary style guideline was used to create a subarticle Concerns and controversies over the 2008 Summer Olympics, while a summary overview of the controversies is retained in the main article.

Philosophy, religion, or politics
For topics about a particular point of view – such as philosophies (Idealism, Naturalism, Existentialism), political outlooks (Capitalism, Marxism), or religion (Islam, Christianity, Atheism) – it will usually be appropriate to have a "Criticism" section or "Criticism of ..." subarticle. Integrating criticism into the main article can cause confusion because readers may misconstrue the critical material as representative of the philosophy's outlook,  the political stance, or the religion's tenets.

Approaches to incorporating controversy and criticism are as follows:

Integrated throughout the article
Often the best approach to incorporating negative criticism into the encyclopedia is to integrate it into the article, in a way that does not disrupt the article's flow. The article should be divided into sections based on topics, timeline, or theme – not viewpoint. Negative criticism should be interwoven throughout the topical or thematic sections. However, for example, when the structure of an article is timeline-based "criticism" can't precede the genesis history of the subject (except possibly for a mentioning in the lede).

"Reception" type section
An acceptable approach to including criticisms in Wikipedia articles is to separate the description of a topic from a description of how the topic was received. Suitable section titles, depending on case, include: "Reception", "Response", "Reviews" and "Reactions". These sections include both negative and positive assessments. This approach usually conforms to the WP neutrality policy, because it avoids being "all negative" or "exclusively laudatory" about the topic.

"Controversy" section
For a specific controversy regarding the topic, when such topic takes a prominent place in the reliable sources on the topic. "Controversy" is not necessarily part of the name of such a section (e.g. Antibiotics#Misuse, Rick Ross (consultant)#Jason Scott deprogramming). Avoid mixed bag section titles like "Controversies" without it being clear in the section title (or in the titles of the subsections of such section) what these controversies are about. If the content of such a section is of the "mixed bag" kind, the section should be handled as a trivia section (see Wikipedia:Manual of Style/Trivia sections).
As of October 2022 about 33,000 articles have controversy sections.

"Criticism" section
A section dedicated to negative material is sometimes appropriate, if the sources treat the negative material as an organic whole, and if readers would be better served by seeing all the negative material in one location. However, sections dedicated to negative material may violate the NPOV policy and may be a troll magnet, which can be harmful if it leads to users with strong opinions dominating the article but may simplify maintenance of the article if unhelpful edits are limited to a single section. In 2006, Jimbo Wales weighed in on the question: "In many cases [criticism sections] are necessary, and in many cases they are not necessary. And I agree with the view expressed by others that often, they are a symptom of bad writing. That is, it isn't that we should not include the criticisms, but that the information should be properly incorporated throughout the article rather than having a troll magnet section of random criticisms."Many criticism sections found in articles are present because editors collected negative material, but have not had the time to properly integrate the negative material into the other sections of the article. Such negative sections should be tagged with a {{POV-section}} or {{criticism-section}} to notify other editors that more work is needed to integrate the material.
Sometimes a section is created to describe a significant criticism made by a notable critic. In these situations, the section title should be something like "View of Maria Smith" or "Reaction of the NY Times", and should avoid the word "criticism" in the section title.

"Accolades" section
Similarly, sections dedicated to positive material may violate the NPOV policy by causing a distortion, albeit in the opposite direction and maybe a promotional editing and public relations editing magnet especially in articles on people, products, businesses and organizations.

Reception history articles
A dedicated "Reception history"  or "History of criticism" article may be acceptable for certain literary, historical, or artistic topics, if  the sources justify it. Such articles should describe the historical progression of the criticism, as well as documenting both the positive and negative criticisms. The "main" article should have a summary style type of section summarizing the "reception history", and properly linking to the subsidiary article (for the Tacitean studies example this is the "Studies and reception history" section in the Tacitus article).

Separate articles devoted to controversies
Articles dedicated to controversies about a topic are generally discouraged, for many of the same reasons discussed for criticism-related material.  Articles dedicated to a controversy may be appropriate if the reliable sources on the topic discuss the controversies as an independent topic.  Examples of articles devoted to a controversy include Global warming controversy, 2008 Olympics controversies, Chiropractic controversy and criticism, and Scientology controversies.

Separate articles devoted to criticism
Creating separate articles with the sole purpose of grouping the criticisms or to elaborate individual points of criticism on a certain topic is generally considered a POV fork.   Wikipedia:Content forking states that "Wikipedia articles should not be split into multiple articles solely so each can advocate a different stance on the subject." For example, the "Criticism" section of Al Gore should not be moved to a separate article such as "Criticism of Al Gore".   Dedicated "Criticism of ..." articles are sometimes appropriate for organizations, businesses, philosophies, religions, or political outlooks, provided the sources justify it; see the "Philosophy, religion, or politics" section above for details.

Essays
Wikipedia:Avoid thread mode
Wikipedia:Be neutral in form
Wikipedia:Controversial articles
Wikipedia:Copyediting reception sections
Wikipedia:Don't "teach the controversy" (the phrase doesn't mean what you think it does)
Wikipedia:Criticisms of society may be consistent with NPOV and reliability
Wikipedia:Pro and con lists

Policy and content
Wikipedia:Neutral point of view § Article structure
Category:Criticisms
Special:PrefixIndex/Criticism of

intitle:"criticism of" site:en.wikipedia.org – Google search for "Criticism of ..." within Wikipedia

Wikipedia:Neutral point of view

All encyclopedic content on Wikipedia must be written from a neutral point of view (NPOV), which means representing fairly, proportionately, and, as far as possible, without editorial bias, all the significant views that have been published by reliable sources on a topic.
NPOV is a fundamental principle of Wikipedia and of other Wikimedia projects. It is also one of Wikipedia's three core content policies; the other two are "Verifiability" and "No original research". These policies jointly determine the type and quality of material acceptable in Wikipedia articles, and because they work in harmony, they should not be interpreted in isolation from one another. Editors are strongly encouraged to familiarize themselves with all three.
This policy is non-negotiable, and the principles upon which it is based cannot be superseded by other policies or guidelines, nor by editor consensus.

Achieving what the Wikipedia community understands as neutrality means carefully and critically analyzing a variety of reliable sources and then attempting to convey to the reader the information contained in them fairly, proportionately, and as far as possible without editorial bias. Wikipedia aims to describe disputes, but not engage in them. The aim is to inform, not influence. Editors, while naturally having their own points of view, should strive in good faith to provide complete information and not to promote one particular point of view over another. As such, the neutral point of view does not mean the exclusion of certain points of view; rather, it means including all verifiable points of view which have sufficient due weight. Observe the following principles to help achieve the level of neutrality that is appropriate for an encyclopedia:

Avoid stating opinions as facts. Usually, articles will contain information about the significant opinions that have been expressed about their subjects. However, these opinions should not be stated in Wikipedia's voice. Rather, they should be attributed in the text to particular sources, or where justified, described as widespread views, etc. For example, an article should not state that "genocide is an evil action" but may state that "genocide has been described by John So-and-so as the epitome of human evil."
Avoid stating seriously contested assertions as facts. If different reliable sources make conflicting assertions about a matter, treat these assertions as opinions rather than facts, and do not present them as direct statements.
Avoid stating facts as opinions. Uncontested and uncontroversial factual assertions made by reliable sources should normally be directly stated in Wikipedia's voice. Unless a topic specifically deals with a disagreement over otherwise uncontested information, there is no need for specific attribution for the assertion, although it is helpful to add a reference link to the source in support of verifiability. Further, the passage should not be worded in any way that makes it appear to be contested.
Prefer nonjudgmental language. A neutral point of view neither sympathizes with nor disparages its subject (or what reliable sources say about the subject), although this must sometimes be balanced against clarity. Present opinions and conflicting findings in a disinterested tone. Do not editorialize. When editorial bias towards one particular point of view can be detected the article needs to be fixed. The only bias that should be evident is the bias attributed to the source.
Indicate the relative prominence of opposing views. Ensure that the reporting of different views on a subject adequately reflects the relative levels of support for those views and that it does not give a false impression of parity, or give undue weight to a particular view. For example, to state that "According to Simon Wiesenthal, the Holocaust was a program of extermination of the Jewish people in Germany, but David Irving disputes this analysis" would be to give apparent parity between the supermajority view and a tiny minority view by assigning each to a single activist in the field.

See the NPOV tutorial and NPOV examples.Generally, do not remove sourced information from the encyclopedia solely because it seems biased. Instead, try to rewrite the passage or section to achieve a more neutral tone. Biased information can usually be balanced with material cited to other sources to produce a more neutral perspective, so such problems should be fixed when possible through the normal editing process. Remove material when you have a good reason to believe it misinforms or misleads readers in ways that cannot be addressed by rewriting the passage. The sections below offer specific guidance on common problems.

Naming
In some cases, the name chosen for a topic can give an appearance of bias. While neutral terms are generally preferable, this must be balanced against clarity. If a name is widely used in reliable sources (particularly those written in English) and is therefore likely to be well recognized by readers, it may be used even though some may regard it as biased. For example, the widely used names "Boston Massacre", "Teapot Dome scandal", and "Jack the Ripper" are legitimate ways of referring to the subjects in question, even though they may appear to pass judgment. The best name to use for a topic may depend on the context in which it is mentioned; it may be appropriate to mention alternative names and the controversies over their use, particularly when the topic in question is the main topic being discussed.
This advice especially applies to article titles. Although multiple terms may be in common usage, a single name should be chosen as the article title, in line with the article titling policy (and relevant guidelines such as on geographical names). Article titles that combine alternative names are discouraged. For example, "Derry/Londonderry", "Aluminium/Aluminum", or "Flat Earth (Round Earth)" should not be used. Instead, alternative names should be given their due prominence within the article itself, and redirects created as appropriate.
Some article titles are descriptive rather than being a name. Descriptive titles should be worded neutrally, so as not to suggest a viewpoint for or against a topic, or to confine the content of the article to views on a particular side of an issue (for example, an article titled "Criticisms of X" might be better renamed "Societal views on X"). Neutral titles encourage multiple viewpoints and responsible article writing.

Article structure
The internal structure of an article may require additional attention to protect neutrality and to avoid problems like POV forking and undue weight. Although specific article structures are not, as a rule, prohibited, care must be taken to ensure the overall presentation is broadly neutral.
Segregation of text or other content into different regions or subsections, based solely on the apparent POV of the content itself, may result in an unencyclopedic structure, such as a back-and-forth dialogue between proponents and opponents. It may also create an apparent hierarchy of fact where details in the main passage appear "true" and "undisputed", whereas other, segregated material is deemed "controversial", and therefore more likely to be false. Try to achieve a more neutral text by folding debates into the narrative, rather than isolating them into sections that ignore or fight against each other.
Pay attention to headers, footnotes, or other formatting elements that might unduly favor one point of view or one aspect of the subject, and watch out for structural or stylistic aspects that make it difficult for a reader to fairly and equally assess the credibility of all relevant and related viewpoints.

Due and undue weight
Neutrality requires that mainspace articles and pages fairly represent all significant viewpoints that have been published by reliable sources, in proportion to the prominence of each viewpoint in those sources. Giving due weight and avoiding giving undue weight means articles should not give minority views or aspects as much of or as detailed a description as more widely held views or widely supported aspects. Generally, the views of tiny minorities should not be included at all, except perhaps in a "see also" to an article about those specific views. For example, the article on the Earth does not directly mention modern support for the flat Earth concept, the view of a distinct (and minuscule) minority; to do so would give undue weight to it.
Undue weight can be given in several ways, including but not limited to the depth of detail, the quantity of text, prominence of placement, the juxtaposition of statements, and the use of imagery. In articles specifically relating to a minority viewpoint, such views may receive more attention and space. However, these pages should still appropriately reference the majority viewpoint wherever relevant and must not represent content strictly from the minority view's perspective. Specifically, it should always be clear which parts of the text describe the minority view. In addition, the majority view should be explained sufficiently to let the reader understand how the minority view differs from it, and controversies regarding aspects of the minority view should be clearly identified and explained. How much detail is required depends on the subject. For instance, articles on historical views such as flat Earth, with few or no modern proponents, may briefly state the modern position and then discuss the history of the idea in great detail, neutrally presenting the history of a now-discredited belief. Other minority views may require a much more extensive description of the majority view to avoid misleading the reader. See fringe theories guideline and the NPOV FAQ.
Wikipedia should not present a dispute as if a view held by a small minority is as significant as the majority view. Views held by a tiny minority should not be represented except in articles devoted to those views (such as the flat Earth). Giving undue weight to the view of a significant minority or including that of a tiny minority might be misleading as to the shape of the dispute. Wikipedia aims to present competing views in proportion to their representation in reliable sources on the subject. This rule applies not only to article text but to images, wikilinks, external links, categories, templates, and all other material as well.

Paraphrased from Jimbo Wales' September 2003 post on the WikiEN-l mailing list:
If a viewpoint is in the majority, then it should be easy to substantiate it with references to commonly accepted reference texts;
If a viewpoint is held by a significant minority, then it should be easy to name prominent adherents;
If a viewpoint is held by an extremely small minority, it does not belong on Wikipedia, regardless of whether it is true, or you can prove it, except perhaps in some ancillary article.Keep in mind that, in determining proper weight, we consider a viewpoint's prevalence in reliable sources, not its prevalence among Wikipedia editors or the general public.
If you can prove a theory that few or none believe, Wikipedia is not the place to present such proof. Once it has been presented and discussed in sources that are reliable, it may be appropriately included. See "No original research" and "Verifiability".

Balance
Neutrality assigns weight to viewpoints in proportion to their prominence in reliable sources. However, when reputable sources contradict one another and are relatively equal in prominence, describe both points of view and work for balance. This involves describing the opposing views clearly, drawing on secondary or tertiary sources that describe the disagreement from a disinterested viewpoint.

Balancing aspects
An article should not give undue weight to minor aspects of its subject but should strive to treat each aspect with a weight proportional to its treatment in the body of reliable, published material on the subject. For example, a description of isolated events, quotes, criticisms, or news reports related to one subject may be verifiable and impartial, but still disproportionate to their overall significance to the article topic. This is a concern especially for recent events that may be in the news.

Giving "equal validity" can create a false balance
See: False balance
While it is important to account for all significant viewpoints on any topic, Wikipedia policy does not state or imply that every minority view, fringe theory, or extraordinary claim needs to be presented along with commonly accepted mainstream scholarship as if they were of equal validity. There are many such beliefs in the world, some popular and some little-known: claims that the Earth is flat, that the Knights Templar possessed the Holy Grail, that the Apollo Moon landings were a hoax, and similar ones. Conspiracy theories, pseudoscience, speculative history, or plausible but unaccepted theories should not be legitimized through comparison to accepted academic scholarship. We do not take a stand on these issues as encyclopedia writers, for or against; we merely omit this information where including it would unduly legitimize it, and otherwise include and describe these ideas in their proper context concerning established scholarship and the beliefs of the wider world.

Selecting sources
In principle, all articles should be based on reliable, independent, published sources with a reputation for fact-checking and accuracy. When writing about a topic, basing content on the best respected and most authoritative reliable sources helps to prevent bias, undue weight, and other NPOV disagreements. Try the library for reputable books and journal articles, and look online for the most reliable resources. If you need help finding high-quality sources, ask other editors on the talk page of the article you are working on, or ask at the reference desk.

Bias in sources
A common argument in a dispute about reliable sources is that one source is biased, meaning another source should be given preference. Some editors argue that biased sources should not be used because they introduce improper POV to an article. However, biased sources are not inherently disallowed based on bias alone, although other aspects of the source may make it invalid. A neutral point of view should be achieved by balancing the bias in sources based on the weight of the opinion in reliable sources and not by excluding sources that do not conform to the editor's point of view. This does not mean any biased source must be used; it may well serve an article better to exclude the material altogether.

Wikipedia describes disputes. Wikipedia does not engage in disputes. A neutral characterization of disputes requires presenting viewpoints with a consistently impartial tone; otherwise, articles end up as partisan commentaries even while presenting all relevant points of view. Even where a topic is presented in terms of facts rather than opinions, inappropriate tones can be introduced through how facts are selected, presented, or organized. Neutral articles are written with a tone that provides an unbiased, accurate, and proportionate representation of all positions included in the article.
The tone of Wikipedia articles should be impartial, neither endorsing nor rejecting a particular point of view. Try not to quote directly from participants engaged in a heated dispute; instead, summarize and present the arguments in an impartial, formal tone.

Describing aesthetic opinions and reputations
Wikipedia articles about art and other creative topics (e.g., musicians, actors, books, etc.) have a tendency to become effusive. This is out of place in an encyclopedia. Aesthetic opinions are diverse and subjective—we might not all agree about who the world's greatest soprano is. However, it is appropriate to note how an artist or a work has been received by prominent experts, critics, and the general public. For instance, the article on Shakespeare should note that he is widely considered one of the greatest authors in the English language by both scholars and the general public. It should not, however, state that Shakespeare is the greatest author in the English language. More generally, it is sometimes permissible to note a subject's reputation when that reputation is widespread and potentially informative or of interest to readers. Articles on creative works should provide an overview of their common interpretations, preferably with citations to experts holding those interpretations. Verifiable public and scholarly critiques provide a useful context for works of art.

Words to watch
There are no forbidden words or expressions on Wikipedia, but certain expressions should be used with care because they may introduce bias. For example, the word claim, as in "Jim claimed he paid for the sandwich", could imply a lack of credibility. Using this or other expressions of doubt may make an article appear to promote one position over another. Try to state the facts more simply without using such loaded words; for example, "Jim said he paid for the sandwich". Strive to eliminate flattering expressions, disparaging, vague, or clichéd, or that endorse a particular point of view (unless those expressions are part of a quote from noteworthy sources).

Attributing and specifying biased statements
Biased statements of opinion can be presented only with in-text attribution. For instance, "John Doe is the best baseball player" expresses an opinion and must not be asserted in Wikipedia as if it were a fact. It can be included as a factual statement about the opinion: "John Doe's baseball skills have been praised by baseball insiders such as Al Kaline and Joe Torre." Opinions must still be verifiable and appropriately cited.
Another approach is to specify or substantiate the statement, by giving those details that actually are factual. For example: "John Doe had the highest batting average in the major leagues from 2003 through 2006." People may still argue over whether he was the best baseball player, but they will not argue over this.
Avoid the temptation to rephrase biased or opinion statements with weasel words, for example, "Many people think John Doe is the best baseball player." Which people? How many? ("Most people think" is acceptable only when supported by at least one published survey.)

Point-of-view forks
A POV fork is an attempt to evade the neutrality policy by creating a new article about a subject that is already treated in an article, often to avoid or highlight negative or positive viewpoints or facts. POV forks are not permitted on Wikipedia.
All facts and significant points of view on a given subject should be treated in one article except in the case of a spinoff sub-article. Some topics are so large that one article cannot reasonably cover all facets of the topic, so a spinoff sub-article is created. For example, Evolution as fact and theory is a sub-article of Evolution, and Creation–evolution controversy is a sub-article of Creationism. This type of split is permissible only if written from a neutral point of view and must not be an attempt to evade the consensus process at another article.

Making necessary assumptions
When writing articles, there may be cases where making some assumptions is necessary to get through a topic. For example, in writing about evolution, it is not helpful to hash out the creation-evolution controversy on every page. There are virtually no topics that could proceed without making some assumptions that someone would find controversial. This is true not only in evolutionary biology but also in philosophy, history, physics, etc.
It is difficult to draw up a rule, but the following principle may help: there is probably not a good reason to discuss some assumption on a given page if that assumption is best discussed in-depth on some other page. However, a brief, unobtrusive pointer might be appropriate.

Wikipedia deals with numerous areas that are frequently subjects of intense debate both in the real world and among editors of the encyclopedia. A proper understanding and application of NPOV is sought in all areas of Wikipedia, but it is often needed most in these.

Fringe theories and pseudoscience
Pseudoscientific theories are presented by proponents as science but characteristically fail to adhere to scientific standards and methods. Conversely, by its very nature, scientific consensus is the majority viewpoint of scientists towards a topic. Thus, when talking about pseudoscientific topics, we should not describe these two opposing viewpoints as being equal to each other. While pseudoscience may in some cases be significant to an article, it should not obfuscate the description of the mainstream views of the scientific community.
Any inclusion of fringe or pseudoscientific views should not give them undue weight. The fringe or pseudoscientific view should be clearly described as such. An explanation of how experts in the relevant field have reacted to such views should be prominently included. This helps us to describe differing views fairly. This applies to all types of fringe subjects, for instance, forms of historical negationism that are considered by more reliable sources to either lack evidence or actively ignore evidence, such as claims that Pope John Paul I was murdered, or that the Apollo Moon landings were faked.
See Wikipedia's established pseudoscience guidelines to help decide whether a topic is appropriately classified as pseudoscience.

Religion
In the case of beliefs and practices, Wikipedia content should not only encompass what motivates individuals who hold these beliefs and practices but also account for how such beliefs and practices developed. Wikipedia articles on history and religion draw from religion's sacred texts as primary sources and modern archaeological, historical, and scientific works as secondary and tertiary sources.
Some adherents of a religion might object to a critical historical treatment of their own faith because in their view such analysis discriminates against their religious beliefs. Their point of view can be mentioned if it can be documented by relevant, reliable sources, yet note there is no contradiction. NPOV policy means Wikipedia editors ought to try to write sentences like this: "Certain Frisbeetarianists (such as the Rev. Goodcatch) believe This and That and consider those to have been tenets of Frisbeetarianism from its earliest days. Certain sects who call themselves Ultimate Frisbeetarianists—influenced by the findings of modern historians and archaeologists (such as Dr. Investigate's textual analysis and Prof. Iconoclast's carbon-dating work)—still believe This, but no longer believe That, and instead believe Something Else."
Several words that have very specific meanings in studies of religion have different meanings in less formal contexts, e.g., fundamentalism, mythology, and (as in the prior paragraph) critical. Wikipedia articles about religious topics should take care to use these words only in their formal senses to avoid causing unnecessary offence or misleading the reader. Conversely, editors should not avoid using terminology that has been established by the majority of the current reliable and relevant sources on a topic out of sympathy for a particular point of view or concern that readers may confuse the formal and informal meanings. Details about particular terms can be found at Wikipedia:Manual of Style/Words to watch.

Common objections or concerns raised to Wikipedia's NPOV policy include the following. Since the NPOV policy is often unfamiliar to newcomers—and is so central to Wikipedia's approach—many issues surrounding it have been covered before very extensively. If you have some new contribution to make to the debate, you could try the policy talk page. Before asking, please review the links below.

Being neutral
"There's no such thing as objectivity"
Everybody with any philosophical sophistication knows we all have biases. So, how can we take the NPOV policy seriously?
Lack of neutrality as an excuse to delete
The NPOV policy is sometimes used as an excuse to delete texts that are perceived as biased. Isn't this a problem?
A simple formulation—what does it mean?
A former section of this policy called "A simple formulation" said, "Assert facts, including facts about opinions—but don't assert opinions themselves." What does this mean?

Balancing different views
Writing for the opponent
I'm not convinced by what you say about "writing for the opponent". I don't want to write for the opponents. Most of them rely on stating as fact many demonstrably false statements. Are you saying that to be neutral in writing an article, I must lie to represent the view I disagree with?
Morally offensive views
What about views that are morally offensive to most readers, such as Holocaust denial, that some people actually hold? Surely we are not to be neutral about them?

Editor disputes
Dealing with biased contributors
I agree with the nonbias policy, but there are some here who seem completely, irremediably biased. I have to go around and clean up after them. What do I do?
Avoiding constant disputes
How can we avoid constant and endless warfare over neutrality issues?

Other objections
Anglo-American focus
The English Wikipedia seems to have an Anglo-American focus. Is this contrary to NPOV?
Not answered here
I have some other objection—where should I complain?

"Neutral Point Of View" is one of the oldest governing concepts on Wikipedia. Originally appearing within Nupedia titled "Non-bias policy", it was drafted by Larry Sanger in 2000. Sanger in 2001 suggested that avoiding bias as one of Wikipedia's "rules to consider". This was codified with the objective of the NPOV policy to produce an unbiased encyclopedia. The original NPOV policy statement on Wikipedia was added by Sanger on December 26, 2001. Jimmy Wales has qualified NPOV as "non-negotiable", consistently, throughout various discussions: 2001 statement, November 2003, April 2006, March 2008
No original research (NOR) and verifiability (V) have their origins in the NPOV policy and the problem of dealing with undue weight and fringe theories. The NOR policy was established in 2003 to address problematic uses of sources. The verifiability policy was established in 2003 to ensure the accuracy of articles by encouraging editors to cite sources. Development of the undue-weight section also started in 2003, for which a mailing-list post by Jimmy Wales in September was instrumental.

Policies and guidelines
Conflict of interest
Fringe theories
Words to watch
No original research
Verifiability

Noticeboards
NPOV noticeboard

Information pages
Essays
Articles
Templates
General NPOV templates:
{{POV}}—message used to attract other editors to assess and fix neutrality problems
{{POV section}}—message that tags only a single section as disputed
{{POV lead}}—message when the article's introduction is questionable
{{POV statement}}—message when only one sentence is questionable
{{NPOV language}}—message used when the neutrality of the style of writing is questioned
{{Political POV}}—message when the political neutrality of an article is questioned
{{Fact or opinion}}—message when a sentence may or may not require in-text attribution (e.g., "Jimmy Wales says")
{{Attribution needed}}—when in-text attribution should be added
Undue-weight templates:
{{Undue weight}}—message used to warn that a part of an article lends undue weight to certain ideas relative to the article as a whole
{{Undue weight section}}—same as above but to tag a section only
{{Undue weight inline}}—same as above but to tag a sentence or paragraph only

Wikipedia:Verifiability

In the English Wikipedia, verifiability means other people using the encyclopedia can check that the information comes from a reliable source. Its content is determined by previously published information rather than editors' beliefs, opinions, experiences, or unpublished original research. Even if you are sure something is true, it must have been previously published in a reliable source before you can add it. If reliable sources disagree with each other, then maintain a neutral point of view and present what the various sources say, giving each side its due weight.
All material in Wikipedia mainspace, including everything in articles, lists, and captions, must be verifiable. All quotations, and any material whose verifiability has been challenged or is likely to be challenged, must include an inline citation to a reliable source that directly supports the material. Any material that needs an inline citation but does not have one may be removed. Please immediately remove contentious material about living people that is unsourced or poorly sourced.
For how to write citations, see citing sources. Verifiability, no original research, and neutral point of view are Wikipedia's core content policies. They work together to determine content, so editors should understand the key points of all three. Articles must also comply with the copyright policy.

All content must be verifiable. The burden to demonstrate verifiability lies with the editor who adds or restores material, and it is satisfied by providing an inline citation to a reliable source that directly supports the contribution.Using inline citations, provide reliable, published sources for:

all quotations
all material whose verifiability has been challenged
all material that is likely to be challenged
all contentious matter about living and recently deceased persons.The cited source must clearly support the material as presented in the article. Cite the source clearly, ideally giving page number(s)—though sometimes a section, chapter, or other division may be appropriate instead; see Wikipedia:Citing sources for details of how to do this.
Any material lacking an inline citation to a reliable source that directly supports the material may be removed and should not be restored without an inline citation to a reliable source. Whether and how quickly material should be initially removed for not having an inline citation to a reliable source depends on the material and the overall state of the article. In some cases, editors may object if you remove material without giving them time to provide references. Consider adding a citation needed tag as an interim step. When tagging or removing material for lacking an inline citation, please state your concern that it may not be possible to find a published reliable source, and the material therefore may not be verifiable. If you think the material is verifiable, you are encouraged to provide an inline citation yourself before considering whether to remove or tag it.
Do not leave unsourced or poorly sourced material in an article if it might damage the reputation of living people or existing groups, and do not move it to the talk page. You should also be aware of how Wikipedia:Biographies of living persons also applies to groups.

What counts as a reliable source
A cited source on Wikipedia is often a specific portion of text (such as a short article or a page in a book). But when editors discuss sources (for example, to debate their appropriateness or reliability) the word source has four related meanings:

The work itself (the article, book:  "That book looks like a useful source for this article.") and works like it ("An obituary can be a useful biographical source", "A recent source is better than an old one")
The creator of the work (the writer, journalist:  "What do we know about that source's reputation?") and people like them ("A medical researcher is a better source than a journalist for..").
The publication (for example, the newspaper, journal, magazine:  "That source covers the arts.") and publications like them ("A newspaper is not a reliable source for medical facts").
The publisher of the work (for example, Cambridge University Press:  "That source publishes reference works.") and publishers like them ("An academic publisher is a good source of reference works").All four can affect reliability.
Base articles on reliable, independent, published sources with a reputation for fact-checking and accuracy. Source material must have been published, the definition of which for the purposes of Wikipedia is made available to the public in some form. Unpublished materials are not considered reliable. Use sources that directly support the material presented in an article and are appropriate to the claims made. The appropriateness of any source depends on the context. Be especially careful when sourcing content related to living people or medicine.
If available, academic and peer-reviewed publications are usually the most reliable sources on topics such as history, medicine, and science.
Editors may also use material from reliable non-academic sources, particularly if it appears in respected mainstream publications. Other reliable sources include:

University-level textbooks
Books published by respected publishing houses
Mainstream (non-fringe) magazines, including specialty ones
Reputable newspapersEditors may also use electronic media, subject to the same criteria (see details in Wikipedia:Identifying reliable sources and Wikipedia:Search engine test).

Best sources
The best sources have a professional structure for checking or analyzing facts, legal issues, evidence, and arguments. The greater the degree of scrutiny given to these issues, the more reliable the source.

Newspaper and magazine blogs
Some newspapers, magazines, and other news organizations host online columns they call blogs. These may be acceptable sources if the writers are professionals, but use them with caution because blogs may not be subject to the news organization's normal fact-checking process. If a news organization publishes an opinion piece in a blog, attribute the statement to the writer, e.g. "Jane Smith wrote ..." Never use the blog comments that are left by the readers as sources. For personal or group blogs that are not reliable sources, see § Self-published sources below.

Reliable sources noticeboard and guideline
To discuss the reliability of a specific source for a particular statement, consult Wikipedia:Reliable sources/Noticeboard, which seeks to apply this policy to particular cases. For a guideline discussing the reliability of particular types of sources, see Wikipedia:Reliable sources. In the case of inconsistency between this policy and the Wikipedia:Reliable sources guideline, or any other guideline related to sourcing, this policy has priority.

Questionable sources
Questionable sources are those that have a poor reputation for checking the facts, lack meaningful editorial oversight, or have an apparent conflict of interest.
Such sources include websites and publications expressing views widely considered by other sources to be promotional, extremist, or relying heavily on unsubstantiated gossip, rumor, or personal opinion. Questionable sources should be used only as sources for material on themselves, such as in articles about themselves; see below. They are not suitable sources for contentious claims about others.
Predatory open access journals are considered questionable due to the absence of quality control in the peer-review process.

Self-published sources
Anyone can create a personal web page, self-publish a book, or claim to be an expert. That is why self-published material such as books, patents, newsletters, personal websites, open wikis, personal or group blogs (as distinguished from newsblogs, above), content farms, Internet forum postings, and social media postings are largely not acceptable as sources. Self-published expert sources may be considered reliable when produced by an established subject-matter expert, whose work in the relevant field has previously been published by reliable, independent publications. Exercise caution when using such sources: if the information in question is suitable for inclusion, someone else will probably have published it in independent, reliable sources. Never use self-published sources as third-party sources about living people, even if the author is an expert, well-known professional researcher, or writer.

Self-published or questionable sources as sources on themselves
Self-published and questionable sources may be used as sources of information about themselves, usually in articles about themselves or their activities, without the self-published source requirement that they are established experts in the field, so long as:

the material is neither unduly self-serving nor an exceptional claim;
it does not involve claims about third parties;
it does not involve claims about events not directly related to the source;
there is no reasonable doubt as to its authenticity; and
the article is not based primarily on such sources.This policy also applies to material published by the source on social networking websites such as Twitter, Tumblr, LinkedIn, Reddit, and Facebook.

Wikipedia and sources that mirror or use it
Do not use articles from Wikipedia (whether English Wikipedia or Wikipedias in other languages) as sources, since Wikipedia is a user-generated source. Also, do not use websites mirroring Wikipedia content or publications relying on material from Wikipedia as sources. Content from a Wikipedia article is not considered reliable unless it is backed up by citing reliable sources. Confirm that these sources support the content, then use them directly.An exception is allowed when Wikipedia itself is being discussed in the article. These may cite an article, guideline, discussion, statistic, or other content from Wikipedia (or a sister project) to support a statement about Wikipedia. Wikipedia or the sister project is a primary source in this case and may be used following the policy for primary sources. Any such use should avoid original research, undue emphasis on Wikipedia's role or views, and inappropriate self-reference. The article text should clarify how the material is sourced from Wikipedia to inform the reader about the potential bias.

Access to sources
Do not reject reliable sources just because they are difficult or costly to access. Some reliable sources are not easily accessible. For example, an online source may require payment, and a print-only source may be available only through libraries. Rare historical sources may even be available only in special museum collections and archives. If you have trouble accessing a source, others may be able to do so on your behalf (see WikiProject Resource Exchange).

Non-English sources
Citing
Citations to non-English reliable sources are allowed on the English Wikipedia. However, because this project is in English, English-language sources are preferred over non-English ones when they are available and of equal quality and relevance. As with sources in English, if a dispute arises involving a citation to a non-English source, editors may request a quotation of relevant portions of the original source be provided, either in text, in a footnote, or on the article talk page. (See Template:Request quotation.)

Quoting
If you quote a non-English reliable source (whether in the main text or in a footnote), a translation into English should accompany the quote. Translations published by reliable sources are preferred over translations by Wikipedians, but translations by Wikipedians are preferred over machine translations. When using a machine translation of source material, editors should be reasonably certain that the translation is accurate and the source is appropriate. Editors should not rely upon machine translations of non-English sources in contentious articles or biographies of living people. If needed, ask an editor who can translate it for you.
The original text is usually included with the translated text in articles when translated by Wikipedians, and the translating editor is usually not cited. When quoting any material, whether in English or in some other language, be careful not to violate copyright; see the fair-use guideline.

Verifiability does not guarantee inclusion
While information must be verifiable for inclusion in an article, not all verifiable information must be included. Consensus may determine that certain information does not improve an article. Such information should be omitted or presented instead in a different article. The responsibility for achieving consensus for inclusion is on those seeking to include disputed content.

Tagging a sentence, section, or article
If you want to request an inline citation for an unsourced statement, you can tag a sentence with the {{citation needed}} template by writing {{cn}} or {{fact}}. Other templates exist for tagging sections or entire articles here. You can also leave a note on the talk page asking for a source, or move the material to the talk page and ask for a source there. To request verification that a reference supports the text, tag it with {{verification needed}}. Material that fails verification may be tagged with {{failed verification}} or removed. It helps other editors to explain your rationale for using templates to tag material in the template, edit summary, or on the talk page.
Take special care with contentious material about living and recently deceased people. Unsourced or poorly sourced material that is contentious, especially text that is negative, derogatory, or potentially damaging, should be removed immediately rather than tagged or moved to the talk page.

Exceptional claims require exceptional sources
Any exceptional claim requires multiple high-quality sources. Warnings (red flags) that should prompt extra caution include:

Surprising or apparently important claims not covered by multiple mainstream sources;
Challenged claims that are supported purely by primary or self-published sources or those with an apparent conflict of interest;
Reports of a statement by someone that seems out of character or against an interest they had previously defended;
Claims contradicted by the prevailing view within the relevant community or that would significantly alter mainstream assumptions—especially in science, medicine, history, politics, and biographies of living and recently dead people. This is especially true when proponents say there is a conspiracy to silence them.

Copyright and plagiarism
Do not plagiarize or breach copyright when using sources. Summarize source material in your own words as much as possible; when quoting or closely paraphrasing a source, use an inline citation, and in-text attribution where appropriate.
Do not link to any source that violates the copyrights of others per contributors' rights and obligations. You can link to websites that display copyrighted works as long as the website has licensed the work or uses the work in a way compliant with fair use. Knowingly directing others to material that violates copyright may be considered contributory copyright infringement. If there is reason to think a source violates copyright, do not cite it. This is particularly relevant when linking to sites such as Scribd or YouTube, where due care should be taken to avoid linking to material violating copyright.

Neutrality
Even when information is cited to reliable sources, you must present it with a neutral point of view (NPOV). Articles should be based on thorough research of sources. All articles must adhere to NPOV, fairly representing all majority and significant-minority viewpoints published by reliable sources, in rough proportion to the prominence of each view. Tiny-minority views need not be included, except in articles devoted to them. If there is a disagreement between sources, use in-text attribution: "John Smith argues X, while Paul Jones maintains Y," followed by an inline citation. Sources themselves do not need to maintain a neutral point of view. Indeed, many reliable sources are not neutral. Our job as editors is simply to summarize what reliable sources say.

Notability
If no reliable, independent sources can be found on a topic, Wikipedia should not have an article on it (i.e., the topic is not notable).
However, notability is based on the existence of suitable sources, not on the state of sourcing in an article (WP:NEXIST).

Original research
The no original research policy (NOR) is closely related to the Verifiability policy. Among its requirements are:

All material in Wikipedia articles must be attributable to a reliable published source. This means a reliable published source must exist for it, whether or not it is cited in the article.
Sources must support the material clearly and directly: drawing inferences from multiple sources to advance a novel position is prohibited by the NOR policy.
Base articles largely on reliable secondary sources. While primary sources are appropriate in some cases, relying on them can be problematic. For more information, see the Primary, secondary, and tertiary sources section of the NOR policy, and the Misuse of primary sources section of the BLP policy.

Guidelines
Reliable sources
Identifying reliable sources (medicine)

Information pages
Resources
Essays

Wales, Jimmy. "Insist on sources", WikiEN-l, July 19, 2006: "I really want to encourage a much stronger culture which says: it is better to have no information, than to have information like this, with no sources."—referring to a rather unlikely statement about the founders of Google throwing pies at each other.

Template:Cite book



Template:Cite journal



Template:Cite news



Template:Cite web



Template:Corporate finance



Template:Lean manufacturing tools



Template:Machine industry



Template talk:Corporate finance

Consider making a category for all the finance templates:

--68.239.240.144 17:07, 11 February 2007 (UTC)

Template talk:Lean manufacturing tools



Template talk:Machine industry

I don't know if this template should be transcluding the Industry category into articles. The Industry category is a high-level category, so I don't think the articles in which this template is included in necessary belong in that category. Wizard191 (talk) 21:21, 12 January 2009 (UTC)

Help:Authority control

Authority control is a way of associating a unique identifier to articles on Wikipedia. This is useful to disambiguate different items with similar or identical headings, as well as establish a single standard title for an item that is commonly known by two or more titles. When used, authority control data links can be found near the bottom of Wikipedia pages, linking to bibliographical records on worldwide library catalogs. Authority control is often used in biographical articles because it is quite common for more than one person to share the same name. It is commonly used in other subject material as well.
Authority control enables researchers to search more easily for pertinent information on the subject of an article, without needing to disambiguate the subject manually. For example, authority control is used on music articles so that the information in the article can be easily cross-referenced with popular databases.
More generally, authority control is a method of creating and maintaining index terms for bibliographical material in a library catalog, similar to the Dewey Decimal System. The links produced by the authority control template on Wikipedia go to authority control data in worldwide library catalogs. As an example, the Wikipedia authority control information for Alexander Graham Bell looks like this:

The abbreviations in the box represent the following: International Standard Name Identifier (ISNI); Virtual International Authority File (VIAF); Library of Congress Control Number (LCCN); and Integrated Authority File (GND), Gemeinsame Normdatei in German. WorldCat is a global cooperative union catalog which itemizes the collections of 72,000 libraries in 170 countries and territories.

The following authority files are supported on the English Wikipedia:

Authority control

Category:Articles with BNF identifiers



Category:Articles with BNFdata identifiers



Category:Articles with GND identifiers



Category:Articles with J9U identifiers



Category:Articles with LCCN identifiers



Category:Articles with LNB identifiers



Category:Articles with incomplete citations from March 2016

This category combines all articles with incomplete citations from March 2016 (2016-03) to enable us to work through the backlog more systematically. It is a member of Category:Articles with incomplete citations.

Category:Articles with unsourced statements from July 2022

This category combines all articles with unsourced statements from July 2022 (2022-07) to enable us to work through the backlog more systematically. It is a member of Category:Articles with unsourced statements.
To add an article to this category add     {{Citation needed|date=July 2022}} to the article. If you omit the date a bot will add it for you at some point.

Please help improve an article in this category by adding references to reliable sources that verify content within the article. Once the reliable source references have been added, the unsourced (citation needed) tag can be removed.

Category:Articles with unsourced statements from June 2017

This category combines all articles with unsourced statements from June 2017 (2017-06) to enable us to work through the backlog more systematically. It is a member of Category:Articles with unsourced statements.
To add an article to this category add     {{Citation needed|date=June 2017}} to the article. If you omit the date a bot will add it for you at some point.

Please help improve an article in this category by adding references to reliable sources that verify content within the article. Once the reliable source references have been added, the unsourced (citation needed) tag can be removed.

Category:Articles with unsourced statements from October 2019

This category combines all articles with unsourced statements from October 2019 (2019-10) to enable us to work through the backlog more systematically. It is a member of Category:Articles with unsourced statements.
To add an article to this category add     {{Citation needed|date=October 2019}} to the article. If you omit the date a bot will add it for you at some point.

Please help improve an article in this category by adding references to reliable sources that verify content within the article. Once the reliable source references have been added, the unsourced (citation needed) tag can be removed.


== Progress ==

Category:CS1 maint: archived copy as title

This is a tracking category for CS1 citations that have |title=Archived copy or |title=Archive copy.
Articles are listed in this category when Module:Citation/CS1 identifies template |title= parameters that use these place-holder titles.  'Archived copy' and 'Archive copy' are commonly provided by bots that are unable to identify the source's correct title.  Articles in this category should be corrected by replacing the place-holder titles with actual titles.  Pages in this category should only be added by Module:Citation/CS1.
Pages with this condition are automatically placed in Category:CS1 maint: archived copy as title.
By default, Citation Style 1 and Citation Style 2 error messages are visible to all readers and maintenance messages are hidden from all readers.
To display maintenance messages in the rendered article, include the following text in your common CSS page (common.css) or your specific skin's CSS page and (skin.css).
(Note to new editors: those CSS pages are specific to you, and control your view of pages, by adding to your user account's CSS code. If you have not yet created such a page, then clicking one of the .css links above will yield a page that starts "Wikipedia does not have a user page with this exact name." Click the "Start the User:username/filename page" link, paste the text below, save the page, follow the instructions at the bottom of the new page on bypassing your browser's cache, and finally, in order to see the previously hidden maintenance messages, refresh the page you were editing earlier.)

To display hidden-by-default error messages:

Even with this CSS installed, older pages in Wikipedia's cache may not have been updated to show these error messages even though the page is listed in one of the tracking categories. A null edit will resolve that issue.
To hide normally-displayed error messages:

You can personalize the display of these messages (such as changing the color), but you will need to ask someone who knows CSS or at the technical village pump if you do not understand how.
Nota bene: these css rules are not obeyed by Navigation popups. They also do not hide script warning messages in the Preview box that begins with "This is only a preview; your changes have not yet been saved".


== Notes ==

Category:CS1 maint: multiple names: authors list

This is a hidden tracking category for CS1 citations that use |author=, or its aliases.
Articles are listed in this category when Module:Citation/CS1 identifies cs1|2 citation templates that appear to use singular forms of author name-list parameters to list multiple authors' names.  Doing so corrupts the citation's metadata.
The citation module code looks for multiple comma or semicolon separator characters in the value assigned to |author=, |last=, their aliases, and enumerated equivalents (e.g. |author2=, |last2=, etc.).  This test displays an error message for multiple authors' names in a single parameter, as well as single author names that include a comma-separated list of post-nominals: |author=FC White, RN, MD, Ph.D.
To fix these errors in citations:

Remove post-nominals.
Provide enumerated author parameters (e.g. either |author2= or |last2= and |first2=) for each author of a cited work.
When multiple separator characters are legitimately present in a name (commonly a corporate, institutional, or governmental author), the name may be wrapped in two sets of parentheses (or <nowiki>...</nowiki> tags) to suppress assignment to this category, like this: |author=((Federal Ministry of Transport, Building, and Urban Development)) or |author=<nowiki>Federal Ministry of Transport, Building, and Urban Development</nowiki>.Editors should not simply replace |author= with |authors=. Using the plural |authors= parameter to replace a singular |author= or |last= parameter that holds multiple authors' names is discouraged because automatically decoding lists of human names is an extraordinarily difficult task.  Because of this difficulty, names listed in |authors= are omitted from the template's COinS metadata. Enumerating the author list with |authorn=, or |lastn= / |firstn=, or, where appropriate, |vauthors=, preserves the associated metadata.
Pages in this category should only be added by Module:Citation/CS1.
Pages with this condition are automatically placed in Category:CS1 maint: multiple names: authors list.
By default, Citation Style 1 and Citation Style 2 error messages are visible to all readers and maintenance messages are hidden from all readers.
To display maintenance messages in the rendered article, include the following text in your common CSS page (common.css) or your specific skin's CSS page and (skin.css).
(Note to new editors: those CSS pages are specific to you, and control your view of pages, by adding to your user account's CSS code. If you have not yet created such a page, then clicking one of the .css links above will yield a page that starts "Wikipedia does not have a user page with this exact name." Click the "Start the User:username/filename page" link, paste the text below, save the page, follow the instructions at the bottom of the new page on bypassing your browser's cache, and finally, in order to see the previously hidden maintenance messages, refresh the page you were editing earlier.)

To display hidden-by-default error messages:

Even with this CSS installed, older pages in Wikipedia's cache may not have been updated to show these error messages even though the page is listed in one of the tracking categories. A null edit will resolve that issue.
To hide normally-displayed error messages:

You can personalize the display of these messages (such as changing the color), but you will need to ask someone who knows CSS or at the technical village pump if you do not understand how.
Nota bene: these css rules are not obeyed by Navigation popups. They also do not hide script warning messages in the Preview box that begins with "This is only a preview; your changes have not yet been saved".


== Notes ==

Category:Secondary sector of the economy



Category:Use mdy dates from October 2015

Wikipedia articles (tagged in this month) that use "mm dd yyyy" date formats, whether by application of the first main contributor rule or by virtue of close national ties to the subject, belong in Category:Use mdy dates. Use {{Mdy}} or {{Use mdy dates}} to add an article to this category. See Wikipedia:MOSNUM.
This system of tagging/categorisation is used as a status monitor of all articles that use mm dd yyyy date formats.

Category:Wikipedia articles needing factual verification from May 2017

This category combines all Wikipedia articles needing factual verification from May 2017 (2017-05) to enable us to work through the backlog more systematically. It is a member of Category:Wikipedia articles needing factual verification.

Category:Wikipedia articles needing factual verification from November 2017

This category combines all Wikipedia articles needing factual verification from November 2017 (2017-11) to enable us to work through the backlog more systematically. It is a member of Category:Wikipedia articles needing factual verification.

Category:Wikipedia articles needing factual verification from October 2019

This category combines all Wikipedia articles needing factual verification from October 2019 (2019-10) to enable us to work through the backlog more systematically. It is a member of Category:Wikipedia articles needing factual verification.

Category:Wikipedia articles needing page number citations from June 2017

This category combines all Wikipedia articles needing page number citations from June 2017 (2017-06) to enable us to work through the backlog more systematically. It is a member of Category:Wikipedia articles needing page number citations.

Category:Wikipedia articles needing page number citations from June 2021

This category combines all Wikipedia articles needing page number citations from June 2021 (2021-06) to enable us to work through the backlog more systematically. It is a member of Category:Wikipedia articles needing page number citations.

Category:Wikipedia articles needing page number citations from May 2017

This category combines all Wikipedia articles needing page number citations from May 2017 (2017-05) to enable us to work through the backlog more systematically. It is a member of Category:Wikipedia articles needing page number citations.

Category:Wikipedia articles needing page number citations from October 2019

This category combines all Wikipedia articles needing page number citations from October 2019 (2019-10) to enable us to work through the backlog more systematically. It is a member of Category:Wikipedia articles needing page number citations.

Category:Wikipedia neutral point of view disputes from June 2021

This category combines all Wikipedia neutral point of view disputes from June 2021 (2021-06) to enable us to work through the backlog more systematically. It is a member of Category:Wikipedia neutral point of view disputes.
To add an article to this category add     {{POV|date=June 2021}} to the article. If you omit the date a bot will add it for you at some point.

